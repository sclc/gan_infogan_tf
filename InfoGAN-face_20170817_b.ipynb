{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN CeleA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import os, glob, cv2, math, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.misc\n",
    "import scipy\n",
    "#from PIL import Image\n",
    "\n",
    "np.random.seed(1)\n",
    "#plt.ion()   # interactive mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load CeleA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html\n",
    "#img_rows, img_cols = 100, 100\n",
    "#img_rows, img_cols = 64, 64\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "def get_im(path):\n",
    "\n",
    "    #img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.imread(path)\n",
    "    #img = plt.imread(path)\n",
    "    #resized = img\n",
    "    resized = cv2.resize(img, (img_cols, img_rows))\n",
    "\n",
    "    return resized\n",
    "\n",
    "def read_train_data_fullname(path):\n",
    "\n",
    "    \n",
    "    files = glob.glob(path)\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_fullname_lfw(path):\n",
    "\n",
    "    root = path\n",
    "    all_folders = os.listdir(root)\n",
    "    path = []\n",
    "    for afolder in all_folders:\n",
    "        path.append(root+\"/\"+afolder+\"/*.jpg\")\n",
    "    #print(path)\n",
    "    files = []\n",
    "    for apath in path:\n",
    "        templist = glob.glob(apath)\n",
    "        for afile in templist:\n",
    "            files.append(afile)\n",
    "\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_batch(filenames, batchsize=5):\n",
    "    \n",
    "    end = min(len(filenames), batchsize)\n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[:end]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        #normalization\n",
    "        #img -= np.mean(img)\n",
    "        #img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    filenames = filenames[end:]\n",
    "    \n",
    "    return train_data, filenames\n",
    "\n",
    "def read_train_data_mini_batch(filenames, startpoint, batchsize=5):\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[startpoint:startpoint+batchsize]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #print(type(img))        \n",
    "        #normalization\n",
    "        img -= np.mean(img)\n",
    "        img /= np.std(img)\n",
    "        #print(img.shape)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "#random mini-batch\n",
    "def read_train_data_random_batch(filenames, batchsize=5):\n",
    "    fullsize=len(filenames)\n",
    "    #http://qiita.com/hamukazu/items/ec1b4659df00f0ce43b1\n",
    "    idset = np.random.randint(0, high=fullsize, size=batchsize)\n",
    "    #print(idset) \n",
    "    train_data = []\n",
    "    \n",
    "    for fid in idset:\n",
    "        fl = filenames[fid]\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        #img[:,:,1] = 0\n",
    "        #img[:,:,2] = 0\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #img = np.reshape(img,[img.shape[0],img.shape[1],1])\n",
    "        # normalization\n",
    "        # https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn\n",
    "        img -= np.mean(img)\n",
    "        img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    \n",
    "    return train_data\n",
    "# must be full path, ~/... not ok\n",
    "# train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data,train_data_filenames = read_train_data_batch(train_data_filenames)\n",
    "\n",
    "#random mini-batch\n",
    "#train_data = read_train_data_random_batch(train_data_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(train_data),len(train_data_filenames))\n",
    "#print(len(train_data[0][1]))\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # need this transpose if there is transpose in read_train_data_xx calls\n",
    "    #inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #inp = std * inp + mean\n",
    "    #plt.imshow(inp,cmap=\"Purples\",interpolation = 'bicubic')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.01)  # pause a bit so that plots are updated\n",
    "\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "#train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "#print(len(train_data_filenames))\n",
    "#print(train_data_filenames[3])\n",
    "#train_data = read_train_data_mini_batch(train_data_filenames,0)  \n",
    "#print(train_data[0].shape)\n",
    "#print(train_data[43][:,:,0:2].shape)\n",
    "\n",
    "#imshow(train_data[2][:,:,0])\n",
    "#imshow(train_data[2][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge_color(images, size))\n",
    "    #return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return images\n",
    "    #return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def merge_color(images, size):\n",
    "    h, w, c = images.shape[1], images.shape[2],images.shape[3]\n",
    "    img = np.zeros((h * size[0], w * size[1],c))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image[:,:,:]\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/depth_to_space\n",
    "# http://qiita.com/tadOne/items/48302a399dcad44c69c8   Tensorflow - padding = VALID/SAMEの違いについて\n",
    "#     so 3 tf.depth_to_space(genX,2) gives 4x2^3 = 32\n",
    "# \n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*448,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,448])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=256,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=128,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=64,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=3,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,64,[4,4],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,128,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,256,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    #print(\"d_out\"+str(d_out))\n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/split\n",
    "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "# https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "# https://www.tensorflow.org/api_docs/python/tf/trainable_variables\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "# https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "# z_lat: one_hot_size + z_size + number_continuous = 10+64+2=76\n",
    "# g_loss def is interesting, my understanding: \n",
    "#        if Dg is the probablity to be told as feak data, then 1-Dg is the probabily of suceessfully cheating, \n",
    "#        so we cal KL(Dg/(1-Dg)), and readuce_mean works as sampling proceduce\n",
    "# \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "z_size = 128 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "#categorical_list = [10]*10 # Each entry in this list defines a categorical variable of a specific size.\n",
    "categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "# categorical_list = [10,10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 1 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "#print(\"latent_cat_in:\", latent_cat_in)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "#print(\"latent_cat_list: \",latent_cat_list)\n",
    "if number_continuous>0:\n",
    "    latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    #print(latent_cat_list[idx])\n",
    "    #print(latent_oh),  woundn't print anything in sess.run()\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "#print(\"1st z_lats: \", z_lats )\n",
    "z_lats.append(z_in)\n",
    "#print(\"2nd z_lats: \", z_lats )\n",
    "if number_continuous>0:\n",
    "    z_lats.append(latent_cont_in)\n",
    "#print(\"3rd z_lats: \", z_lats )\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "#print(\"z_lat: \", z_lat )\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "#print (Gz.shape)\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "#g_loss = -tf.reduce_mean(tf.log((Dg/(1.-Dg)))) #KL Divergence optimizer\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) \n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    #print (\"latent_var: \", latent_var)\n",
    "    #print (\"tf.log(QgCat[idx]): \",tf.log(QgCat[idx]))\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "#print (len(tvars))\n",
    "#for i in tvars:\n",
    "#    print(i)\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.9)\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "#\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:1 Gen Loss: 10.8093 Disc Loss: 1.50684 Q Losses: [0.21452199, 2.2875094]\n",
      "epoch:0 batch_done:2 Gen Loss: 1.58906 Disc Loss: 1.13432 Q Losses: [0.16069031, 2.2909784]\n",
      "epoch:0 batch_done:3 Gen Loss: 6.36265 Disc Loss: 4.44809 Q Losses: [0.15094638, 2.3174233]\n",
      "epoch:0 batch_done:4 Gen Loss: 8.99172 Disc Loss: 0.0846714 Q Losses: [0.14221787, 2.2839708]\n",
      "epoch:0 batch_done:5 Gen Loss: 7.67117 Disc Loss: 0.267323 Q Losses: [0.12819034, 2.2600918]\n",
      "epoch:0 batch_done:6 Gen Loss: 4.51652 Disc Loss: 0.18706 Q Losses: [0.1161695, 2.2617178]\n",
      "epoch:0 batch_done:7 Gen Loss: 11.6681 Disc Loss: 0.455332 Q Losses: [0.10314247, 2.2648273]\n",
      "epoch:0 batch_done:8 Gen Loss: 13.2076 Disc Loss: 0.148518 Q Losses: [0.086699218, 2.2257361]\n",
      "epoch:0 batch_done:9 Gen Loss: 10.7992 Disc Loss: 0.12668 Q Losses: [0.086579859, 2.2222574]\n",
      "epoch:0 batch_done:10 Gen Loss: 6.70204 Disc Loss: 0.0171134 Q Losses: [0.1168533, 2.2004037]\n",
      "epoch:0 batch_done:11 Gen Loss: 9.91904 Disc Loss: 0.11805 Q Losses: [0.12382295, 2.2309318]\n",
      "epoch:0 batch_done:12 Gen Loss: 9.52889 Disc Loss: 0.0251036 Q Losses: [0.062157899, 2.2094173]\n",
      "epoch:0 batch_done:13 Gen Loss: 11.3472 Disc Loss: 0.088537 Q Losses: [0.10199645, 2.1864421]\n",
      "epoch:0 batch_done:14 Gen Loss: 8.38011 Disc Loss: 0.157207 Q Losses: [0.071331486, 2.2176204]\n",
      "epoch:0 batch_done:15 Gen Loss: 26.3245 Disc Loss: 0.519068 Q Losses: [0.086153999, 2.1640806]\n",
      "epoch:0 batch_done:16 Gen Loss: 23.6687 Disc Loss: 1.18038 Q Losses: [0.053648576, 2.1335936]\n",
      "epoch:0 batch_done:17 Gen Loss: 18.8271 Disc Loss: 0.0576152 Q Losses: [0.056661174, 2.1676216]\n",
      "epoch:0 batch_done:18 Gen Loss: 10.8347 Disc Loss: 0.00924118 Q Losses: [0.055244885, 2.1511354]\n",
      "epoch:0 batch_done:19 Gen Loss: 17.7271 Disc Loss: 0.149655 Q Losses: [0.058816206, 2.1586432]\n",
      "epoch:0 batch_done:20 Gen Loss: 18.0231 Disc Loss: 0.00115088 Q Losses: [0.070876248, 2.0928848]\n",
      "epoch:0 batch_done:21 Gen Loss: 15.262 Disc Loss: 0.0160716 Q Losses: [0.045625024, 2.0982361]\n",
      "epoch:0 batch_done:22 Gen Loss: 11.0285 Disc Loss: 0.012489 Q Losses: [0.055819001, 2.1325955]\n",
      "epoch:0 batch_done:23 Gen Loss: 7.59691 Disc Loss: 0.0195119 Q Losses: [0.054227598, 2.0854042]\n",
      "epoch:0 batch_done:24 Gen Loss: 19.2621 Disc Loss: 0.10615 Q Losses: [0.068610348, 2.0998607]\n",
      "epoch:0 batch_done:25 Gen Loss: 22.2746 Disc Loss: 0.0451386 Q Losses: [0.039873347, 2.0363479]\n",
      "epoch:0 batch_done:26 Gen Loss: 20.9984 Disc Loss: 0.00762264 Q Losses: [0.036360502, 2.0222621]\n",
      "epoch:0 batch_done:27 Gen Loss: 17.5104 Disc Loss: 0.00183696 Q Losses: [0.058257006, 2.0595784]\n",
      "epoch:0 batch_done:28 Gen Loss: 13.5077 Disc Loss: 0.000441946 Q Losses: [0.04138916, 2.0035677]\n",
      "epoch:0 batch_done:29 Gen Loss: 10.2445 Disc Loss: 0.000668637 Q Losses: [0.03486333, 1.9685702]\n",
      "epoch:0 batch_done:30 Gen Loss: 7.70532 Disc Loss: 0.00286279 Q Losses: [0.052434027, 1.9240929]\n",
      "epoch:0 batch_done:31 Gen Loss: 6.38988 Disc Loss: 0.0089136 Q Losses: [0.051543683, 1.9327581]\n",
      "epoch:0 batch_done:32 Gen Loss: 6.91442 Disc Loss: 0.00764565 Q Losses: [0.055097505, 1.8665419]\n",
      "epoch:0 batch_done:33 Gen Loss: 6.88915 Disc Loss: 0.00873033 Q Losses: [0.034262881, 1.8309002]\n",
      "epoch:0 batch_done:34 Gen Loss: 6.68893 Disc Loss: 0.00700843 Q Losses: [0.055254035, 1.793088]\n",
      "epoch:0 batch_done:35 Gen Loss: 6.74186 Disc Loss: 0.017137 Q Losses: [0.053242031, 1.7816644]\n",
      "epoch:0 batch_done:36 Gen Loss: 7.2782 Disc Loss: 0.00875856 Q Losses: [0.047231074, 1.7556127]\n",
      "epoch:0 batch_done:37 Gen Loss: 7.59593 Disc Loss: 0.00836618 Q Losses: [0.047741503, 1.7449919]\n",
      "epoch:0 batch_done:38 Gen Loss: 7.68839 Disc Loss: 0.0118851 Q Losses: [0.069583759, 1.6220319]\n",
      "epoch:0 batch_done:39 Gen Loss: 7.6418 Disc Loss: 0.00981617 Q Losses: [0.054075025, 1.6855756]\n",
      "epoch:0 batch_done:40 Gen Loss: 7.48194 Disc Loss: 0.00752289 Q Losses: [0.047286727, 1.6342289]\n",
      "epoch:0 batch_done:41 Gen Loss: 12.3513 Disc Loss: 0.0410807 Q Losses: [0.053141292, 1.6035423]\n",
      "epoch:0 batch_done:42 Gen Loss: 12.8834 Disc Loss: 0.00805555 Q Losses: [0.070513681, 1.6060423]\n",
      "epoch:0 batch_done:43 Gen Loss: 11.3168 Disc Loss: 0.00777397 Q Losses: [0.064135745, 1.5556881]\n",
      "epoch:0 batch_done:44 Gen Loss: 8.3981 Disc Loss: 0.0222192 Q Losses: [0.055351615, 1.5474908]\n",
      "epoch:0 batch_done:45 Gen Loss: 6.7231 Disc Loss: 0.00643564 Q Losses: [0.052967817, 1.5002151]\n",
      "epoch:0 batch_done:46 Gen Loss: 8.71143 Disc Loss: 0.0161056 Q Losses: [0.05324237, 1.483851]\n",
      "epoch:0 batch_done:47 Gen Loss: 8.80941 Disc Loss: 0.0059496 Q Losses: [0.063526325, 1.4335061]\n",
      "epoch:0 batch_done:48 Gen Loss: 8.30505 Disc Loss: 0.00683088 Q Losses: [0.057715021, 1.3919063]\n",
      "epoch:0 batch_done:49 Gen Loss: 7.50594 Disc Loss: 0.00622248 Q Losses: [0.075187072, 1.3541229]\n",
      "epoch:0 batch_done:50 Gen Loss: 7.84056 Disc Loss: 0.0140781 Q Losses: [0.054702051, 1.3889387]\n",
      "epoch:0 batch_done:51 Gen Loss: 8.91149 Disc Loss: 0.00950034 Q Losses: [0.051795773, 1.4067237]\n",
      "epoch:0 batch_done:52 Gen Loss: 9.3834 Disc Loss: 0.00537644 Q Losses: [0.054031003, 1.3436636]\n",
      "epoch:0 batch_done:53 Gen Loss: 9.10442 Disc Loss: 0.00494442 Q Losses: [0.049947694, 1.2545384]\n",
      "epoch:0 batch_done:54 Gen Loss: 7.1556 Disc Loss: 0.00558057 Q Losses: [0.059114143, 1.336077]\n",
      "epoch:0 batch_done:55 Gen Loss: 9.06901 Disc Loss: 0.0150811 Q Losses: [0.05409934, 1.2888808]\n",
      "epoch:0 batch_done:56 Gen Loss: 9.73558 Disc Loss: 0.00439149 Q Losses: [0.053360719, 1.2995059]\n",
      "epoch:0 batch_done:57 Gen Loss: 8.42798 Disc Loss: 0.0175011 Q Losses: [0.038393103, 1.2482278]\n",
      "epoch:0 batch_done:58 Gen Loss: 7.51877 Disc Loss: 0.00221713 Q Losses: [0.049885523, 1.2369673]\n",
      "epoch:0 batch_done:59 Gen Loss: 7.16072 Disc Loss: 0.0252814 Q Losses: [0.074543536, 1.1441791]\n",
      "epoch:0 batch_done:60 Gen Loss: 8.69782 Disc Loss: 0.00873786 Q Losses: [0.048599154, 1.1808121]\n",
      "epoch:0 batch_done:61 Gen Loss: 8.86077 Disc Loss: 0.0022628 Q Losses: [0.041822068, 1.1882105]\n",
      "epoch:0 batch_done:62 Gen Loss: 8.28659 Disc Loss: 0.0034079 Q Losses: [0.059116535, 1.2124782]\n",
      "epoch:0 batch_done:63 Gen Loss: 8.3919 Disc Loss: 0.00663234 Q Losses: [0.045870915, 1.1905994]\n",
      "epoch:0 batch_done:64 Gen Loss: 9.67828 Disc Loss: 0.00568723 Q Losses: [0.044990562, 1.1548375]\n",
      "epoch:0 batch_done:65 Gen Loss: 9.46493 Disc Loss: 0.00124594 Q Losses: [0.064540051, 1.128055]\n",
      "epoch:0 batch_done:66 Gen Loss: 7.48731 Disc Loss: 0.00218434 Q Losses: [0.065733403, 1.129302]\n",
      "epoch:0 batch_done:67 Gen Loss: 9.44487 Disc Loss: 0.0122141 Q Losses: [0.064090461, 1.0988847]\n",
      "epoch:0 batch_done:68 Gen Loss: 9.25843 Disc Loss: 0.00550859 Q Losses: [0.056113381, 1.0996702]\n",
      "epoch:0 batch_done:69 Gen Loss: 7.76256 Disc Loss: 0.0196549 Q Losses: [0.058940776, 1.0920652]\n",
      "epoch:0 batch_done:70 Gen Loss: 6.25952 Disc Loss: 0.0168024 Q Losses: [0.074891239, 1.0414641]\n",
      "epoch:0 batch_done:71 Gen Loss: 11.9 Disc Loss: 0.03039 Q Losses: [0.036562819, 1.1448078]\n",
      "epoch:0 batch_done:72 Gen Loss: 12.8908 Disc Loss: 0.000733505 Q Losses: [0.06672176, 1.0995998]\n",
      "epoch:0 batch_done:73 Gen Loss: 10.5455 Disc Loss: 0.013733 Q Losses: [0.075492337, 1.0792325]\n",
      "epoch:0 batch_done:74 Gen Loss: 6.6125 Disc Loss: 0.0118362 Q Losses: [0.043404944, 1.0459392]\n",
      "epoch:0 batch_done:75 Gen Loss: 12.3127 Disc Loss: 0.0184751 Q Losses: [0.064703926, 1.0897989]\n",
      "epoch:0 batch_done:76 Gen Loss: 13.254 Disc Loss: 0.00490968 Q Losses: [0.076136813, 1.0655184]\n",
      "epoch:0 batch_done:77 Gen Loss: 11.3124 Disc Loss: 0.00800025 Q Losses: [0.075692005, 1.0112176]\n",
      "epoch:0 batch_done:78 Gen Loss: 8.32025 Disc Loss: 0.0015584 Q Losses: [0.054580607, 0.98892093]\n",
      "epoch:0 batch_done:79 Gen Loss: 7.64631 Disc Loss: 0.00629123 Q Losses: [0.057556778, 1.0073169]\n",
      "epoch:0 batch_done:80 Gen Loss: 9.03331 Disc Loss: 0.00958128 Q Losses: [0.058574684, 1.0848]\n",
      "epoch:0 batch_done:81 Gen Loss: 9.01328 Disc Loss: 0.00524863 Q Losses: [0.055459924, 0.95785874]\n",
      "epoch:0 batch_done:82 Gen Loss: 8.67771 Disc Loss: 0.00443785 Q Losses: [0.060334481, 0.96771944]\n",
      "epoch:0 batch_done:83 Gen Loss: 8.42251 Disc Loss: 0.00649031 Q Losses: [0.069286764, 0.95145512]\n",
      "epoch:0 batch_done:84 Gen Loss: 6.91777 Disc Loss: 0.0186639 Q Losses: [0.045655712, 0.94727182]\n",
      "epoch:0 batch_done:85 Gen Loss: 12.7331 Disc Loss: 0.019369 Q Losses: [0.049227308, 0.95812076]\n",
      "epoch:0 batch_done:86 Gen Loss: 12.4901 Disc Loss: 0.0113572 Q Losses: [0.054324672, 0.98773396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:87 Gen Loss: 9.55773 Disc Loss: 0.00733913 Q Losses: [0.068099394, 0.97649074]\n",
      "epoch:0 batch_done:88 Gen Loss: 7.09193 Disc Loss: 0.00732735 Q Losses: [0.054263115, 0.89886761]\n",
      "epoch:0 batch_done:89 Gen Loss: 16.1441 Disc Loss: 0.0247117 Q Losses: [0.059781365, 0.92551726]\n",
      "epoch:0 batch_done:90 Gen Loss: 17.7336 Disc Loss: 0.00091038 Q Losses: [0.057572167, 0.92443705]\n",
      "epoch:0 batch_done:91 Gen Loss: 15.6753 Disc Loss: 0.00349718 Q Losses: [0.065654255, 0.92350382]\n",
      "epoch:0 batch_done:92 Gen Loss: 11.7957 Disc Loss: 0.00794005 Q Losses: [0.061219182, 0.95430517]\n",
      "epoch:0 batch_done:93 Gen Loss: 10.2037 Disc Loss: 0.00331589 Q Losses: [0.062206741, 0.84793365]\n",
      "epoch:0 batch_done:94 Gen Loss: 8.29435 Disc Loss: 0.00143072 Q Losses: [0.054034472, 0.84798622]\n",
      "epoch:0 batch_done:95 Gen Loss: 8.24482 Disc Loss: 0.00743813 Q Losses: [0.043041974, 0.89094514]\n",
      "epoch:0 batch_done:96 Gen Loss: 12.4938 Disc Loss: 0.0142093 Q Losses: [0.068678454, 0.86815917]\n",
      "epoch:0 batch_done:97 Gen Loss: 11.9584 Disc Loss: 0.00950958 Q Losses: [0.059925117, 0.7945534]\n",
      "epoch:0 batch_done:98 Gen Loss: 10.2602 Disc Loss: 0.00188669 Q Losses: [0.059596002, 0.81617653]\n",
      "epoch:0 batch_done:99 Gen Loss: 8.05556 Disc Loss: 0.00205243 Q Losses: [0.062171374, 0.79988396]\n",
      "epoch:0 batch_done:100 Gen Loss: 8.0057 Disc Loss: 0.00247929 Q Losses: [0.03960409, 0.78108531]\n",
      "epoch:0 batch_done:101 Gen Loss: 8.3128 Disc Loss: 0.00314141 Q Losses: [0.072151415, 0.78780246]\n",
      "epoch:0 batch_done:102 Gen Loss: 9.16698 Disc Loss: 0.00637108 Q Losses: [0.056829803, 0.77132308]\n",
      "epoch:0 batch_done:103 Gen Loss: 8.23529 Disc Loss: 0.012544 Q Losses: [0.071405634, 0.77842891]\n",
      "epoch:0 batch_done:104 Gen Loss: 8.36262 Disc Loss: 0.00270899 Q Losses: [0.04236538, 0.74230343]\n",
      "epoch:0 batch_done:105 Gen Loss: 10.6069 Disc Loss: 0.0088542 Q Losses: [0.047297619, 0.76802492]\n",
      "epoch:0 batch_done:106 Gen Loss: 10.2064 Disc Loss: 0.00347721 Q Losses: [0.074221939, 0.76370788]\n",
      "epoch:0 batch_done:107 Gen Loss: 8.63929 Disc Loss: 0.00437405 Q Losses: [0.043889198, 0.73649228]\n",
      "epoch:0 batch_done:108 Gen Loss: 7.6947 Disc Loss: 0.0058012 Q Losses: [0.065183572, 0.78566825]\n",
      "epoch:0 batch_done:109 Gen Loss: 11.929 Disc Loss: 0.0132086 Q Losses: [0.046400707, 0.70266604]\n",
      "epoch:0 batch_done:110 Gen Loss: 11.6545 Disc Loss: 0.00344697 Q Losses: [0.058183737, 0.73017001]\n",
      "epoch:0 batch_done:111 Gen Loss: 9.36983 Disc Loss: 0.00737828 Q Losses: [0.061318472, 0.70386547]\n",
      "epoch:0 batch_done:112 Gen Loss: 9.65357 Disc Loss: 0.00939493 Q Losses: [0.044970889, 0.69431645]\n",
      "epoch:0 batch_done:113 Gen Loss: 9.937 Disc Loss: 0.00581807 Q Losses: [0.053993829, 0.69068277]\n",
      "epoch:0 batch_done:114 Gen Loss: 8.29392 Disc Loss: 0.009911 Q Losses: [0.079024769, 0.6864928]\n",
      "epoch:0 batch_done:115 Gen Loss: 19.9601 Disc Loss: 0.0389237 Q Losses: [0.04949379, 0.70886397]\n",
      "epoch:0 batch_done:116 Gen Loss: 22.1911 Disc Loss: 0.00562302 Q Losses: [0.049436808, 0.68631971]\n",
      "epoch:0 batch_done:117 Gen Loss: 17.3037 Disc Loss: 0.019394 Q Losses: [0.059904609, 0.66998547]\n",
      "epoch:0 batch_done:118 Gen Loss: 10.4223 Disc Loss: 0.00151329 Q Losses: [0.05090322, 0.67512095]\n",
      "epoch:0 batch_done:119 Gen Loss: 12.8749 Disc Loss: 0.014395 Q Losses: [0.04277461, 0.71319002]\n",
      "epoch:0 batch_done:120 Gen Loss: 12.1222 Disc Loss: 0.00170708 Q Losses: [0.058064837, 0.69352126]\n",
      "epoch:0 batch_done:121 Gen Loss: 10.0871 Disc Loss: 0.000986765 Q Losses: [0.052963838, 0.65749764]\n",
      "epoch:0 batch_done:122 Gen Loss: 8.78886 Disc Loss: 0.00281089 Q Losses: [0.046835631, 0.65259039]\n",
      "epoch:0 batch_done:123 Gen Loss: 9.31144 Disc Loss: 0.00511405 Q Losses: [0.058998764, 0.64323747]\n",
      "epoch:0 batch_done:124 Gen Loss: 9.0885 Disc Loss: 0.00473725 Q Losses: [0.046627663, 0.64849186]\n",
      "epoch:0 batch_done:125 Gen Loss: 8.28308 Disc Loss: 0.00476888 Q Losses: [0.044746593, 0.63173413]\n",
      "epoch:0 batch_done:126 Gen Loss: 8.61616 Disc Loss: 0.00401402 Q Losses: [0.059927698, 0.58976543]\n",
      "epoch:0 batch_done:127 Gen Loss: 8.7028 Disc Loss: 0.0100801 Q Losses: [0.040229984, 0.60144448]\n",
      "epoch:0 batch_done:128 Gen Loss: 8.34579 Disc Loss: 0.00641133 Q Losses: [0.059775382, 0.60501939]\n",
      "epoch:0 batch_done:129 Gen Loss: 9.53179 Disc Loss: 0.0107039 Q Losses: [0.059780683, 0.62517631]\n",
      "epoch:0 batch_done:130 Gen Loss: 9.75857 Disc Loss: 0.0016019 Q Losses: [0.037040234, 0.59277558]\n",
      "epoch:0 batch_done:131 Gen Loss: 9.30729 Disc Loss: 0.00206061 Q Losses: [0.053071216, 0.60509825]\n",
      "epoch:0 batch_done:132 Gen Loss: 9.1425 Disc Loss: 0.00219072 Q Losses: [0.063060284, 0.55622447]\n",
      "epoch:0 batch_done:133 Gen Loss: 8.41085 Disc Loss: 0.00692223 Q Losses: [0.053764142, 0.56124127]\n",
      "epoch:0 batch_done:134 Gen Loss: 9.46663 Disc Loss: 0.00572766 Q Losses: [0.040016398, 0.56253731]\n",
      "epoch:0 batch_done:135 Gen Loss: 9.47457 Disc Loss: 0.00299353 Q Losses: [0.05089438, 0.5671646]\n",
      "epoch:0 batch_done:136 Gen Loss: 8.78727 Disc Loss: 0.00481942 Q Losses: [0.050275173, 0.6026234]\n",
      "epoch:0 batch_done:137 Gen Loss: 8.90752 Disc Loss: 0.00274825 Q Losses: [0.045118578, 0.51940441]\n",
      "epoch:0 batch_done:138 Gen Loss: 8.69155 Disc Loss: 0.00408613 Q Losses: [0.073053949, 0.52184737]\n",
      "epoch:0 batch_done:139 Gen Loss: 8.43585 Disc Loss: 0.00172627 Q Losses: [0.043515667, 0.53881001]\n",
      "epoch:0 batch_done:140 Gen Loss: 8.67356 Disc Loss: 0.00160889 Q Losses: [0.052804742, 0.49640828]\n",
      "epoch:0 batch_done:141 Gen Loss: 8.84541 Disc Loss: 0.00270282 Q Losses: [0.044389796, 0.51824027]\n",
      "epoch:0 batch_done:142 Gen Loss: 8.91788 Disc Loss: 0.00197128 Q Losses: [0.043739192, 0.51812023]\n",
      "epoch:0 batch_done:143 Gen Loss: 7.18178 Disc Loss: 0.024577 Q Losses: [0.05465439, 0.50038105]\n",
      "epoch:0 batch_done:144 Gen Loss: 13.6936 Disc Loss: 0.0112515 Q Losses: [0.050058413, 0.50499618]\n",
      "epoch:0 batch_done:145 Gen Loss: 14.5475 Disc Loss: 0.00321878 Q Losses: [0.0445779, 0.49575764]\n",
      "epoch:0 batch_done:146 Gen Loss: 13.1688 Disc Loss: 0.00327066 Q Losses: [0.032387145, 0.50056827]\n",
      "epoch:0 batch_done:147 Gen Loss: 10.724 Disc Loss: 0.000921936 Q Losses: [0.038383938, 0.4876864]\n",
      "epoch:0 batch_done:148 Gen Loss: 8.24261 Disc Loss: 0.000811305 Q Losses: [0.050257005, 0.47683036]\n",
      "epoch:0 batch_done:149 Gen Loss: 10.2194 Disc Loss: 0.00552285 Q Losses: [0.045818828, 0.49929899]\n",
      "epoch:0 batch_done:150 Gen Loss: 9.85273 Disc Loss: 0.00404489 Q Losses: [0.048355989, 0.49996442]\n",
      "epoch:0 batch_done:151 Gen Loss: 9.41779 Disc Loss: 0.0023422 Q Losses: [0.035799958, 0.49355274]\n",
      "epoch:0 batch_done:152 Gen Loss: 13.5575 Disc Loss: 0.00862683 Q Losses: [0.046129726, 0.48533037]\n",
      "epoch:0 batch_done:153 Gen Loss: 13.1862 Disc Loss: 0.00129665 Q Losses: [0.054648399, 0.48626688]\n",
      "epoch:0 batch_done:154 Gen Loss: 9.20096 Disc Loss: 0.00506241 Q Losses: [0.040217668, 0.46749675]\n",
      "epoch:0 batch_done:155 Gen Loss: 8.56593 Disc Loss: 0.00494201 Q Losses: [0.047297258, 0.46331841]\n",
      "epoch:0 batch_done:156 Gen Loss: 16.9909 Disc Loss: 0.0140788 Q Losses: [0.060688537, 0.47894204]\n",
      "epoch:0 batch_done:157 Gen Loss: 14.885 Disc Loss: 0.0226582 Q Losses: [0.045589887, 0.45677042]\n",
      "epoch:0 batch_done:158 Gen Loss: 11.0268 Disc Loss: 0.00107371 Q Losses: [0.056242794, 0.438629]\n",
      "epoch:0 batch_done:159 Gen Loss: 13.9418 Disc Loss: 0.00898105 Q Losses: [0.046023753, 0.43072143]\n",
      "epoch:0 batch_done:160 Gen Loss: 13.8774 Disc Loss: 0.000361615 Q Losses: [0.03546926, 0.44242412]\n",
      "epoch:0 batch_done:161 Gen Loss: 11.9092 Disc Loss: 0.00056887 Q Losses: [0.045600727, 0.45460758]\n",
      "epoch:0 batch_done:162 Gen Loss: 8.91793 Disc Loss: 0.00243203 Q Losses: [0.039629623, 0.42286733]\n",
      "epoch:0 batch_done:163 Gen Loss: 8.2328 Disc Loss: 0.00445195 Q Losses: [0.054160073, 0.42500514]\n",
      "epoch:0 batch_done:164 Gen Loss: 10.2716 Disc Loss: 0.00682185 Q Losses: [0.038103044, 0.4240135]\n",
      "epoch:0 batch_done:165 Gen Loss: 10.067 Disc Loss: 0.00225659 Q Losses: [0.058003008, 0.44806233]\n",
      "epoch:0 batch_done:166 Gen Loss: 6.83119 Disc Loss: 0.0127603 Q Losses: [0.035568081, 0.45854765]\n",
      "epoch:0 batch_done:167 Gen Loss: 10.5093 Disc Loss: 0.00710686 Q Losses: [0.039910957, 0.42020813]\n",
      "epoch:0 batch_done:168 Gen Loss: 11.2968 Disc Loss: 0.000363182 Q Losses: [0.041959696, 0.42089659]\n",
      "epoch:0 batch_done:169 Gen Loss: 10.4972 Disc Loss: 0.000638297 Q Losses: [0.03721543, 0.39997119]\n",
      "epoch:0 batch_done:170 Gen Loss: 9.55355 Disc Loss: 0.000322967 Q Losses: [0.04630363, 0.39964518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:171 Gen Loss: 8.51881 Disc Loss: 0.000766772 Q Losses: [0.036033295, 0.38278079]\n",
      "epoch:0 batch_done:172 Gen Loss: 8.48498 Disc Loss: 0.00131874 Q Losses: [0.036467992, 0.39346576]\n",
      "epoch:0 batch_done:173 Gen Loss: 9.8202 Disc Loss: 0.00312103 Q Losses: [0.033531755, 0.39052206]\n",
      "epoch:0 batch_done:174 Gen Loss: 9.80705 Disc Loss: 0.00142974 Q Losses: [0.044168826, 0.39599335]\n",
      "epoch:0 batch_done:175 Gen Loss: 8.97582 Disc Loss: 0.00167022 Q Losses: [0.037259065, 0.3826679]\n",
      "epoch:0 batch_done:176 Gen Loss: 8.38162 Disc Loss: 0.00288368 Q Losses: [0.050628573, 0.35419488]\n",
      "epoch:0 batch_done:177 Gen Loss: 5.96773 Disc Loss: 0.0184207 Q Losses: [0.029275533, 0.38465458]\n",
      "epoch:0 batch_done:178 Gen Loss: 35.9029 Disc Loss: 0.0381076 Q Losses: [0.033647586, 0.40425986]\n",
      "epoch:0 batch_done:179 Gen Loss: 28.8039 Disc Loss: 0.175909 Q Losses: [0.038307779, 0.38645226]\n",
      "epoch:0 batch_done:180 Gen Loss: 22.1451 Disc Loss: 3.35276e-08 Q Losses: [0.04002618, 0.4037385]\n",
      "epoch:0 batch_done:181 Gen Loss: 15.1685 Disc Loss: 7.07805e-08 Q Losses: [0.033267543, 0.37371334]\n",
      "epoch:0 batch_done:182 Gen Loss: 9.3632 Disc Loss: 4.42035e-05 Q Losses: [0.037619874, 0.36224842]\n",
      "epoch:0 batch_done:183 Gen Loss: 28.3008 Disc Loss: 0.0288836 Q Losses: [0.02410499, 0.36517394]\n",
      "epoch:0 batch_done:184 Gen Loss: 33.8646 Disc Loss: -0.0 Q Losses: [0.040332712, 0.38809744]\n",
      "epoch:0 batch_done:185 Gen Loss: 33.126 Disc Loss: 3.72529e-09 Q Losses: [0.036451671, 0.34808451]\n",
      "epoch:0 batch_done:186 Gen Loss: 29.6975 Disc Loss: -0.0 Q Losses: [0.040028427, 0.33931446]\n",
      "epoch:0 batch_done:187 Gen Loss: 24.0147 Disc Loss: 1.11759e-08 Q Losses: [0.038711958, 0.36435306]\n",
      "epoch:0 batch_done:188 Gen Loss: 16.9675 Disc Loss: 7.91624e-08 Q Losses: [0.031183697, 0.37875947]\n",
      "epoch:0 batch_done:189 Gen Loss: 9.58669 Disc Loss: 0.000105288 Q Losses: [0.044529419, 0.35860717]\n",
      "epoch:0 batch_done:190 Gen Loss: 10.0319 Disc Loss: 0.00445741 Q Losses: [0.034134634, 0.37593436]\n",
      "epoch:0 batch_done:191 Gen Loss: 10.5364 Disc Loss: 0.00206681 Q Losses: [0.027639344, 0.3705591]\n",
      "epoch:0 batch_done:192 Gen Loss: 10.2806 Disc Loss: 0.000423747 Q Losses: [0.047056064, 0.35694122]\n",
      "epoch:0 batch_done:193 Gen Loss: 9.22921 Disc Loss: 0.000732103 Q Losses: [0.042883389, 0.33814183]\n",
      "epoch:0 batch_done:194 Gen Loss: 11.3363 Disc Loss: 0.0045187 Q Losses: [0.040437277, 0.33946574]\n",
      "epoch:0 batch_done:195 Gen Loss: 11.1625 Disc Loss: 0.000429295 Q Losses: [0.039784074, 0.3532654]\n",
      "epoch:0 batch_done:196 Gen Loss: 9.85669 Disc Loss: 0.000453037 Q Losses: [0.035322946, 0.37447351]\n",
      "epoch:0 batch_done:197 Gen Loss: 9.54626 Disc Loss: 0.00211107 Q Losses: [0.040213205, 0.34203309]\n",
      "epoch:0 batch_done:198 Gen Loss: 9.74207 Disc Loss: 0.00130687 Q Losses: [0.035676636, 0.31602049]\n",
      "epoch:0 batch_done:199 Gen Loss: 9.57022 Disc Loss: 0.000701282 Q Losses: [0.034354743, 0.34266767]\n",
      "epoch:0 batch_done:200 Gen Loss: 9.06562 Disc Loss: 0.000978004 Q Losses: [0.038397387, 0.33379111]\n",
      "epoch:0 batch_done:201 Gen Loss: 8.13515 Disc Loss: 0.00323916 Q Losses: [0.032397602, 0.33795023]\n",
      "epoch:0 batch_done:202 Gen Loss: 8.53946 Disc Loss: 0.0019292 Q Losses: [0.022704341, 0.31150725]\n",
      "epoch:0 batch_done:203 Gen Loss: 9.19469 Disc Loss: 0.00205672 Q Losses: [0.037299588, 0.30194551]\n",
      "epoch:0 batch_done:204 Gen Loss: 7.5189 Disc Loss: 0.00636179 Q Losses: [0.029335294, 0.28663963]\n",
      "epoch:0 batch_done:205 Gen Loss: 16.3484 Disc Loss: 0.0111782 Q Losses: [0.034710385, 0.32379705]\n",
      "epoch:0 batch_done:206 Gen Loss: 19.8308 Disc Loss: 2.67547e-05 Q Losses: [0.045396406, 0.33269915]\n",
      "epoch:0 batch_done:207 Gen Loss: 18.4039 Disc Loss: 0.00133062 Q Losses: [0.032616295, 0.29586002]\n",
      "epoch:1 batch_done:1 Gen Loss: 14.3535 Disc Loss: 0.000673592 Q Losses: [0.034351289, 0.34530926]\n",
      "epoch:1 batch_done:2 Gen Loss: 10.4474 Disc Loss: 0.000190583 Q Losses: [0.027133076, 0.32244122]\n",
      "epoch:1 batch_done:3 Gen Loss: 9.6097 Disc Loss: 0.00394483 Q Losses: [0.031199712, 0.33664942]\n",
      "epoch:1 batch_done:4 Gen Loss: 10.1348 Disc Loss: 0.00128463 Q Losses: [0.039324991, 0.29563785]\n",
      "epoch:1 batch_done:5 Gen Loss: 10.1823 Disc Loss: 0.00402321 Q Losses: [0.034247473, 0.32392618]\n",
      "epoch:1 batch_done:6 Gen Loss: 10.1189 Disc Loss: 0.00211253 Q Losses: [0.037522729, 0.30531693]\n",
      "epoch:1 batch_done:7 Gen Loss: 10.4474 Disc Loss: 0.00201011 Q Losses: [0.026214825, 0.30353689]\n",
      "epoch:1 batch_done:8 Gen Loss: 10.2705 Disc Loss: 0.000701705 Q Losses: [0.038610704, 0.31712967]\n",
      "epoch:1 batch_done:9 Gen Loss: 9.34019 Disc Loss: 0.00168288 Q Losses: [0.039323401, 0.3037191]\n",
      "epoch:1 batch_done:10 Gen Loss: 10.2842 Disc Loss: 0.00552054 Q Losses: [0.044087902, 0.27426946]\n",
      "epoch:1 batch_done:11 Gen Loss: 10.4299 Disc Loss: 0.000714148 Q Losses: [0.016862884, 0.30427331]\n",
      "epoch:1 batch_done:12 Gen Loss: 10.0944 Disc Loss: 0.00143059 Q Losses: [0.042415291, 0.28061402]\n",
      "epoch:1 batch_done:13 Gen Loss: 10.2743 Disc Loss: 0.00161494 Q Losses: [0.04957011, 0.28974736]\n",
      "epoch:1 batch_done:14 Gen Loss: 9.2782 Disc Loss: 0.00404943 Q Losses: [0.037827991, 0.30309227]\n",
      "epoch:1 batch_done:15 Gen Loss: 11.0775 Disc Loss: 0.00300434 Q Losses: [0.031681094, 0.28389746]\n",
      "epoch:1 batch_done:16 Gen Loss: 11.0179 Disc Loss: 0.000670602 Q Losses: [0.029699078, 0.29771352]\n",
      "epoch:1 batch_done:17 Gen Loss: 9.14784 Disc Loss: 0.00417099 Q Losses: [0.035255395, 0.28033805]\n",
      "epoch:1 batch_done:18 Gen Loss: 8.30602 Disc Loss: 0.00151859 Q Losses: [0.027089333, 0.26617599]\n",
      "epoch:1 batch_done:19 Gen Loss: 10.9899 Disc Loss: 0.0035531 Q Losses: [0.032425351, 0.27206701]\n",
      "epoch:1 batch_done:20 Gen Loss: 10.8983 Disc Loss: 0.00112624 Q Losses: [0.030059613, 0.26191771]\n",
      "epoch:1 batch_done:21 Gen Loss: 10.2697 Disc Loss: 0.000688262 Q Losses: [0.031673595, 0.26630083]\n",
      "epoch:1 batch_done:22 Gen Loss: 10.1169 Disc Loss: 0.00148446 Q Losses: [0.019451767, 0.31347209]\n",
      "epoch:1 batch_done:23 Gen Loss: 15.5893 Disc Loss: 0.0205081 Q Losses: [0.030857703, 0.27577615]\n",
      "epoch:1 batch_done:24 Gen Loss: 17.0526 Disc Loss: 0.000442877 Q Losses: [0.035083514, 0.27522278]\n",
      "epoch:1 batch_done:25 Gen Loss: 15.141 Disc Loss: 0.000179079 Q Losses: [0.030524962, 0.25775591]\n",
      "epoch:1 batch_done:26 Gen Loss: 11.109 Disc Loss: 0.000152325 Q Losses: [0.029486241, 0.31474212]\n",
      "epoch:1 batch_done:27 Gen Loss: 8.87322 Disc Loss: 0.00153566 Q Losses: [0.026694171, 0.28152204]\n",
      "epoch:1 batch_done:28 Gen Loss: 13.1389 Disc Loss: 0.00772692 Q Losses: [0.032920502, 0.27690327]\n",
      "epoch:1 batch_done:29 Gen Loss: 10.4427 Disc Loss: 0.00817906 Q Losses: [0.03303685, 0.25718373]\n",
      "epoch:1 batch_done:30 Gen Loss: 12.6026 Disc Loss: 0.00321506 Q Losses: [0.031230658, 0.32516485]\n",
      "epoch:1 batch_done:31 Gen Loss: 12.0906 Disc Loss: 0.000230891 Q Losses: [0.038594939, 0.26356503]\n",
      "epoch:1 batch_done:32 Gen Loss: 8.63921 Disc Loss: 0.00409022 Q Losses: [0.028573846, 0.28308669]\n",
      "epoch:1 batch_done:33 Gen Loss: 40.2668 Disc Loss: 0.0278054 Q Losses: [0.03208011, 0.30568725]\n",
      "epoch:1 batch_done:34 Gen Loss: 37.717 Disc Loss: 0.0638 Q Losses: [0.032317575, 0.27345857]\n",
      "epoch:1 batch_done:35 Gen Loss: 33.996 Disc Loss: 9.38157e-05 Q Losses: [0.036407478, 0.24058777]\n",
      "epoch:1 batch_done:36 Gen Loss: 29.4696 Disc Loss: 4.53289e-05 Q Losses: [0.028298575, 0.22583467]\n",
      "epoch:1 batch_done:37 Gen Loss: 26.0261 Disc Loss: 0.00022087 Q Losses: [0.033582728, 0.26350966]\n",
      "epoch:1 batch_done:38 Gen Loss: 21.5003 Disc Loss: 3.31815e-05 Q Losses: [0.022142146, 0.25818908]\n",
      "epoch:1 batch_done:39 Gen Loss: 18.1087 Disc Loss: 1.3728e-06 Q Losses: [0.031454623, 0.2573787]\n",
      "epoch:1 batch_done:40 Gen Loss: 15.9521 Disc Loss: 1.42492e-07 Q Losses: [0.034001537, 0.2340043]\n",
      "epoch:1 batch_done:41 Gen Loss: 12.7702 Disc Loss: 2.87407e-06 Q Losses: [0.02374682, 0.23843157]\n",
      "epoch:1 batch_done:42 Gen Loss: 10.0673 Disc Loss: 4.55111e-05 Q Losses: [0.02775375, 0.23255172]\n",
      "epoch:1 batch_done:43 Gen Loss: 8.26446 Disc Loss: 0.00180452 Q Losses: [0.03053714, 0.26506412]\n",
      "epoch:1 batch_done:44 Gen Loss: 9.39874 Disc Loss: 0.00246708 Q Losses: [0.031254165, 0.2269284]\n",
      "epoch:1 batch_done:45 Gen Loss: 9.91208 Disc Loss: 0.000634434 Q Losses: [0.033903435, 0.23061472]\n",
      "epoch:1 batch_done:46 Gen Loss: 9.41698 Disc Loss: 0.001108 Q Losses: [0.023662649, 0.25697374]\n",
      "epoch:1 batch_done:47 Gen Loss: 9.48021 Disc Loss: 0.00080489 Q Losses: [0.027195202, 0.29379076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:48 Gen Loss: 9.4298 Disc Loss: 0.000611601 Q Losses: [0.030157564, 0.23599227]\n",
      "epoch:1 batch_done:49 Gen Loss: 9.40932 Disc Loss: 0.000425807 Q Losses: [0.035700642, 0.24148746]\n",
      "epoch:1 batch_done:50 Gen Loss: 9.13361 Disc Loss: 0.000841045 Q Losses: [0.026576821, 0.22764781]\n",
      "epoch:1 batch_done:51 Gen Loss: 10.4615 Disc Loss: 0.00228071 Q Losses: [0.021198526, 0.21071537]\n",
      "epoch:1 batch_done:52 Gen Loss: 10.535 Disc Loss: 0.000483788 Q Losses: [0.03012019, 0.2142621]\n",
      "epoch:1 batch_done:53 Gen Loss: 9.99862 Disc Loss: 0.000285629 Q Losses: [0.029734172, 0.20253363]\n",
      "epoch:1 batch_done:54 Gen Loss: 8.57687 Disc Loss: 0.00507256 Q Losses: [0.022817694, 0.21704213]\n",
      "epoch:1 batch_done:55 Gen Loss: 11.9244 Disc Loss: 0.00403203 Q Losses: [0.016613595, 0.21390724]\n",
      "epoch:1 batch_done:56 Gen Loss: 11.8545 Disc Loss: 0.00258152 Q Losses: [0.025919404, 0.21779111]\n",
      "epoch:1 batch_done:57 Gen Loss: 11.5118 Disc Loss: 5.45525e-05 Q Losses: [0.039278317, 0.21777847]\n",
      "epoch:1 batch_done:58 Gen Loss: 9.74144 Disc Loss: 0.000170913 Q Losses: [0.039780803, 0.24078149]\n",
      "epoch:1 batch_done:59 Gen Loss: 9.9401 Disc Loss: 0.0027686 Q Losses: [0.043948773, 0.24203029]\n",
      "epoch:1 batch_done:60 Gen Loss: 10.471 Disc Loss: 0.000302468 Q Losses: [0.027026907, 0.24564633]\n",
      "epoch:1 batch_done:61 Gen Loss: 9.51846 Disc Loss: 0.000756151 Q Losses: [0.015456846, 0.20424235]\n",
      "epoch:1 batch_done:62 Gen Loss: 9.38873 Disc Loss: 0.00134512 Q Losses: [0.036949348, 0.19382554]\n",
      "epoch:1 batch_done:63 Gen Loss: 24.3364 Disc Loss: 0.0112165 Q Losses: [0.033085041, 0.21317473]\n",
      "epoch:1 batch_done:64 Gen Loss: 19.5955 Disc Loss: 0.068449 Q Losses: [0.025206245, 0.20948386]\n",
      "epoch:1 batch_done:65 Gen Loss: 15.193 Disc Loss: 4.63784e-05 Q Losses: [0.028185502, 0.22816208]\n",
      "epoch:1 batch_done:66 Gen Loss: 10.8127 Disc Loss: 2.33542e-05 Q Losses: [0.027872894, 0.23047781]\n",
      "epoch:1 batch_done:67 Gen Loss: 8.86753 Disc Loss: 0.000130571 Q Losses: [0.032098792, 0.20346373]\n",
      "epoch:1 batch_done:68 Gen Loss: 9.727 Disc Loss: 0.00198988 Q Losses: [0.031354733, 0.21949802]\n",
      "epoch:1 batch_done:69 Gen Loss: 10.0968 Disc Loss: 0.00241164 Q Losses: [0.026884077, 0.2234585]\n",
      "epoch:1 batch_done:70 Gen Loss: 15.5377 Disc Loss: 0.00418493 Q Losses: [0.032471146, 0.28447381]\n",
      "epoch:1 batch_done:71 Gen Loss: 17.9535 Disc Loss: 9.56434e-06 Q Losses: [0.029103303, 0.25173104]\n",
      "epoch:1 batch_done:72 Gen Loss: 17.5336 Disc Loss: 1.89945e-05 Q Losses: [0.026280575, 0.25473058]\n",
      "epoch:1 batch_done:73 Gen Loss: 15.7333 Disc Loss: 9.15572e-06 Q Losses: [0.03236898, 0.33352664]\n",
      "epoch:1 batch_done:74 Gen Loss: 16.5702 Disc Loss: 8.10899e-06 Q Losses: [0.036846861, 0.26312849]\n",
      "epoch:1 batch_done:75 Gen Loss: 16.5805 Disc Loss: 1.71028e-05 Q Losses: [0.0316355, 0.22961211]\n",
      "epoch:1 batch_done:76 Gen Loss: 15.156 Disc Loss: 1.47988e-05 Q Losses: [0.027225312, 0.19546142]\n",
      "epoch:1 batch_done:77 Gen Loss: 13.4508 Disc Loss: 2.74249e-05 Q Losses: [0.029347934, 0.20449317]\n",
      "epoch:1 batch_done:78 Gen Loss: 11.2478 Disc Loss: 2.34541e-05 Q Losses: [0.031681508, 0.22206244]\n",
      "epoch:1 batch_done:79 Gen Loss: 9.94112 Disc Loss: 6.72008e-05 Q Losses: [0.034098193, 0.20139128]\n",
      "epoch:1 batch_done:80 Gen Loss: 8.759 Disc Loss: 0.000280979 Q Losses: [0.017440163, 0.2107597]\n",
      "epoch:1 batch_done:81 Gen Loss: 10.0902 Disc Loss: 0.00213813 Q Losses: [0.031595103, 0.2020703]\n",
      "epoch:1 batch_done:82 Gen Loss: 10.3562 Disc Loss: 0.000482124 Q Losses: [0.018248767, 0.18718007]\n",
      "epoch:1 batch_done:83 Gen Loss: 9.87259 Disc Loss: 0.000428571 Q Losses: [0.028011661, 0.21823192]\n",
      "epoch:1 batch_done:84 Gen Loss: 9.50606 Disc Loss: 0.000597553 Q Losses: [0.024779623, 0.18641895]\n",
      "epoch:1 batch_done:85 Gen Loss: 9.59529 Disc Loss: 0.000413905 Q Losses: [0.028062768, 0.18260552]\n",
      "epoch:1 batch_done:86 Gen Loss: 9.70553 Disc Loss: 0.000211039 Q Losses: [0.02817901, 0.19132662]\n",
      "epoch:1 batch_done:87 Gen Loss: 9.21824 Disc Loss: 0.000337388 Q Losses: [0.028392235, 0.18175052]\n",
      "epoch:1 batch_done:88 Gen Loss: 9.36424 Disc Loss: 0.00124587 Q Losses: [0.036486164, 0.19360864]\n",
      "epoch:1 batch_done:89 Gen Loss: 9.78722 Disc Loss: 0.000378479 Q Losses: [0.037739772, 0.23067668]\n",
      "epoch:1 batch_done:90 Gen Loss: 9.47499 Disc Loss: 0.000473892 Q Losses: [0.026766453, 0.2013268]\n",
      "epoch:1 batch_done:91 Gen Loss: 9.44411 Disc Loss: 0.00081172 Q Losses: [0.017822038, 0.18640327]\n",
      "epoch:1 batch_done:92 Gen Loss: 9.85487 Disc Loss: 0.000343954 Q Losses: [0.026379369, 0.16797087]\n",
      "epoch:1 batch_done:93 Gen Loss: 9.56934 Disc Loss: 0.000422006 Q Losses: [0.0295996, 0.16674629]\n",
      "epoch:1 batch_done:94 Gen Loss: 9.54155 Disc Loss: 0.000491956 Q Losses: [0.01864467, 0.18612106]\n",
      "epoch:1 batch_done:95 Gen Loss: 7.6746 Disc Loss: 0.0353838 Q Losses: [0.025872692, 0.19153714]\n",
      "epoch:1 batch_done:96 Gen Loss: 9.02877 Disc Loss: 0.00118631 Q Losses: [0.022346694, 0.18237819]\n",
      "epoch:1 batch_done:97 Gen Loss: 9.70861 Disc Loss: 0.00035921 Q Losses: [0.013953299, 0.19075865]\n",
      "epoch:1 batch_done:98 Gen Loss: 9.95938 Disc Loss: 0.000149914 Q Losses: [0.021313194, 0.15855089]\n",
      "epoch:1 batch_done:99 Gen Loss: 9.40029 Disc Loss: 0.000224099 Q Losses: [0.019193448, 0.15868735]\n",
      "epoch:1 batch_done:100 Gen Loss: 9.28917 Disc Loss: 0.000253486 Q Losses: [0.020276258, 0.1574147]\n",
      "epoch:1 batch_done:101 Gen Loss: 9.43245 Disc Loss: 0.000985028 Q Losses: [0.024525888, 0.16140628]\n",
      "epoch:1 batch_done:102 Gen Loss: 9.86574 Disc Loss: 0.000508179 Q Losses: [0.017110711, 0.19301936]\n",
      "epoch:1 batch_done:103 Gen Loss: 9.84689 Disc Loss: 0.000307242 Q Losses: [0.018705551, 0.18360579]\n",
      "epoch:1 batch_done:104 Gen Loss: 9.47013 Disc Loss: 0.000432813 Q Losses: [0.025696207, 0.19820267]\n",
      "epoch:1 batch_done:105 Gen Loss: 9.53465 Disc Loss: 0.000806065 Q Losses: [0.021135349, 0.15316893]\n",
      "epoch:1 batch_done:106 Gen Loss: 10.156 Disc Loss: 0.00113364 Q Losses: [0.023264028, 0.16733843]\n",
      "epoch:1 batch_done:107 Gen Loss: 12.5338 Disc Loss: 0.00258614 Q Losses: [0.020428514, 0.2055105]\n",
      "epoch:1 batch_done:108 Gen Loss: 12.4072 Disc Loss: 0.000126781 Q Losses: [0.028505582, 0.19977131]\n",
      "epoch:1 batch_done:109 Gen Loss: 10.5372 Disc Loss: 0.000211229 Q Losses: [0.033587139, 0.20874462]\n",
      "epoch:1 batch_done:110 Gen Loss: 15.5419 Disc Loss: 0.00540282 Q Losses: [0.018925851, 0.17194819]\n",
      "epoch:1 batch_done:111 Gen Loss: 15.9845 Disc Loss: 2.18231e-05 Q Losses: [0.024938516, 0.15971282]\n",
      "epoch:1 batch_done:112 Gen Loss: 13.8856 Disc Loss: 3.25443e-05 Q Losses: [0.034182765, 0.14248922]\n",
      "epoch:1 batch_done:113 Gen Loss: 11.0308 Disc Loss: 0.000119308 Q Losses: [0.01946545, 0.1718781]\n",
      "epoch:1 batch_done:114 Gen Loss: 22.7088 Disc Loss: 0.010364 Q Losses: [0.03684232, 0.22060393]\n",
      "epoch:1 batch_done:115 Gen Loss: 0.0772786 Disc Loss: 0.149223 Q Losses: [0.069871239, 0.18438354]\n",
      "epoch:1 batch_done:116 Gen Loss: inf Disc Loss: 11.047 Q Losses: [0.16344883, 2.303781]\n",
      "epoch:1 batch_done:117 Gen Loss: nan Disc Loss: nan Q Losses: [0.17450292, 2.3020878]\n",
      "epoch:1 batch_done:118 Gen Loss: nan Disc Loss: nan Q Losses: [0.18164171, 2.3024836]\n",
      "epoch:1 batch_done:119 Gen Loss: nan Disc Loss: nan Q Losses: [0.17134015, 2.3034704]\n",
      "epoch:1 batch_done:120 Gen Loss: nan Disc Loss: nan Q Losses: [0.13424264, 2.3025794]\n",
      "epoch:1 batch_done:121 Gen Loss: nan Disc Loss: nan Q Losses: [0.14799964, 2.3020248]\n",
      "epoch:1 batch_done:122 Gen Loss: nan Disc Loss: nan Q Losses: [0.17951894, 2.3026965]\n",
      "epoch:1 batch_done:123 Gen Loss: nan Disc Loss: nan Q Losses: [0.1784896, 2.3031194]\n",
      "epoch:1 batch_done:124 Gen Loss: nan Disc Loss: nan Q Losses: [0.18459046, 2.3038182]\n",
      "epoch:1 batch_done:125 Gen Loss: nan Disc Loss: nan Q Losses: [0.18887338, 2.3022919]\n",
      "epoch:1 batch_done:126 Gen Loss: nan Disc Loss: nan Q Losses: [0.15899402, 2.3035207]\n",
      "epoch:1 batch_done:127 Gen Loss: nan Disc Loss: nan Q Losses: [0.19192903, 2.3024311]\n",
      "epoch:1 batch_done:128 Gen Loss: nan Disc Loss: nan Q Losses: [0.17232239, 2.3033528]\n",
      "epoch:1 batch_done:129 Gen Loss: nan Disc Loss: nan Q Losses: [0.15366715, 2.3020139]\n",
      "epoch:1 batch_done:130 Gen Loss: nan Disc Loss: nan Q Losses: [0.16237946, 2.3020191]\n",
      "epoch:1 batch_done:131 Gen Loss: nan Disc Loss: nan Q Losses: [0.16333938, 2.3028538]\n",
      "epoch:1 batch_done:132 Gen Loss: nan Disc Loss: nan Q Losses: [0.14560542, 2.302999]\n",
      "epoch:1 batch_done:133 Gen Loss: nan Disc Loss: nan Q Losses: [0.17697732, 2.303978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:134 Gen Loss: nan Disc Loss: nan Q Losses: [0.18191117, 2.3015685]\n",
      "epoch:1 batch_done:135 Gen Loss: nan Disc Loss: nan Q Losses: [0.15215217, 2.3019886]\n",
      "epoch:1 batch_done:136 Gen Loss: nan Disc Loss: nan Q Losses: [0.17277305, 2.3028955]\n",
      "epoch:1 batch_done:137 Gen Loss: nan Disc Loss: nan Q Losses: [0.1389202, 2.3024936]\n",
      "epoch:1 batch_done:138 Gen Loss: nan Disc Loss: nan Q Losses: [0.15831023, 2.3017516]\n",
      "epoch:1 batch_done:139 Gen Loss: nan Disc Loss: nan Q Losses: [0.16180117, 2.3036289]\n",
      "epoch:1 batch_done:140 Gen Loss: nan Disc Loss: nan Q Losses: [0.19563991, 2.3031483]\n",
      "epoch:1 batch_done:141 Gen Loss: nan Disc Loss: nan Q Losses: [0.16302338, 2.3041024]\n",
      "epoch:1 batch_done:142 Gen Loss: nan Disc Loss: nan Q Losses: [0.15476435, 2.3032959]\n",
      "epoch:1 batch_done:143 Gen Loss: nan Disc Loss: nan Q Losses: [0.16259921, 2.3016219]\n",
      "epoch:1 batch_done:144 Gen Loss: nan Disc Loss: nan Q Losses: [0.12351868, 2.3006711]\n",
      "epoch:1 batch_done:145 Gen Loss: nan Disc Loss: nan Q Losses: [0.22389957, 2.3015213]\n",
      "epoch:1 batch_done:146 Gen Loss: nan Disc Loss: nan Q Losses: [0.1674253, 2.3020792]\n",
      "epoch:1 batch_done:147 Gen Loss: nan Disc Loss: nan Q Losses: [0.16948475, 2.3007796]\n",
      "epoch:1 batch_done:148 Gen Loss: nan Disc Loss: nan Q Losses: [0.14802456, 2.3017931]\n",
      "epoch:1 batch_done:149 Gen Loss: nan Disc Loss: nan Q Losses: [0.19620845, 2.3025355]\n",
      "epoch:1 batch_done:150 Gen Loss: nan Disc Loss: nan Q Losses: [0.17326483, 2.3024631]\n",
      "epoch:1 batch_done:151 Gen Loss: nan Disc Loss: nan Q Losses: [0.13441795, 2.3015165]\n",
      "epoch:1 batch_done:152 Gen Loss: nan Disc Loss: nan Q Losses: [0.17611751, 2.3031442]\n",
      "epoch:1 batch_done:153 Gen Loss: nan Disc Loss: nan Q Losses: [0.18150191, 2.3024576]\n",
      "epoch:1 batch_done:154 Gen Loss: nan Disc Loss: nan Q Losses: [0.14876015, 2.3026972]\n",
      "epoch:1 batch_done:155 Gen Loss: nan Disc Loss: nan Q Losses: [0.16884983, 2.3037615]\n",
      "epoch:1 batch_done:156 Gen Loss: nan Disc Loss: nan Q Losses: [0.15568118, 2.3026547]\n",
      "epoch:1 batch_done:157 Gen Loss: nan Disc Loss: nan Q Losses: [0.17423365, 2.3020458]\n",
      "epoch:1 batch_done:158 Gen Loss: nan Disc Loss: nan Q Losses: [0.17432564, 2.3017776]\n",
      "epoch:1 batch_done:159 Gen Loss: nan Disc Loss: nan Q Losses: [0.21042271, 2.302413]\n",
      "epoch:1 batch_done:160 Gen Loss: nan Disc Loss: nan Q Losses: [0.13807112, 2.3023379]\n",
      "epoch:1 batch_done:161 Gen Loss: nan Disc Loss: nan Q Losses: [0.18914145, 2.3045936]\n",
      "epoch:1 batch_done:162 Gen Loss: nan Disc Loss: nan Q Losses: [0.18927458, 2.3031781]\n",
      "epoch:1 batch_done:163 Gen Loss: nan Disc Loss: nan Q Losses: [0.21688819, 2.3042769]\n",
      "epoch:1 batch_done:164 Gen Loss: nan Disc Loss: nan Q Losses: [0.12797599, 2.3029068]\n",
      "epoch:1 batch_done:165 Gen Loss: nan Disc Loss: nan Q Losses: [0.168423, 2.3022385]\n",
      "epoch:1 batch_done:166 Gen Loss: nan Disc Loss: nan Q Losses: [0.17220844, 2.30302]\n",
      "epoch:1 batch_done:167 Gen Loss: nan Disc Loss: nan Q Losses: [0.16279508, 2.3023019]\n",
      "epoch:1 batch_done:168 Gen Loss: nan Disc Loss: nan Q Losses: [0.14690198, 2.3024375]\n",
      "epoch:1 batch_done:169 Gen Loss: nan Disc Loss: nan Q Losses: [0.17898831, 2.3031006]\n",
      "epoch:1 batch_done:170 Gen Loss: nan Disc Loss: nan Q Losses: [0.14991742, 2.3004351]\n",
      "epoch:1 batch_done:171 Gen Loss: nan Disc Loss: nan Q Losses: [0.14778593, 2.3027148]\n",
      "epoch:1 batch_done:172 Gen Loss: nan Disc Loss: nan Q Losses: [0.17676446, 2.304213]\n",
      "epoch:1 batch_done:173 Gen Loss: nan Disc Loss: nan Q Losses: [0.16499431, 2.3016167]\n",
      "epoch:1 batch_done:174 Gen Loss: nan Disc Loss: nan Q Losses: [0.15942296, 2.3026168]\n",
      "epoch:1 batch_done:175 Gen Loss: nan Disc Loss: nan Q Losses: [0.15305594, 2.3026009]\n",
      "epoch:1 batch_done:176 Gen Loss: nan Disc Loss: nan Q Losses: [0.14689323, 2.3012705]\n",
      "epoch:1 batch_done:177 Gen Loss: nan Disc Loss: nan Q Losses: [0.17913392, 2.3027215]\n",
      "epoch:1 batch_done:178 Gen Loss: nan Disc Loss: nan Q Losses: [0.15131223, 2.3049226]\n",
      "epoch:1 batch_done:179 Gen Loss: nan Disc Loss: nan Q Losses: [0.17875022, 2.3027639]\n",
      "epoch:1 batch_done:180 Gen Loss: nan Disc Loss: nan Q Losses: [0.15318295, 2.3032415]\n",
      "epoch:1 batch_done:181 Gen Loss: nan Disc Loss: nan Q Losses: [0.16018394, 2.3010209]\n",
      "epoch:1 batch_done:182 Gen Loss: nan Disc Loss: nan Q Losses: [0.13840199, 2.3040035]\n",
      "epoch:1 batch_done:183 Gen Loss: nan Disc Loss: nan Q Losses: [0.14015219, 2.3041966]\n",
      "epoch:1 batch_done:184 Gen Loss: nan Disc Loss: nan Q Losses: [0.14732188, 2.3012605]\n",
      "epoch:1 batch_done:185 Gen Loss: nan Disc Loss: nan Q Losses: [0.17137051, 2.3035252]\n",
      "epoch:1 batch_done:186 Gen Loss: nan Disc Loss: nan Q Losses: [0.18657942, 2.3038294]\n",
      "epoch:1 batch_done:187 Gen Loss: nan Disc Loss: nan Q Losses: [0.17017633, 2.3017211]\n",
      "epoch:1 batch_done:188 Gen Loss: nan Disc Loss: nan Q Losses: [0.15648875, 2.3030655]\n",
      "epoch:1 batch_done:189 Gen Loss: nan Disc Loss: nan Q Losses: [0.15379168, 2.3044219]\n",
      "epoch:1 batch_done:190 Gen Loss: nan Disc Loss: nan Q Losses: [0.16271041, 2.3031092]\n",
      "epoch:1 batch_done:191 Gen Loss: nan Disc Loss: nan Q Losses: [0.19899499, 2.3027449]\n",
      "epoch:1 batch_done:192 Gen Loss: nan Disc Loss: nan Q Losses: [0.17340536, 2.3023429]\n",
      "epoch:1 batch_done:193 Gen Loss: nan Disc Loss: nan Q Losses: [0.14594747, 2.3027666]\n",
      "epoch:1 batch_done:194 Gen Loss: nan Disc Loss: nan Q Losses: [0.16358474, 2.301616]\n",
      "epoch:1 batch_done:195 Gen Loss: nan Disc Loss: nan Q Losses: [0.14317837, 2.3019919]\n",
      "epoch:1 batch_done:196 Gen Loss: nan Disc Loss: nan Q Losses: [0.16015755, 2.3030849]\n",
      "epoch:1 batch_done:197 Gen Loss: nan Disc Loss: nan Q Losses: [0.18961205, 2.3039794]\n",
      "epoch:1 batch_done:198 Gen Loss: nan Disc Loss: nan Q Losses: [0.20011041, 2.3021193]\n",
      "epoch:1 batch_done:199 Gen Loss: nan Disc Loss: nan Q Losses: [0.16199891, 2.301568]\n",
      "epoch:1 batch_done:200 Gen Loss: nan Disc Loss: nan Q Losses: [0.15380865, 2.3023043]\n",
      "epoch:1 batch_done:201 Gen Loss: nan Disc Loss: nan Q Losses: [0.15953143, 2.3028121]\n",
      "epoch:1 batch_done:202 Gen Loss: nan Disc Loss: nan Q Losses: [0.15437277, 2.3029556]\n",
      "epoch:1 batch_done:203 Gen Loss: nan Disc Loss: nan Q Losses: [0.13863364, 2.3028946]\n",
      "epoch:1 batch_done:204 Gen Loss: nan Disc Loss: nan Q Losses: [0.16444573, 2.3028772]\n",
      "epoch:1 batch_done:205 Gen Loss: nan Disc Loss: nan Q Losses: [0.18148671, 2.3035529]\n",
      "epoch:1 batch_done:206 Gen Loss: nan Disc Loss: nan Q Losses: [0.15325952, 2.3042426]\n",
      "epoch:1 batch_done:207 Gen Loss: nan Disc Loss: nan Q Losses: [0.16839847, 2.3027229]\n",
      "epoch:2 batch_done:1 Gen Loss: nan Disc Loss: nan Q Losses: [0.1942258, 2.3009434]\n",
      "epoch:2 batch_done:2 Gen Loss: nan Disc Loss: nan Q Losses: [0.14358696, 2.3034406]\n",
      "epoch:2 batch_done:3 Gen Loss: nan Disc Loss: nan Q Losses: [0.17122164, 2.3023264]\n",
      "epoch:2 batch_done:4 Gen Loss: nan Disc Loss: nan Q Losses: [0.17396858, 2.3020315]\n",
      "epoch:2 batch_done:5 Gen Loss: nan Disc Loss: nan Q Losses: [0.1873481, 2.3031406]\n",
      "epoch:2 batch_done:6 Gen Loss: nan Disc Loss: nan Q Losses: [0.1483621, 2.3033721]\n",
      "epoch:2 batch_done:7 Gen Loss: nan Disc Loss: nan Q Losses: [0.18738765, 2.3019161]\n",
      "epoch:2 batch_done:8 Gen Loss: nan Disc Loss: nan Q Losses: [0.16607538, 2.3039598]\n",
      "epoch:2 batch_done:9 Gen Loss: nan Disc Loss: nan Q Losses: [0.16741526, 2.3025229]\n",
      "epoch:2 batch_done:10 Gen Loss: nan Disc Loss: nan Q Losses: [0.15942644, 2.3027568]\n",
      "epoch:2 batch_done:11 Gen Loss: nan Disc Loss: nan Q Losses: [0.15342313, 2.3036406]\n",
      "epoch:2 batch_done:12 Gen Loss: nan Disc Loss: nan Q Losses: [0.17168657, 2.3029439]\n",
      "epoch:2 batch_done:13 Gen Loss: nan Disc Loss: nan Q Losses: [0.18735063, 2.3021789]\n",
      "epoch:2 batch_done:14 Gen Loss: nan Disc Loss: nan Q Losses: [0.16012204, 2.3028109]\n",
      "epoch:2 batch_done:15 Gen Loss: nan Disc Loss: nan Q Losses: [0.14657277, 2.3024383]\n",
      "epoch:2 batch_done:16 Gen Loss: nan Disc Loss: nan Q Losses: [0.18398625, 2.3041663]\n",
      "epoch:2 batch_done:17 Gen Loss: nan Disc Loss: nan Q Losses: [0.14076129, 2.3036823]\n",
      "epoch:2 batch_done:18 Gen Loss: nan Disc Loss: nan Q Losses: [0.16599184, 2.3029642]\n",
      "epoch:2 batch_done:19 Gen Loss: nan Disc Loss: nan Q Losses: [0.16201603, 2.3048873]\n",
      "epoch:2 batch_done:20 Gen Loss: nan Disc Loss: nan Q Losses: [0.19435295, 2.3036139]\n",
      "epoch:2 batch_done:21 Gen Loss: nan Disc Loss: nan Q Losses: [0.19627684, 2.3018365]\n",
      "epoch:2 batch_done:22 Gen Loss: nan Disc Loss: nan Q Losses: [0.1799316, 2.3022909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:23 Gen Loss: nan Disc Loss: nan Q Losses: [0.19516993, 2.3030441]\n",
      "epoch:2 batch_done:24 Gen Loss: nan Disc Loss: nan Q Losses: [0.18309525, 2.3023591]\n",
      "epoch:2 batch_done:25 Gen Loss: nan Disc Loss: nan Q Losses: [0.15322037, 2.3028216]\n",
      "epoch:2 batch_done:26 Gen Loss: nan Disc Loss: nan Q Losses: [0.16256255, 2.303494]\n",
      "epoch:2 batch_done:27 Gen Loss: nan Disc Loss: nan Q Losses: [0.16095647, 2.3019819]\n",
      "epoch:2 batch_done:28 Gen Loss: nan Disc Loss: nan Q Losses: [0.15136617, 2.3029158]\n",
      "epoch:2 batch_done:29 Gen Loss: nan Disc Loss: nan Q Losses: [0.15625297, 2.3022563]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fae07538ba4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_Q\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_cont_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_cat_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update to optimize mutual information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "# blow up after 81800\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/Session#run\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "c_val = 10\n",
    "\n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 1000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        train_data_filenames=permutation(train_data_filenames) # mini-batch\n",
    "        data_left = len(train_data_filenames)\n",
    "        batch_counter = 0\n",
    "        while data_left>0:\n",
    "            batch_size_to_train = min(batch_size, data_left)          \n",
    "            \n",
    "            zs = np.random.uniform(-1.0,1.0,size=[batch_size_to_train,z_size]).astype(np.float32) #Generate a random z batch\n",
    "            #print(\"zs shape:\",zs.shape)\n",
    "            \n",
    "            #lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "            lcat = np.random.randint(0,c_val,[batch_size_to_train,len(categorical_list)]) #Generate random c batch\n",
    "            \n",
    "            lcont = np.random.uniform(-1,1,[batch_size_to_train,number_continuous]) #\n",
    "\n",
    "            #xs = read_train_data_random_batch(train_data_filenames, batchsize=batch_size_to_train)\n",
    "            xs = read_train_data_mini_batch(train_data_filenames, batch_counter*batch_size, batch_size_to_train)\n",
    "            \n",
    "            _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "            _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "            _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "\n",
    "            data_left = data_left - batch_size_to_train\n",
    "            batch_counter +=1\n",
    "            if batch_counter%1 == 0 or data_left == 0:\n",
    "                z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32)\n",
    "                lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "                a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "                b = np.reshape(a,[c_val*c_val,1])\n",
    "                lcont_sample = b\n",
    "                samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample})\n",
    "                if not os.path.exists(sample_directory):\n",
    "                    os.makedirs(sample_directory)\n",
    "                save_images(np.reshape(samples[0:100],[100,32,32,3]),[10,10],sample_directory+'/fig'\\\n",
    "                            +str(i)+'_'+str(batch_counter)+'.png')\n",
    "                \n",
    "                print (\"epoch:\"+str(i)+\" batch_done:\"+str(batch_counter) \\\n",
    "                       +\" Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            \n",
    "             \n",
    "        \"\"\"\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "            #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "            latent_fixed = np.ones((c_val*c_val,1))\n",
    "            lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "            \n",
    "            #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "            #b = np.reshape(a,[100,1])\n",
    "            b = np.reshape(a,[c_val*c_val,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            #\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "            save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig'+str(i)+'.png')\n",
    "        \"\"\"\n",
    "        \n",
    "        if i % 10 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model on \", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\"\"\"\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "c_val = 10\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "    #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #lcat_sample = np.reshape(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]),[100,1])\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "    #print(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]))\n",
    "    #latent_fixed = np.ones((c_val*c_val,1))*50\n",
    "    latent_fixed = np.zeros((c_val*c_val,1))\n",
    "    #lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "    # good shape\n",
    "    lcat_sample = np.hstack([lcat_sample,latent_fixed])\n",
    "            \n",
    "    #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #a = a = np.ones((c_val*c_val,1))*-0.5\n",
    "    #a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #b = np.reshape(a,[100,1])\n",
    "    b = np.reshape(a,[c_val*c_val,1])\n",
    "    #c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)\n",
    "    c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)+8\n",
    "    #angle\n",
    "    lcont_sample = np.hstack([b,c])\n",
    "    # width\n",
    "    #lcont_sample = np.hstack([c,b])\n",
    "    \n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test_4'+'.png')\n",
    "    save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig_test_13'+'.png')\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35tf]",
   "language": "python",
   "name": "conda-env-dlpy35tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
