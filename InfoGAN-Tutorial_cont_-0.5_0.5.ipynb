{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorials walks through an implementation of InfoGAN as described in [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n",
    "\n",
    "To learn more about InfoGAN, see this [Medium post](https://medium.com/p/dd710852db46) on them. To lean more about GANs generally, see [this one](https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.692jyamki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/depth_to_space\n",
    "# http://qiita.com/tadOne/items/48302a399dcad44c69c8   Tensorflow - padding = VALID/SAMEの違いについて\n",
    "#     so 3 tf.depth_to_space(genX,2) gives 4x2^3 = 32\n",
    "# \n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=128,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=64,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=32,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,32,[3,3],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,64,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,128,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/split\n",
    "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "# https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "# https://www.tensorflow.org/api_docs/python/tf/trainable_variables\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "# https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "# z_lat: one_hot_size + z_size + number_continuous = 10+64+2=76\n",
    "# g_loss def is interesting, my understanding: \n",
    "#        if Dg is the probablity to be told as feak data, then 1-Dg is the probabily of suceessfully cheating, \n",
    "#        so we cal KL(Dg/(1-Dg)), and readuce_mean works as sampling proceduce\n",
    "# \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 64 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "# categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "categorical_list = [10,10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 2 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "#print(\"latent_cat_in:\", latent_cat_in)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "#print(\"latent_cat_list: \",latent_cat_list)\n",
    "latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    #print(latent_cat_list[idx])\n",
    "    #print(latent_oh),  woundn't print anything in sess.run()\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "#print(\"1st z_lats: \", z_lats )\n",
    "z_lats.append(z_in)\n",
    "#print(\"2nd z_lats: \", z_lats )\n",
    "z_lats.append(latent_cont_in)\n",
    "#print(\"3rd z_lats: \", z_lats )\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "#print(\"z_lat: \", z_lat )\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log((Dg/(1-Dg)))) #KL Divergence optimizer\n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    #print (\"latent_var: \", latent_var)\n",
    "    #print (\"tf.log(QgCat[idx]): \",tf.log(QgCat[idx]))\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "#print (len(tvars))\n",
    "#for i in tvars:\n",
    "#    print(i)\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.002,beta1=0.5  )\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.88683 Disc Loss: 1.38475 Q Losses: [0.33055681, 2.3019431]\n",
      "Gen Loss: 5.01192 Disc Loss: 1.48685 Q Losses: [0.32546675, 2.3138018]\n",
      "Gen Loss: 2.83209 Disc Loss: 0.382227 Q Losses: [0.30639446, 2.2880154]\n",
      "Gen Loss: 4.39966 Disc Loss: 0.283994 Q Losses: [0.31002137, 2.2910056]\n",
      "Gen Loss: 3.75773 Disc Loss: 0.669019 Q Losses: [0.3462739, 2.2979536]\n",
      "Gen Loss: 2.788 Disc Loss: 0.449476 Q Losses: [0.31907439, 2.3015063]\n",
      "Gen Loss: 0.570638 Disc Loss: 1.98395 Q Losses: [0.35817301, 2.2944474]\n",
      "Gen Loss: 5.02872 Disc Loss: 1.55026 Q Losses: [0.34132946, 2.3008089]\n",
      "Gen Loss: 1.00852 Disc Loss: 0.647349 Q Losses: [0.31863058, 2.3029485]\n",
      "Gen Loss: 4.22208 Disc Loss: 0.929014 Q Losses: [0.32986876, 2.3078885]\n",
      "Gen Loss: 1.99651 Disc Loss: 0.525141 Q Losses: [0.30726612, 2.2989049]\n",
      "Saved Model on  1000\n",
      "Gen Loss: 0.978517 Disc Loss: 0.656552 Q Losses: [0.30708298, 2.3002746]\n",
      "Gen Loss: 1.68426 Disc Loss: 0.569623 Q Losses: [0.29816145, 2.3183336]\n",
      "Gen Loss: 0.963222 Disc Loss: 0.5945 Q Losses: [0.34431702, 2.3101726]\n",
      "Gen Loss: 0.639202 Disc Loss: 0.842699 Q Losses: [0.29107571, 2.3040981]\n",
      "Gen Loss: 1.52214 Disc Loss: 0.884742 Q Losses: [0.33477187, 2.3118477]\n",
      "Gen Loss: 1.69644 Disc Loss: 0.793933 Q Losses: [0.37790191, 2.3100805]\n",
      "Gen Loss: -0.16722 Disc Loss: 0.750118 Q Losses: [0.3551172, 2.3063183]\n",
      "Gen Loss: 1.3121 Disc Loss: 0.952713 Q Losses: [0.36178058, 2.3016191]\n",
      "Gen Loss: 3.34029 Disc Loss: 1.37363 Q Losses: [0.35208291, 2.3123188]\n",
      "Gen Loss: 0.653718 Disc Loss: 0.796868 Q Losses: [0.33891177, 2.3049808]\n",
      "Saved Model on  2000\n",
      "Gen Loss: 1.58569 Disc Loss: 0.682222 Q Losses: [0.30457056, 2.3076134]\n",
      "Gen Loss: 1.88919 Disc Loss: 0.900882 Q Losses: [0.32601348, 2.3070996]\n",
      "Gen Loss: 2.03009 Disc Loss: 0.524323 Q Losses: [0.29401442, 2.2953482]\n",
      "Gen Loss: 1.70308 Disc Loss: 0.852424 Q Losses: [0.35583317, 2.3142934]\n",
      "Gen Loss: 1.7728 Disc Loss: 0.983273 Q Losses: [0.33943951, 2.2879052]\n",
      "Gen Loss: 0.0761349 Disc Loss: 1.18767 Q Losses: [0.34290934, 2.2745595]\n",
      "Gen Loss: 1.67127 Disc Loss: 1.22257 Q Losses: [0.30240384, 2.2384851]\n",
      "Gen Loss: 0.989979 Disc Loss: 1.27268 Q Losses: [0.31342214, 2.2219865]\n",
      "Gen Loss: 0.899377 Disc Loss: 1.19941 Q Losses: [0.30934596, 2.1384935]\n",
      "Gen Loss: 0.218082 Disc Loss: 1.21664 Q Losses: [0.30767667, 2.1583288]\n",
      "Saved Model on  3000\n",
      "Gen Loss: 1.22516 Disc Loss: 1.20452 Q Losses: [0.34709826, 2.1060362]\n",
      "Gen Loss: 0.244676 Disc Loss: 1.27796 Q Losses: [0.38713881, 2.1056314]\n",
      "Gen Loss: 0.482776 Disc Loss: 1.24381 Q Losses: [0.28376326, 2.1795797]\n",
      "Gen Loss: 0.376172 Disc Loss: 1.06155 Q Losses: [0.3121061, 2.15204]\n",
      "Gen Loss: 0.0864234 Disc Loss: 1.23492 Q Losses: [0.30554163, 2.1193821]\n",
      "Gen Loss: 0.888777 Disc Loss: 1.05956 Q Losses: [0.31929433, 2.1680613]\n",
      "Gen Loss: 0.117276 Disc Loss: 1.06799 Q Losses: [0.29995358, 2.1094553]\n",
      "Gen Loss: 0.760516 Disc Loss: 1.00749 Q Losses: [0.33249658, 2.2222638]\n",
      "Gen Loss: -0.692926 Disc Loss: 1.40896 Q Losses: [0.3450895, 2.1621165]\n",
      "Gen Loss: 0.673976 Disc Loss: 0.975464 Q Losses: [0.34108683, 2.256855]\n",
      "Saved Model on  4000\n",
      "Gen Loss: 0.207915 Disc Loss: 1.14344 Q Losses: [0.32663679, 2.2421613]\n",
      "Gen Loss: -0.217426 Disc Loss: 1.16008 Q Losses: [0.31913352, 2.2110019]\n",
      "Gen Loss: 0.525094 Disc Loss: 1.1416 Q Losses: [0.31521523, 2.2559762]\n",
      "Gen Loss: -2.23055 Disc Loss: 1.62881 Q Losses: [0.3318423, 2.1888695]\n",
      "Gen Loss: 1.55545 Disc Loss: 0.858568 Q Losses: [0.36119783, 2.2290828]\n",
      "Gen Loss: 0.316744 Disc Loss: 0.999256 Q Losses: [0.33023208, 2.117342]\n",
      "Gen Loss: -0.484428 Disc Loss: 1.51061 Q Losses: [0.32725954, 2.2652478]\n",
      "Gen Loss: 2.43186 Disc Loss: 1.06948 Q Losses: [0.36609909, 2.2062445]\n",
      "Gen Loss: 0.97787 Disc Loss: 0.857463 Q Losses: [0.33629167, 2.2090545]\n",
      "Gen Loss: 0.887629 Disc Loss: 0.921506 Q Losses: [0.29465303, 2.1774416]\n",
      "Saved Model on  5000\n",
      "Gen Loss: 1.31961 Disc Loss: 0.985322 Q Losses: [0.33198404, 2.1920896]\n",
      "Gen Loss: 0.125668 Disc Loss: 1.01241 Q Losses: [0.2976858, 2.1634049]\n",
      "Gen Loss: 1.40178 Disc Loss: 1.15941 Q Losses: [0.28406262, 2.175051]\n",
      "Gen Loss: -0.17583 Disc Loss: 0.79715 Q Losses: [0.33538094, 2.2388101]\n",
      "Gen Loss: -0.884753 Disc Loss: 1.52956 Q Losses: [0.29572076, 2.113297]\n",
      "Gen Loss: 0.295082 Disc Loss: 1.503 Q Losses: [0.31151563, 2.1154687]\n",
      "Gen Loss: 0.714183 Disc Loss: 0.806116 Q Losses: [0.29199177, 2.1943324]\n",
      "Gen Loss: 0.328857 Disc Loss: 1.07256 Q Losses: [0.3264856, 2.1369939]\n",
      "Gen Loss: 1.13592 Disc Loss: 0.967331 Q Losses: [0.31707025, 2.1790342]\n",
      "Gen Loss: 2.17849 Disc Loss: 1.26202 Q Losses: [0.32788324, 2.1763124]\n",
      "Saved Model on  6000\n",
      "Gen Loss: 2.08997 Disc Loss: 1.23779 Q Losses: [0.30146474, 2.0922222]\n",
      "Gen Loss: 0.693316 Disc Loss: 0.790392 Q Losses: [0.3605237, 2.176764]\n",
      "Gen Loss: 1.76436 Disc Loss: 0.840618 Q Losses: [0.30627373, 2.1354547]\n",
      "Gen Loss: 0.256173 Disc Loss: 1.05694 Q Losses: [0.29075906, 2.1208355]\n",
      "Gen Loss: 1.83723 Disc Loss: 0.930436 Q Losses: [0.33299202, 2.1154037]\n",
      "Gen Loss: 1.70589 Disc Loss: 0.892188 Q Losses: [0.28171891, 2.0761161]\n",
      "Gen Loss: 1.22742 Disc Loss: 0.765179 Q Losses: [0.3140716, 2.0711954]\n",
      "Gen Loss: 0.0542632 Disc Loss: 1.4425 Q Losses: [0.2753737, 2.0180545]\n",
      "Gen Loss: 0.0862169 Disc Loss: 0.701539 Q Losses: [0.30791077, 1.9846867]\n",
      "Gen Loss: 0.947566 Disc Loss: 0.887096 Q Losses: [0.35263112, 2.1146624]\n",
      "Saved Model on  7000\n",
      "Gen Loss: 4.10756 Disc Loss: 1.45593 Q Losses: [0.37015831, 2.1049004]\n",
      "Gen Loss: 3.7827 Disc Loss: 1.55376 Q Losses: [0.30834699, 2.0560141]\n",
      "Gen Loss: 1.88673 Disc Loss: 0.79565 Q Losses: [0.30494606, 2.0497437]\n",
      "Gen Loss: 0.110297 Disc Loss: 0.903782 Q Losses: [0.33690661, 2.0179343]\n",
      "Gen Loss: 2.61639 Disc Loss: 1.18295 Q Losses: [0.32103875, 2.0455897]\n",
      "Gen Loss: -1.35373 Disc Loss: 1.35496 Q Losses: [0.31744492, 2.0495975]\n",
      "Gen Loss: 0.420614 Disc Loss: 0.825542 Q Losses: [0.32931536, 2.0677576]\n",
      "Gen Loss: 2.06128 Disc Loss: 0.780538 Q Losses: [0.30691221, 2.0357869]\n",
      "Gen Loss: 0.29767 Disc Loss: 0.93434 Q Losses: [0.32696477, 2.0861802]\n",
      "Gen Loss: 1.93915 Disc Loss: 0.626762 Q Losses: [0.35053754, 1.9485068]\n",
      "Saved Model on  8000\n",
      "Gen Loss: 3.39243 Disc Loss: 1.09131 Q Losses: [0.40334439, 2.0070009]\n",
      "Gen Loss: 2.83021 Disc Loss: 0.833799 Q Losses: [0.32857916, 1.9486189]\n",
      "Gen Loss: -0.0986411 Disc Loss: 0.844026 Q Losses: [0.37412298, 1.997252]\n",
      "Gen Loss: 2.68993 Disc Loss: 0.71048 Q Losses: [0.31032982, 1.9952288]\n",
      "Gen Loss: 0.434985 Disc Loss: 0.906749 Q Losses: [0.38102627, 2.0010965]\n",
      "Gen Loss: 3.27486 Disc Loss: 1.10586 Q Losses: [0.25460583, 2.0012088]\n",
      "Gen Loss: -0.309773 Disc Loss: 1.47368 Q Losses: [0.31962377, 1.9718081]\n",
      "Gen Loss: 0.749794 Disc Loss: 0.837685 Q Losses: [0.29475337, 1.9708657]\n",
      "Gen Loss: 0.400682 Disc Loss: 0.875965 Q Losses: [0.35316563, 2.0545604]\n",
      "Gen Loss: -0.129575 Disc Loss: 0.838074 Q Losses: [0.32343137, 1.975451]\n",
      "Saved Model on  9000\n",
      "Gen Loss: 0.918941 Disc Loss: 0.627794 Q Losses: [0.26422387, 1.9846787]\n",
      "Gen Loss: -0.283494 Disc Loss: 0.802885 Q Losses: [0.34254885, 1.9575343]\n",
      "Gen Loss: 0.811475 Disc Loss: 0.652425 Q Losses: [0.28384179, 1.9980795]\n",
      "Gen Loss: 1.85158 Disc Loss: 1.04297 Q Losses: [0.32772756, 1.982898]\n",
      "Gen Loss: 3.45812 Disc Loss: 0.603743 Q Losses: [0.32633868, 1.9318511]\n",
      "Gen Loss: 1.47954 Disc Loss: 0.892194 Q Losses: [0.30342937, 1.9850132]\n",
      "Gen Loss: 2.10392 Disc Loss: 0.672493 Q Losses: [0.34049183, 1.9024037]\n",
      "Gen Loss: 0.0515582 Disc Loss: 1.33278 Q Losses: [0.34507823, 2.0121951]\n",
      "Gen Loss: 3.48324 Disc Loss: 1.26694 Q Losses: [0.32416669, 1.9354224]\n",
      "Gen Loss: 0.109593 Disc Loss: 1.10993 Q Losses: [0.34271926, 1.8706894]\n",
      "Saved Model on  10000\n",
      "Gen Loss: 0.134669 Disc Loss: 0.800979 Q Losses: [0.32117772, 1.8752792]\n",
      "Gen Loss: 3.51353 Disc Loss: 1.43296 Q Losses: [0.29946834, 2.0284729]\n",
      "Gen Loss: 3.38226 Disc Loss: 1.07324 Q Losses: [0.31200862, 1.9527061]\n",
      "Gen Loss: 0.928139 Disc Loss: 0.72002 Q Losses: [0.31025004, 1.8079886]\n",
      "Gen Loss: 0.568601 Disc Loss: 0.648294 Q Losses: [0.37225693, 1.8971183]\n",
      "Gen Loss: 2.94183 Disc Loss: 1.29981 Q Losses: [0.33769953, 1.9075539]\n",
      "Gen Loss: 4.3946 Disc Loss: 2.66874 Q Losses: [0.32037538, 1.9133863]\n",
      "Gen Loss: 0.672424 Disc Loss: 0.96831 Q Losses: [0.31647801, 1.9794064]\n",
      "Gen Loss: -0.529994 Disc Loss: 1.42336 Q Losses: [0.32231724, 1.8416426]\n",
      "Gen Loss: -0.529412 Disc Loss: 2.3996 Q Losses: [0.33769071, 1.8583736]\n",
      "Saved Model on  11000\n",
      "Gen Loss: 4.48148 Disc Loss: 1.34963 Q Losses: [0.33553323, 1.9663472]\n",
      "Gen Loss: -0.1448 Disc Loss: 0.749072 Q Losses: [0.37465578, 1.8957198]\n",
      "Gen Loss: 1.41324 Disc Loss: 0.59904 Q Losses: [0.31816706, 1.8135871]\n",
      "Gen Loss: 1.23551 Disc Loss: 0.973718 Q Losses: [0.29487669, 1.8661101]\n",
      "Gen Loss: 4.74704 Disc Loss: 1.11921 Q Losses: [0.28404826, 1.9295536]\n",
      "Gen Loss: -0.453318 Disc Loss: 0.968927 Q Losses: [0.29563719, 1.9556127]\n",
      "Gen Loss: 0.957703 Disc Loss: 0.62776 Q Losses: [0.32753652, 1.8012754]\n",
      "Gen Loss: 1.82973 Disc Loss: 0.578607 Q Losses: [0.35244879, 1.9259287]\n",
      "Gen Loss: 3.76262 Disc Loss: 0.650358 Q Losses: [0.31690633, 1.836574]\n",
      "Gen Loss: 1.09791 Disc Loss: 0.534584 Q Losses: [0.28136104, 1.822489]\n",
      "Saved Model on  12000\n",
      "Gen Loss: 2.44812 Disc Loss: 0.628398 Q Losses: [0.3279632, 1.7540728]\n",
      "Gen Loss: 3.50577 Disc Loss: 0.897453 Q Losses: [0.30743197, 1.8160495]\n",
      "Gen Loss: 4.54658 Disc Loss: 0.422989 Q Losses: [0.38188004, 1.8700037]\n",
      "Gen Loss: 0.7268 Disc Loss: 0.76227 Q Losses: [0.29607391, 1.8191742]\n",
      "Gen Loss: 4.3781 Disc Loss: 1.44379 Q Losses: [0.28193152, 1.8443551]\n",
      "Gen Loss: 2.68167 Disc Loss: 0.83724 Q Losses: [0.34197876, 1.8621027]\n",
      "Gen Loss: 4.24834 Disc Loss: 1.22895 Q Losses: [0.33419329, 1.8426716]\n",
      "Gen Loss: 3.09365 Disc Loss: 0.816036 Q Losses: [0.27643648, 1.7583654]\n",
      "Gen Loss: 0.818112 Disc Loss: 0.62174 Q Losses: [0.34775412, 1.7932932]\n",
      "Gen Loss: 2.66339 Disc Loss: 0.582601 Q Losses: [0.25715488, 1.8893329]\n",
      "Saved Model on  13000\n",
      "Gen Loss: 2.43657 Disc Loss: 0.857124 Q Losses: [0.31636405, 1.8706019]\n",
      "Gen Loss: 3.18823 Disc Loss: 0.482844 Q Losses: [0.36155397, 1.7487464]\n",
      "Gen Loss: 0.484879 Disc Loss: 0.69144 Q Losses: [0.31023669, 1.8742756]\n",
      "Gen Loss: -1.16895 Disc Loss: 1.06281 Q Losses: [0.31218082, 1.8826847]\n",
      "Gen Loss: 3.04413 Disc Loss: 0.337849 Q Losses: [0.37084889, 1.7026639]\n",
      "Gen Loss: 1.59474 Disc Loss: 0.729273 Q Losses: [0.27265465, 1.740352]\n",
      "Gen Loss: 1.17007 Disc Loss: 0.626407 Q Losses: [0.33247784, 1.7554728]\n",
      "Gen Loss: -1.39357 Disc Loss: 1.08666 Q Losses: [0.33999383, 1.8031273]\n",
      "Gen Loss: 1.1875 Disc Loss: 0.629658 Q Losses: [0.31418049, 1.7824908]\n",
      "Gen Loss: -0.362035 Disc Loss: 0.76846 Q Losses: [0.29332054, 1.7706233]\n",
      "Saved Model on  14000\n",
      "Gen Loss: 3.49335 Disc Loss: 1.08242 Q Losses: [0.27701655, 1.8167796]\n",
      "Gen Loss: 1.13385 Disc Loss: 0.594669 Q Losses: [0.33258593, 1.6361699]\n",
      "Gen Loss: 2.03985 Disc Loss: 0.544227 Q Losses: [0.32177922, 1.7736776]\n",
      "Gen Loss: 3.91962 Disc Loss: 0.778786 Q Losses: [0.27506846, 1.6460201]\n",
      "Gen Loss: 2.51312 Disc Loss: 0.591341 Q Losses: [0.33193809, 1.7550867]\n",
      "Gen Loss: 1.00603 Disc Loss: 0.646583 Q Losses: [0.27192485, 1.79308]\n",
      "Gen Loss: 3.2391 Disc Loss: 0.582641 Q Losses: [0.28335029, 1.7309234]\n",
      "Gen Loss: 1.7692 Disc Loss: 0.667594 Q Losses: [0.32073921, 1.7578228]\n",
      "Gen Loss: 2.03454 Disc Loss: 0.440805 Q Losses: [0.30229962, 1.6956906]\n",
      "Gen Loss: 1.92725 Disc Loss: 0.448502 Q Losses: [0.34983823, 1.738709]\n",
      "Saved Model on  15000\n",
      "Gen Loss: 1.23598 Disc Loss: 1.57865 Q Losses: [0.31857058, 1.7174184]\n",
      "Gen Loss: 4.39643 Disc Loss: 0.413034 Q Losses: [0.32903776, 1.7415557]\n",
      "Gen Loss: -0.957207 Disc Loss: 0.751415 Q Losses: [0.28229672, 1.7559726]\n",
      "Gen Loss: 1.52939 Disc Loss: 0.655068 Q Losses: [0.32872105, 1.6793284]\n",
      "Gen Loss: 1.14864 Disc Loss: 0.488755 Q Losses: [0.28799027, 1.8016081]\n",
      "Gen Loss: 0.626688 Disc Loss: 0.622166 Q Losses: [0.25989395, 1.6870975]\n",
      "Gen Loss: 1.04653 Disc Loss: 0.621057 Q Losses: [0.31541571, 1.7659166]\n",
      "Gen Loss: 4.54332 Disc Loss: 0.941418 Q Losses: [0.33819699, 1.772491]\n",
      "Gen Loss: 3.12131 Disc Loss: 0.377698 Q Losses: [0.34316385, 1.6853623]\n",
      "Gen Loss: 3.17867 Disc Loss: 0.577325 Q Losses: [0.30579954, 1.7306767]\n",
      "Saved Model on  16000\n",
      "Gen Loss: 0.816472 Disc Loss: 0.922701 Q Losses: [0.3603819, 1.7088526]\n",
      "Gen Loss: -0.599639 Disc Loss: 1.22767 Q Losses: [0.36422473, 1.6594838]\n",
      "Gen Loss: 4.19261 Disc Loss: 0.526613 Q Losses: [0.29792199, 1.750774]\n",
      "Gen Loss: 3.35108 Disc Loss: 0.25291 Q Losses: [0.25798088, 1.5762738]\n",
      "Gen Loss: -0.704792 Disc Loss: 4.78849 Q Losses: [0.30621654, 1.747117]\n",
      "Gen Loss: 2.23737 Disc Loss: 0.369404 Q Losses: [0.26376081, 1.7384999]\n",
      "Gen Loss: 2.45244 Disc Loss: 0.42583 Q Losses: [0.28674936, 1.6128328]\n",
      "Gen Loss: 3.50974 Disc Loss: 0.477022 Q Losses: [0.32138312, 1.5846083]\n",
      "Gen Loss: 1.00865 Disc Loss: 0.830017 Q Losses: [0.3123543, 1.6543553]\n",
      "Gen Loss: -1.61207 Disc Loss: 2.15289 Q Losses: [0.30908877, 1.5562496]\n",
      "Saved Model on  17000\n",
      "Gen Loss: 1.97896 Disc Loss: 0.644028 Q Losses: [0.2701979, 1.6097385]\n",
      "Gen Loss: 2.04988 Disc Loss: 0.398868 Q Losses: [0.33276838, 1.64419]\n",
      "Gen Loss: 5.3677 Disc Loss: 2.28676 Q Losses: [0.33348632, 1.6267706]\n",
      "Gen Loss: 1.27442 Disc Loss: 0.498802 Q Losses: [0.2960805, 1.6305358]\n",
      "Gen Loss: 4.01901 Disc Loss: 0.524929 Q Losses: [0.32919553, 1.6830628]\n",
      "Gen Loss: 2.83492 Disc Loss: 0.399245 Q Losses: [0.34702897, 1.6764042]\n",
      "Gen Loss: 3.66235 Disc Loss: 0.291194 Q Losses: [0.26047957, 1.6441442]\n",
      "Gen Loss: 3.79027 Disc Loss: 0.275503 Q Losses: [0.2784645, 1.6838062]\n",
      "Gen Loss: 2.92464 Disc Loss: 0.323476 Q Losses: [0.31454712, 1.5575852]\n",
      "Gen Loss: 2.2842 Disc Loss: 0.572129 Q Losses: [0.32503545, 1.5992937]\n",
      "Saved Model on  18000\n",
      "Gen Loss: 1.41471 Disc Loss: 0.450494 Q Losses: [0.31725711, 1.5259304]\n",
      "Gen Loss: 2.28353 Disc Loss: 0.440098 Q Losses: [0.33411211, 1.7294424]\n",
      "Gen Loss: 0.838021 Disc Loss: 0.604392 Q Losses: [0.29334766, 1.581954]\n",
      "Gen Loss: 2.12413 Disc Loss: 0.69617 Q Losses: [0.30863819, 1.5395502]\n",
      "Gen Loss: 2.62078 Disc Loss: 0.303093 Q Losses: [0.30855614, 1.6828727]\n",
      "Gen Loss: 3.09368 Disc Loss: 0.359743 Q Losses: [0.24569447, 1.5253286]\n",
      "Gen Loss: 2.53506 Disc Loss: 0.365279 Q Losses: [0.30697879, 1.6271123]\n",
      "Gen Loss: 2.54249 Disc Loss: 0.540998 Q Losses: [0.29702839, 1.6525559]\n",
      "Gen Loss: 3.27945 Disc Loss: 0.489628 Q Losses: [0.30436885, 1.5248613]\n",
      "Gen Loss: 2.19115 Disc Loss: 0.318512 Q Losses: [0.32300803, 1.5931642]\n",
      "Saved Model on  19000\n",
      "Gen Loss: 2.55915 Disc Loss: 0.523738 Q Losses: [0.27562347, 1.5343997]\n",
      "Gen Loss: 3.51933 Disc Loss: 0.389018 Q Losses: [0.23852777, 1.688941]\n",
      "Gen Loss: 1.82652 Disc Loss: 0.578614 Q Losses: [0.26794481, 1.6049571]\n",
      "Gen Loss: 2.46917 Disc Loss: 0.684801 Q Losses: [0.31353909, 1.5922935]\n",
      "Gen Loss: 2.28786 Disc Loss: 0.637456 Q Losses: [0.32391435, 1.4804819]\n",
      "Gen Loss: 2.16685 Disc Loss: 0.958189 Q Losses: [0.34321985, 1.484754]\n",
      "Gen Loss: 4.73259 Disc Loss: 0.426065 Q Losses: [0.29248077, 1.442987]\n",
      "Gen Loss: 6.36638 Disc Loss: 1.0261 Q Losses: [0.34119558, 1.5546138]\n",
      "Gen Loss: 3.36917 Disc Loss: 0.303946 Q Losses: [0.33278018, 1.6335592]\n",
      "Gen Loss: 3.67065 Disc Loss: 0.300281 Q Losses: [0.31903562, 1.6074847]\n",
      "Saved Model on  20000\n",
      "Gen Loss: 3.45938 Disc Loss: 0.451417 Q Losses: [0.25167453, 1.4594088]\n",
      "Gen Loss: -0.273775 Disc Loss: 0.541135 Q Losses: [0.31415364, 1.6655991]\n",
      "Gen Loss: 1.42343 Disc Loss: 0.541025 Q Losses: [0.26387966, 1.5924714]\n",
      "Gen Loss: -0.372732 Disc Loss: 0.879175 Q Losses: [0.31099939, 1.4389057]\n",
      "Gen Loss: 1.43455 Disc Loss: 0.717138 Q Losses: [0.30050328, 1.5578048]\n",
      "Gen Loss: 1.56373 Disc Loss: 0.482123 Q Losses: [0.28411722, 1.4692448]\n",
      "Gen Loss: 3.49855 Disc Loss: 0.393459 Q Losses: [0.25190353, 1.5614996]\n",
      "Gen Loss: 4.95468 Disc Loss: 0.830667 Q Losses: [0.28560749, 1.5666516]\n",
      "Gen Loss: 0.18913 Disc Loss: 0.741525 Q Losses: [0.27097344, 1.5256793]\n",
      "Gen Loss: 3.81068 Disc Loss: 0.584896 Q Losses: [0.30529204, 1.5273364]\n",
      "Saved Model on  21000\n",
      "Gen Loss: 1.58314 Disc Loss: 0.614892 Q Losses: [0.29137275, 1.5239995]\n",
      "Gen Loss: -0.670573 Disc Loss: 0.682948 Q Losses: [0.28567398, 1.5138352]\n",
      "Gen Loss: 2.86176 Disc Loss: 0.540696 Q Losses: [0.34735644, 1.5227746]\n",
      "Gen Loss: 3.54139 Disc Loss: 0.417627 Q Losses: [0.31149793, 1.487473]\n",
      "Gen Loss: 2.59917 Disc Loss: 0.875597 Q Losses: [0.31212926, 1.6299477]\n",
      "Gen Loss: 2.57475 Disc Loss: 0.350849 Q Losses: [0.29183376, 1.4612665]\n",
      "Gen Loss: -1.59061 Disc Loss: 2.2302 Q Losses: [0.30820137, 1.4458017]\n",
      "Gen Loss: 3.18944 Disc Loss: 0.326287 Q Losses: [0.31480724, 1.3734003]\n",
      "Gen Loss: 1.37285 Disc Loss: 0.553268 Q Losses: [0.28564084, 1.5281477]\n",
      "Gen Loss: 2.14422 Disc Loss: 0.491409 Q Losses: [0.32489747, 1.4923139]\n",
      "Saved Model on  22000\n",
      "Gen Loss: 0.781001 Disc Loss: 0.475215 Q Losses: [0.34040901, 1.3593649]\n",
      "Gen Loss: 2.7745 Disc Loss: 0.380415 Q Losses: [0.28138876, 1.4924705]\n",
      "Gen Loss: -0.579419 Disc Loss: 1.06253 Q Losses: [0.29036766, 1.4627918]\n",
      "Gen Loss: 1.61084 Disc Loss: 0.613927 Q Losses: [0.25907075, 1.3857193]\n",
      "Gen Loss: 1.69601 Disc Loss: 0.343251 Q Losses: [0.36504668, 1.4167902]\n",
      "Gen Loss: 2.22364 Disc Loss: 0.558852 Q Losses: [0.31773999, 1.3501482]\n",
      "Gen Loss: 2.93535 Disc Loss: 0.437099 Q Losses: [0.25842002, 1.5014377]\n",
      "Gen Loss: 2.84909 Disc Loss: 0.403026 Q Losses: [0.33864596, 1.4227282]\n",
      "Gen Loss: 0.555518 Disc Loss: 0.545319 Q Losses: [0.30851176, 1.4093511]\n",
      "Gen Loss: 2.81699 Disc Loss: 0.989348 Q Losses: [0.33802253, 1.4028202]\n",
      "Saved Model on  23000\n",
      "Gen Loss: 4.03693 Disc Loss: 0.474268 Q Losses: [0.29723865, 1.3842866]\n",
      "Gen Loss: 1.9326 Disc Loss: 0.348322 Q Losses: [0.27256596, 1.3432174]\n",
      "Gen Loss: 3.18477 Disc Loss: 0.392752 Q Losses: [0.30912104, 1.3216823]\n",
      "Gen Loss: -0.96025 Disc Loss: 1.26598 Q Losses: [0.27849364, 1.4535961]\n",
      "Gen Loss: 1.80526 Disc Loss: 0.443959 Q Losses: [0.2846126, 1.3793801]\n",
      "Gen Loss: 3.5446 Disc Loss: 0.33284 Q Losses: [0.27697384, 1.3272867]\n",
      "Gen Loss: 3.33861 Disc Loss: 0.362129 Q Losses: [0.26485968, 1.4513749]\n",
      "Gen Loss: 4.27272 Disc Loss: 0.555962 Q Losses: [0.28627235, 1.5071831]\n",
      "Gen Loss: 0.623484 Disc Loss: 0.734042 Q Losses: [0.31758663, 1.358433]\n",
      "Gen Loss: 0.646213 Disc Loss: 1.19897 Q Losses: [0.32190287, 1.3468077]\n",
      "Saved Model on  24000\n",
      "Gen Loss: 2.51029 Disc Loss: 0.409333 Q Losses: [0.24862543, 1.3388759]\n",
      "Gen Loss: -0.382293 Disc Loss: 0.959428 Q Losses: [0.31671679, 1.3876815]\n",
      "Gen Loss: 3.09754 Disc Loss: 0.631176 Q Losses: [0.25828242, 1.2530453]\n",
      "Gen Loss: 4.32403 Disc Loss: 0.510493 Q Losses: [0.32714981, 1.3907526]\n",
      "Gen Loss: -0.186081 Disc Loss: 0.651089 Q Losses: [0.29707611, 1.3354111]\n",
      "Gen Loss: 3.17281 Disc Loss: 0.478005 Q Losses: [0.27772096, 1.2233069]\n",
      "Gen Loss: 2.34124 Disc Loss: 0.552583 Q Losses: [0.2908687, 1.3702021]\n",
      "Gen Loss: 3.53112 Disc Loss: 0.283346 Q Losses: [0.29994676, 1.2956634]\n",
      "Gen Loss: 2.74489 Disc Loss: 0.439373 Q Losses: [0.23841783, 1.390062]\n",
      "Gen Loss: 3.00668 Disc Loss: 0.515312 Q Losses: [0.28121459, 1.3763421]\n",
      "Saved Model on  25000\n",
      "Gen Loss: 3.17085 Disc Loss: 0.383395 Q Losses: [0.28221238, 1.2996016]\n",
      "Gen Loss: 1.942 Disc Loss: 0.44083 Q Losses: [0.31575891, 1.2556612]\n",
      "Gen Loss: 0.634233 Disc Loss: 0.68853 Q Losses: [0.34696355, 1.2708429]\n",
      "Gen Loss: 2.02622 Disc Loss: 0.444331 Q Losses: [0.31797224, 1.3668725]\n",
      "Gen Loss: 2.38374 Disc Loss: 0.407411 Q Losses: [0.27820605, 1.2787275]\n",
      "Gen Loss: 1.2081 Disc Loss: 0.445396 Q Losses: [0.29311222, 1.2319384]\n",
      "Gen Loss: 3.5707 Disc Loss: 0.312586 Q Losses: [0.27131087, 1.2609138]\n",
      "Gen Loss: 3.08527 Disc Loss: 0.327782 Q Losses: [0.29389143, 1.3203783]\n",
      "Gen Loss: 2.02811 Disc Loss: 0.423595 Q Losses: [0.28851116, 1.0869185]\n",
      "Gen Loss: 4.05926 Disc Loss: 0.35798 Q Losses: [0.33852389, 1.4097198]\n",
      "Saved Model on  26000\n",
      "Gen Loss: 1.92264 Disc Loss: 0.433909 Q Losses: [0.29020596, 1.283622]\n",
      "Gen Loss: 2.18567 Disc Loss: 0.335706 Q Losses: [0.33414704, 1.2065326]\n",
      "Gen Loss: 4.81511 Disc Loss: 0.379181 Q Losses: [0.30051863, 1.0836567]\n",
      "Gen Loss: 0.748598 Disc Loss: 0.567184 Q Losses: [0.31870395, 1.3620071]\n",
      "Gen Loss: 4.5483 Disc Loss: 0.398909 Q Losses: [0.33962381, 1.3124993]\n",
      "Gen Loss: 3.08072 Disc Loss: 0.346712 Q Losses: [0.27165151, 1.2529035]\n",
      "Gen Loss: 2.88853 Disc Loss: 0.347299 Q Losses: [0.36911893, 1.2966678]\n",
      "Gen Loss: 2.56985 Disc Loss: 0.419379 Q Losses: [0.29823717, 1.266999]\n",
      "Gen Loss: 3.14056 Disc Loss: 0.21727 Q Losses: [0.31918126, 1.2807417]\n",
      "Gen Loss: 2.60233 Disc Loss: 0.383064 Q Losses: [0.32461184, 1.2368472]\n",
      "Saved Model on  27000\n",
      "Gen Loss: 3.96822 Disc Loss: 0.36883 Q Losses: [0.31078622, 1.0559435]\n",
      "Gen Loss: -1.93334 Disc Loss: 2.06032 Q Losses: [0.27399933, 1.1374952]\n",
      "Gen Loss: 2.70864 Disc Loss: 0.38294 Q Losses: [0.26453382, 1.2522352]\n",
      "Gen Loss: 2.35344 Disc Loss: 0.269195 Q Losses: [0.33249462, 1.2647868]\n",
      "Gen Loss: 2.84959 Disc Loss: 0.895697 Q Losses: [0.34013039, 1.2012136]\n",
      "Gen Loss: 2.43427 Disc Loss: 0.287415 Q Losses: [0.27178144, 1.2213194]\n",
      "Gen Loss: 3.058 Disc Loss: 0.384485 Q Losses: [0.30767167, 1.1667509]\n",
      "Gen Loss: 3.30464 Disc Loss: 0.491516 Q Losses: [0.22108513, 1.1614888]\n",
      "Gen Loss: 1.11577 Disc Loss: 0.578735 Q Losses: [0.27793509, 1.3482354]\n",
      "Gen Loss: 3.14416 Disc Loss: 0.359823 Q Losses: [0.29649979, 1.2908454]\n",
      "Saved Model on  28000\n",
      "Gen Loss: 2.19618 Disc Loss: 0.426717 Q Losses: [0.29541022, 1.1728022]\n",
      "Gen Loss: 4.25773 Disc Loss: 0.272871 Q Losses: [0.30564195, 1.2394435]\n",
      "Gen Loss: 2.84326 Disc Loss: 0.248351 Q Losses: [0.28345847, 1.2750845]\n",
      "Gen Loss: 3.90294 Disc Loss: 0.316677 Q Losses: [0.33252591, 1.3780668]\n",
      "Gen Loss: 1.9697 Disc Loss: 0.404255 Q Losses: [0.31250289, 1.2429446]\n",
      "Gen Loss: 3.32085 Disc Loss: 0.271101 Q Losses: [0.31315455, 1.1103501]\n",
      "Gen Loss: 2.02658 Disc Loss: 0.355273 Q Losses: [0.27823871, 1.2957797]\n",
      "Gen Loss: 3.52172 Disc Loss: 0.546119 Q Losses: [0.29661036, 1.2235343]\n",
      "Gen Loss: -1.75055 Disc Loss: 0.768341 Q Losses: [0.24601331, 1.1799643]\n",
      "Gen Loss: 3.54239 Disc Loss: 0.298455 Q Losses: [0.32499498, 1.0679158]\n",
      "Saved Model on  29000\n",
      "Gen Loss: 2.98384 Disc Loss: 0.418576 Q Losses: [0.27520189, 1.2585721]\n",
      "Gen Loss: 3.05975 Disc Loss: 0.39496 Q Losses: [0.33488566, 1.1983188]\n",
      "Gen Loss: 4.4052 Disc Loss: 0.497077 Q Losses: [0.28596029, 1.1683313]\n",
      "Gen Loss: 3.32009 Disc Loss: 0.444441 Q Losses: [0.26247272, 1.3381855]\n",
      "Gen Loss: 3.21923 Disc Loss: 0.286195 Q Losses: [0.24788845, 1.0886716]\n",
      "Gen Loss: 4.10646 Disc Loss: 0.508617 Q Losses: [0.30173305, 1.1011153]\n",
      "Gen Loss: 2.76962 Disc Loss: 0.363085 Q Losses: [0.25950044, 1.2963252]\n",
      "Gen Loss: -4.24862 Disc Loss: 1.37147 Q Losses: [0.2558462, 1.2404852]\n",
      "Gen Loss: 2.46953 Disc Loss: 0.41815 Q Losses: [0.30075333, 1.1380913]\n"
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "# blow up after 81800\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/Session#run\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "c_val = 10\n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 30000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        #zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        zs = np.random.uniform(-5.0,5.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        #print(\"zs shape:\",zs.shape)\n",
    "        #lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "        lcat = np.random.randint(0,c_val,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "        #print(\"lcat\", lcat)\n",
    "        #print(\"lcat shape: \", lcat.shape)\n",
    "        lcont = np.random.uniform(-1,1,[batch_size,number_continuous]) #\n",
    "        \n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        \n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "        _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "            #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "            latent_fixed = np.ones((c_val*c_val,1))\n",
    "            lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "            #lcat_sample = np.hstack([lcat_sample,latent_fixed])\n",
    "            \n",
    "            #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "            #b = np.reshape(a,[100,1])\n",
    "            b = np.reshape(a,[c_val*c_val,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            #\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "            save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model on \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-29000.cptk\n"
     ]
    }
   ],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "c_val = 10\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    z_sample = np.random.uniform(-5.0,5.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "    #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #lcat_sample = np.reshape(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]),[100,1])\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "    #print(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]))\n",
    "    #latent_fixed = np.ones((c_val*c_val,1))*50\n",
    "    latent_fixed = np.zeros((c_val*c_val,1))\n",
    "    #lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "    lcat_sample = np.hstack([lcat_sample,latent_fixed])\n",
    "            \n",
    "    #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #a = a = np.ones((c_val*c_val,1))*-0.5\n",
    "    #a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #b = np.reshape(a,[100,1])\n",
    "    b = np.reshape(a,[c_val*c_val,1])\n",
    "    #c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)\n",
    "    c = np.zeros_like(b)+0.5\n",
    "    #c = np.zeros_like(b)+8\n",
    "    #lcont_sample = np.hstack([b,c])\n",
    "    lcont_sample = np.hstack([c,b])\n",
    "    \n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test_4'+'.png')\n",
    "    save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig_test_12'+'.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35]",
   "language": "python",
   "name": "conda-env-dlpy35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
