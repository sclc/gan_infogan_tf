{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorials walks through an implementation of InfoGAN as described in [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n",
    "\n",
    "To learn more about InfoGAN, see this [Medium post](https://medium.com/p/dd710852db46) on them. To lean more about GANs generally, see [this one](https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.692jyamki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/depth_to_space\n",
    "# http://qiita.com/tadOne/items/48302a399dcad44c69c8   Tensorflow - padding = VALID/SAMEの違いについて\n",
    "#     so 3 tf.depth_to_space(genX,2) gives 4x2^3 = 32\n",
    "# \n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=128,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=64,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=32,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,32,[3,3],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,64,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,128,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/split\n",
    "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "# https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "# https://www.tensorflow.org/api_docs/python/tf/trainable_variables\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "# https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "# z_lat: one_hot_size + z_size + number_continuous = 10+64+2=76\n",
    "# g_loss def is interesting, my understanding: \n",
    "#        if Dg is the probablity to be told as feak data, then 1-Dg is the probabily of suceessfully cheating, \n",
    "#        so we cal KL(Dg/(1-Dg)), and readuce_mean works as sampling proceduce\n",
    "# \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 64 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "# categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "categorical_list = [10,10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 2 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "#print(\"latent_cat_in:\", latent_cat_in)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "#print(\"latent_cat_list: \",latent_cat_list)\n",
    "latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    #print(latent_cat_list[idx])\n",
    "    #print(latent_oh),  woundn't print anything in sess.run()\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "#print(\"1st z_lats: \", z_lats )\n",
    "z_lats.append(z_in)\n",
    "#print(\"2nd z_lats: \", z_lats )\n",
    "z_lats.append(latent_cont_in)\n",
    "#print(\"3rd z_lats: \", z_lats )\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "#print(\"z_lat: \", z_lat )\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log((Dg/(1-Dg)))) #KL Divergence optimizer\n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    #print (\"latent_var: \", latent_var)\n",
    "    #print (\"tf.log(QgCat[idx]): \",tf.log(QgCat[idx]))\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "#print (len(tvars))\n",
    "#for i in tvars:\n",
    "#    print(i)\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.002,beta1=0.5)\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 2.95689 Disc Loss: 1.35589 Q Losses: [0.38443068, 2.3201294]\n",
      "Gen Loss: 4.57285 Disc Loss: 0.401731 Q Losses: [0.28363293, 2.2566705]\n",
      "Gen Loss: 0.333817 Disc Loss: 1.66211 Q Losses: [0.30599505, 2.3136113]\n",
      "Gen Loss: 3.24278 Disc Loss: 0.441559 Q Losses: [0.27389646, 2.2756143]\n",
      "Gen Loss: 3.78939 Disc Loss: 0.284224 Q Losses: [0.2178901, 2.2373662]\n",
      "Gen Loss: 2.28797 Disc Loss: 0.612261 Q Losses: [0.19002303, 2.2524757]\n",
      "Gen Loss: 0.384185 Disc Loss: 0.85043 Q Losses: [0.17324576, 2.2355111]\n",
      "Gen Loss: 6.0405 Disc Loss: 3.06843 Q Losses: [0.13084136, 2.1701374]\n",
      "Gen Loss: 1.65668 Disc Loss: 1.25725 Q Losses: [0.16160688, 2.0884387]\n",
      "Gen Loss: 0.693242 Disc Loss: 0.835535 Q Losses: [0.13886118, 1.8785381]\n",
      "Gen Loss: 1.94354 Disc Loss: 0.815342 Q Losses: [0.12573269, 1.8651717]\n",
      "Saved Model on  1000\n",
      "Gen Loss: 1.81809 Disc Loss: 0.601826 Q Losses: [0.11081026, 1.6222768]\n",
      "Gen Loss: -0.886611 Disc Loss: 1.36294 Q Losses: [0.11593219, 1.6118336]\n",
      "Gen Loss: 3.21888 Disc Loss: 0.7391 Q Losses: [0.11300539, 1.54123]\n",
      "Gen Loss: -0.200948 Disc Loss: 1.25781 Q Losses: [0.098800726, 1.2196757]\n",
      "Gen Loss: 0.327631 Disc Loss: 0.996986 Q Losses: [0.092257321, 1.0873734]\n",
      "Gen Loss: 1.65931 Disc Loss: 1.00869 Q Losses: [0.08173506, 1.0043226]\n",
      "Gen Loss: 0.731237 Disc Loss: 0.835235 Q Losses: [0.080716029, 0.85714716]\n",
      "Gen Loss: 0.706605 Disc Loss: 0.931659 Q Losses: [0.096197143, 0.77914822]\n",
      "Gen Loss: -0.598905 Disc Loss: 1.58069 Q Losses: [0.061804816, 0.78693765]\n",
      "Gen Loss: 1.75786 Disc Loss: 0.941468 Q Losses: [0.091489144, 0.68918514]\n",
      "Saved Model on  2000\n",
      "Gen Loss: 1.72702 Disc Loss: 0.680692 Q Losses: [0.076855004, 0.67887187]\n",
      "Gen Loss: -1.236 Disc Loss: 1.31507 Q Losses: [0.06330812, 0.61967134]\n",
      "Gen Loss: 0.910349 Disc Loss: 0.815716 Q Losses: [0.074456885, 0.65947473]\n",
      "Gen Loss: -1.55679 Disc Loss: 1.35167 Q Losses: [0.090775952, 0.52166092]\n",
      "Gen Loss: 1.83741 Disc Loss: 0.823433 Q Losses: [0.071761057, 0.39046016]\n",
      "Gen Loss: 1.71689 Disc Loss: 0.901409 Q Losses: [0.071805999, 0.41180947]\n",
      "Gen Loss: 1.03111 Disc Loss: 1.05999 Q Losses: [0.083751328, 0.28823301]\n",
      "Gen Loss: 1.33301 Disc Loss: 0.770109 Q Losses: [0.086524993, 0.44241202]\n",
      "Gen Loss: 1.20623 Disc Loss: 1.04228 Q Losses: [0.076413468, 0.3415072]\n",
      "Gen Loss: 1.5099 Disc Loss: 0.889819 Q Losses: [0.060041003, 0.26947024]\n",
      "Saved Model on  3000\n",
      "Gen Loss: 0.926864 Disc Loss: 0.970352 Q Losses: [0.055535335, 0.32278103]\n",
      "Gen Loss: 0.520512 Disc Loss: 0.967694 Q Losses: [0.094623387, 0.28826839]\n",
      "Gen Loss: 0.246723 Disc Loss: 0.753901 Q Losses: [0.077762596, 0.40541548]\n",
      "Gen Loss: -0.348134 Disc Loss: 1.01631 Q Losses: [0.07488472, 0.25900397]\n",
      "Gen Loss: 0.749276 Disc Loss: 0.850596 Q Losses: [0.084287152, 0.22911915]\n",
      "Gen Loss: 0.860859 Disc Loss: 1.04765 Q Losses: [0.081950106, 0.3173812]\n",
      "Gen Loss: 0.703316 Disc Loss: 0.937313 Q Losses: [0.070561238, 0.21820064]\n",
      "Gen Loss: 2.44814 Disc Loss: 1.59994 Q Losses: [0.068165414, 0.3727544]\n",
      "Gen Loss: 2.11398 Disc Loss: 1.08977 Q Losses: [0.056650076, 0.29060507]\n",
      "Gen Loss: 1.91452 Disc Loss: 1.08586 Q Losses: [0.059974179, 0.17740564]\n",
      "Saved Model on  4000\n",
      "Gen Loss: 1.07179 Disc Loss: 0.860876 Q Losses: [0.073025502, 0.24987288]\n",
      "Gen Loss: -0.139406 Disc Loss: 0.908526 Q Losses: [0.067689314, 0.23366708]\n",
      "Gen Loss: 1.57523 Disc Loss: 0.989608 Q Losses: [0.069983065, 0.25016657]\n",
      "Gen Loss: 0.20754 Disc Loss: 1.09136 Q Losses: [0.072801948, 0.20832556]\n",
      "Gen Loss: 0.758781 Disc Loss: 0.899143 Q Losses: [0.067290023, 0.1583084]\n",
      "Gen Loss: 0.450186 Disc Loss: 1.30285 Q Losses: [0.077778295, 0.16947779]\n",
      "Gen Loss: 1.30085 Disc Loss: 0.780873 Q Losses: [0.061083186, 0.20337179]\n",
      "Gen Loss: -0.426925 Disc Loss: 1.08305 Q Losses: [0.069343999, 0.24413657]\n",
      "Gen Loss: 1.03734 Disc Loss: 0.781415 Q Losses: [0.083616294, 0.22794271]\n",
      "Gen Loss: 0.469806 Disc Loss: 0.858593 Q Losses: [0.077992797, 0.27209681]\n",
      "Saved Model on  5000\n",
      "Gen Loss: 1.08266 Disc Loss: 0.869481 Q Losses: [0.073802523, 0.13592623]\n",
      "Gen Loss: 0.654249 Disc Loss: 0.967561 Q Losses: [0.071287341, 0.1337966]\n",
      "Gen Loss: 1.12938 Disc Loss: 0.858304 Q Losses: [0.072724827, 0.22792371]\n",
      "Gen Loss: -0.0763806 Disc Loss: 1.20889 Q Losses: [0.07284642, 0.22670794]\n",
      "Gen Loss: 1.92692 Disc Loss: 1.00447 Q Losses: [0.066504851, 0.22097722]\n",
      "Gen Loss: -0.349771 Disc Loss: 1.32937 Q Losses: [0.070591949, 0.1628958]\n",
      "Gen Loss: 1.76852 Disc Loss: 0.908435 Q Losses: [0.0842219, 0.31139156]\n",
      "Gen Loss: 0.724419 Disc Loss: 0.916075 Q Losses: [0.074620798, 0.20552242]\n",
      "Gen Loss: 0.894333 Disc Loss: 0.870324 Q Losses: [0.084117517, 0.22388709]\n",
      "Gen Loss: 0.957008 Disc Loss: 0.755336 Q Losses: [0.076347962, 0.21890606]\n",
      "Saved Model on  6000\n",
      "Gen Loss: 1.07692 Disc Loss: 0.834585 Q Losses: [0.070317328, 0.070404954]\n",
      "Gen Loss: 1.10225 Disc Loss: 1.0144 Q Losses: [0.088943541, 0.19972028]\n",
      "Gen Loss: 1.81986 Disc Loss: 0.910373 Q Losses: [0.081747495, 0.11665814]\n",
      "Gen Loss: 0.713114 Disc Loss: 0.876432 Q Losses: [0.085209236, 0.13642262]\n",
      "Gen Loss: 0.700974 Disc Loss: 0.860544 Q Losses: [0.086652562, 0.20163777]\n",
      "Gen Loss: 2.07029 Disc Loss: 0.813942 Q Losses: [0.070062965, 0.10600297]\n",
      "Gen Loss: -0.0864755 Disc Loss: 0.96108 Q Losses: [0.072947353, 0.1246878]\n",
      "Gen Loss: 1.92907 Disc Loss: 0.75707 Q Losses: [0.06362015, 0.14034586]\n",
      "Gen Loss: 3.68572 Disc Loss: 1.2032 Q Losses: [0.078606352, 0.14475204]\n",
      "Gen Loss: 1.2737 Disc Loss: 0.846797 Q Losses: [0.079484344, 0.096968412]\n",
      "Saved Model on  7000\n",
      "Gen Loss: 1.85415 Disc Loss: 0.71984 Q Losses: [0.080441386, 0.19384211]\n",
      "Gen Loss: 0.314141 Disc Loss: 1.07676 Q Losses: [0.075812824, 0.19807087]\n",
      "Gen Loss: 0.924232 Disc Loss: 0.80129 Q Losses: [0.079784006, 0.17789137]\n",
      "Gen Loss: 1.67381 Disc Loss: 1.02253 Q Losses: [0.077135943, 0.12873276]\n",
      "Gen Loss: 2.24764 Disc Loss: 0.97966 Q Losses: [0.07395833, 0.17474408]\n",
      "Gen Loss: 1.65638 Disc Loss: 0.771878 Q Losses: [0.074000418, 0.10790375]\n",
      "Gen Loss: 0.39044 Disc Loss: 0.929374 Q Losses: [0.068061434, 0.14341705]\n",
      "Gen Loss: 2.33673 Disc Loss: 1.79918 Q Losses: [0.056367397, 0.22391441]\n",
      "Gen Loss: -0.28759 Disc Loss: 0.999957 Q Losses: [0.066280998, 0.12668447]\n",
      "Gen Loss: 1.20192 Disc Loss: 0.770093 Q Losses: [0.058699716, 0.15656546]\n",
      "Saved Model on  8000\n",
      "Gen Loss: 0.480711 Disc Loss: 1.03237 Q Losses: [0.0707675, 0.20118284]\n",
      "Gen Loss: 1.74428 Disc Loss: 0.886816 Q Losses: [0.073070779, 0.12488886]\n",
      "Gen Loss: 2.54382 Disc Loss: 0.821749 Q Losses: [0.081573874, 0.13601628]\n",
      "Gen Loss: 1.15168 Disc Loss: 0.772559 Q Losses: [0.059129648, 0.1032474]\n",
      "Gen Loss: 2.70578 Disc Loss: 0.840984 Q Losses: [0.059918068, 0.15499157]\n",
      "Gen Loss: 1.38871 Disc Loss: 0.869177 Q Losses: [0.064986452, 0.091273963]\n",
      "Gen Loss: 0.884087 Disc Loss: 0.655266 Q Losses: [0.065419257, 0.12539271]\n",
      "Gen Loss: 1.79719 Disc Loss: 1.06245 Q Losses: [0.060142495, 0.15176105]\n",
      "Gen Loss: 1.67654 Disc Loss: 0.673371 Q Losses: [0.075370476, 0.22957531]\n",
      "Gen Loss: 0.262368 Disc Loss: 0.934552 Q Losses: [0.052711509, 0.09339954]\n",
      "Saved Model on  9000\n",
      "Gen Loss: 1.42418 Disc Loss: 0.997727 Q Losses: [0.057704642, 0.12160433]\n",
      "Gen Loss: 2.17138 Disc Loss: 1.32571 Q Losses: [0.048851177, 0.080483213]\n",
      "Gen Loss: 2.25413 Disc Loss: 0.958226 Q Losses: [0.055919796, 0.17874552]\n",
      "Gen Loss: 0.705613 Disc Loss: 1.00235 Q Losses: [0.066343397, 0.085236706]\n",
      "Gen Loss: 1.81576 Disc Loss: 0.746275 Q Losses: [0.059842438, 0.091762125]\n",
      "Gen Loss: -0.0437703 Disc Loss: 1.59554 Q Losses: [0.061731428, 0.09716776]\n",
      "Gen Loss: 2.84388 Disc Loss: 0.943187 Q Losses: [0.036917932, 0.095735341]\n",
      "Gen Loss: 0.179242 Disc Loss: 1.05618 Q Losses: [0.069953218, 0.14413212]\n",
      "Gen Loss: 1.2877 Disc Loss: 0.884866 Q Losses: [0.055412538, 0.14970928]\n",
      "Gen Loss: -0.238297 Disc Loss: 1.00538 Q Losses: [0.063517988, 0.055682987]\n",
      "Saved Model on  10000\n",
      "Gen Loss: 0.307073 Disc Loss: 1.02142 Q Losses: [0.050997756, 0.064212568]\n",
      "Gen Loss: -1.02313 Disc Loss: 1.32248 Q Losses: [0.059235081, 0.097629592]\n",
      "Gen Loss: 1.29373 Disc Loss: 1.18615 Q Losses: [0.084152147, 0.15521665]\n",
      "Gen Loss: 1.90992 Disc Loss: 0.882651 Q Losses: [0.061162293, 0.16438019]\n",
      "Gen Loss: 1.20426 Disc Loss: 0.633776 Q Losses: [0.069562428, 0.11666682]\n",
      "Gen Loss: 2.85268 Disc Loss: 0.776231 Q Losses: [0.061702013, 0.098963037]\n",
      "Gen Loss: 0.167382 Disc Loss: 0.929193 Q Losses: [0.060372427, 0.16807687]\n",
      "Gen Loss: 1.69653 Disc Loss: 0.623725 Q Losses: [0.059761692, 0.16108112]\n",
      "Gen Loss: 3.28199 Disc Loss: 1.04874 Q Losses: [0.085686609, 0.16547714]\n",
      "Gen Loss: 1.61176 Disc Loss: 0.818779 Q Losses: [0.065126851, 0.08904767]\n",
      "Saved Model on  11000\n",
      "Gen Loss: 0.0916265 Disc Loss: 0.898884 Q Losses: [0.060852986, 0.048157725]\n",
      "Gen Loss: 1.57564 Disc Loss: 0.854989 Q Losses: [0.050826449, 0.096997157]\n",
      "Gen Loss: 1.01682 Disc Loss: 0.864745 Q Losses: [0.05265056, 0.072637558]\n",
      "Gen Loss: 3.01646 Disc Loss: 1.10828 Q Losses: [0.078385904, 0.20222205]\n",
      "Gen Loss: 2.11051 Disc Loss: 0.901485 Q Losses: [0.056562021, 0.071529396]\n",
      "Gen Loss: 0.569624 Disc Loss: 0.896669 Q Losses: [0.056783568, 0.1761788]\n",
      "Gen Loss: 2.01658 Disc Loss: 0.788846 Q Losses: [0.050073549, 0.091761515]\n",
      "Gen Loss: 0.910491 Disc Loss: 0.772959 Q Losses: [0.062143676, 0.20302035]\n",
      "Gen Loss: 0.329793 Disc Loss: 0.924459 Q Losses: [0.047922146, 0.087066568]\n",
      "Gen Loss: 1.29655 Disc Loss: 0.83642 Q Losses: [0.06409587, 0.10369597]\n",
      "Saved Model on  12000\n",
      "Gen Loss: 1.04861 Disc Loss: 0.674499 Q Losses: [0.064893365, 0.24900451]\n",
      "Gen Loss: -0.570251 Disc Loss: 1.15178 Q Losses: [0.068982914, 0.069543734]\n",
      "Gen Loss: 0.121535 Disc Loss: 0.974596 Q Losses: [0.059550039, 0.11802465]\n",
      "Gen Loss: 1.69347 Disc Loss: 0.788728 Q Losses: [0.061221328, 0.095161378]\n",
      "Gen Loss: 0.731297 Disc Loss: 0.78687 Q Losses: [0.061965264, 0.088760652]\n",
      "Gen Loss: 3.49049 Disc Loss: 0.960004 Q Losses: [0.073074378, 0.095914327]\n",
      "Gen Loss: 0.515054 Disc Loss: 0.668609 Q Losses: [0.051333316, 0.061214276]\n",
      "Gen Loss: 3.42319 Disc Loss: 1.30832 Q Losses: [0.057941988, 0.099924654]\n",
      "Gen Loss: 3.44063 Disc Loss: 1.10749 Q Losses: [0.049235329, 0.09767694]\n",
      "Gen Loss: 1.12029 Disc Loss: 0.854182 Q Losses: [0.052817199, 0.085229412]\n",
      "Saved Model on  13000\n",
      "Gen Loss: 1.40224 Disc Loss: 0.745449 Q Losses: [0.05539785, 0.23293017]\n",
      "Gen Loss: -0.994654 Disc Loss: 1.61414 Q Losses: [0.065751731, 0.10295295]\n",
      "Gen Loss: 0.527389 Disc Loss: 1.16621 Q Losses: [0.058238428, 0.10577358]\n",
      "Gen Loss: 2.28279 Disc Loss: 1.10022 Q Losses: [0.052363202, 0.090206817]\n",
      "Gen Loss: 1.56094 Disc Loss: 0.818377 Q Losses: [0.065386772, 0.11997738]\n",
      "Gen Loss: 1.87837 Disc Loss: 0.722995 Q Losses: [0.07037136, 0.053499594]\n",
      "Gen Loss: -0.423907 Disc Loss: 1.10688 Q Losses: [0.056725107, 0.11311782]\n",
      "Gen Loss: -0.116116 Disc Loss: 1.49339 Q Losses: [0.054307386, 0.14053845]\n",
      "Gen Loss: 3.65025 Disc Loss: 1.28052 Q Losses: [0.061188724, 0.10702608]\n",
      "Gen Loss: 0.0104965 Disc Loss: 1.19838 Q Losses: [0.07046859, 0.15004575]\n",
      "Saved Model on  14000\n",
      "Gen Loss: -1.86267 Disc Loss: 1.56898 Q Losses: [0.048361018, 0.10403804]\n",
      "Gen Loss: 1.05201 Disc Loss: 0.673595 Q Losses: [0.053188037, 0.14387938]\n",
      "Gen Loss: -0.080289 Disc Loss: 1.03135 Q Losses: [0.053252891, 0.05746787]\n",
      "Gen Loss: 1.4732 Disc Loss: 0.691638 Q Losses: [0.042276718, 0.102247]\n",
      "Gen Loss: 0.486848 Disc Loss: 1.14087 Q Losses: [0.057182133, 0.096966505]\n",
      "Gen Loss: 2.94595 Disc Loss: 0.603035 Q Losses: [0.048811518, 0.081966877]\n",
      "Gen Loss: 0.420807 Disc Loss: 0.703594 Q Losses: [0.047573917, 0.11552967]\n",
      "Gen Loss: 1.46109 Disc Loss: 0.606206 Q Losses: [0.077486701, 0.057050772]\n",
      "Gen Loss: 2.82574 Disc Loss: 0.675317 Q Losses: [0.055045307, 0.053147376]\n",
      "Gen Loss: 4.33542 Disc Loss: 0.75481 Q Losses: [0.068588778, 0.10459669]\n",
      "Saved Model on  15000\n",
      "Gen Loss: 0.445005 Disc Loss: 0.655649 Q Losses: [0.053499155, 0.12660848]\n",
      "Gen Loss: 1.50288 Disc Loss: 0.684753 Q Losses: [0.068351008, 0.15937072]\n",
      "Gen Loss: 1.65948 Disc Loss: 0.745701 Q Losses: [0.046724766, 0.12021162]\n",
      "Gen Loss: 3.18286 Disc Loss: 1.19532 Q Losses: [0.054148588, 0.11304671]\n",
      "Gen Loss: 1.0075 Disc Loss: 0.739877 Q Losses: [0.059925914, 0.081349418]\n",
      "Gen Loss: 2.29877 Disc Loss: 0.739886 Q Losses: [0.072400138, 0.056303393]\n",
      "Gen Loss: 2.33311 Disc Loss: 0.584894 Q Losses: [0.05228807, 0.095438555]\n",
      "Gen Loss: 1.39419 Disc Loss: 0.538814 Q Losses: [0.043048464, 0.050514951]\n",
      "Gen Loss: -0.343435 Disc Loss: 0.783401 Q Losses: [0.043780297, 0.086011156]\n",
      "Gen Loss: 3.3845 Disc Loss: 0.570803 Q Losses: [0.05262031, 0.069350541]\n",
      "Saved Model on  16000\n",
      "Gen Loss: 1.43737 Disc Loss: 0.605848 Q Losses: [0.053843386, 0.065398745]\n",
      "Gen Loss: 2.7136 Disc Loss: 0.582322 Q Losses: [0.054014139, 0.090126649]\n",
      "Gen Loss: 3.08892 Disc Loss: 1.77383 Q Losses: [0.062935561, 0.064645037]\n",
      "Gen Loss: 2.80066 Disc Loss: 0.819363 Q Losses: [0.043847397, 0.091272376]\n",
      "Gen Loss: 3.45 Disc Loss: 0.748944 Q Losses: [0.048541665, 0.077997983]\n",
      "Gen Loss: 1.93146 Disc Loss: 0.577645 Q Losses: [0.04991677, 0.052260365]\n",
      "Gen Loss: 3.78391 Disc Loss: 1.53998 Q Losses: [0.041365601, 0.078445069]\n",
      "Gen Loss: 1.91391 Disc Loss: 0.826243 Q Losses: [0.060084883, 0.1410242]\n",
      "Gen Loss: 1.85252 Disc Loss: 0.642722 Q Losses: [0.055832081, 0.083409548]\n",
      "Gen Loss: 2.11331 Disc Loss: 0.549056 Q Losses: [0.06262286, 0.13134252]\n",
      "Saved Model on  17000\n",
      "Gen Loss: 1.0546 Disc Loss: 0.715338 Q Losses: [0.050267138, 0.14057332]\n",
      "Gen Loss: 1.67916 Disc Loss: 0.596212 Q Losses: [0.052348755, 0.097640723]\n",
      "Gen Loss: 1.46901 Disc Loss: 0.70742 Q Losses: [0.043072168, 0.083130978]\n",
      "Gen Loss: 3.60931 Disc Loss: 0.814056 Q Losses: [0.059545249, 0.10965887]\n",
      "Gen Loss: 2.12268 Disc Loss: 0.611172 Q Losses: [0.051835865, 0.065241054]\n",
      "Gen Loss: -0.559073 Disc Loss: 1.0915 Q Losses: [0.051776007, 0.070391655]\n",
      "Gen Loss: 2.31439 Disc Loss: 0.845949 Q Losses: [0.05397968, 0.0922518]\n",
      "Gen Loss: 2.39006 Disc Loss: 0.920692 Q Losses: [0.053331494, 0.11783271]\n",
      "Gen Loss: -1.51819 Disc Loss: 1.6546 Q Losses: [0.052972563, 0.095811248]\n",
      "Gen Loss: 2.13755 Disc Loss: 0.601567 Q Losses: [0.038554385, 0.039921042]\n",
      "Saved Model on  18000\n",
      "Gen Loss: 2.33128 Disc Loss: 0.563848 Q Losses: [0.071158975, 0.10491855]\n",
      "Gen Loss: 0.684585 Disc Loss: 0.621848 Q Losses: [0.051080905, 0.058552574]\n",
      "Gen Loss: 3.18621 Disc Loss: 0.696471 Q Losses: [0.06908983, 0.13168769]\n",
      "Gen Loss: -1.26913 Disc Loss: 1.84997 Q Losses: [0.056429066, 0.046360046]\n",
      "Gen Loss: 2.94575 Disc Loss: 0.722831 Q Losses: [0.045902565, 0.069866993]\n",
      "Gen Loss: 2.26068 Disc Loss: 0.570636 Q Losses: [0.054635286, 0.10121363]\n",
      "Gen Loss: 1.72169 Disc Loss: 0.788785 Q Losses: [0.049809571, 0.090688601]\n",
      "Gen Loss: 2.21528 Disc Loss: 0.670117 Q Losses: [0.054347873, 0.069035195]\n",
      "Gen Loss: 0.0931043 Disc Loss: 0.736262 Q Losses: [0.059175875, 0.084314778]\n",
      "Gen Loss: 2.52178 Disc Loss: 0.596798 Q Losses: [0.049450237, 0.11399966]\n",
      "Saved Model on  19000\n",
      "Gen Loss: -1.2053 Disc Loss: 1.48002 Q Losses: [0.054016292, 0.12215545]\n",
      "Gen Loss: 1.46276 Disc Loss: 0.906838 Q Losses: [0.049731307, 0.14694437]\n",
      "Gen Loss: 1.88276 Disc Loss: 0.941946 Q Losses: [0.047402136, 0.081015892]\n",
      "Gen Loss: -0.394361 Disc Loss: 1.4446 Q Losses: [0.057183698, 0.031475119]\n",
      "Gen Loss: 3.02703 Disc Loss: 0.668853 Q Losses: [0.047974259, 0.12600434]\n",
      "Gen Loss: 0.0753719 Disc Loss: 0.754563 Q Losses: [0.055941563, 0.068048209]\n",
      "Gen Loss: -0.425613 Disc Loss: 1.64637 Q Losses: [0.055561066, 0.085710153]\n",
      "Gen Loss: 2.16525 Disc Loss: 0.911477 Q Losses: [0.055321131, 0.13405697]\n",
      "Gen Loss: -0.475053 Disc Loss: 1.23134 Q Losses: [0.050827127, 0.13137089]\n",
      "Gen Loss: 0.545518 Disc Loss: 0.750747 Q Losses: [0.049356937, 0.091923252]\n",
      "Saved Model on  20000\n",
      "Gen Loss: 3.40699 Disc Loss: 0.428743 Q Losses: [0.046523426, 0.10234068]\n",
      "Gen Loss: 2.89867 Disc Loss: 0.637548 Q Losses: [0.039110895, 0.094187789]\n",
      "Gen Loss: 2.41883 Disc Loss: 0.617455 Q Losses: [0.043394674, 0.073735222]\n",
      "Gen Loss: 2.77685 Disc Loss: 0.640185 Q Losses: [0.059601307, 0.069194861]\n",
      "Gen Loss: 4.53775 Disc Loss: 1.40904 Q Losses: [0.050093211, 0.023702143]\n",
      "Gen Loss: 1.26905 Disc Loss: 0.71691 Q Losses: [0.048734374, 0.030115034]\n",
      "Gen Loss: 0.796055 Disc Loss: 0.865612 Q Losses: [0.044711918, 0.11951283]\n",
      "Gen Loss: 4.43115 Disc Loss: 1.56915 Q Losses: [0.038163964, 0.033594713]\n",
      "Gen Loss: 2.17484 Disc Loss: 0.58794 Q Losses: [0.048380483, 0.023332825]\n",
      "Gen Loss: 1.11423 Disc Loss: 0.642631 Q Losses: [0.037043914, 0.09954375]\n",
      "Saved Model on  21000\n",
      "Gen Loss: 1.80133 Disc Loss: 0.97036 Q Losses: [0.050722286, 0.088951215]\n",
      "Gen Loss: 1.65564 Disc Loss: 0.466779 Q Losses: [0.038876928, 0.067880549]\n",
      "Gen Loss: 4.53215 Disc Loss: 1.69741 Q Losses: [0.050643157, 0.089254454]\n",
      "Gen Loss: 1.80922 Disc Loss: 0.465346 Q Losses: [0.059755035, 0.055827536]\n",
      "Gen Loss: 2.06984 Disc Loss: 0.631152 Q Losses: [0.042098813, 0.071885549]\n",
      "Gen Loss: 2.85579 Disc Loss: 0.661708 Q Losses: [0.047314979, 0.094257779]\n",
      "Gen Loss: 2.14376 Disc Loss: 0.682567 Q Losses: [0.046508998, 0.039739564]\n",
      "Gen Loss: -0.952244 Disc Loss: 1.05389 Q Losses: [0.048584335, 0.074177213]\n",
      "Gen Loss: 1.10775 Disc Loss: 0.735262 Q Losses: [0.049196962, 0.12351334]\n",
      "Gen Loss: 1.96957 Disc Loss: 0.473228 Q Losses: [0.051037036, 0.053590387]\n",
      "Saved Model on  22000\n",
      "Gen Loss: 0.717495 Disc Loss: 1.10775 Q Losses: [0.045632184, 0.030720713]\n",
      "Gen Loss: 2.58221 Disc Loss: 0.434768 Q Losses: [0.05982808, 0.104957]\n",
      "Gen Loss: 1.71593 Disc Loss: 0.62267 Q Losses: [0.048849788, 0.13875039]\n",
      "Gen Loss: 2.21175 Disc Loss: 0.498259 Q Losses: [0.057293963, 0.18030381]\n",
      "Gen Loss: 1.07301 Disc Loss: 0.614148 Q Losses: [0.035872683, 0.044877019]\n",
      "Gen Loss: 2.16817 Disc Loss: 0.489457 Q Losses: [0.048067577, 0.10069626]\n",
      "Gen Loss: 2.60858 Disc Loss: 0.60935 Q Losses: [0.059435938, 0.15248354]\n",
      "Gen Loss: 3.81665 Disc Loss: 0.896576 Q Losses: [0.062063619, 0.047065858]\n",
      "Gen Loss: 2.57574 Disc Loss: 0.514154 Q Losses: [0.059836231, 0.073469706]\n",
      "Gen Loss: 4.00815 Disc Loss: 0.501514 Q Losses: [0.061825119, 0.1397208]\n",
      "Saved Model on  23000\n",
      "Gen Loss: 0.98888 Disc Loss: 0.76527 Q Losses: [0.08735121, 0.08250954]\n",
      "Gen Loss: -0.124674 Disc Loss: 0.67934 Q Losses: [0.043848, 0.041690245]\n",
      "Gen Loss: 2.31113 Disc Loss: 0.434654 Q Losses: [0.054457199, 0.092383504]\n",
      "Gen Loss: 0.912578 Disc Loss: 0.692127 Q Losses: [0.045566574, 0.023981003]\n",
      "Gen Loss: 5.02397 Disc Loss: 1.18176 Q Losses: [0.037789423, 0.028327482]\n",
      "Gen Loss: 2.84092 Disc Loss: 0.46651 Q Losses: [0.053138334, 0.046444368]\n",
      "Gen Loss: 2.85283 Disc Loss: 0.687324 Q Losses: [0.064569831, 0.036188424]\n",
      "Gen Loss: 3.64923 Disc Loss: 0.605176 Q Losses: [0.044194959, 0.10847821]\n",
      "Gen Loss: 2.43664 Disc Loss: 0.593167 Q Losses: [0.045695264, 0.06955687]\n",
      "Gen Loss: 0.782064 Disc Loss: 0.583972 Q Losses: [0.039341047, 0.075445421]\n",
      "Saved Model on  24000\n",
      "Gen Loss: 0.942406 Disc Loss: 0.781585 Q Losses: [0.042644486, 0.081845365]\n",
      "Gen Loss: 3.80665 Disc Loss: 0.731568 Q Losses: [0.039710402, 0.044020269]\n",
      "Gen Loss: 4.01603 Disc Loss: 0.479553 Q Losses: [0.03310737, 0.04716751]\n",
      "Gen Loss: 2.79855 Disc Loss: 0.614915 Q Losses: [0.052588314, 0.016419619]\n",
      "Gen Loss: 1.77147 Disc Loss: 0.609561 Q Losses: [0.047860559, 0.10845126]\n",
      "Gen Loss: 3.86618 Disc Loss: 0.551081 Q Losses: [0.04786336, 0.036401693]\n",
      "Gen Loss: 2.88668 Disc Loss: 0.690686 Q Losses: [0.046713062, 0.048118912]\n",
      "Gen Loss: 0.756592 Disc Loss: 0.825625 Q Losses: [0.048958533, 0.031274065]\n",
      "Gen Loss: 2.12862 Disc Loss: 0.619796 Q Losses: [0.041618414, 0.042126302]\n",
      "Gen Loss: 4.94466 Disc Loss: 0.943577 Q Losses: [0.051399566, 0.061254725]\n",
      "Saved Model on  25000\n",
      "Gen Loss: 1.17912 Disc Loss: 0.610676 Q Losses: [0.045391094, 0.047546461]\n",
      "Gen Loss: 4.37736 Disc Loss: 0.441397 Q Losses: [0.04054432, 0.036332168]\n",
      "Gen Loss: 2.44046 Disc Loss: 0.34283 Q Losses: [0.047144301, 0.067653231]\n",
      "Gen Loss: 3.3675 Disc Loss: 0.692209 Q Losses: [0.038783208, 0.073163606]\n",
      "Gen Loss: 1.89764 Disc Loss: 0.702473 Q Losses: [0.064722419, 0.076465473]\n",
      "Gen Loss: 2.83303 Disc Loss: 0.443261 Q Losses: [0.053414248, 0.059961978]\n",
      "Gen Loss: 2.65698 Disc Loss: 0.566746 Q Losses: [0.049967971, 0.031426426]\n",
      "Gen Loss: 2.85243 Disc Loss: 0.461283 Q Losses: [0.048624299, 0.11295597]\n",
      "Gen Loss: 1.9676 Disc Loss: 0.473388 Q Losses: [0.03782561, 0.082238182]\n",
      "Gen Loss: 1.25029 Disc Loss: 0.881125 Q Losses: [0.050504342, 0.067836881]\n",
      "Saved Model on  26000\n",
      "Gen Loss: 2.52279 Disc Loss: 0.858466 Q Losses: [0.030775474, 0.02293805]\n",
      "Gen Loss: 2.1669 Disc Loss: 0.435157 Q Losses: [0.047388781, 0.024215659]\n",
      "Gen Loss: 4.07008 Disc Loss: 0.837895 Q Losses: [0.032475077, 0.089968339]\n",
      "Gen Loss: 2.99767 Disc Loss: 0.510121 Q Losses: [0.040217206, 0.1138657]\n",
      "Gen Loss: 1.38654 Disc Loss: 0.600737 Q Losses: [0.04786662, 0.081696212]\n",
      "Gen Loss: 2.36743 Disc Loss: 0.547394 Q Losses: [0.045507491, 0.026062181]\n",
      "Gen Loss: 1.90622 Disc Loss: 0.544178 Q Losses: [0.037183359, 0.023491617]\n",
      "Gen Loss: 6.54938 Disc Loss: 1.1266 Q Losses: [0.040558554, 0.030013062]\n",
      "Gen Loss: 0.101695 Disc Loss: 0.840043 Q Losses: [0.051790953, 0.061656069]\n",
      "Gen Loss: 2.12133 Disc Loss: 0.663595 Q Losses: [0.044288658, 0.037341349]\n",
      "Saved Model on  27000\n",
      "Gen Loss: 2.41524 Disc Loss: 0.531081 Q Losses: [0.056708749, 0.078299887]\n",
      "Gen Loss: 2.41748 Disc Loss: 0.499396 Q Losses: [0.042045906, 0.044012532]\n",
      "Gen Loss: 1.72118 Disc Loss: 1.02745 Q Losses: [0.042166729, 0.071020648]\n",
      "Gen Loss: 2.03877 Disc Loss: 0.455883 Q Losses: [0.043822706, 0.050340772]\n",
      "Gen Loss: 1.34532 Disc Loss: 0.56317 Q Losses: [0.07001695, 0.034176074]\n",
      "Gen Loss: 0.804155 Disc Loss: 0.728472 Q Losses: [0.040437087, 0.071605831]\n",
      "Gen Loss: 2.51545 Disc Loss: 0.90606 Q Losses: [0.049931381, 0.063021071]\n",
      "Gen Loss: 1.6239 Disc Loss: 0.476784 Q Losses: [0.043684743, 0.037091099]\n",
      "Gen Loss: 1.46031 Disc Loss: 0.76542 Q Losses: [0.034076866, 0.12026102]\n",
      "Gen Loss: 0.0904156 Disc Loss: 0.629122 Q Losses: [0.040443808, 0.013901627]\n",
      "Saved Model on  28000\n",
      "Gen Loss: 3.35811 Disc Loss: 0.552161 Q Losses: [0.03807709, 0.071255386]\n",
      "Gen Loss: 5.21985 Disc Loss: 0.784953 Q Losses: [0.059035838, 0.07590685]\n",
      "Gen Loss: 1.73921 Disc Loss: 0.482813 Q Losses: [0.044432923, 0.073197097]\n",
      "Gen Loss: 5.2813 Disc Loss: 0.894491 Q Losses: [0.06059061, 0.091454558]\n",
      "Gen Loss: 3.13437 Disc Loss: 0.403676 Q Losses: [0.050857693, 0.074367404]\n",
      "Gen Loss: 3.6374 Disc Loss: 0.753757 Q Losses: [0.056592729, 0.05270895]\n",
      "Gen Loss: 1.35873 Disc Loss: 0.638537 Q Losses: [0.047489833, 0.080293126]\n",
      "Gen Loss: 4.10808 Disc Loss: 0.803141 Q Losses: [0.036005177, 0.030295989]\n",
      "Gen Loss: -0.0321485 Disc Loss: 1.12738 Q Losses: [0.043908823, 0.089718401]\n",
      "Gen Loss: 3.3939 Disc Loss: 0.482068 Q Losses: [0.04952981, 0.080166012]\n",
      "Saved Model on  29000\n",
      "Gen Loss: -0.706263 Disc Loss: 1.78937 Q Losses: [0.048207235, 0.020696953]\n",
      "Gen Loss: 3.41992 Disc Loss: 1.05787 Q Losses: [0.04602886, 0.041757155]\n",
      "Gen Loss: 1.65025 Disc Loss: 1.01791 Q Losses: [0.040523905, 0.06124096]\n",
      "Gen Loss: 4.64919 Disc Loss: 0.868727 Q Losses: [0.04379563, 0.041176613]\n",
      "Gen Loss: 2.61969 Disc Loss: 0.413187 Q Losses: [0.052187361, 0.0760976]\n",
      "Gen Loss: 1.99586 Disc Loss: 0.508031 Q Losses: [0.058340758, 0.11958909]\n",
      "Gen Loss: 2.77478 Disc Loss: 0.667165 Q Losses: [0.052797373, 0.10446179]\n",
      "Gen Loss: 2.50335 Disc Loss: 0.566218 Q Losses: [0.047879342, 0.049244162]\n",
      "Gen Loss: 4.87722 Disc Loss: 1.79379 Q Losses: [0.054026313, 0.06608554]\n",
      "Gen Loss: 3.43766 Disc Loss: 1.18098 Q Losses: [0.060142256, 0.045411028]\n",
      "Saved Model on  30000\n",
      "Gen Loss: 2.26804 Disc Loss: 0.588301 Q Losses: [0.0390772, 0.067335472]\n",
      "Gen Loss: 2.67703 Disc Loss: 0.486513 Q Losses: [0.041102916, 0.074399836]\n",
      "Gen Loss: 3.23898 Disc Loss: 0.580554 Q Losses: [0.045250848, 0.037805017]\n",
      "Gen Loss: 1.29271 Disc Loss: 0.481641 Q Losses: [0.033599988, 0.023596713]\n",
      "Gen Loss: 6.36389 Disc Loss: 1.49042 Q Losses: [0.046444654, 0.080433153]\n",
      "Gen Loss: 3.38143 Disc Loss: 0.495418 Q Losses: [0.052588165, 0.042199738]\n",
      "Gen Loss: 3.10197 Disc Loss: 0.544621 Q Losses: [0.042284194, 0.12965553]\n",
      "Gen Loss: 3.52014 Disc Loss: 0.630538 Q Losses: [0.055438958, 0.070413858]\n",
      "Gen Loss: 1.78425 Disc Loss: 0.508876 Q Losses: [0.045280639, 0.068330407]\n",
      "Gen Loss: 1.71272 Disc Loss: 0.416023 Q Losses: [0.062668353, 0.0495717]\n",
      "Saved Model on  31000\n",
      "Gen Loss: 3.14598 Disc Loss: 0.904897 Q Losses: [0.047663264, 0.069314264]\n",
      "Gen Loss: 0.33753 Disc Loss: 0.763048 Q Losses: [0.036446419, 0.014442425]\n",
      "Gen Loss: 2.58049 Disc Loss: 0.55101 Q Losses: [0.041870289, 0.044837289]\n",
      "Gen Loss: 4.15328 Disc Loss: 1.02864 Q Losses: [0.050434094, 0.055048414]\n",
      "Gen Loss: 0.858554 Disc Loss: 0.44121 Q Losses: [0.035484456, 0.066959821]\n",
      "Gen Loss: 3.93545 Disc Loss: 0.712733 Q Losses: [0.071135715, 0.018093441]\n",
      "Gen Loss: 0.79636 Disc Loss: 0.689626 Q Losses: [0.04303164, 0.034117229]\n",
      "Gen Loss: 1.94839 Disc Loss: 0.461085 Q Losses: [0.039565742, 0.033051141]\n",
      "Gen Loss: 4.19426 Disc Loss: 0.801178 Q Losses: [0.035273232, 0.086462155]\n",
      "Gen Loss: 0.498083 Disc Loss: 0.766417 Q Losses: [0.062581204, 0.044146065]\n",
      "Saved Model on  32000\n",
      "Gen Loss: 2.81044 Disc Loss: 0.637902 Q Losses: [0.061580241, 0.10108709]\n",
      "Gen Loss: 2.77256 Disc Loss: 0.344708 Q Losses: [0.045034431, 0.021440782]\n",
      "Gen Loss: 0.608861 Disc Loss: 1.6985 Q Losses: [0.05459464, 0.09219563]\n",
      "Gen Loss: 1.23304 Disc Loss: 0.390691 Q Losses: [0.036989935, 0.053936977]\n",
      "Gen Loss: 2.28266 Disc Loss: 0.538461 Q Losses: [0.051922478, 0.045796163]\n",
      "Gen Loss: 3.47147 Disc Loss: 0.508013 Q Losses: [0.056850489, 0.101741]\n",
      "Gen Loss: 3.7503 Disc Loss: 0.53295 Q Losses: [0.048238087, 0.028405786]\n",
      "Gen Loss: -0.442274 Disc Loss: 1.16201 Q Losses: [0.056340951, 0.052172996]\n",
      "Gen Loss: 3.65728 Disc Loss: 0.509333 Q Losses: [0.057954203, 0.10086623]\n",
      "Gen Loss: 1.21117 Disc Loss: 0.673747 Q Losses: [0.0580635, 0.072753631]\n",
      "Saved Model on  33000\n",
      "Gen Loss: 4.58449 Disc Loss: 0.538495 Q Losses: [0.039938912, 0.082060352]\n",
      "Gen Loss: 1.48069 Disc Loss: 0.513705 Q Losses: [0.043855537, 0.032315455]\n",
      "Gen Loss: 3.73997 Disc Loss: 0.75774 Q Losses: [0.049822379, 0.10956562]\n",
      "Gen Loss: 2.45653 Disc Loss: 0.511546 Q Losses: [0.039344594, 0.028131604]\n",
      "Gen Loss: 0.982647 Disc Loss: 0.701421 Q Losses: [0.034843542, 0.033400781]\n",
      "Gen Loss: 5.47749 Disc Loss: 1.48694 Q Losses: [0.03839229, 0.075240113]\n",
      "Gen Loss: 0.920896 Disc Loss: 0.733856 Q Losses: [0.045632523, 0.046536066]\n",
      "Gen Loss: 3.1117 Disc Loss: 0.429865 Q Losses: [0.029546019, 0.11262222]\n",
      "Gen Loss: 2.59692 Disc Loss: 0.492562 Q Losses: [0.050184932, 0.084428042]\n",
      "Gen Loss: 4.23108 Disc Loss: 0.584763 Q Losses: [0.04507371, 0.044255815]\n",
      "Saved Model on  34000\n",
      "Gen Loss: 3.03973 Disc Loss: 0.485146 Q Losses: [0.056320958, 0.10710405]\n",
      "Gen Loss: 2.50073 Disc Loss: 0.620755 Q Losses: [0.032713454, 0.045293048]\n",
      "Gen Loss: 4.48998 Disc Loss: 0.700359 Q Losses: [0.048248433, 0.049368829]\n",
      "Gen Loss: 3.98127 Disc Loss: 0.523896 Q Losses: [0.068849623, 0.10215366]\n",
      "Gen Loss: 1.32184 Disc Loss: 0.553706 Q Losses: [0.048524875, 0.063963205]\n",
      "Gen Loss: 2.88492 Disc Loss: 0.459592 Q Losses: [0.041262507, 0.022658462]\n",
      "Gen Loss: 1.44572 Disc Loss: 0.698429 Q Losses: [0.035331439, 0.10738844]\n",
      "Gen Loss: 2.13107 Disc Loss: 0.537812 Q Losses: [0.038276099, 0.082944699]\n",
      "Gen Loss: 4.35718 Disc Loss: 0.459721 Q Losses: [0.045652509, 0.085404553]\n",
      "Gen Loss: 4.98768 Disc Loss: 0.563026 Q Losses: [0.039781578, 0.048821099]\n",
      "Saved Model on  35000\n",
      "Gen Loss: 2.97042 Disc Loss: 0.667932 Q Losses: [0.041842975, 0.067445695]\n",
      "Gen Loss: 3.57427 Disc Loss: 0.430726 Q Losses: [0.059942126, 0.038793266]\n",
      "Gen Loss: 2.26868 Disc Loss: 0.585025 Q Losses: [0.048451133, 0.017243965]\n",
      "Gen Loss: 1.71188 Disc Loss: 0.517804 Q Losses: [0.047826957, 0.10904793]\n",
      "Gen Loss: 2.90109 Disc Loss: 0.552225 Q Losses: [0.057183925, 0.070453681]\n",
      "Gen Loss: -0.166341 Disc Loss: 0.683264 Q Losses: [0.03833811, 0.031155122]\n",
      "Gen Loss: 5.21298 Disc Loss: 1.5133 Q Losses: [0.042019699, 0.057706758]\n",
      "Gen Loss: 3.13138 Disc Loss: 0.545611 Q Losses: [0.051252138, 0.050488293]\n",
      "Gen Loss: 3.06189 Disc Loss: 0.468225 Q Losses: [0.040218882, 0.049688421]\n",
      "Gen Loss: 3.05738 Disc Loss: 0.40001 Q Losses: [0.04524909, 0.016945666]\n",
      "Saved Model on  36000\n",
      "Gen Loss: 1.84396 Disc Loss: 0.331683 Q Losses: [0.04764878, 0.031407814]\n",
      "Gen Loss: 2.56463 Disc Loss: 0.455901 Q Losses: [0.049903452, 0.052528903]\n",
      "Gen Loss: 2.35011 Disc Loss: 0.52217 Q Losses: [0.045818768, 0.11845608]\n",
      "Gen Loss: 3.30243 Disc Loss: 0.278385 Q Losses: [0.045660812, 0.049006693]\n",
      "Gen Loss: 2.61862 Disc Loss: 0.461315 Q Losses: [0.043309361, 0.059923545]\n",
      "Gen Loss: 3.863 Disc Loss: 0.410574 Q Losses: [0.043723959, 0.086004965]\n",
      "Gen Loss: 2.79498 Disc Loss: 0.483034 Q Losses: [0.054309323, 0.095879093]\n",
      "Gen Loss: 2.50602 Disc Loss: 0.448943 Q Losses: [0.049130909, 0.036932182]\n",
      "Gen Loss: 2.51208 Disc Loss: 0.531828 Q Losses: [0.050213188, 0.10738473]\n",
      "Gen Loss: 3.33721 Disc Loss: 0.707154 Q Losses: [0.052451424, 0.041320998]\n",
      "Saved Model on  37000\n",
      "Gen Loss: 3.06808 Disc Loss: 0.455684 Q Losses: [0.042598668, 0.017768811]\n",
      "Gen Loss: 0.0347497 Disc Loss: 0.756355 Q Losses: [0.047896884, 0.040534463]\n",
      "Gen Loss: 5.80206 Disc Loss: 0.759594 Q Losses: [0.035617363, 0.070263244]\n",
      "Gen Loss: 4.75613 Disc Loss: 0.875162 Q Losses: [0.053177949, 0.063087568]\n",
      "Gen Loss: 2.27584 Disc Loss: 0.528345 Q Losses: [0.053079739, 0.028131749]\n",
      "Gen Loss: 0.523622 Disc Loss: 0.630794 Q Losses: [0.045603573, 0.065729007]\n",
      "Gen Loss: 3.75477 Disc Loss: 0.522348 Q Losses: [0.032627136, 0.10142785]\n",
      "Gen Loss: 0.833716 Disc Loss: 1.05055 Q Losses: [0.041998938, 0.10841329]\n",
      "Gen Loss: 3.68814 Disc Loss: 0.491076 Q Losses: [0.037402183, 0.010382871]\n",
      "Gen Loss: 1.87996 Disc Loss: 0.500277 Q Losses: [0.048147559, 0.048658971]\n",
      "Saved Model on  38000\n",
      "Gen Loss: 2.45491 Disc Loss: 0.422559 Q Losses: [0.053401206, 0.067582086]\n",
      "Gen Loss: 1.43762 Disc Loss: 0.594949 Q Losses: [0.043241687, 0.09069214]\n",
      "Gen Loss: 3.27737 Disc Loss: 0.334862 Q Losses: [0.036457539, 0.066202186]\n",
      "Gen Loss: 2.39089 Disc Loss: 0.466619 Q Losses: [0.04041636, 0.050703995]\n",
      "Gen Loss: 3.46157 Disc Loss: 0.464712 Q Losses: [0.046917401, 0.033411145]\n",
      "Gen Loss: 4.06905 Disc Loss: 0.460157 Q Losses: [0.038497217, 0.018980946]\n",
      "Gen Loss: 2.18373 Disc Loss: 0.603321 Q Losses: [0.061679389, 0.060773589]\n",
      "Gen Loss: 2.27334 Disc Loss: 0.900808 Q Losses: [0.037627146, 0.03550737]\n",
      "Gen Loss: 2.78873 Disc Loss: 0.496991 Q Losses: [0.044738524, 0.049839072]\n",
      "Gen Loss: 5.44291 Disc Loss: 0.80879 Q Losses: [0.042893939, 0.14006907]\n",
      "Saved Model on  39000\n",
      "Gen Loss: 3.47348 Disc Loss: 0.367852 Q Losses: [0.042912729, 0.052676164]\n",
      "Gen Loss: 1.49824 Disc Loss: 0.462012 Q Losses: [0.051427685, 0.018749323]\n",
      "Gen Loss: 4.48265 Disc Loss: 0.660836 Q Losses: [0.047346544, 0.11386359]\n",
      "Gen Loss: 2.67867 Disc Loss: 0.432946 Q Losses: [0.066802554, 0.06121055]\n",
      "Gen Loss: 3.76894 Disc Loss: 0.776298 Q Losses: [0.038781811, 0.041469537]\n",
      "Gen Loss: 3.23311 Disc Loss: 0.5853 Q Losses: [0.048807539, 0.070918322]\n",
      "Gen Loss: 1.97292 Disc Loss: 0.358465 Q Losses: [0.040010169, 0.018676152]\n",
      "Gen Loss: 1.06349 Disc Loss: 0.515184 Q Losses: [0.04755713, 0.053608671]\n",
      "Gen Loss: 0.827854 Disc Loss: 0.732378 Q Losses: [0.035465565, 0.068023652]\n",
      "Gen Loss: -0.524779 Disc Loss: 0.968737 Q Losses: [0.040282, 0.035521794]\n",
      "Saved Model on  40000\n",
      "Gen Loss: 2.27011 Disc Loss: 0.499698 Q Losses: [0.040427029, 0.046799593]\n",
      "Gen Loss: 5.14313 Disc Loss: 0.683183 Q Losses: [0.03113302, 0.058155201]\n",
      "Gen Loss: 3.88718 Disc Loss: 0.421432 Q Losses: [0.042247355, 0.046413001]\n",
      "Gen Loss: 3.794 Disc Loss: 0.275204 Q Losses: [0.040541343, 0.026241256]\n",
      "Gen Loss: 1.49703 Disc Loss: 0.451292 Q Losses: [0.042992435, 0.068481095]\n",
      "Gen Loss: 2.5918 Disc Loss: 0.336928 Q Losses: [0.04037831, 0.060661368]\n",
      "Gen Loss: 3.07766 Disc Loss: 0.317517 Q Losses: [0.041989215, 0.012585687]\n",
      "Gen Loss: 3.22333 Disc Loss: 0.44474 Q Losses: [0.05113478, 0.042593539]\n",
      "Gen Loss: 2.79942 Disc Loss: 0.485159 Q Losses: [0.044780634, 0.042795639]\n",
      "Gen Loss: 3.24646 Disc Loss: 0.511184 Q Losses: [0.054221332, 0.062332116]\n",
      "Saved Model on  41000\n",
      "Gen Loss: 3.91174 Disc Loss: 0.491632 Q Losses: [0.044053458, 0.074960507]\n",
      "Gen Loss: 1.69125 Disc Loss: 0.562684 Q Losses: [0.047763221, 0.045096032]\n",
      "Gen Loss: 3.3455 Disc Loss: 0.26661 Q Losses: [0.050067395, 0.078852162]\n",
      "Gen Loss: 4.2684 Disc Loss: 0.8007 Q Losses: [0.045418222, 0.012264375]\n",
      "Gen Loss: 1.79378 Disc Loss: 0.610565 Q Losses: [0.03817261, 0.052560411]\n",
      "Gen Loss: 0.666488 Disc Loss: 1.48254 Q Losses: [0.047233395, 0.038919713]\n",
      "Gen Loss: 3.10323 Disc Loss: 0.468573 Q Losses: [0.041231684, 0.022726554]\n",
      "Gen Loss: 1.89925 Disc Loss: 0.591842 Q Losses: [0.041758183, 0.049591575]\n",
      "Gen Loss: 2.89368 Disc Loss: 0.869712 Q Losses: [0.04702428, 0.042529482]\n",
      "Gen Loss: 2.37131 Disc Loss: 0.611784 Q Losses: [0.034296788, 0.040466804]\n",
      "Saved Model on  42000\n",
      "Gen Loss: 3.17747 Disc Loss: 0.603121 Q Losses: [0.050134227, 0.033542246]\n",
      "Gen Loss: 3.42071 Disc Loss: 0.490653 Q Losses: [0.051505819, 0.029015755]\n",
      "Gen Loss: 2.55698 Disc Loss: 0.499677 Q Losses: [0.053098917, 0.12288371]\n",
      "Gen Loss: 3.19937 Disc Loss: 0.441177 Q Losses: [0.038919564, 0.1039168]\n",
      "Gen Loss: 3.30644 Disc Loss: 0.379266 Q Losses: [0.042168427, 0.053571679]\n",
      "Gen Loss: 2.68436 Disc Loss: 0.727311 Q Losses: [0.036345325, 0.047935329]\n",
      "Gen Loss: 3.01481 Disc Loss: 0.488494 Q Losses: [0.05556963, 0.064724192]\n",
      "Gen Loss: 2.50981 Disc Loss: 0.423754 Q Losses: [0.037766084, 0.029170807]\n",
      "Gen Loss: 1.85715 Disc Loss: 0.444345 Q Losses: [0.039951056, 0.034764305]\n",
      "Gen Loss: 3.51965 Disc Loss: 0.418418 Q Losses: [0.065968961, 0.022313323]\n",
      "Saved Model on  43000\n",
      "Gen Loss: 2.3296 Disc Loss: 0.421655 Q Losses: [0.047207128, 0.10323995]\n",
      "Gen Loss: 2.01668 Disc Loss: 0.418028 Q Losses: [0.048928179, 0.025843486]\n",
      "Gen Loss: 2.5874 Disc Loss: 0.566064 Q Losses: [0.067859158, 0.039975766]\n",
      "Gen Loss: 2.23533 Disc Loss: 0.755043 Q Losses: [0.050909735, 0.048283365]\n",
      "Gen Loss: 1.58626 Disc Loss: 0.722701 Q Losses: [0.04203479, 0.010638738]\n",
      "Gen Loss: 3.17483 Disc Loss: 0.381234 Q Losses: [0.055352572, 0.042197518]\n",
      "Gen Loss: 3.44313 Disc Loss: 0.345886 Q Losses: [0.044124007, 0.042026617]\n",
      "Gen Loss: 3.0232 Disc Loss: 0.451878 Q Losses: [0.048848197, 0.036209825]\n",
      "Gen Loss: 0.770378 Disc Loss: 0.84279 Q Losses: [0.052806333, 0.062819526]\n",
      "Gen Loss: 5.03446 Disc Loss: 0.536699 Q Losses: [0.036387004, 0.018547142]\n",
      "Saved Model on  44000\n",
      "Gen Loss: 1.75389 Disc Loss: 0.483436 Q Losses: [0.041304052, 0.038566321]\n",
      "Gen Loss: 2.06255 Disc Loss: 0.665325 Q Losses: [0.043597575, 0.060409945]\n",
      "Gen Loss: 3.8374 Disc Loss: 0.460121 Q Losses: [0.053642634, 0.077787429]\n",
      "Gen Loss: 3.80954 Disc Loss: 0.389104 Q Losses: [0.056065246, 0.016218746]\n",
      "Gen Loss: 1.68705 Disc Loss: 0.773193 Q Losses: [0.033951208, 0.09071862]\n",
      "Gen Loss: 2.70011 Disc Loss: 0.558158 Q Losses: [0.040040568, 0.023551784]\n",
      "Gen Loss: 4.10996 Disc Loss: 0.401497 Q Losses: [0.047101893, 0.025774589]\n",
      "Gen Loss: 2.32395 Disc Loss: 0.400122 Q Losses: [0.04219858, 0.04076796]\n",
      "Gen Loss: -0.0828582 Disc Loss: 0.834677 Q Losses: [0.044341903, 0.027582314]\n",
      "Gen Loss: 3.13757 Disc Loss: 0.304215 Q Losses: [0.048083901, 0.069155306]\n",
      "Saved Model on  45000\n",
      "Gen Loss: 1.6816 Disc Loss: 0.902753 Q Losses: [0.055691727, 0.083544955]\n",
      "Gen Loss: 3.78047 Disc Loss: 0.396263 Q Losses: [0.050913718, 0.073267221]\n",
      "Gen Loss: 2.25624 Disc Loss: 0.623268 Q Losses: [0.044311307, 0.075896986]\n",
      "Gen Loss: 3.83272 Disc Loss: 0.214283 Q Losses: [0.04884135, 0.040776346]\n",
      "Gen Loss: 2.18934 Disc Loss: 0.382675 Q Losses: [0.034018867, 0.035647679]\n",
      "Gen Loss: 3.81225 Disc Loss: 0.393835 Q Losses: [0.044662751, 0.136718]\n",
      "Gen Loss: 2.62108 Disc Loss: 0.597355 Q Losses: [0.063942671, 0.019353803]\n",
      "Gen Loss: 2.20589 Disc Loss: 0.61018 Q Losses: [0.048520282, 0.051457968]\n",
      "Gen Loss: 2.48664 Disc Loss: 0.487496 Q Losses: [0.039984666, 0.044159599]\n",
      "Gen Loss: 4.67469 Disc Loss: 0.329592 Q Losses: [0.05500745, 0.05246871]\n",
      "Saved Model on  46000\n",
      "Gen Loss: 3.23512 Disc Loss: 0.278568 Q Losses: [0.059938662, 0.037353918]\n",
      "Gen Loss: 4.03446 Disc Loss: 0.369257 Q Losses: [0.032868393, 0.060644336]\n",
      "Gen Loss: 4.8721 Disc Loss: 0.486863 Q Losses: [0.056515176, 0.025754629]\n",
      "Gen Loss: 4.2616 Disc Loss: 0.371875 Q Losses: [0.048384897, 0.035687022]\n",
      "Gen Loss: 3.45666 Disc Loss: 0.618313 Q Losses: [0.091310181, 0.058057182]\n",
      "Gen Loss: 5.50081 Disc Loss: 0.964141 Q Losses: [0.040805981, 0.10008767]\n",
      "Gen Loss: 3.50952 Disc Loss: 0.33093 Q Losses: [0.035775289, 0.041864194]\n",
      "Gen Loss: 3.76568 Disc Loss: 0.400814 Q Losses: [0.055859014, 0.069363199]\n",
      "Gen Loss: 3.64658 Disc Loss: 0.458514 Q Losses: [0.049699567, 0.028326174]\n",
      "Gen Loss: 3.55781 Disc Loss: 0.385279 Q Losses: [0.041533507, 0.058238536]\n",
      "Saved Model on  47000\n",
      "Gen Loss: 5.02986 Disc Loss: 0.275302 Q Losses: [0.043007519, 0.08442641]\n",
      "Gen Loss: 1.89357 Disc Loss: 0.501875 Q Losses: [0.039166212, 0.069119245]\n",
      "Gen Loss: 3.36269 Disc Loss: 0.309694 Q Losses: [0.05602916, 0.065397993]\n",
      "Gen Loss: 5.50852 Disc Loss: 0.753613 Q Losses: [0.0489146, 0.071351372]\n",
      "Gen Loss: 2.49317 Disc Loss: 0.348098 Q Losses: [0.041140676, 0.029624252]\n",
      "Gen Loss: 3.88604 Disc Loss: 0.407235 Q Losses: [0.032398123, 0.052677423]\n",
      "Gen Loss: 2.24852 Disc Loss: 0.447354 Q Losses: [0.041850403, 0.044489644]\n",
      "Gen Loss: 3.1017 Disc Loss: 0.398446 Q Losses: [0.045295779, 0.03300862]\n",
      "Gen Loss: 4.77092 Disc Loss: 0.636452 Q Losses: [0.036262121, 0.11278372]\n",
      "Gen Loss: 3.36911 Disc Loss: 0.381931 Q Losses: [0.04610423, 0.040089522]\n",
      "Saved Model on  48000\n",
      "Gen Loss: 4.52646 Disc Loss: 0.581648 Q Losses: [0.042026505, 0.038318772]\n",
      "Gen Loss: 2.66958 Disc Loss: 0.426485 Q Losses: [0.046754293, 0.087965056]\n",
      "Gen Loss: 6.10776 Disc Loss: 0.682953 Q Losses: [0.033627324, 0.031955898]\n",
      "Gen Loss: 1.84049 Disc Loss: 0.73907 Q Losses: [0.03854382, 0.017028816]\n",
      "Gen Loss: 2.91186 Disc Loss: 0.43457 Q Losses: [0.045424104, 0.056414425]\n",
      "Gen Loss: 4.84175 Disc Loss: 0.409734 Q Losses: [0.04179861, 0.082414463]\n",
      "Gen Loss: 3.71476 Disc Loss: 0.5245 Q Losses: [0.047550458, 0.058031626]\n",
      "Gen Loss: 1.35448 Disc Loss: 0.827099 Q Losses: [0.052695096, 0.046305217]\n",
      "Gen Loss: 3.57982 Disc Loss: 0.440887 Q Losses: [0.05930965, 0.12559921]\n",
      "Gen Loss: 3.3183 Disc Loss: 0.621354 Q Losses: [0.039385259, 0.036287799]\n",
      "Saved Model on  49000\n",
      "Gen Loss: 3.81678 Disc Loss: 0.244317 Q Losses: [0.050233264, 0.049300492]\n",
      "Gen Loss: 2.67855 Disc Loss: 0.292023 Q Losses: [0.054575711, 0.052490488]\n",
      "Gen Loss: 3.50916 Disc Loss: 0.405707 Q Losses: [0.046242427, 0.06301216]\n",
      "Gen Loss: 3.28093 Disc Loss: 0.504538 Q Losses: [0.05039607, 0.075308464]\n",
      "Gen Loss: 3.78367 Disc Loss: 0.382221 Q Losses: [0.041836284, 0.099851198]\n",
      "Gen Loss: 2.11039 Disc Loss: 0.4804 Q Losses: [0.055399608, 0.078442775]\n",
      "Gen Loss: 3.72606 Disc Loss: 0.427878 Q Losses: [0.042261437, 0.026889445]\n",
      "Gen Loss: 3.40344 Disc Loss: 0.393111 Q Losses: [0.066686466, 0.052755967]\n",
      "Gen Loss: 6.09386 Disc Loss: 0.571599 Q Losses: [0.032505132, 0.041431751]\n",
      "Gen Loss: 1.99173 Disc Loss: 0.385572 Q Losses: [0.044074658, 0.011944894]\n",
      "Saved Model on  50000\n",
      "Gen Loss: 2.9392 Disc Loss: 0.419713 Q Losses: [0.046934053, 0.051804323]\n",
      "Gen Loss: 3.06975 Disc Loss: 0.335107 Q Losses: [0.031122729, 0.079068288]\n",
      "Gen Loss: 4.12729 Disc Loss: 0.332123 Q Losses: [0.02990716, 0.097134523]\n",
      "Gen Loss: 3.68057 Disc Loss: 0.334036 Q Losses: [0.039565809, 0.081081189]\n",
      "Gen Loss: 5.15737 Disc Loss: 0.380583 Q Losses: [0.031794969, 0.073713623]\n",
      "Gen Loss: 3.13962 Disc Loss: 0.507266 Q Losses: [0.062043764, 0.071004279]\n",
      "Gen Loss: 3.22175 Disc Loss: 0.306564 Q Losses: [0.044539139, 0.02573096]\n",
      "Gen Loss: 1.11576 Disc Loss: 0.647585 Q Losses: [0.046851184, 0.074292377]\n",
      "Gen Loss: 3.91825 Disc Loss: 0.294772 Q Losses: [0.04792811, 0.037734594]\n",
      "Gen Loss: 2.94432 Disc Loss: 0.410976 Q Losses: [0.044241164, 0.050769586]\n",
      "Saved Model on  51000\n",
      "Gen Loss: 4.52848 Disc Loss: 0.454938 Q Losses: [0.050595857, 0.029161882]\n",
      "Gen Loss: 2.2497 Disc Loss: 0.275241 Q Losses: [0.045338254, 0.041469935]\n",
      "Gen Loss: 3.66688 Disc Loss: 0.322707 Q Losses: [0.048908316, 0.12978667]\n",
      "Gen Loss: 4.45276 Disc Loss: 0.287125 Q Losses: [0.060446948, 0.09247791]\n",
      "Gen Loss: 2.64405 Disc Loss: 0.448754 Q Losses: [0.045052566, 0.077568106]\n",
      "Gen Loss: 2.32978 Disc Loss: 0.244577 Q Losses: [0.044828765, 0.085931882]\n",
      "Gen Loss: 3.12737 Disc Loss: 0.261367 Q Losses: [0.04516843, 0.018587615]\n",
      "Gen Loss: 4.27284 Disc Loss: 0.211766 Q Losses: [0.041314323, 0.16342756]\n",
      "Gen Loss: 4.22346 Disc Loss: 0.269436 Q Losses: [0.053136762, 0.072220616]\n",
      "Gen Loss: 3.70578 Disc Loss: 0.376143 Q Losses: [0.041623943, 0.081148356]\n",
      "Saved Model on  52000\n",
      "Gen Loss: 3.73293 Disc Loss: 0.441726 Q Losses: [0.040190607, 0.026431125]\n",
      "Gen Loss: 4.7852 Disc Loss: 0.635245 Q Losses: [0.045837395, 0.073220953]\n",
      "Gen Loss: 3.53324 Disc Loss: 0.313186 Q Losses: [0.04720673, 0.036572024]\n",
      "Gen Loss: 4.35411 Disc Loss: 0.219573 Q Losses: [0.044724591, 0.073121756]\n",
      "Gen Loss: 1.61114 Disc Loss: 1.30938 Q Losses: [0.045004383, 0.032235675]\n",
      "Gen Loss: 5.78371 Disc Loss: 0.60841 Q Losses: [0.038486261, 0.12476101]\n",
      "Gen Loss: 0.807604 Disc Loss: 0.844981 Q Losses: [0.061431177, 0.028073274]\n",
      "Gen Loss: 2.99956 Disc Loss: 0.587322 Q Losses: [0.027539993, 0.060918275]\n",
      "Gen Loss: 3.33564 Disc Loss: 0.662158 Q Losses: [0.034672834, 0.031866807]\n",
      "Gen Loss: 2.49361 Disc Loss: 0.597189 Q Losses: [0.031562056, 0.037229333]\n",
      "Saved Model on  53000\n",
      "Gen Loss: 2.25129 Disc Loss: 0.756315 Q Losses: [0.045333087, 0.019522551]\n",
      "Gen Loss: 2.979 Disc Loss: 0.410929 Q Losses: [0.052452207, 0.0526301]\n",
      "Gen Loss: 0.519208 Disc Loss: 0.662369 Q Losses: [0.066339947, 0.014880905]\n",
      "Gen Loss: 4.51156 Disc Loss: 0.433121 Q Losses: [0.035478335, 0.013626553]\n",
      "Gen Loss: 2.14889 Disc Loss: 0.330313 Q Losses: [0.048588339, 0.084152684]\n",
      "Gen Loss: 4.64331 Disc Loss: 0.230912 Q Losses: [0.051203586, 0.039038084]\n",
      "Gen Loss: 4.79999 Disc Loss: 0.428796 Q Losses: [0.045781936, 0.068444736]\n",
      "Gen Loss: 3.72854 Disc Loss: 0.282954 Q Losses: [0.067130581, 0.0084801726]\n",
      "Gen Loss: 4.29394 Disc Loss: 0.478034 Q Losses: [0.049054712, 0.057870477]\n",
      "Gen Loss: 4.06771 Disc Loss: 0.565322 Q Losses: [0.040899083, 0.055777621]\n",
      "Saved Model on  54000\n",
      "Gen Loss: 3.13976 Disc Loss: 0.422487 Q Losses: [0.042624995, 0.029586261]\n",
      "Gen Loss: 3.55943 Disc Loss: 0.352834 Q Losses: [0.047478557, 0.068810895]\n",
      "Gen Loss: 3.23819 Disc Loss: 0.269937 Q Losses: [0.039894573, 0.064858392]\n",
      "Gen Loss: 6.04835 Disc Loss: 0.519438 Q Losses: [0.030125711, 0.023538124]\n",
      "Gen Loss: 4.0978 Disc Loss: 0.225031 Q Losses: [0.038185373, 0.042237692]\n",
      "Gen Loss: 4.03441 Disc Loss: 0.29631 Q Losses: [0.038126491, 0.011837201]\n",
      "Gen Loss: 3.78082 Disc Loss: 0.330234 Q Losses: [0.041700803, 0.049589891]\n",
      "Gen Loss: 4.7195 Disc Loss: 0.445344 Q Losses: [0.038684547, 0.06842313]\n",
      "Gen Loss: 2.91295 Disc Loss: 0.323715 Q Losses: [0.028677788, 0.023045883]\n",
      "Gen Loss: 3.68474 Disc Loss: 0.373571 Q Losses: [0.038953763, 0.019933807]\n",
      "Saved Model on  55000\n",
      "Gen Loss: 3.22983 Disc Loss: 0.4393 Q Losses: [0.043337323, 0.062771805]\n",
      "Gen Loss: 2.65397 Disc Loss: 0.464592 Q Losses: [0.04727016, 0.0074371048]\n",
      "Gen Loss: 2.74685 Disc Loss: 0.331207 Q Losses: [0.052682672, 0.078309909]\n",
      "Gen Loss: 3.76998 Disc Loss: 0.316784 Q Losses: [0.05676122, 0.018602576]\n",
      "Gen Loss: 4.60078 Disc Loss: 0.241452 Q Losses: [0.055740759, 0.036284499]\n",
      "Gen Loss: 1.78597 Disc Loss: 0.846587 Q Losses: [0.057443932, 0.059216637]\n",
      "Gen Loss: 4.14473 Disc Loss: 0.334323 Q Losses: [0.04214656, 0.077484615]\n",
      "Gen Loss: 2.49481 Disc Loss: 0.502761 Q Losses: [0.039059184, 0.044783264]\n",
      "Gen Loss: 3.17379 Disc Loss: 0.332255 Q Losses: [0.044883162, 0.031230222]\n",
      "Gen Loss: 0.4871 Disc Loss: 0.797696 Q Losses: [0.042002723, 0.077925116]\n",
      "Saved Model on  56000\n",
      "Gen Loss: 5.1806 Disc Loss: 0.600806 Q Losses: [0.046426043, 0.02000008]\n",
      "Gen Loss: 4.5792 Disc Loss: 0.659529 Q Losses: [0.037861928, 0.016890446]\n",
      "Gen Loss: 1.43503 Disc Loss: 0.949621 Q Losses: [0.050913163, 0.045778245]\n",
      "Gen Loss: 3.61619 Disc Loss: 0.416485 Q Losses: [0.04343253, 0.053826284]\n",
      "Gen Loss: 3.67839 Disc Loss: 0.363824 Q Losses: [0.051234715, 0.10736637]\n",
      "Gen Loss: 2.54081 Disc Loss: 0.752495 Q Losses: [0.048343234, 0.060747579]\n",
      "Gen Loss: 5.38145 Disc Loss: 0.928016 Q Losses: [0.039439056, 0.047526881]\n",
      "Gen Loss: 5.63934 Disc Loss: 0.899809 Q Losses: [0.033996325, 0.047089733]\n",
      "Gen Loss: 8.10454 Disc Loss: 1.46545 Q Losses: [0.037625849, 0.020903455]\n",
      "Gen Loss: 1.70131 Disc Loss: 0.668356 Q Losses: [0.067947432, 0.060784373]\n",
      "Saved Model on  57000\n",
      "Gen Loss: 0.973964 Disc Loss: 1.48929 Q Losses: [0.042927623, 0.035560049]\n",
      "Gen Loss: 4.3049 Disc Loss: 0.297244 Q Losses: [0.051578425, 0.035188273]\n",
      "Gen Loss: 4.99771 Disc Loss: 0.350606 Q Losses: [0.039885871, 0.044815421]\n",
      "Gen Loss: 3.22669 Disc Loss: 0.398754 Q Losses: [0.041885369, 0.046348192]\n",
      "Gen Loss: 4.33295 Disc Loss: 0.346744 Q Losses: [0.031131372, 0.02719545]\n",
      "Gen Loss: 4.00049 Disc Loss: 0.343776 Q Losses: [0.059026483, 0.03083327]\n",
      "Gen Loss: 4.84019 Disc Loss: 0.354742 Q Losses: [0.030190047, 0.027267408]\n",
      "Gen Loss: 2.57375 Disc Loss: 0.316023 Q Losses: [0.046801135, 0.051559292]\n",
      "Gen Loss: -0.669486 Disc Loss: 1.0145 Q Losses: [0.036375102, 0.050730981]\n",
      "Gen Loss: 4.12556 Disc Loss: 0.19451 Q Losses: [0.050602689, 0.18436469]\n",
      "Saved Model on  58000\n",
      "Gen Loss: 0.777892 Disc Loss: 0.725404 Q Losses: [0.048918407, 0.044269651]\n",
      "Gen Loss: 3.66677 Disc Loss: 0.450771 Q Losses: [0.039030038, 0.13611943]\n",
      "Gen Loss: 4.02665 Disc Loss: 0.25136 Q Losses: [0.040785722, 0.086111099]\n",
      "Gen Loss: 4.6576 Disc Loss: 0.402748 Q Losses: [0.032881964, 0.058432102]\n",
      "Gen Loss: 4.47794 Disc Loss: 0.329996 Q Losses: [0.035966635, 0.018135397]\n",
      "Gen Loss: -0.20636 Disc Loss: 1.08875 Q Losses: [0.02515148, 0.032507934]\n",
      "Gen Loss: 3.45822 Disc Loss: 0.380665 Q Losses: [0.046343375, 0.060520697]\n",
      "Gen Loss: 4.17621 Disc Loss: 0.254667 Q Losses: [0.047060438, 0.010056837]\n",
      "Gen Loss: 3.59343 Disc Loss: 0.221518 Q Losses: [0.037864782, 0.0085435221]\n",
      "Gen Loss: 4.55586 Disc Loss: 0.381208 Q Losses: [0.045102313, 0.036585808]\n",
      "Saved Model on  59000\n",
      "Gen Loss: 5.13536 Disc Loss: 0.593191 Q Losses: [0.038516954, 0.029812507]\n",
      "Gen Loss: 3.33993 Disc Loss: 0.416487 Q Losses: [0.040506598, 0.011511276]\n",
      "Gen Loss: 4.11005 Disc Loss: 0.274052 Q Losses: [0.057638183, 0.030614629]\n",
      "Gen Loss: 5.05878 Disc Loss: 0.681353 Q Losses: [0.044961639, 0.064249501]\n",
      "Gen Loss: 4.99862 Disc Loss: 0.18193 Q Losses: [0.040376976, 0.063403904]\n",
      "Gen Loss: 2.82861 Disc Loss: 0.486338 Q Losses: [0.03765437, 0.11770798]\n",
      "Gen Loss: 5.06707 Disc Loss: 0.474955 Q Losses: [0.041140594, 0.027713174]\n",
      "Gen Loss: 3.11303 Disc Loss: 0.393414 Q Losses: [0.043136984, 0.032862853]\n",
      "Gen Loss: 3.26343 Disc Loss: 0.633718 Q Losses: [0.039974637, 0.048595242]\n",
      "Gen Loss: 3.74876 Disc Loss: 0.209297 Q Losses: [0.051421173, 0.095729649]\n",
      "Saved Model on  60000\n",
      "Gen Loss: 2.93154 Disc Loss: 0.820766 Q Losses: [0.027633639, 0.012508366]\n",
      "Gen Loss: 3.42164 Disc Loss: 0.341569 Q Losses: [0.042692341, 0.054109935]\n",
      "Gen Loss: 3.13333 Disc Loss: 0.506619 Q Losses: [0.031156531, 0.040431984]\n",
      "Gen Loss: 4.51131 Disc Loss: 0.498618 Q Losses: [0.040271942, 0.016148556]\n",
      "Gen Loss: 3.06131 Disc Loss: 0.52038 Q Losses: [0.03341458, 0.021074956]\n",
      "Gen Loss: 2.76983 Disc Loss: 0.174449 Q Losses: [0.062545247, 0.0065786252]\n",
      "Gen Loss: 5.77842 Disc Loss: 0.537823 Q Losses: [0.030786762, 0.061407961]\n",
      "Gen Loss: 1.06231 Disc Loss: 0.536166 Q Losses: [0.04621888, 0.006961178]\n",
      "Gen Loss: 4.82961 Disc Loss: 0.344237 Q Losses: [0.036690757, 0.12529196]\n",
      "Gen Loss: 3.58746 Disc Loss: 0.200253 Q Losses: [0.052472118, 0.015130571]\n",
      "Saved Model on  61000\n",
      "Gen Loss: 3.60018 Disc Loss: 0.210574 Q Losses: [0.037884049, 0.097272411]\n",
      "Gen Loss: 0.967675 Disc Loss: 0.969097 Q Losses: [0.034080826, 0.054387651]\n",
      "Gen Loss: 6.53064 Disc Loss: 0.617288 Q Losses: [0.049467169, 0.028254833]\n",
      "Gen Loss: 4.13494 Disc Loss: 0.276061 Q Losses: [0.03987829, 0.014065156]\n",
      "Gen Loss: 4.22018 Disc Loss: 0.227948 Q Losses: [0.061155096, 0.028901309]\n",
      "Gen Loss: 5.16073 Disc Loss: 0.235441 Q Losses: [0.040875852, 0.022745673]\n",
      "Gen Loss: 2.51369 Disc Loss: 0.632653 Q Losses: [0.040131859, 0.047024347]\n",
      "Gen Loss: 3.52802 Disc Loss: 0.320924 Q Losses: [0.04546427, 0.07332547]\n",
      "Gen Loss: 6.16304 Disc Loss: 0.5168 Q Losses: [0.034835316, 0.033058114]\n",
      "Gen Loss: 4.18685 Disc Loss: 0.179869 Q Losses: [0.039529338, 0.095593482]\n",
      "Saved Model on  62000\n",
      "Gen Loss: 3.74358 Disc Loss: 0.23895 Q Losses: [0.048725799, 0.00418561]\n",
      "Gen Loss: 5.80548 Disc Loss: 0.385988 Q Losses: [0.040956475, 0.041535009]\n",
      "Gen Loss: 3.93774 Disc Loss: 0.143159 Q Losses: [0.044922099, 0.039967529]\n",
      "Gen Loss: -0.13642 Disc Loss: 1.04465 Q Losses: [0.064023137, 0.066758491]\n",
      "Gen Loss: 2.79963 Disc Loss: 0.369432 Q Losses: [0.046624139, 0.024579199]\n",
      "Gen Loss: -0.749797 Disc Loss: 1.42522 Q Losses: [0.030961053, 0.083772421]\n",
      "Gen Loss: 4.60958 Disc Loss: 0.478036 Q Losses: [0.038358092, 0.015344117]\n",
      "Gen Loss: 2.38549 Disc Loss: 1.88469 Q Losses: [0.047904413, 0.015600047]\n",
      "Gen Loss: 2.76836 Disc Loss: 0.613075 Q Losses: [0.049427472, 0.060499284]\n",
      "Gen Loss: 3.83869 Disc Loss: 0.429111 Q Losses: [0.051091835, 0.022978347]\n",
      "Saved Model on  63000\n",
      "Gen Loss: 2.40087 Disc Loss: 0.367349 Q Losses: [0.040368851, 0.017209008]\n",
      "Gen Loss: 5.24286 Disc Loss: 0.644426 Q Losses: [0.046592988, 0.056264974]\n",
      "Gen Loss: 5.90614 Disc Loss: 0.272998 Q Losses: [0.042259313, 0.077012196]\n",
      "Gen Loss: 3.82142 Disc Loss: 0.256755 Q Losses: [0.032664455, 0.041324358]\n",
      "Gen Loss: 3.95692 Disc Loss: 0.496585 Q Losses: [0.032102488, 0.053284332]\n",
      "Gen Loss: 6.49788 Disc Loss: 0.125567 Q Losses: [0.077422336, 0.017934207]\n",
      "Gen Loss: 2.97277 Disc Loss: 0.380718 Q Losses: [0.034880478, 0.065864041]\n",
      "Gen Loss: 4.5168 Disc Loss: 0.244509 Q Losses: [0.033420492, 0.020073632]\n",
      "Gen Loss: 3.43478 Disc Loss: 0.441614 Q Losses: [0.048331257, 0.037082694]\n",
      "Gen Loss: 3.53896 Disc Loss: 0.190004 Q Losses: [0.046182156, 0.020639826]\n",
      "Saved Model on  64000\n",
      "Gen Loss: 3.57409 Disc Loss: 0.354571 Q Losses: [0.035843149, 0.050179578]\n",
      "Gen Loss: 3.23031 Disc Loss: 0.318423 Q Losses: [0.04820849, 0.03462743]\n",
      "Gen Loss: 5.74587 Disc Loss: 0.51942 Q Losses: [0.033745978, 0.066028811]\n",
      "Gen Loss: 5.97821 Disc Loss: 0.487175 Q Losses: [0.061216764, 0.020446679]\n",
      "Gen Loss: 2.24373 Disc Loss: 0.350733 Q Losses: [0.036780324, 0.033991136]\n",
      "Gen Loss: 2.67199 Disc Loss: 0.264081 Q Losses: [0.068606883, 0.021332124]\n",
      "Gen Loss: 4.20131 Disc Loss: 0.501003 Q Losses: [0.056353137, 0.060177278]\n",
      "Gen Loss: 2.89498 Disc Loss: 0.377915 Q Losses: [0.046355598, 0.068423882]\n",
      "Gen Loss: 3.63375 Disc Loss: 0.294535 Q Losses: [0.034618989, 0.040125098]\n",
      "Gen Loss: 5.3512 Disc Loss: 0.277275 Q Losses: [0.044539668, 0.04759983]\n",
      "Saved Model on  65000\n",
      "Gen Loss: 5.28909 Disc Loss: 0.600411 Q Losses: [0.056652229, 0.007306308]\n",
      "Gen Loss: 4.82638 Disc Loss: 0.388459 Q Losses: [0.059529066, 0.07415995]\n",
      "Gen Loss: 4.49983 Disc Loss: 0.303145 Q Losses: [0.039152179, 0.035923149]\n",
      "Gen Loss: -0.649413 Disc Loss: 0.643399 Q Losses: [0.047137894, 0.042650633]\n",
      "Gen Loss: 4.52821 Disc Loss: 0.28222 Q Losses: [0.035791289, 0.060692616]\n",
      "Gen Loss: 6.71106 Disc Loss: 0.464147 Q Losses: [0.049222343, 0.053917278]\n",
      "Gen Loss: 2.68338 Disc Loss: 0.564867 Q Losses: [0.038493983, 0.066806935]\n",
      "Gen Loss: 3.83478 Disc Loss: 0.540361 Q Losses: [0.040658593, 0.030028041]\n",
      "Gen Loss: 4.90987 Disc Loss: 0.170428 Q Losses: [0.05965383, 0.013906582]\n",
      "Gen Loss: 4.84901 Disc Loss: 0.222601 Q Losses: [0.032845698, 0.019393351]\n",
      "Saved Model on  66000\n",
      "Gen Loss: 4.55827 Disc Loss: 0.321721 Q Losses: [0.031701237, 0.027706746]\n",
      "Gen Loss: 4.52528 Disc Loss: 0.377715 Q Losses: [0.035945747, 0.039031342]\n",
      "Gen Loss: 5.20454 Disc Loss: 0.37154 Q Losses: [0.029776229, 0.028368711]\n",
      "Gen Loss: 4.40679 Disc Loss: 0.266391 Q Losses: [0.03197968, 0.084437355]\n",
      "Gen Loss: 3.63965 Disc Loss: 0.317034 Q Losses: [0.038614303, 0.014083214]\n",
      "Gen Loss: 2.45298 Disc Loss: 0.28895 Q Losses: [0.043432631, 0.022732761]\n",
      "Gen Loss: 1.15903 Disc Loss: 0.865739 Q Losses: [0.067021176, 0.032140095]\n",
      "Gen Loss: 9.19067 Disc Loss: 3.16306 Q Losses: [0.035007205, 0.023873519]\n",
      "Gen Loss: 7.10308 Disc Loss: 0.663856 Q Losses: [0.043703809, 0.026243417]\n",
      "Gen Loss: 4.80706 Disc Loss: 0.188468 Q Losses: [0.048667267, 0.0079145832]\n",
      "Saved Model on  67000\n",
      "Gen Loss: 3.29626 Disc Loss: 0.30466 Q Losses: [0.045466483, 0.031370901]\n",
      "Gen Loss: 2.74012 Disc Loss: 0.337473 Q Losses: [0.033597201, 0.060696866]\n",
      "Gen Loss: 4.04534 Disc Loss: 0.403536 Q Losses: [0.03383863, 0.011010275]\n",
      "Gen Loss: 3.21593 Disc Loss: 0.217918 Q Losses: [0.040602297, 0.046205223]\n",
      "Gen Loss: 3.72353 Disc Loss: 0.270308 Q Losses: [0.031402998, 0.04144831]\n",
      "Gen Loss: 6.26343 Disc Loss: 0.56809 Q Losses: [0.041281771, 0.059427366]\n",
      "Gen Loss: 3.56326 Disc Loss: 0.290378 Q Losses: [0.049279854, 0.034739047]\n",
      "Gen Loss: 3.4144 Disc Loss: 0.295783 Q Losses: [0.044584788, 0.038922012]\n",
      "Gen Loss: 3.60243 Disc Loss: 0.363586 Q Losses: [0.036469534, 0.018034384]\n",
      "Gen Loss: 4.43716 Disc Loss: 0.177848 Q Losses: [0.03240617, 0.011667684]\n",
      "Saved Model on  68000\n",
      "Gen Loss: 2.31449 Disc Loss: 0.588472 Q Losses: [0.034810252, 0.058235899]\n",
      "Gen Loss: 4.08327 Disc Loss: 0.245199 Q Losses: [0.03800055, 0.077937447]\n",
      "Gen Loss: 2.29426 Disc Loss: 0.567702 Q Losses: [0.042649865, 0.20535809]\n",
      "Gen Loss: 5.60108 Disc Loss: 0.473897 Q Losses: [0.034023337, 0.02419441]\n",
      "Gen Loss: 4.49092 Disc Loss: 0.261485 Q Losses: [0.042099148, 0.01329757]\n",
      "Gen Loss: 3.93479 Disc Loss: 0.421656 Q Losses: [0.025662718, 0.051755629]\n",
      "Gen Loss: 8.73303 Disc Loss: 1.02763 Q Losses: [0.028323807, 0.011118116]\n",
      "Gen Loss: 2.89473 Disc Loss: 0.412464 Q Losses: [0.042233422, 0.0093119554]\n",
      "Gen Loss: 5.52116 Disc Loss: 0.373043 Q Losses: [0.037608467, 0.044252757]\n",
      "Gen Loss: 3.05755 Disc Loss: 0.33904 Q Losses: [0.037623152, 0.0283507]\n",
      "Saved Model on  69000\n",
      "Gen Loss: 3.80556 Disc Loss: 0.225402 Q Losses: [0.053553462, 0.01948018]\n",
      "Gen Loss: 5.44637 Disc Loss: 0.312331 Q Losses: [0.03321749, 0.036817383]\n",
      "Gen Loss: 4.3938 Disc Loss: 0.437882 Q Losses: [0.048424836, 0.065677077]\n",
      "Gen Loss: 4.27946 Disc Loss: 0.314632 Q Losses: [0.043800212, 0.042546507]\n",
      "Gen Loss: 4.91504 Disc Loss: 0.263973 Q Losses: [0.032134928, 0.031970985]\n",
      "Gen Loss: 4.08364 Disc Loss: 0.299501 Q Losses: [0.045081131, 0.027382182]\n",
      "Gen Loss: 4.30915 Disc Loss: 0.160829 Q Losses: [0.038131773, 0.023996346]\n",
      "Gen Loss: 4.80599 Disc Loss: 0.163702 Q Losses: [0.043751076, 0.043419402]\n",
      "Gen Loss: 4.07571 Disc Loss: 0.352236 Q Losses: [0.048610248, 0.059211463]\n",
      "Gen Loss: 3.82893 Disc Loss: 0.214956 Q Losses: [0.050154772, 0.047734071]\n",
      "Saved Model on  70000\n",
      "Gen Loss: 4.63395 Disc Loss: 0.574593 Q Losses: [0.030985177, 0.019083792]\n",
      "Gen Loss: 1.38611 Disc Loss: 0.844511 Q Losses: [0.060914643, 0.049383521]\n",
      "Gen Loss: 2.90232 Disc Loss: 0.303829 Q Losses: [0.044214956, 0.026393533]\n",
      "Gen Loss: 5.09518 Disc Loss: 0.170112 Q Losses: [0.030670624, 0.079018719]\n",
      "Gen Loss: 3.2261 Disc Loss: 0.429784 Q Losses: [0.038526669, 0.012157436]\n",
      "Gen Loss: 4.61912 Disc Loss: 0.466156 Q Losses: [0.052244209, 0.020314714]\n",
      "Gen Loss: 4.47334 Disc Loss: 0.193499 Q Losses: [0.052704256, 0.079167262]\n",
      "Gen Loss: 4.14099 Disc Loss: 0.389942 Q Losses: [0.036048599, 0.030107588]\n",
      "Gen Loss: 5.91348 Disc Loss: 0.792402 Q Losses: [0.037111443, 0.026586553]\n",
      "Gen Loss: 4.10388 Disc Loss: 0.368046 Q Losses: [0.034474112, 0.039149236]\n",
      "Saved Model on  71000\n",
      "Gen Loss: 6.81286 Disc Loss: 1.21325 Q Losses: [0.044770315, 0.029030496]\n",
      "Gen Loss: 7.18667 Disc Loss: 0.487952 Q Losses: [0.040992148, 0.069100171]\n",
      "Gen Loss: 5.5804 Disc Loss: 0.170745 Q Losses: [0.058320943, 0.0038704446]\n",
      "Gen Loss: 6.42367 Disc Loss: 0.374163 Q Losses: [0.052452326, 0.030538678]\n",
      "Gen Loss: 4.91289 Disc Loss: 0.161762 Q Losses: [0.054091327, 0.075924747]\n",
      "Gen Loss: 4.54274 Disc Loss: 0.356675 Q Losses: [0.037309058, 0.038141213]\n",
      "Gen Loss: 4.39937 Disc Loss: 0.437807 Q Losses: [0.035950426, 0.029403478]\n",
      "Gen Loss: 4.66214 Disc Loss: 0.330891 Q Losses: [0.024781108, 0.0188515]\n",
      "Gen Loss: 3.49294 Disc Loss: 0.166188 Q Losses: [0.064522266, 0.049180567]\n",
      "Gen Loss: 2.99792 Disc Loss: 0.286182 Q Losses: [0.054321352, 0.020980177]\n",
      "Saved Model on  72000\n",
      "Gen Loss: 4.75854 Disc Loss: 0.414007 Q Losses: [0.048090659, 0.051454667]\n",
      "Gen Loss: 4.69893 Disc Loss: 0.211949 Q Losses: [0.041450989, 0.072367743]\n",
      "Gen Loss: 7.37668 Disc Loss: 0.583675 Q Losses: [0.031811707, 0.020445861]\n",
      "Gen Loss: -0.0949893 Disc Loss: 1.01938 Q Losses: [0.038403787, 0.047989026]\n",
      "Gen Loss: 5.82686 Disc Loss: 0.421121 Q Losses: [0.040452607, 0.029475868]\n",
      "Gen Loss: 2.81488 Disc Loss: 0.320714 Q Losses: [0.043646857, 0.026995795]\n",
      "Gen Loss: 4.94347 Disc Loss: 0.209914 Q Losses: [0.028205719, 0.034067906]\n",
      "Gen Loss: 3.92715 Disc Loss: 0.229403 Q Losses: [0.032164738, 0.010664837]\n",
      "Gen Loss: 4.95974 Disc Loss: 0.292248 Q Losses: [0.037236653, 0.035230283]\n",
      "Gen Loss: 3.6427 Disc Loss: 0.392497 Q Losses: [0.038736105, 0.011902076]\n",
      "Saved Model on  73000\n",
      "Gen Loss: 4.57438 Disc Loss: 0.221507 Q Losses: [0.039174303, 0.024135841]\n",
      "Gen Loss: 4.2737 Disc Loss: 0.330687 Q Losses: [0.034893773, 0.038274009]\n",
      "Gen Loss: 4.44275 Disc Loss: 0.246974 Q Losses: [0.041528195, 0.0061288662]\n",
      "Gen Loss: 5.34823 Disc Loss: 0.113744 Q Losses: [0.042430215, 0.030013125]\n",
      "Gen Loss: 5.5269 Disc Loss: 0.141175 Q Losses: [0.05169367, 0.030470829]\n",
      "Gen Loss: 4.15089 Disc Loss: 0.302373 Q Losses: [0.048444904, 0.041814819]\n",
      "Gen Loss: 5.72012 Disc Loss: 0.196424 Q Losses: [0.050571341, 0.058785792]\n",
      "Gen Loss: 1.21306 Disc Loss: 0.645657 Q Losses: [0.038292751, 0.10777724]\n",
      "Gen Loss: 6.13087 Disc Loss: 0.363189 Q Losses: [0.036782876, 0.02797756]\n",
      "Gen Loss: 6.13595 Disc Loss: 0.568762 Q Losses: [0.036283448, 0.033637442]\n",
      "Saved Model on  74000\n",
      "Gen Loss: 4.08383 Disc Loss: 0.49537 Q Losses: [0.038512819, 0.071698859]\n",
      "Gen Loss: 4.64573 Disc Loss: 0.239566 Q Losses: [0.052937869, 0.033837114]\n",
      "Gen Loss: 4.12684 Disc Loss: 0.209439 Q Losses: [0.043252811, 0.028745715]\n",
      "Gen Loss: 5.19046 Disc Loss: 0.342546 Q Losses: [0.03153158, 0.06813316]\n",
      "Gen Loss: 1.89479 Disc Loss: 0.429359 Q Losses: [0.03266098, 0.031367108]\n",
      "Gen Loss: 3.5497 Disc Loss: 0.175442 Q Losses: [0.050441153, 0.013039593]\n",
      "Gen Loss: 4.62482 Disc Loss: 0.308863 Q Losses: [0.037918862, 0.013003112]\n",
      "Gen Loss: 6.34782 Disc Loss: 0.510241 Q Losses: [0.025083933, 0.056532539]\n",
      "Gen Loss: 6.66089 Disc Loss: 0.404647 Q Losses: [0.033055283, 0.038315289]\n",
      "Gen Loss: 3.80251 Disc Loss: 0.520345 Q Losses: [0.036664926, 0.011349837]\n",
      "Saved Model on  75000\n",
      "Gen Loss: 3.92343 Disc Loss: 0.242223 Q Losses: [0.032996014, 0.14403722]\n",
      "Gen Loss: 5.33325 Disc Loss: 0.215895 Q Losses: [0.03492783, 0.015001012]\n",
      "Gen Loss: 2.946 Disc Loss: 0.328796 Q Losses: [0.028026093, 0.018758431]\n",
      "Gen Loss: 4.05691 Disc Loss: 0.150606 Q Losses: [0.04167692, 0.0345673]\n",
      "Gen Loss: 5.453 Disc Loss: 0.167369 Q Losses: [0.038116626, 0.0027969072]\n",
      "Gen Loss: 5.54921 Disc Loss: 0.200742 Q Losses: [0.038063545, 0.014608947]\n",
      "Gen Loss: 5.74743 Disc Loss: 0.336065 Q Losses: [0.033555388, 0.017400328]\n",
      "Gen Loss: 4.92905 Disc Loss: 0.587761 Q Losses: [0.035274006, 0.027088128]\n",
      "Gen Loss: 3.42326 Disc Loss: 0.294674 Q Losses: [0.038498923, 0.016974801]\n",
      "Gen Loss: 6.26949 Disc Loss: 0.389052 Q Losses: [0.062745266, 0.052696258]\n",
      "Saved Model on  76000\n",
      "Gen Loss: 2.60068 Disc Loss: 0.789513 Q Losses: [0.055667024, 0.080293335]\n",
      "Gen Loss: 6.63407 Disc Loss: 0.696153 Q Losses: [0.03122323, 0.0071337614]\n",
      "Gen Loss: 4.54606 Disc Loss: 0.201343 Q Losses: [0.029833021, 0.014594866]\n",
      "Gen Loss: 9.39089 Disc Loss: 1.827 Q Losses: [0.030062702, 0.09733592]\n",
      "Gen Loss: 4.39854 Disc Loss: 0.383299 Q Losses: [0.045704685, 0.010664892]\n",
      "Gen Loss: 3.56843 Disc Loss: 0.121572 Q Losses: [0.059369016, 0.021605177]\n",
      "Gen Loss: 3.83015 Disc Loss: 0.27651 Q Losses: [0.039726608, 0.025384087]\n",
      "Gen Loss: 5.09028 Disc Loss: 0.198022 Q Losses: [0.037428118, 0.023759572]\n",
      "Gen Loss: 5.17215 Disc Loss: 0.810166 Q Losses: [0.035709549, 0.019364707]\n",
      "Gen Loss: 4.2155 Disc Loss: 0.150096 Q Losses: [0.04894501, 0.052725419]\n",
      "Saved Model on  77000\n",
      "Gen Loss: 5.36394 Disc Loss: 0.188751 Q Losses: [0.031795785, 0.019860281]\n",
      "Gen Loss: 2.85627 Disc Loss: 0.343449 Q Losses: [0.030945711, 0.015717104]\n",
      "Gen Loss: 4.30723 Disc Loss: 0.220755 Q Losses: [0.065804109, 0.067465082]\n",
      "Gen Loss: 3.79545 Disc Loss: 0.202933 Q Losses: [0.055291131, 0.034600265]\n",
      "Gen Loss: 1.29462 Disc Loss: 0.514757 Q Losses: [0.029025553, 0.036471866]\n",
      "Gen Loss: 2.78135 Disc Loss: 0.458136 Q Losses: [0.062571041, 0.016345443]\n",
      "Gen Loss: 4.84231 Disc Loss: 0.395823 Q Losses: [0.049360856, 0.085859455]\n",
      "Gen Loss: 2.95986 Disc Loss: 0.349236 Q Losses: [0.028591078, 0.018568967]\n",
      "Gen Loss: 5.43903 Disc Loss: 0.58513 Q Losses: [0.038874932, 0.010999283]\n",
      "Gen Loss: 5.38997 Disc Loss: 0.10801 Q Losses: [0.033913393, 0.080937698]\n",
      "Saved Model on  78000\n",
      "Gen Loss: 5.63251 Disc Loss: 0.287614 Q Losses: [0.045116145, 0.043671455]\n",
      "Gen Loss: 5.16536 Disc Loss: 0.237143 Q Losses: [0.054569811, 0.025207125]\n",
      "Gen Loss: 3.92057 Disc Loss: 0.209558 Q Losses: [0.052466061, 0.016519226]\n",
      "Gen Loss: 5.36443 Disc Loss: 0.362043 Q Losses: [0.033628706, 0.058992989]\n",
      "Gen Loss: 3.97227 Disc Loss: 0.272556 Q Losses: [0.045518748, 0.063773215]\n",
      "Gen Loss: 5.12024 Disc Loss: 0.174141 Q Losses: [0.046785001, 0.024885094]\n",
      "Gen Loss: 4.7989 Disc Loss: 0.262633 Q Losses: [0.052910171, 0.0056602592]\n",
      "Gen Loss: 4.46321 Disc Loss: 0.518213 Q Losses: [0.073022403, 0.036706928]\n",
      "Gen Loss: 5.77114 Disc Loss: 0.217108 Q Losses: [0.045198694, 0.014355117]\n",
      "Gen Loss: 6.9498 Disc Loss: 0.814763 Q Losses: [0.041494414, 0.018191572]\n",
      "Saved Model on  79000\n",
      "Gen Loss: 4.2624 Disc Loss: 0.46848 Q Losses: [0.026816597, 0.022724057]\n",
      "Gen Loss: 2.57844 Disc Loss: 0.279442 Q Losses: [0.03909814, 0.0064365971]\n",
      "Gen Loss: 5.81441 Disc Loss: 0.498058 Q Losses: [0.037412457, 0.080708742]\n",
      "Gen Loss: 4.27235 Disc Loss: 0.235129 Q Losses: [0.039185375, 0.055823423]\n",
      "Gen Loss: 5.16642 Disc Loss: 0.497005 Q Losses: [0.025916794, 0.012025575]\n",
      "Gen Loss: 3.62931 Disc Loss: 0.222523 Q Losses: [0.031439271, 0.055908859]\n",
      "Gen Loss: 5.87582 Disc Loss: 0.273924 Q Losses: [0.030191395, 0.040762939]\n",
      "Gen Loss: 5.31282 Disc Loss: 0.163986 Q Losses: [0.057215236, 0.019663658]\n",
      "Gen Loss: 4.81736 Disc Loss: 0.161688 Q Losses: [0.035976, 0.035211138]\n",
      "Gen Loss: 5.78929 Disc Loss: 0.147212 Q Losses: [0.035372987, 0.03502715]\n",
      "Saved Model on  80000\n",
      "Gen Loss: 5.32938 Disc Loss: 0.269015 Q Losses: [0.058552008, 0.016138146]\n",
      "Gen Loss: 2.65392 Disc Loss: 0.411815 Q Losses: [0.027629107, 0.006442925]\n",
      "Gen Loss: 1.07533 Disc Loss: 0.580277 Q Losses: [0.029275689, 0.031634547]\n",
      "Gen Loss: 2.22591 Disc Loss: 0.421667 Q Losses: [0.043153487, 0.023771139]\n",
      "Gen Loss: 5.05896 Disc Loss: 0.198453 Q Losses: [0.030965796, 0.039763294]\n",
      "Gen Loss: 6.38912 Disc Loss: 0.255683 Q Losses: [0.03516924, 0.025879795]\n",
      "Gen Loss: 4.59548 Disc Loss: 0.174048 Q Losses: [0.039280541, 0.033001576]\n",
      "Gen Loss: 5.84293 Disc Loss: 1.09912 Q Losses: [0.031640522, 0.057306856]\n",
      "Gen Loss: 2.74456 Disc Loss: 0.370948 Q Losses: [0.052438769, 0.049356319]\n"
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "# blow up after 81800\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/Session#run\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "c_val = 10\n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 81000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        #print(\"zs shape:\",zs.shape)\n",
    "        #lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "        lcat = np.random.randint(0,c_val,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "        #print(\"lcat\", lcat)\n",
    "        #print(\"lcat shape: \", lcat.shape)\n",
    "        lcont = np.random.uniform(-1,1,[batch_size,number_continuous]) #\n",
    "        \n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        \n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "        _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "            #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "            latent_fixed = np.ones((c_val*c_val,1))\n",
    "            lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "            \n",
    "            #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "            #b = np.reshape(a,[100,1])\n",
    "            b = np.reshape(a,[c_val*c_val,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            #\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "            save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model on \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-80000.cptk\n"
     ]
    }
   ],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #lcat_sample = np.reshape(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #print(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]))\n",
    "    a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    b = np.reshape(a,[100,1])\n",
    "    c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)+8\n",
    "    lcont_sample = np.hstack([b,c])\n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test_4'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35]",
   "language": "python",
   "name": "conda-env-dlpy35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
