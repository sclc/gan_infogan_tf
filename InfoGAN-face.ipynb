{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN CeleA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import os, glob, cv2, math, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.misc\n",
    "import scipy\n",
    "#from PIL import Image\n",
    "\n",
    "np.random.seed(1)\n",
    "#plt.ion()   # interactive mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load CeleA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html\n",
    "#img_rows, img_cols = 100, 100\n",
    "#img_rows, img_cols = 64, 64\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "def get_im(path):\n",
    "\n",
    "    #img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.imread(path)\n",
    "    #img = plt.imread(path)\n",
    "    #resized = img\n",
    "    resized = cv2.resize(img, (img_cols, img_rows))\n",
    "\n",
    "    return resized\n",
    "\n",
    "def read_train_data_fullname(path):\n",
    "\n",
    "    \n",
    "    files = glob.glob(path)\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_fullname_lfw(path):\n",
    "\n",
    "    root = path\n",
    "    all_folders = os.listdir(root)\n",
    "    path = []\n",
    "    for afolder in all_folders:\n",
    "        path.append(root+\"/\"+afolder+\"/*.jpg\")\n",
    "    #print(path)\n",
    "    files = []\n",
    "    for apath in path:\n",
    "        templist = glob.glob(apath)\n",
    "        for afile in templist:\n",
    "            files.append(afile)\n",
    "\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_batch(filenames, batchsize=5):\n",
    "    \n",
    "    end = min(len(filenames), batchsize)\n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[:end]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        #normalization\n",
    "        #img -= np.mean(img)\n",
    "        #img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    filenames = filenames[end:]\n",
    "    \n",
    "    return train_data, filenames\n",
    "\n",
    "def read_train_data_mini_batch(filenames, startpoint, batchsize=5):\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[startpoint:startpoint+batchsize]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #print(type(img))        \n",
    "        #normalization\n",
    "        img -= np.mean(img)\n",
    "        img /= np.std(img)\n",
    "        #print(img.shape)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "#random mini-batch\n",
    "def read_train_data_random_batch(filenames, batchsize=5):\n",
    "    fullsize=len(filenames)\n",
    "    #http://qiita.com/hamukazu/items/ec1b4659df00f0ce43b1\n",
    "    idset = np.random.randint(0, high=fullsize, size=batchsize)\n",
    "    #print(idset) \n",
    "    train_data = []\n",
    "    \n",
    "    for fid in idset:\n",
    "        fl = filenames[fid]\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        #img[:,:,1] = 0\n",
    "        #img[:,:,2] = 0\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #img = np.reshape(img,[img.shape[0],img.shape[1],1])\n",
    "        # normalization\n",
    "        # https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn\n",
    "        img -= np.mean(img)\n",
    "        img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    \n",
    "    return train_data\n",
    "# must be full path, ~/... not ok\n",
    "# train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data,train_data_filenames = read_train_data_batch(train_data_filenames)\n",
    "\n",
    "#random mini-batch\n",
    "#train_data = read_train_data_random_batch(train_data_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(train_data),len(train_data_filenames))\n",
    "#print(len(train_data[0][1]))\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # need this transpose if there is transpose in read_train_data_xx calls\n",
    "    #inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #inp = std * inp + mean\n",
    "    #plt.imshow(inp,cmap=\"Purples\",interpolation = 'bicubic')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.01)  # pause a bit so that plots are updated\n",
    "\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "#train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "#print(len(train_data_filenames))\n",
    "#print(train_data_filenames[3])\n",
    "#train_data = read_train_data_mini_batch(train_data_filenames,0)  \n",
    "#print(train_data[0].shape)\n",
    "#print(train_data[43][:,:,0:2].shape)\n",
    "\n",
    "#imshow(train_data[2][:,:,0])\n",
    "#imshow(train_data[2][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge_color(images, size))\n",
    "    #return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    #return images\n",
    "    # https://zhuanlan.zhihu.com/p/25051313\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def merge_color(images, size):\n",
    "    h, w, c = images.shape[1], images.shape[2],images.shape[3]\n",
    "    img = np.zeros((h * size[0], w * size[1],c))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image[:,:,:]\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/depth_to_space\n",
    "# http://qiita.com/tadOne/items/48302a399dcad44c69c8   Tensorflow - padding = VALID/SAMEの違いについて\n",
    "#     so 3 tf.depth_to_space(genX,2) gives 4x2^3 = 32\n",
    "# \n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*448,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,448])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=256,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=128,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=64,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=3,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,64,[4,4],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,128,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,256,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    #print(\"d_out\"+str(d_out))\n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/split\n",
    "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "# https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "# https://www.tensorflow.org/api_docs/python/tf/trainable_variables\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "# https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "# z_lat: one_hot_size + z_size + number_continuous = 10+64+2=76\n",
    "# g_loss def is interesting, my understanding: \n",
    "#        if Dg is the probablity to be told as feak data, then 1-Dg is the probabily of suceessfully cheating, \n",
    "#        so we cal KL(Dg/(1-Dg)), and readuce_mean works as sampling proceduce\n",
    "# \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "z_size = 128 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "#categorical_list = [10]*10 # Each entry in this list defines a categorical variable of a specific size.\n",
    "categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "# categorical_list = [10,10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 1 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "#print(\"latent_cat_in:\", latent_cat_in)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "#print(\"latent_cat_list: \",latent_cat_list)\n",
    "if number_continuous>0:\n",
    "    latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    #print(latent_cat_list[idx])\n",
    "    #print(latent_oh),  woundn't print anything in sess.run()\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "#print(\"1st z_lats: \", z_lats )\n",
    "z_lats.append(z_in)\n",
    "#print(\"2nd z_lats: \", z_lats )\n",
    "if number_continuous>0:\n",
    "    z_lats.append(latent_cont_in)\n",
    "#print(\"3rd z_lats: \", z_lats )\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "#print(\"z_lat: \", z_lat )\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "#print (Gz.shape)\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "#g_loss = -tf.reduce_mean(tf.log((Dg/(1.-Dg)))) #KL Divergence optimizer\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) \n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    #print (\"latent_var: \", latent_var)\n",
    "    #print (\"tf.log(QgCat[idx]): \",tf.log(QgCat[idx]))\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "#print (len(tvars))\n",
    "#for i in tvars:\n",
    "#    print(i)\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.4)\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "#\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:1 Gen Loss: 3.97692 Disc Loss: 1.65535 Q Losses: [0.2209909, 2.3021722]\n",
      "epoch:0 batch_done:2 Gen Loss: 10.6725 Disc Loss: 0.983071 Q Losses: [0.15692919, 2.2700653]\n",
      "epoch:0 batch_done:3 Gen Loss: 7.2922 Disc Loss: 0.411932 Q Losses: [0.13837145, 2.3361697]\n",
      "epoch:0 batch_done:4 Gen Loss: 12.2727 Disc Loss: 1.28163 Q Losses: [0.13925597, 2.2842112]\n",
      "epoch:0 batch_done:5 Gen Loss: 5.10141 Disc Loss: 2.20942 Q Losses: [0.12259919, 2.2668846]\n",
      "epoch:0 batch_done:6 Gen Loss: 11.0006 Disc Loss: 1.5264 Q Losses: [0.11910344, 2.2542524]\n",
      "epoch:0 batch_done:7 Gen Loss: 11.0247 Disc Loss: 0.289347 Q Losses: [0.11063288, 2.2532749]\n",
      "epoch:0 batch_done:8 Gen Loss: 5.92379 Disc Loss: 0.151191 Q Losses: [0.10034138, 2.2489665]\n",
      "epoch:0 batch_done:9 Gen Loss: 14.6506 Disc Loss: 2.32099 Q Losses: [0.095879316, 2.2115786]\n",
      "epoch:0 batch_done:10 Gen Loss: 10.8081 Disc Loss: 2.18877 Q Losses: [0.11042821, 2.2312627]\n",
      "epoch:0 batch_done:11 Gen Loss: 2.65694 Disc Loss: 0.640113 Q Losses: [0.11398821, 2.2541769]\n",
      "epoch:0 batch_done:12 Gen Loss: 13.9797 Disc Loss: 5.62279 Q Losses: [0.060877576, 2.226413]\n",
      "epoch:0 batch_done:13 Gen Loss: 13.8924 Disc Loss: 1.00735 Q Losses: [0.10178046, 2.2356229]\n",
      "epoch:0 batch_done:14 Gen Loss: 7.52878 Disc Loss: 0.542609 Q Losses: [0.082941025, 2.2091765]\n",
      "epoch:0 batch_done:15 Gen Loss: 11.5261 Disc Loss: 0.86852 Q Losses: [0.09239234, 2.207999]\n",
      "epoch:0 batch_done:16 Gen Loss: 7.25289 Disc Loss: 0.513278 Q Losses: [0.060977288, 2.1776719]\n",
      "epoch:0 batch_done:17 Gen Loss: 17.5319 Disc Loss: 2.65017 Q Losses: [0.070362046, 2.1818023]\n",
      "epoch:0 batch_done:18 Gen Loss: 13.785 Disc Loss: 2.52637 Q Losses: [0.075110301, 2.1078646]\n",
      "epoch:0 batch_done:19 Gen Loss: 6.22611 Disc Loss: 0.285247 Q Losses: [0.065657556, 2.1489627]\n",
      "epoch:0 batch_done:20 Gen Loss: 15.1997 Disc Loss: 2.24373 Q Losses: [0.052663811, 2.1396432]\n",
      "epoch:0 batch_done:21 Gen Loss: 13.1711 Disc Loss: 0.440349 Q Losses: [0.065725684, 2.0643828]\n",
      "epoch:0 batch_done:22 Gen Loss: 5.92371 Disc Loss: 0.483342 Q Losses: [0.062759057, 2.0794344]\n",
      "epoch:0 batch_done:23 Gen Loss: 18.2449 Disc Loss: 2.9526 Q Losses: [0.053015281, 2.0671899]\n",
      "epoch:0 batch_done:24 Gen Loss: 14.2286 Disc Loss: 2.87508 Q Losses: [0.046142198, 2.1068845]\n",
      "epoch:0 batch_done:25 Gen Loss: 3.3758 Disc Loss: 0.650672 Q Losses: [0.046292335, 2.0649953]\n",
      "epoch:0 batch_done:26 Gen Loss: 13.3075 Disc Loss: 5.04862 Q Losses: [0.047766544, 2.0991836]\n",
      "epoch:0 batch_done:27 Gen Loss: 12.9516 Disc Loss: 0.488673 Q Losses: [0.057588816, 2.0502357]\n",
      "epoch:0 batch_done:28 Gen Loss: 6.0273 Disc Loss: 0.540138 Q Losses: [0.047884829, 2.03175]\n",
      "epoch:0 batch_done:29 Gen Loss: 14.126 Disc Loss: 2.92288 Q Losses: [0.044668049, 1.9643878]\n",
      "epoch:0 batch_done:30 Gen Loss: 11.9893 Disc Loss: 0.954578 Q Losses: [0.046141461, 1.9863169]\n",
      "epoch:0 batch_done:31 Gen Loss: 4.05718 Disc Loss: 0.619719 Q Losses: [0.042829923, 1.9789393]\n",
      "epoch:0 batch_done:32 Gen Loss: 15.1203 Disc Loss: 2.10745 Q Losses: [0.038559973, 1.9508193]\n",
      "epoch:0 batch_done:33 Gen Loss: 14.892 Disc Loss: 1.14916 Q Losses: [0.039428964, 1.9396546]\n",
      "epoch:0 batch_done:34 Gen Loss: 8.89347 Disc Loss: 0.694059 Q Losses: [0.046280868, 1.8621976]\n",
      "epoch:0 batch_done:35 Gen Loss: 4.53588 Disc Loss: 0.385775 Q Losses: [0.051533394, 1.8750741]\n",
      "epoch:0 batch_done:36 Gen Loss: 13.5412 Disc Loss: 0.857126 Q Losses: [0.041548233, 1.8536108]\n",
      "epoch:0 batch_done:37 Gen Loss: 11.5557 Disc Loss: 0.789584 Q Losses: [0.052022386, 1.7889717]\n",
      "epoch:0 batch_done:38 Gen Loss: 5.57582 Disc Loss: 0.222224 Q Losses: [0.042190664, 1.8016602]\n",
      "epoch:0 batch_done:39 Gen Loss: 11.3768 Disc Loss: 0.795956 Q Losses: [0.03941676, 1.8005258]\n",
      "epoch:0 batch_done:40 Gen Loss: 9.54774 Disc Loss: 0.420041 Q Losses: [0.040743701, 1.7848029]\n",
      "epoch:0 batch_done:41 Gen Loss: 4.37619 Disc Loss: 0.358059 Q Losses: [0.043285917, 1.7711693]\n",
      "epoch:0 batch_done:42 Gen Loss: 17.9261 Disc Loss: 1.56082 Q Losses: [0.040510986, 1.6729078]\n",
      "epoch:0 batch_done:43 Gen Loss: 15.8263 Disc Loss: 1.51138 Q Losses: [0.062382825, 1.7044053]\n",
      "epoch:0 batch_done:44 Gen Loss: 7.48036 Disc Loss: 0.429201 Q Losses: [0.058618683, 1.6981212]\n",
      "epoch:0 batch_done:45 Gen Loss: 12.4518 Disc Loss: 0.634153 Q Losses: [0.04818029, 1.6481009]\n",
      "epoch:0 batch_done:46 Gen Loss: 11.8968 Disc Loss: 0.177971 Q Losses: [0.037390359, 1.6014218]\n",
      "epoch:0 batch_done:47 Gen Loss: 7.2229 Disc Loss: 0.20559 Q Losses: [0.038767822, 1.5914866]\n",
      "epoch:0 batch_done:48 Gen Loss: 4.42617 Disc Loss: 0.165011 Q Losses: [0.040511329, 1.5254723]\n",
      "epoch:0 batch_done:49 Gen Loss: 13.2684 Disc Loss: 0.594902 Q Losses: [0.058837995, 1.4863836]\n",
      "epoch:0 batch_done:50 Gen Loss: 11.8899 Disc Loss: 0.440621 Q Losses: [0.051418178, 1.416023]\n",
      "epoch:0 batch_done:51 Gen Loss: 4.74815 Disc Loss: 0.464539 Q Losses: [0.051052567, 1.4746234]\n",
      "epoch:0 batch_done:52 Gen Loss: 22.284 Disc Loss: 1.61472 Q Losses: [0.041971985, 1.4596078]\n",
      "epoch:0 batch_done:53 Gen Loss: 17.2446 Disc Loss: 3.76571 Q Losses: [0.038947597, 1.4847314]\n",
      "epoch:0 batch_done:54 Gen Loss: 7.4348 Disc Loss: 0.50107 Q Losses: [0.047764122, 1.4589499]\n",
      "epoch:0 batch_done:55 Gen Loss: 6.82872 Disc Loss: 0.284874 Q Losses: [0.04980237, 1.3927377]\n",
      "epoch:0 batch_done:56 Gen Loss: 10.4783 Disc Loss: 0.198781 Q Losses: [0.038097933, 1.4377224]\n",
      "epoch:0 batch_done:57 Gen Loss: 7.81887 Disc Loss: 0.276091 Q Losses: [0.04148902, 1.3647225]\n",
      "epoch:0 batch_done:58 Gen Loss: 6.73321 Disc Loss: 0.138097 Q Losses: [0.040788159, 1.3644145]\n",
      "epoch:0 batch_done:59 Gen Loss: 4.94015 Disc Loss: 0.428365 Q Losses: [0.05395443, 1.2782608]\n",
      "epoch:0 batch_done:60 Gen Loss: 9.36919 Disc Loss: 0.341844 Q Losses: [0.048959494, 1.2852038]\n",
      "epoch:0 batch_done:61 Gen Loss: 8.55613 Disc Loss: 0.263154 Q Losses: [0.040053371, 1.2558603]\n",
      "epoch:0 batch_done:62 Gen Loss: 4.70783 Disc Loss: 0.14186 Q Losses: [0.06546019, 1.2418226]\n",
      "epoch:0 batch_done:63 Gen Loss: 9.94324 Disc Loss: 0.263143 Q Losses: [0.050213978, 1.1968738]\n",
      "epoch:0 batch_done:64 Gen Loss: 6.351 Disc Loss: 0.456506 Q Losses: [0.041999042, 1.1930593]\n",
      "epoch:0 batch_done:65 Gen Loss: 3.70578 Disc Loss: 0.202157 Q Losses: [0.052775502, 1.1589026]\n",
      "epoch:0 batch_done:66 Gen Loss: 22.0678 Disc Loss: 0.819773 Q Losses: [0.050144665, 1.1715183]\n",
      "epoch:0 batch_done:67 Gen Loss: 21.3501 Disc Loss: 1.42768 Q Losses: [0.049750105, 1.109776]\n",
      "epoch:0 batch_done:68 Gen Loss: 15.9021 Disc Loss: 0.34491 Q Losses: [0.051884949, 1.1526576]\n",
      "epoch:0 batch_done:69 Gen Loss: 8.38821 Disc Loss: 0.105294 Q Losses: [0.047635712, 1.1048928]\n",
      "epoch:0 batch_done:70 Gen Loss: 11.0741 Disc Loss: 0.281769 Q Losses: [0.049049318, 1.1169436]\n",
      "epoch:0 batch_done:71 Gen Loss: 9.79667 Disc Loss: 0.0502184 Q Losses: [0.042280663, 1.1658158]\n",
      "epoch:0 batch_done:72 Gen Loss: 14.1638 Disc Loss: 0.321504 Q Losses: [0.0547469, 1.1147292]\n",
      "epoch:0 batch_done:73 Gen Loss: 11.1511 Disc Loss: 0.345324 Q Losses: [0.067345589, 1.1054405]\n",
      "epoch:0 batch_done:74 Gen Loss: 4.37304 Disc Loss: 0.412591 Q Losses: [0.046531621, 1.0628674]\n",
      "epoch:0 batch_done:75 Gen Loss: 31.3797 Disc Loss: 1.19193 Q Losses: [0.052951332, 1.0641971]\n",
      "epoch:0 batch_done:76 Gen Loss: 31.3596 Disc Loss: 2.45478 Q Losses: [0.06024646, 1.0951277]\n",
      "epoch:0 batch_done:77 Gen Loss: 22.9414 Disc Loss: 0.829768 Q Losses: [0.056448467, 1.0633745]\n",
      "epoch:0 batch_done:78 Gen Loss: 11.7402 Disc Loss: 0.0608774 Q Losses: [0.054049291, 1.0557201]\n",
      "epoch:0 batch_done:79 Gen Loss: 5.88295 Disc Loss: 0.0865072 Q Losses: [0.053724207, 1.0094138]\n",
      "epoch:0 batch_done:80 Gen Loss: 30.6523 Disc Loss: 1.24025 Q Losses: [0.053145271, 1.032089]\n",
      "epoch:0 batch_done:81 Gen Loss: 31.9085 Disc Loss: 0.846404 Q Losses: [0.062266581, 1.0105715]\n",
      "epoch:0 batch_done:82 Gen Loss: 21.4567 Disc Loss: 1.10141 Q Losses: [0.062519595, 1.0674195]\n",
      "epoch:0 batch_done:83 Gen Loss: 11.6262 Disc Loss: 0.638383 Q Losses: [0.056374542, 1.0358937]\n",
      "epoch:0 batch_done:84 Gen Loss: 4.87643 Disc Loss: 0.0643895 Q Losses: [0.043669578, 1.0154817]\n",
      "epoch:0 batch_done:85 Gen Loss: 6.25619 Disc Loss: 0.281888 Q Losses: [0.045947962, 0.94735557]\n",
      "epoch:0 batch_done:86 Gen Loss: 13.7909 Disc Loss: 0.204465 Q Losses: [0.04432831, 0.93183213]\n",
      "epoch:0 batch_done:87 Gen Loss: 12.007 Disc Loss: 0.106534 Q Losses: [0.050734352, 0.92741966]\n",
      "epoch:0 batch_done:88 Gen Loss: 4.92228 Disc Loss: 0.40401 Q Losses: [0.056586776, 0.88694924]\n",
      "epoch:0 batch_done:89 Gen Loss: 36.6467 Disc Loss: 3.22037 Q Losses: [0.039671328, 0.89773852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:90 Gen Loss: 33.3241 Disc Loss: 4.86764 Q Losses: [0.041059278, 0.90894228]\n",
      "epoch:0 batch_done:91 Gen Loss: 14.155 Disc Loss: 2.84692 Q Losses: [0.063347615, 0.89060712]\n",
      "epoch:0 batch_done:92 Gen Loss: 2.53305 Disc Loss: 0.197935 Q Losses: [0.056810722, 0.89171875]\n",
      "epoch:0 batch_done:93 Gen Loss: 32.5968 Disc Loss: 1.9267 Q Losses: [0.061246894, 0.81116462]\n",
      "epoch:0 batch_done:94 Gen Loss: 33.9135 Disc Loss: 0.777237 Q Losses: [0.057007972, 0.83628476]\n",
      "epoch:0 batch_done:95 Gen Loss: 16.6724 Disc Loss: 0.878904 Q Losses: [0.041109014, 0.89850664]\n",
      "epoch:0 batch_done:96 Gen Loss: 2.3732 Disc Loss: 0.468039 Q Losses: [0.07167162, 0.90092003]\n",
      "epoch:0 batch_done:97 Gen Loss: 30.7628 Disc Loss: 1.88397 Q Losses: [0.050247967, 0.82712775]\n",
      "epoch:0 batch_done:98 Gen Loss: 24.5014 Disc Loss: 1.22442 Q Losses: [0.065144889, 0.86233675]\n",
      "epoch:0 batch_done:99 Gen Loss: 9.22656 Disc Loss: 1.01283 Q Losses: [0.052137263, 0.83936095]\n",
      "epoch:0 batch_done:100 Gen Loss: 2.0657 Disc Loss: 0.218286 Q Losses: [0.040241122, 0.82857984]\n",
      "epoch:0 batch_done:101 Gen Loss: 22.486 Disc Loss: 1.5825 Q Losses: [0.060182601, 0.79646242]\n",
      "epoch:0 batch_done:102 Gen Loss: 21.7853 Disc Loss: 3.2936 Q Losses: [0.044987448, 0.74106508]\n",
      "epoch:0 batch_done:103 Gen Loss: 14.7836 Disc Loss: 1.14333 Q Losses: [0.071401082, 0.74121445]\n",
      "epoch:0 batch_done:104 Gen Loss: 6.61653 Disc Loss: 0.229621 Q Losses: [0.051844407, 0.74282396]\n",
      "epoch:0 batch_done:105 Gen Loss: 3.55939 Disc Loss: 0.110872 Q Losses: [0.044813603, 0.75011313]\n",
      "epoch:0 batch_done:106 Gen Loss: 10.4035 Disc Loss: 0.344425 Q Losses: [0.065926991, 0.73500955]\n",
      "epoch:0 batch_done:107 Gen Loss: 10.7812 Disc Loss: 0.165919 Q Losses: [0.04412245, 0.70171952]\n",
      "epoch:0 batch_done:108 Gen Loss: 7.03573 Disc Loss: 0.220616 Q Losses: [0.046358399, 0.70687151]\n",
      "epoch:0 batch_done:109 Gen Loss: 4.02812 Disc Loss: 0.186987 Q Losses: [0.034759898, 0.68535525]\n",
      "epoch:0 batch_done:110 Gen Loss: 23.0531 Disc Loss: 1.07836 Q Losses: [0.058121566, 0.66893399]\n",
      "epoch:0 batch_done:111 Gen Loss: 22.5977 Disc Loss: 2.11862 Q Losses: [0.045931511, 0.67114556]\n",
      "epoch:0 batch_done:112 Gen Loss: 13.1933 Disc Loss: 1.17677 Q Losses: [0.045093559, 0.65171027]\n",
      "epoch:0 batch_done:113 Gen Loss: 2.90463 Disc Loss: 0.232601 Q Losses: [0.048875093, 0.68948406]\n",
      "epoch:0 batch_done:114 Gen Loss: 30.9237 Disc Loss: 4.61548 Q Losses: [0.069367163, 0.69126433]\n",
      "epoch:0 batch_done:115 Gen Loss: 31.9471 Disc Loss: 1.98586 Q Losses: [0.043551154, 0.64993793]\n",
      "epoch:0 batch_done:116 Gen Loss: 19.8718 Disc Loss: 2.1088 Q Losses: [0.038929105, 0.67325795]\n",
      "epoch:0 batch_done:117 Gen Loss: 6.80928 Disc Loss: 0.0911302 Q Losses: [0.052254181, 0.6976524]\n",
      "epoch:0 batch_done:118 Gen Loss: 13.1959 Disc Loss: 0.495816 Q Losses: [0.056154791, 0.66660035]\n",
      "epoch:0 batch_done:119 Gen Loss: 12.7388 Disc Loss: 0.366193 Q Losses: [0.047544606, 0.70510358]\n",
      "epoch:0 batch_done:120 Gen Loss: 8.54721 Disc Loss: 0.231338 Q Losses: [0.050893925, 0.65863538]\n",
      "epoch:0 batch_done:121 Gen Loss: 2.47292 Disc Loss: 0.387383 Q Losses: [0.0498816, 0.62010229]\n",
      "epoch:0 batch_done:122 Gen Loss: 23.6179 Disc Loss: 1.69101 Q Losses: [0.044261567, 0.61343563]\n",
      "epoch:0 batch_done:123 Gen Loss: 24.6178 Disc Loss: 2.08231 Q Losses: [0.041702561, 0.61269081]\n",
      "epoch:0 batch_done:124 Gen Loss: 16.3697 Disc Loss: 1.60914 Q Losses: [0.032007616, 0.60157919]\n",
      "epoch:0 batch_done:125 Gen Loss: 7.10617 Disc Loss: 0.0815354 Q Losses: [0.043478951, 0.59629107]\n",
      "epoch:0 batch_done:126 Gen Loss: 17.667 Disc Loss: 1.015 Q Losses: [0.052071337, 0.58129609]\n",
      "epoch:0 batch_done:127 Gen Loss: 17.631 Disc Loss: 0.440649 Q Losses: [0.034724973, 0.55911374]\n",
      "epoch:0 batch_done:128 Gen Loss: 10.035 Disc Loss: 0.433724 Q Losses: [0.042389832, 0.58094943]\n",
      "epoch:0 batch_done:129 Gen Loss: 8.78028 Disc Loss: 0.432181 Q Losses: [0.050425354, 0.57702208]\n",
      "epoch:0 batch_done:130 Gen Loss: 6.7355 Disc Loss: 0.394678 Q Losses: [0.037900113, 0.57119316]\n",
      "epoch:0 batch_done:131 Gen Loss: 7.07541 Disc Loss: 0.298503 Q Losses: [0.046291109, 0.5610745]\n",
      "epoch:0 batch_done:132 Gen Loss: 5.66197 Disc Loss: 0.282806 Q Losses: [0.050566353, 0.546354]\n",
      "epoch:0 batch_done:133 Gen Loss: 7.315 Disc Loss: 0.145776 Q Losses: [0.035575174, 0.55186737]\n",
      "epoch:0 batch_done:134 Gen Loss: 5.61793 Disc Loss: 0.19327 Q Losses: [0.035560247, 0.53011942]\n",
      "epoch:0 batch_done:135 Gen Loss: 5.03066 Disc Loss: 0.289865 Q Losses: [0.035473865, 0.53081763]\n",
      "epoch:0 batch_done:136 Gen Loss: 6.04256 Disc Loss: 0.218053 Q Losses: [0.041440133, 0.52981991]\n",
      "epoch:0 batch_done:137 Gen Loss: 6.41718 Disc Loss: 0.0783094 Q Losses: [0.030892827, 0.50906909]\n",
      "epoch:0 batch_done:138 Gen Loss: 5.50769 Disc Loss: 0.152653 Q Losses: [0.062564835, 0.50803709]\n",
      "epoch:0 batch_done:139 Gen Loss: 6.30897 Disc Loss: 0.135224 Q Losses: [0.03817226, 0.51475847]\n",
      "epoch:0 batch_done:140 Gen Loss: 6.59375 Disc Loss: 0.0398362 Q Losses: [0.050945602, 0.48712516]\n",
      "epoch:0 batch_done:141 Gen Loss: 4.95657 Disc Loss: 0.178196 Q Losses: [0.034499183, 0.49589381]\n",
      "epoch:0 batch_done:142 Gen Loss: 8.24793 Disc Loss: 0.242153 Q Losses: [0.041358795, 0.48059085]\n",
      "epoch:0 batch_done:143 Gen Loss: 6.62722 Disc Loss: 0.184029 Q Losses: [0.045395918, 0.48877007]\n",
      "epoch:0 batch_done:144 Gen Loss: 5.85633 Disc Loss: 0.137133 Q Losses: [0.050458632, 0.4824717]\n",
      "epoch:0 batch_done:145 Gen Loss: 13.7792 Disc Loss: 0.337287 Q Losses: [0.036181457, 0.4589276]\n",
      "epoch:0 batch_done:146 Gen Loss: 12.4332 Disc Loss: 0.473604 Q Losses: [0.029569352, 0.48745877]\n",
      "epoch:0 batch_done:147 Gen Loss: 7.29699 Disc Loss: 0.135501 Q Losses: [0.036274303, 0.47135085]\n",
      "epoch:0 batch_done:148 Gen Loss: 6.34786 Disc Loss: 0.110322 Q Losses: [0.04923344, 0.45329097]\n",
      "epoch:0 batch_done:149 Gen Loss: 11.2824 Disc Loss: 0.228626 Q Losses: [0.038364515, 0.49132448]\n",
      "epoch:0 batch_done:150 Gen Loss: 9.75924 Disc Loss: 0.186542 Q Losses: [0.039840177, 0.47046077]\n",
      "epoch:0 batch_done:151 Gen Loss: 6.03192 Disc Loss: 0.153319 Q Losses: [0.043514155, 0.44892585]\n",
      "epoch:0 batch_done:152 Gen Loss: 7.29712 Disc Loss: 0.13669 Q Losses: [0.036416002, 0.44546342]\n",
      "epoch:0 batch_done:153 Gen Loss: 6.57392 Disc Loss: 0.0699664 Q Losses: [0.05361012, 0.48180956]\n",
      "epoch:0 batch_done:154 Gen Loss: 8.76706 Disc Loss: 0.202167 Q Losses: [0.033962995, 0.44087487]\n",
      "epoch:0 batch_done:155 Gen Loss: 7.6564 Disc Loss: 0.0858591 Q Losses: [0.037717849, 0.45888278]\n",
      "epoch:0 batch_done:156 Gen Loss: 5.60999 Disc Loss: 0.0890178 Q Losses: [0.055902302, 0.46438712]\n",
      "epoch:0 batch_done:157 Gen Loss: 5.92686 Disc Loss: 0.0585012 Q Losses: [0.031177541, 0.4280988]\n",
      "epoch:0 batch_done:158 Gen Loss: 6.8224 Disc Loss: 0.117798 Q Losses: [0.042915739, 0.42303109]\n",
      "epoch:0 batch_done:159 Gen Loss: 6.75792 Disc Loss: 0.0415063 Q Losses: [0.035352506, 0.42639363]\n",
      "epoch:0 batch_done:160 Gen Loss: 5.7439 Disc Loss: 0.172537 Q Losses: [0.035453945, 0.41187128]\n",
      "epoch:0 batch_done:161 Gen Loss: 7.98941 Disc Loss: 0.163029 Q Losses: [0.035461918, 0.44627053]\n",
      "epoch:0 batch_done:162 Gen Loss: 7.26219 Disc Loss: 0.0997563 Q Losses: [0.032804806, 0.42089665]\n",
      "epoch:0 batch_done:163 Gen Loss: 6.12478 Disc Loss: 0.103732 Q Losses: [0.044093288, 0.40947708]\n",
      "epoch:0 batch_done:164 Gen Loss: 4.95407 Disc Loss: 0.114032 Q Losses: [0.034323949, 0.39444914]\n",
      "epoch:0 batch_done:165 Gen Loss: 12.3472 Disc Loss: 0.162216 Q Losses: [0.037927747, 0.40118074]\n",
      "epoch:0 batch_done:166 Gen Loss: 10.4185 Disc Loss: 0.188191 Q Losses: [0.02692591, 0.40015721]\n",
      "epoch:0 batch_done:167 Gen Loss: 7.65441 Disc Loss: 0.168478 Q Losses: [0.034974866, 0.40439591]\n",
      "epoch:0 batch_done:168 Gen Loss: 5.84575 Disc Loss: 0.0407035 Q Losses: [0.041350141, 0.40371007]\n",
      "epoch:0 batch_done:169 Gen Loss: 5.5282 Disc Loss: 0.0472658 Q Losses: [0.031956412, 0.39112729]\n",
      "epoch:0 batch_done:170 Gen Loss: 6.55248 Disc Loss: 0.0223647 Q Losses: [0.047780506, 0.39231279]\n",
      "epoch:0 batch_done:171 Gen Loss: 37.2853 Disc Loss: 1.01717 Q Losses: [0.030305479, 0.40973359]\n",
      "epoch:0 batch_done:172 Gen Loss: 34.4979 Disc Loss: 4.00709 Q Losses: [0.037531052, 0.37670526]\n",
      "epoch:0 batch_done:173 Gen Loss: 28.5728 Disc Loss: 0.279024 Q Losses: [0.035489667, 0.38208538]\n",
      "epoch:0 batch_done:174 Gen Loss: 21.1066 Disc Loss: 0.0222268 Q Losses: [0.037147172, 0.39160019]\n",
      "epoch:0 batch_done:175 Gen Loss: 12.9185 Disc Loss: 0.0011284 Q Losses: [0.040312763, 0.37118325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:176 Gen Loss: 5.37506 Disc Loss: 0.00347742 Q Losses: [0.04479963, 0.36037925]\n",
      "epoch:0 batch_done:177 Gen Loss: 26.6788 Disc Loss: 0.597342 Q Losses: [0.029297462, 0.37689903]\n",
      "epoch:0 batch_done:178 Gen Loss: 27.7698 Disc Loss: 0.286969 Q Losses: [0.047075674, 0.40553933]\n",
      "epoch:0 batch_done:179 Gen Loss: 17.2314 Disc Loss: 0.801434 Q Losses: [0.04028663, 0.4209488]\n",
      "epoch:0 batch_done:180 Gen Loss: 8.7777 Disc Loss: 0.0264434 Q Losses: [0.033635534, 0.47222084]\n",
      "epoch:0 batch_done:181 Gen Loss: 5.46594 Disc Loss: 0.0279857 Q Losses: [0.050270788, 0.45643392]\n",
      "epoch:0 batch_done:182 Gen Loss: 4.49545 Disc Loss: 0.0284025 Q Losses: [0.0397055, 0.39179748]\n",
      "epoch:0 batch_done:183 Gen Loss: 5.0188 Disc Loss: 0.0438434 Q Losses: [0.029009255, 0.35765955]\n",
      "epoch:0 batch_done:184 Gen Loss: 18.9641 Disc Loss: 0.356099 Q Losses: [0.037482325, 0.38721508]\n",
      "epoch:0 batch_done:185 Gen Loss: 16.0383 Disc Loss: 0.860929 Q Losses: [0.034430098, 0.36784133]\n",
      "epoch:0 batch_done:186 Gen Loss: 9.16033 Disc Loss: 0.568431 Q Losses: [0.032437645, 0.37486094]\n",
      "epoch:0 batch_done:187 Gen Loss: 3.38442 Disc Loss: 0.0527989 Q Losses: [0.033401988, 0.3598308]\n",
      "epoch:0 batch_done:188 Gen Loss: 23.4178 Disc Loss: 0.522808 Q Losses: [0.035162091, 0.34391874]\n",
      "epoch:0 batch_done:189 Gen Loss: 26.1185 Disc Loss: 0.145978 Q Losses: [0.030815331, 0.35771489]\n",
      "epoch:0 batch_done:190 Gen Loss: 17.7645 Disc Loss: 0.599678 Q Losses: [0.033260923, 0.36187977]\n",
      "epoch:0 batch_done:191 Gen Loss: 4.99813 Disc Loss: 0.554435 Q Losses: [0.034489281, 0.34692577]\n",
      "epoch:0 batch_done:192 Gen Loss: 30.0346 Disc Loss: 0.718852 Q Losses: [0.035795435, 0.35871756]\n",
      "epoch:0 batch_done:193 Gen Loss: 25.9864 Disc Loss: 0.626393 Q Losses: [0.039306637, 0.33596551]\n",
      "epoch:0 batch_done:194 Gen Loss: 18.3843 Disc Loss: 0.487914 Q Losses: [0.034859885, 0.3359639]\n",
      "epoch:0 batch_done:195 Gen Loss: 12.8073 Disc Loss: 0.0238058 Q Losses: [0.033484526, 0.35604084]\n",
      "epoch:0 batch_done:196 Gen Loss: 7.7121 Disc Loss: 0.0144688 Q Losses: [0.032552369, 0.33467317]\n",
      "epoch:0 batch_done:197 Gen Loss: 4.23657 Disc Loss: 0.0107504 Q Losses: [0.027926829, 0.31946933]\n",
      "epoch:0 batch_done:198 Gen Loss: 12.7158 Disc Loss: 0.222089 Q Losses: [0.029995576, 0.31809783]\n",
      "epoch:0 batch_done:199 Gen Loss: 14.5085 Disc Loss: 0.0880739 Q Losses: [0.027731353, 0.3357867]\n",
      "epoch:0 batch_done:200 Gen Loss: 11.7011 Disc Loss: 0.0742243 Q Losses: [0.032900676, 0.30136061]\n",
      "epoch:0 batch_done:201 Gen Loss: 6.73565 Disc Loss: 0.0660213 Q Losses: [0.027076613, 0.30473161]\n",
      "epoch:0 batch_done:202 Gen Loss: 17.5965 Disc Loss: 0.342342 Q Losses: [0.021365836, 0.28306413]\n",
      "epoch:0 batch_done:203 Gen Loss: 17.414 Disc Loss: 0.373138 Q Losses: [0.028952662, 0.28613874]\n",
      "epoch:0 batch_done:204 Gen Loss: 7.48924 Disc Loss: 1.24199 Q Losses: [0.026171908, 0.28050494]\n",
      "epoch:0 batch_done:205 Gen Loss: 12.939 Disc Loss: 0.284043 Q Losses: [0.025560798, 0.28962383]\n",
      "epoch:0 batch_done:206 Gen Loss: 12.557 Disc Loss: 0.112864 Q Losses: [0.036433294, 0.30210227]\n",
      "epoch:0 batch_done:207 Gen Loss: 8.79939 Disc Loss: 0.13701 Q Losses: [0.024448637, 0.28681839]\n",
      "epoch:1 batch_done:1 Gen Loss: 5.37537 Disc Loss: 0.138015 Q Losses: [0.028497349, 0.30000636]\n",
      "epoch:1 batch_done:2 Gen Loss: 17.8034 Disc Loss: 0.243578 Q Losses: [0.030708523, 0.28084475]\n",
      "epoch:1 batch_done:3 Gen Loss: 17.1741 Disc Loss: 0.542498 Q Losses: [0.020528093, 0.28790098]\n",
      "epoch:1 batch_done:4 Gen Loss: 13.0337 Disc Loss: 0.0857431 Q Losses: [0.027030624, 0.27958834]\n",
      "epoch:1 batch_done:5 Gen Loss: 8.6159 Disc Loss: 0.00955315 Q Losses: [0.029397912, 0.29634625]\n",
      "epoch:1 batch_done:6 Gen Loss: 4.28718 Disc Loss: 0.175643 Q Losses: [0.023444938, 0.27182451]\n",
      "epoch:1 batch_done:7 Gen Loss: 28.6231 Disc Loss: 0.599863 Q Losses: [0.028631318, 0.26993668]\n",
      "epoch:1 batch_done:8 Gen Loss: 29.0875 Disc Loss: 0.977278 Q Losses: [0.027860148, 0.26028866]\n",
      "epoch:1 batch_done:9 Gen Loss: 23.0427 Disc Loss: 0.56014 Q Losses: [0.026381934, 0.28146726]\n",
      "epoch:1 batch_done:10 Gen Loss: 16.7059 Disc Loss: 0.0725119 Q Losses: [0.027794793, 0.26267534]\n",
      "epoch:1 batch_done:11 Gen Loss: 10.6118 Disc Loss: 0.0446068 Q Losses: [0.025395796, 0.26606175]\n",
      "epoch:1 batch_done:12 Gen Loss: 5.9761 Disc Loss: 0.0019462 Q Losses: [0.035858486, 0.27381638]\n",
      "epoch:1 batch_done:13 Gen Loss: 5.37405 Disc Loss: 0.0380575 Q Losses: [0.025702298, 0.27654961]\n",
      "epoch:1 batch_done:14 Gen Loss: 11.2517 Disc Loss: 0.138402 Q Losses: [0.02607777, 0.28091222]\n",
      "epoch:1 batch_done:15 Gen Loss: 11.0116 Disc Loss: 0.0176558 Q Losses: [0.032377429, 0.26001012]\n",
      "epoch:1 batch_done:16 Gen Loss: 8.26358 Disc Loss: 0.0036585 Q Losses: [0.023876768, 0.25835389]\n",
      "epoch:1 batch_done:17 Gen Loss: 5.71478 Disc Loss: 0.06744 Q Losses: [0.024142155, 0.2434714]\n",
      "epoch:1 batch_done:18 Gen Loss: 12.5654 Disc Loss: 0.182722 Q Losses: [0.016964402, 0.24584818]\n",
      "epoch:1 batch_done:19 Gen Loss: 12.8746 Disc Loss: 0.204222 Q Losses: [0.020333793, 0.24819414]\n",
      "epoch:1 batch_done:20 Gen Loss: 9.46123 Disc Loss: 0.221441 Q Losses: [0.01932605, 0.23204035]\n",
      "epoch:1 batch_done:21 Gen Loss: 4.65686 Disc Loss: 0.140683 Q Losses: [0.026816193, 0.24171537]\n",
      "epoch:1 batch_done:22 Gen Loss: 4.159 Disc Loss: 0.0994932 Q Losses: [0.018816367, 0.26158288]\n",
      "epoch:1 batch_done:23 Gen Loss: 36.6049 Disc Loss: 1.21828 Q Losses: [0.027495187, 0.24022359]\n",
      "epoch:1 batch_done:24 Gen Loss: 23.5942 Disc Loss: 2.94434 Q Losses: [0.015203575, 0.26232755]\n",
      "epoch:1 batch_done:25 Gen Loss: 9.74874 Disc Loss: 0.12231 Q Losses: [0.025413144, 0.2813319]\n",
      "epoch:1 batch_done:26 Gen Loss: 2.15656 Disc Loss: 0.00458206 Q Losses: [0.031425748, 0.30808964]\n",
      "epoch:1 batch_done:27 Gen Loss: 3.35323 Disc Loss: 0.0514382 Q Losses: [0.0341383, 0.25662652]\n",
      "epoch:1 batch_done:28 Gen Loss: 7.02989 Disc Loss: 0.0524364 Q Losses: [0.027780417, 0.26977849]\n",
      "epoch:1 batch_done:29 Gen Loss: 10.772 Disc Loss: 0.0289838 Q Losses: [0.028964186, 0.24095753]\n",
      "epoch:1 batch_done:30 Gen Loss: 9.78115 Disc Loss: 0.0492295 Q Losses: [0.024318213, 0.27109843]\n",
      "epoch:1 batch_done:31 Gen Loss: 6.14488 Disc Loss: 0.0328887 Q Losses: [0.029133696, 0.23446445]\n",
      "epoch:1 batch_done:32 Gen Loss: 3.95292 Disc Loss: 0.0995885 Q Losses: [0.023849927, 0.2337023]\n",
      "epoch:1 batch_done:33 Gen Loss: 8.97875 Disc Loss: 0.0649349 Q Losses: [0.021387305, 0.23445192]\n",
      "epoch:1 batch_done:34 Gen Loss: 12.2513 Disc Loss: 0.00663668 Q Losses: [0.03029022, 0.22059473]\n",
      "epoch:1 batch_done:35 Gen Loss: 6.38854 Disc Loss: 0.0470846 Q Losses: [0.020572471, 0.22559874]\n",
      "epoch:1 batch_done:36 Gen Loss: 5.15373 Disc Loss: 0.0532845 Q Losses: [0.020551551, 0.22726026]\n",
      "epoch:1 batch_done:37 Gen Loss: 6.47862 Disc Loss: 0.0918421 Q Losses: [0.030498229, 0.22819185]\n",
      "epoch:1 batch_done:38 Gen Loss: 45.2851 Disc Loss: 2.74899 Q Losses: [0.025461977, 0.23614512]\n",
      "epoch:1 batch_done:39 Gen Loss: 30.4747 Disc Loss: 4.63769 Q Losses: [0.026907351, 0.23263216]\n",
      "epoch:1 batch_done:40 Gen Loss: 14.1087 Disc Loss: 0.525522 Q Losses: [0.032375954, 0.22110026]\n",
      "epoch:1 batch_done:41 Gen Loss: 2.82153 Disc Loss: 0.0141105 Q Losses: [0.023911277, 0.24382903]\n",
      "epoch:1 batch_done:42 Gen Loss: 37.5381 Disc Loss: 1.14212 Q Losses: [0.021490447, 0.2397317]\n",
      "epoch:1 batch_done:43 Gen Loss: 37.3177 Disc Loss: 2.29144 Q Losses: [0.029746797, 0.22927254]\n",
      "epoch:1 batch_done:44 Gen Loss: 26.3842 Disc Loss: 1.32906 Q Losses: [0.026721422, 0.20805511]\n",
      "epoch:1 batch_done:45 Gen Loss: 14.2986 Disc Loss: 0.358168 Q Losses: [0.027209911, 0.21215038]\n",
      "epoch:1 batch_done:46 Gen Loss: 7.419 Disc Loss: 0.00712957 Q Losses: [0.018695582, 0.26875848]\n",
      "epoch:1 batch_done:47 Gen Loss: 3.58938 Disc Loss: 0.00875188 Q Losses: [0.022271197, 0.23129447]\n",
      "epoch:1 batch_done:48 Gen Loss: 30.0419 Disc Loss: 0.842999 Q Losses: [0.022202473, 0.20198958]\n",
      "epoch:1 batch_done:49 Gen Loss: 30.3583 Disc Loss: 1.63307 Q Losses: [0.029374197, 0.237103]\n",
      "epoch:1 batch_done:50 Gen Loss: 21.7063 Disc Loss: 0.837885 Q Losses: [0.02252743, 0.21500215]\n",
      "epoch:1 batch_done:51 Gen Loss: 12.0252 Disc Loss: 0.1241 Q Losses: [0.016096368, 0.22519678]\n",
      "epoch:1 batch_done:52 Gen Loss: 4.5803 Disc Loss: 0.129287 Q Losses: [0.031449758, 0.2112624]\n",
      "epoch:1 batch_done:53 Gen Loss: 16.647 Disc Loss: 0.350837 Q Losses: [0.027154483, 0.19992246]\n",
      "epoch:1 batch_done:54 Gen Loss: 20.0181 Disc Loss: 0.008374 Q Losses: [0.017408423, 0.21152779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:55 Gen Loss: 16.1963 Disc Loss: 0.357467 Q Losses: [0.015795611, 0.21564101]\n",
      "epoch:1 batch_done:56 Gen Loss: 10.4581 Disc Loss: 0.0807641 Q Losses: [0.017458092, 0.19386521]\n",
      "epoch:1 batch_done:57 Gen Loss: 4.82153 Disc Loss: 0.0356646 Q Losses: [0.026029818, 0.19694282]\n",
      "epoch:1 batch_done:58 Gen Loss: 34.132 Disc Loss: 1.06864 Q Losses: [0.021729067, 0.19329049]\n",
      "epoch:1 batch_done:59 Gen Loss: 37.1161 Disc Loss: 1.3671 Q Losses: [0.022324938, 0.21441987]\n",
      "epoch:1 batch_done:60 Gen Loss: 25.2984 Disc Loss: 1.43301 Q Losses: [0.020200614, 0.20780179]\n",
      "epoch:1 batch_done:61 Gen Loss: 9.8901 Disc Loss: 0.0102783 Q Losses: [0.021233507, 0.19355419]\n",
      "epoch:1 batch_done:62 Gen Loss: 2.62085 Disc Loss: 0.0179449 Q Losses: [0.029470418, 0.20428476]\n",
      "epoch:1 batch_done:63 Gen Loss: 26.7449 Disc Loss: 0.513204 Q Losses: [0.029343808, 0.20411414]\n",
      "epoch:1 batch_done:64 Gen Loss: 26.3312 Disc Loss: 0.347303 Q Losses: [0.022258574, 0.18539158]\n",
      "epoch:1 batch_done:65 Gen Loss: 19.6178 Disc Loss: 0.278603 Q Losses: [0.017684344, 0.20014158]\n",
      "epoch:1 batch_done:66 Gen Loss: 8.03451 Disc Loss: 0.616638 Q Losses: [0.022324769, 0.18787876]\n",
      "epoch:1 batch_done:67 Gen Loss: 7.06892 Disc Loss: 0.0837696 Q Losses: [0.017186726, 0.17851968]\n",
      "epoch:1 batch_done:68 Gen Loss: 10.9644 Disc Loss: 0.0626313 Q Losses: [0.01908257, 0.17538664]\n",
      "epoch:1 batch_done:69 Gen Loss: 10.2038 Disc Loss: 0.0113722 Q Losses: [0.020735674, 0.1858186]\n",
      "epoch:1 batch_done:70 Gen Loss: 6.43712 Disc Loss: 0.0127797 Q Losses: [0.019272374, 0.20852152]\n",
      "epoch:1 batch_done:71 Gen Loss: 23.9274 Disc Loss: 0.470126 Q Losses: [0.016416367, 0.18465269]\n",
      "epoch:1 batch_done:72 Gen Loss: 24.8676 Disc Loss: 0.781908 Q Losses: [0.015548771, 0.20314056]\n",
      "epoch:1 batch_done:73 Gen Loss: 18.1119 Disc Loss: 0.688761 Q Losses: [0.019240066, 0.19544309]\n",
      "epoch:1 batch_done:74 Gen Loss: 12.1261 Disc Loss: 0.0226569 Q Losses: [0.016531887, 0.18037091]\n",
      "epoch:1 batch_done:75 Gen Loss: 7.15941 Disc Loss: 0.00600624 Q Losses: [0.016862424, 0.16092026]\n",
      "epoch:1 batch_done:76 Gen Loss: 5.88054 Disc Loss: 0.00401935 Q Losses: [0.020603966, 0.16853124]\n",
      "epoch:1 batch_done:77 Gen Loss: 5.62885 Disc Loss: 0.00362258 Q Losses: [0.016177839, 0.16994628]\n",
      "epoch:1 batch_done:78 Gen Loss: 42.8765 Disc Loss: 2.44555 Q Losses: [0.022152048, 0.19197392]\n",
      "epoch:1 batch_done:79 Gen Loss: 36.8795 Disc Loss: 4.39993 Q Losses: [0.015035488, 0.1614622]\n",
      "epoch:1 batch_done:80 Gen Loss: 6.91161 Disc Loss: 1.68602 Q Losses: [0.022264596, 0.2639482]\n",
      "epoch:1 batch_done:81 Gen Loss: 0.0055409 Disc Loss: 0.370333 Q Losses: [0.027309332, 0.21009858]\n",
      "epoch:1 batch_done:82 Gen Loss: 15.5008 Disc Loss: 0.0160678 Q Losses: [0.0242166, 0.16761562]\n",
      "epoch:1 batch_done:83 Gen Loss: 14.9493 Disc Loss: 0.0179988 Q Losses: [0.025005359, 0.1769975]\n",
      "epoch:1 batch_done:84 Gen Loss: 10.4504 Disc Loss: 0.0853917 Q Losses: [0.014755484, 0.16980073]\n",
      "epoch:1 batch_done:85 Gen Loss: 4.98668 Disc Loss: 0.0119006 Q Losses: [0.017029539, 0.1647242]\n",
      "epoch:1 batch_done:86 Gen Loss: 31.9581 Disc Loss: 0.83594 Q Losses: [0.014573036, 0.1783924]\n",
      "epoch:1 batch_done:87 Gen Loss: 33.2817 Disc Loss: 0.807534 Q Losses: [0.020661578, 0.17057806]\n",
      "epoch:1 batch_done:88 Gen Loss: 26.9328 Disc Loss: 0.88119 Q Losses: [0.019202432, 0.17534956]\n",
      "epoch:1 batch_done:89 Gen Loss: 20.9824 Disc Loss: 0.120094 Q Losses: [0.021625213, 0.18297744]\n",
      "epoch:1 batch_done:90 Gen Loss: 15.6706 Disc Loss: 0.00460005 Q Losses: [0.017334737, 0.16913402]\n",
      "epoch:1 batch_done:91 Gen Loss: 10.7326 Disc Loss: 0.0313654 Q Losses: [0.012548249, 0.18478319]\n",
      "epoch:1 batch_done:92 Gen Loss: 6.38298 Disc Loss: 0.0192359 Q Losses: [0.01964359, 0.16834731]\n",
      "epoch:1 batch_done:93 Gen Loss: 7.78687 Disc Loss: 0.123627 Q Losses: [0.029980231, 0.15570506]\n",
      "epoch:1 batch_done:94 Gen Loss: 9.65828 Disc Loss: 0.0812988 Q Losses: [0.023889314, 0.16351828]\n",
      "epoch:1 batch_done:95 Gen Loss: 8.16918 Disc Loss: 0.0648707 Q Losses: [0.017267391, 0.17552942]\n",
      "epoch:1 batch_done:96 Gen Loss: 6.68154 Disc Loss: 0.13908 Q Losses: [0.020145424, 0.1550917]\n",
      "epoch:1 batch_done:97 Gen Loss: 12.5313 Disc Loss: 0.225787 Q Losses: [0.013920464, 0.16651368]\n",
      "epoch:1 batch_done:98 Gen Loss: 11.7582 Disc Loss: 0.346969 Q Losses: [0.018733049, 0.1542016]\n",
      "epoch:1 batch_done:99 Gen Loss: 5.71642 Disc Loss: 0.545647 Q Losses: [0.016168721, 0.15352198]\n",
      "epoch:1 batch_done:100 Gen Loss: 21.0032 Disc Loss: 0.491679 Q Losses: [0.015614135, 0.1466428]\n",
      "epoch:1 batch_done:101 Gen Loss: 22.6216 Disc Loss: 0.250522 Q Losses: [0.017417727, 0.15683366]\n",
      "epoch:1 batch_done:102 Gen Loss: 18.7511 Disc Loss: 0.274712 Q Losses: [0.018496409, 0.16855769]\n",
      "epoch:1 batch_done:103 Gen Loss: 14.3819 Disc Loss: 0.0324709 Q Losses: [0.018955886, 0.13536829]\n",
      "epoch:1 batch_done:104 Gen Loss: 12.3361 Disc Loss: 0.023693 Q Losses: [0.015483489, 0.14544593]\n",
      "epoch:1 batch_done:105 Gen Loss: 11.1272 Disc Loss: 0.0039907 Q Losses: [0.011690727, 0.13360026]\n",
      "epoch:1 batch_done:106 Gen Loss: 5.33428 Disc Loss: 0.0438785 Q Losses: [0.016982898, 0.13769877]\n",
      "epoch:1 batch_done:107 Gen Loss: 6.85297 Disc Loss: 0.0427423 Q Losses: [0.017118216, 0.15001488]\n",
      "epoch:1 batch_done:108 Gen Loss: 8.85047 Disc Loss: 0.0849798 Q Losses: [0.017097492, 0.15687877]\n",
      "epoch:1 batch_done:109 Gen Loss: 5.96776 Disc Loss: 0.0114869 Q Losses: [0.019283455, 0.14696383]\n",
      "epoch:1 batch_done:110 Gen Loss: 5.75237 Disc Loss: 0.0271551 Q Losses: [0.016344186, 0.14088452]\n",
      "epoch:1 batch_done:111 Gen Loss: 8.49224 Disc Loss: 0.0894698 Q Losses: [0.014817342, 0.14195547]\n",
      "epoch:1 batch_done:112 Gen Loss: 38.6148 Disc Loss: 1.05077 Q Losses: [0.023777053, 0.13939521]\n",
      "epoch:1 batch_done:113 Gen Loss: 35.9372 Disc Loss: 4.46809 Q Losses: [0.016809274, 0.14335987]\n",
      "epoch:1 batch_done:114 Gen Loss: 27.9125 Disc Loss: 0.59979 Q Losses: [0.016840488, 0.17888552]\n",
      "epoch:1 batch_done:115 Gen Loss: 19.7134 Disc Loss: 0.000872403 Q Losses: [0.015067083, 0.15435553]\n",
      "epoch:1 batch_done:116 Gen Loss: 13.6086 Disc Loss: 0.000363693 Q Losses: [0.017761616, 0.13821954]\n",
      "epoch:1 batch_done:117 Gen Loss: 7.60317 Disc Loss: 0.000965376 Q Losses: [0.024325488, 0.12852412]\n",
      "epoch:1 batch_done:118 Gen Loss: 6.26101 Disc Loss: 0.0567675 Q Losses: [0.012062138, 0.14948255]\n",
      "epoch:1 batch_done:119 Gen Loss: 7.0872 Disc Loss: 0.0180155 Q Losses: [0.01518506, 0.1328229]\n",
      "epoch:1 batch_done:120 Gen Loss: 7.493 Disc Loss: 0.00309751 Q Losses: [0.012982104, 0.13085958]\n",
      "epoch:1 batch_done:121 Gen Loss: 6.26903 Disc Loss: 0.0105455 Q Losses: [0.015599763, 0.13870448]\n",
      "epoch:1 batch_done:122 Gen Loss: 5.91102 Disc Loss: 0.025577 Q Losses: [0.012195994, 0.15114412]\n",
      "epoch:1 batch_done:123 Gen Loss: 27.8981 Disc Loss: 0.514762 Q Losses: [0.016171226, 0.13741143]\n",
      "epoch:1 batch_done:124 Gen Loss: 29.469 Disc Loss: 0.42268 Q Losses: [0.014194869, 0.13833699]\n",
      "epoch:1 batch_done:125 Gen Loss: 15.7858 Disc Loss: 1.36542 Q Losses: [0.020987775, 0.14782399]\n",
      "epoch:1 batch_done:126 Gen Loss: 2.06418 Disc Loss: 0.108547 Q Losses: [0.019515712, 0.14537729]\n",
      "epoch:1 batch_done:127 Gen Loss: 11.7956 Disc Loss: 0.0581255 Q Losses: [0.016269077, 0.14007539]\n",
      "epoch:1 batch_done:128 Gen Loss: 10.2988 Disc Loss: 0.00467621 Q Losses: [0.016444262, 0.14137052]\n",
      "epoch:1 batch_done:129 Gen Loss: 5.46295 Disc Loss: 0.0627994 Q Losses: [0.016968954, 0.1196318]\n",
      "epoch:1 batch_done:130 Gen Loss: 15.7561 Disc Loss: 0.136006 Q Losses: [0.015182765, 0.14063016]\n",
      "epoch:1 batch_done:131 Gen Loss: 15.8716 Disc Loss: 0.272278 Q Losses: [0.012286181, 0.13958678]\n",
      "epoch:1 batch_done:132 Gen Loss: 14.2415 Disc Loss: 0.00429065 Q Losses: [0.010874834, 0.12918136]\n",
      "epoch:1 batch_done:133 Gen Loss: 12.4428 Disc Loss: 0.0491927 Q Losses: [0.020562559, 0.12727937]\n",
      "epoch:1 batch_done:134 Gen Loss: 7.63904 Disc Loss: 0.0476813 Q Losses: [0.01448452, 0.13684836]\n",
      "epoch:1 batch_done:135 Gen Loss: 4.92718 Disc Loss: 0.0710191 Q Losses: [0.018173192, 0.12348898]\n",
      "epoch:1 batch_done:136 Gen Loss: 11.18 Disc Loss: 0.110777 Q Losses: [0.012769617, 0.17744882]\n",
      "epoch:1 batch_done:137 Gen Loss: 10.2434 Disc Loss: 0.077876 Q Losses: [0.012380727, 0.11238149]\n",
      "epoch:1 batch_done:138 Gen Loss: 8.2543 Disc Loss: 0.0928333 Q Losses: [0.013325686, 0.12722315]\n",
      "epoch:1 batch_done:139 Gen Loss: 6.89797 Disc Loss: 0.0343666 Q Losses: [0.016871294, 0.13049491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:140 Gen Loss: 6.52589 Disc Loss: 0.064535 Q Losses: [0.021165196, 0.13622178]\n",
      "epoch:1 batch_done:141 Gen Loss: 31.4293 Disc Loss: 0.356229 Q Losses: [0.019496107, 0.13823506]\n",
      "epoch:1 batch_done:142 Gen Loss: 31.3211 Disc Loss: 0.251205 Q Losses: [0.017552853, 0.12145579]\n",
      "epoch:1 batch_done:143 Gen Loss: 20.2397 Disc Loss: 0.701937 Q Losses: [0.014877701, 0.12607917]\n",
      "epoch:1 batch_done:144 Gen Loss: 5.46077 Disc Loss: 0.00900299 Q Losses: [0.014964756, 0.16517666]\n",
      "epoch:1 batch_done:145 Gen Loss: 43.4247 Disc Loss: 4.55544 Q Losses: [0.023899473, 0.17001411]\n",
      "epoch:1 batch_done:146 Gen Loss: 28.8302 Disc Loss: 3.27709 Q Losses: [0.018575443, 0.14639716]\n",
      "epoch:1 batch_done:147 Gen Loss: 12.9159 Disc Loss: 0.754349 Q Losses: [0.027956584, 0.14567956]\n",
      "epoch:1 batch_done:148 Gen Loss: 2.36265 Disc Loss: 0.340757 Q Losses: [0.018393341, 0.19793651]\n",
      "epoch:1 batch_done:149 Gen Loss: 13.8243 Disc Loss: 0.348788 Q Losses: [0.026354574, 0.16645341]\n",
      "epoch:1 batch_done:150 Gen Loss: 13.5911 Disc Loss: 0.362342 Q Losses: [0.015057183, 0.16058579]\n",
      "epoch:1 batch_done:151 Gen Loss: 7.32471 Disc Loss: 0.29989 Q Losses: [0.029984547, 0.14939351]\n",
      "epoch:1 batch_done:152 Gen Loss: 5.25961 Disc Loss: 0.133718 Q Losses: [0.017499726, 0.17367931]\n",
      "epoch:1 batch_done:153 Gen Loss: 26.2278 Disc Loss: 0.775195 Q Losses: [0.029257122, 0.13564746]\n",
      "epoch:1 batch_done:154 Gen Loss: 31.4109 Disc Loss: 0.571914 Q Losses: [0.020302823, 0.14741734]\n",
      "epoch:1 batch_done:155 Gen Loss: 26.9861 Disc Loss: 1.01499 Q Losses: [0.025890414, 0.13799942]\n",
      "epoch:1 batch_done:156 Gen Loss: 16.7182 Disc Loss: 1.1733 Q Losses: [0.016084272, 0.14650905]\n",
      "epoch:1 batch_done:157 Gen Loss: 2.56896 Disc Loss: 1.12285 Q Losses: [0.016456515, 0.143948]\n",
      "epoch:1 batch_done:158 Gen Loss: 45.4892 Disc Loss: 3.49046 Q Losses: [0.023753043, 0.14550558]\n",
      "epoch:1 batch_done:159 Gen Loss: 54.8095 Disc Loss: 1.53003 Q Losses: [0.022315603, 0.138221]\n",
      "epoch:1 batch_done:160 Gen Loss: 43.4 Disc Loss: 3.11829 Q Losses: [0.0099728499, 0.13021195]\n",
      "epoch:1 batch_done:161 Gen Loss: 25.124 Disc Loss: 1.13966 Q Losses: [0.019456754, 0.13627873]\n",
      "epoch:1 batch_done:162 Gen Loss: 9.26084 Disc Loss: 0.248219 Q Losses: [0.027094681, 0.12284195]\n",
      "epoch:1 batch_done:163 Gen Loss: 8.40608 Disc Loss: 0.342283 Q Losses: [0.02308137, 0.13252693]\n",
      "epoch:1 batch_done:164 Gen Loss: 7.12688 Disc Loss: 0.300486 Q Losses: [0.0090987859, 0.11634687]\n",
      "epoch:1 batch_done:165 Gen Loss: 13.8439 Disc Loss: 0.219321 Q Losses: [0.016966691, 0.12619151]\n",
      "epoch:1 batch_done:166 Gen Loss: 12.572 Disc Loss: 0.290071 Q Losses: [0.014650146, 0.11837325]\n",
      "epoch:1 batch_done:167 Gen Loss: 6.37966 Disc Loss: 0.701778 Q Losses: [0.019665442, 0.10160498]\n",
      "epoch:1 batch_done:168 Gen Loss: 37.1185 Disc Loss: 1.34181 Q Losses: [0.014516193, 0.1246237]\n",
      "epoch:1 batch_done:169 Gen Loss: 39.0422 Disc Loss: 2.18001 Q Losses: [0.020221295, 0.11900239]\n",
      "epoch:1 batch_done:170 Gen Loss: 32.4615 Disc Loss: 1.44669 Q Losses: [0.012166828, 0.13101439]\n",
      "epoch:1 batch_done:171 Gen Loss: 24.7993 Disc Loss: 0.375865 Q Losses: [0.012010332, 0.12391063]\n",
      "epoch:1 batch_done:172 Gen Loss: 15.3485 Disc Loss: 0.442518 Q Losses: [0.01850065, 0.12111714]\n",
      "epoch:1 batch_done:173 Gen Loss: 6.76106 Disc Loss: 0.0471699 Q Losses: [0.019420829, 0.1132957]\n",
      "epoch:1 batch_done:174 Gen Loss: 28.7166 Disc Loss: 1.00253 Q Losses: [0.013463235, 0.11318958]\n",
      "epoch:1 batch_done:175 Gen Loss: 35.1036 Disc Loss: 0.703759 Q Losses: [0.0081257736, 0.11629868]\n",
      "epoch:1 batch_done:176 Gen Loss: 31.1445 Disc Loss: 1.06094 Q Losses: [0.015646955, 0.11268957]\n",
      "epoch:1 batch_done:177 Gen Loss: 25.0739 Disc Loss: 0.264597 Q Losses: [0.013722692, 0.10847091]\n",
      "epoch:1 batch_done:178 Gen Loss: 17.9459 Disc Loss: 0.230171 Q Losses: [0.0094449185, 0.13421591]\n",
      "epoch:1 batch_done:179 Gen Loss: 9.93108 Disc Loss: 0.110514 Q Losses: [0.016831646, 0.12137483]\n",
      "epoch:1 batch_done:180 Gen Loss: 5.4723 Disc Loss: 0.0693718 Q Losses: [0.012328774, 0.14354311]\n",
      "epoch:1 batch_done:181 Gen Loss: 37.5759 Disc Loss: 1.32756 Q Losses: [0.015974931, 0.10947821]\n",
      "epoch:1 batch_done:182 Gen Loss: 37.9519 Disc Loss: 3.55793 Q Losses: [0.0090038748, 0.11368406]\n",
      "epoch:1 batch_done:183 Gen Loss: 27.5832 Disc Loss: 2.16805 Q Losses: [0.017630262, 0.12349467]\n",
      "epoch:1 batch_done:184 Gen Loss: 17.7097 Disc Loss: 0.150365 Q Losses: [0.015168259, 0.12713924]\n",
      "epoch:1 batch_done:185 Gen Loss: 9.69285 Disc Loss: 0.00520845 Q Losses: [0.018793136, 0.11491054]\n",
      "epoch:1 batch_done:186 Gen Loss: 4.14977 Disc Loss: 0.0346184 Q Losses: [0.024865467, 0.10657355]\n",
      "epoch:1 batch_done:187 Gen Loss: 17.952 Disc Loss: 0.318959 Q Losses: [0.0089684837, 0.11100179]\n",
      "epoch:1 batch_done:188 Gen Loss: 19.7721 Disc Loss: 0.118612 Q Losses: [0.012475829, 0.099353828]\n",
      "epoch:1 batch_done:189 Gen Loss: 16.0686 Disc Loss: 0.247022 Q Losses: [0.017418526, 0.099929467]\n",
      "epoch:1 batch_done:190 Gen Loss: 11.9657 Disc Loss: 0.0933505 Q Losses: [0.01510197, 0.10223311]\n",
      "epoch:1 batch_done:191 Gen Loss: 6.10288 Disc Loss: 0.00956673 Q Losses: [0.0308658, 0.10386702]\n",
      "epoch:1 batch_done:192 Gen Loss: 13.4562 Disc Loss: 0.269641 Q Losses: [0.011288471, 0.10848116]\n",
      "epoch:1 batch_done:193 Gen Loss: 14.2769 Disc Loss: 0.183963 Q Losses: [0.011538309, 0.090724498]\n",
      "epoch:1 batch_done:194 Gen Loss: 9.12209 Disc Loss: 0.680397 Q Losses: [0.012701348, 0.091008469]\n",
      "epoch:1 batch_done:195 Gen Loss: 6.42009 Disc Loss: 0.26848 Q Losses: [0.011153153, 0.091261372]\n",
      "epoch:1 batch_done:196 Gen Loss: 4.56169 Disc Loss: 0.00544416 Q Losses: [0.012397349, 0.10366288]\n",
      "epoch:1 batch_done:197 Gen Loss: 4.1436 Disc Loss: 0.153091 Q Losses: [0.017304964, 0.10195629]\n",
      "epoch:1 batch_done:198 Gen Loss: 27.2199 Disc Loss: 0.939423 Q Losses: [0.01748256, 0.11866626]\n",
      "epoch:1 batch_done:199 Gen Loss: 30.3469 Disc Loss: 1.12987 Q Losses: [0.01515093, 0.088992387]\n",
      "epoch:1 batch_done:200 Gen Loss: 23.1932 Disc Loss: 1.36327 Q Losses: [0.011080923, 0.095613874]\n",
      "epoch:1 batch_done:201 Gen Loss: 15.4588 Disc Loss: 0.547406 Q Losses: [0.016730007, 0.08646284]\n",
      "epoch:1 batch_done:202 Gen Loss: 11.2174 Disc Loss: 0.0951986 Q Losses: [0.012971203, 0.094312787]\n",
      "epoch:1 batch_done:203 Gen Loss: 8.00132 Disc Loss: 0.000181168 Q Losses: [0.011816428, 0.086240776]\n",
      "epoch:1 batch_done:204 Gen Loss: 4.87902 Disc Loss: 0.00283133 Q Losses: [0.013242105, 0.086100057]\n",
      "epoch:1 batch_done:205 Gen Loss: 5.70758 Disc Loss: 0.0531676 Q Losses: [0.0099197067, 0.086280495]\n",
      "epoch:1 batch_done:206 Gen Loss: 9.33786 Disc Loss: 0.000354484 Q Losses: [0.013239393, 0.09316054]\n",
      "epoch:1 batch_done:207 Gen Loss: 11.9906 Disc Loss: 0.215175 Q Losses: [0.026927425, 0.098836944]\n",
      "epoch:2 batch_done:1 Gen Loss: 17.2851 Disc Loss: 0.00126393 Q Losses: [0.017905377, 0.10650656]\n",
      "epoch:2 batch_done:2 Gen Loss: 12.9065 Disc Loss: 0.0631503 Q Losses: [0.010433709, 0.085518338]\n",
      "epoch:2 batch_done:3 Gen Loss: 10.2356 Disc Loss: 0.0417302 Q Losses: [0.021158502, 0.096183389]\n",
      "epoch:2 batch_done:4 Gen Loss: 10.9709 Disc Loss: 0.272794 Q Losses: [0.014040064, 0.093264624]\n",
      "epoch:2 batch_done:5 Gen Loss: 12.8935 Disc Loss: 0.0854415 Q Losses: [0.01627492, 0.10105576]\n",
      "epoch:2 batch_done:6 Gen Loss: 15.8425 Disc Loss: 0.0496657 Q Losses: [0.010134669, 0.074643552]\n",
      "epoch:2 batch_done:7 Gen Loss: 7.59159 Disc Loss: 0.0107806 Q Losses: [0.016121909, 0.087587424]\n",
      "epoch:2 batch_done:8 Gen Loss: 7.64443 Disc Loss: 0.0205564 Q Losses: [0.0079793613, 0.1131328]\n",
      "epoch:2 batch_done:9 Gen Loss: 4.91343 Disc Loss: 0.0848894 Q Losses: [0.012499405, 0.091995671]\n",
      "epoch:2 batch_done:10 Gen Loss: 35.179 Disc Loss: 2.07111 Q Losses: [0.01219658, 0.084739342]\n",
      "epoch:2 batch_done:11 Gen Loss: 36.6919 Disc Loss: 3.02963 Q Losses: [0.019286774, 0.093305342]\n",
      "epoch:2 batch_done:12 Gen Loss: 31.3145 Disc Loss: 0.393286 Q Losses: [0.016628388, 0.096226975]\n",
      "epoch:2 batch_done:13 Gen Loss: 23.8424 Disc Loss: 0.0965744 Q Losses: [0.015061127, 0.082812458]\n",
      "epoch:2 batch_done:14 Gen Loss: 15.9331 Disc Loss: 0.0315589 Q Losses: [0.020288115, 0.090660505]\n",
      "epoch:2 batch_done:15 Gen Loss: 10.0253 Disc Loss: 0.0122007 Q Losses: [0.013178667, 0.10467497]\n",
      "epoch:2 batch_done:16 Gen Loss: 5.30845 Disc Loss: 0.00479558 Q Losses: [0.011856971, 0.098351404]\n",
      "epoch:2 batch_done:17 Gen Loss: 6.66742 Disc Loss: 0.0950035 Q Losses: [0.01128974, 0.084374592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:18 Gen Loss: 7.66352 Disc Loss: 0.00571887 Q Losses: [0.033603914, 0.084162414]\n",
      "epoch:2 batch_done:19 Gen Loss: 8.14763 Disc Loss: 0.00698735 Q Losses: [0.012397666, 0.090187132]\n",
      "epoch:2 batch_done:20 Gen Loss: 7.70699 Disc Loss: 0.0211296 Q Losses: [0.014518169, 0.093572713]\n",
      "epoch:2 batch_done:21 Gen Loss: 6.67673 Disc Loss: 0.00234582 Q Losses: [0.013410295, 0.087158956]\n",
      "epoch:2 batch_done:22 Gen Loss: 6.11695 Disc Loss: 0.0128483 Q Losses: [0.013497517, 0.094188452]\n",
      "epoch:2 batch_done:23 Gen Loss: 6.0416 Disc Loss: 0.0161558 Q Losses: [0.0097863153, 0.085599765]\n",
      "epoch:2 batch_done:24 Gen Loss: 7.88623 Disc Loss: 0.0050156 Q Losses: [0.014220238, 0.093529105]\n",
      "epoch:2 batch_done:25 Gen Loss: 7.52377 Disc Loss: 0.0142872 Q Losses: [0.019084536, 0.080991343]\n",
      "epoch:2 batch_done:26 Gen Loss: 4.8878 Disc Loss: 0.0225304 Q Losses: [0.0077832402, 0.081772283]\n",
      "epoch:2 batch_done:27 Gen Loss: 6.56476 Disc Loss: 0.0718586 Q Losses: [0.011044593, 0.086178318]\n",
      "epoch:2 batch_done:28 Gen Loss: 10.2808 Disc Loss: 0.125356 Q Losses: [0.010081979, 0.072199821]\n",
      "epoch:2 batch_done:29 Gen Loss: 11.9132 Disc Loss: 0.160137 Q Losses: [0.010199394, 0.078029491]\n",
      "epoch:2 batch_done:30 Gen Loss: 10.9426 Disc Loss: 0.0900555 Q Losses: [0.011606022, 0.074566573]\n",
      "epoch:2 batch_done:31 Gen Loss: 7.38793 Disc Loss: 0.0312144 Q Losses: [0.011877753, 0.082493268]\n",
      "epoch:2 batch_done:32 Gen Loss: 11.5542 Disc Loss: 0.0133155 Q Losses: [0.012780393, 0.082857408]\n",
      "epoch:2 batch_done:33 Gen Loss: 13.3238 Disc Loss: 0.252464 Q Losses: [0.012417898, 0.083274499]\n",
      "epoch:2 batch_done:34 Gen Loss: 17.4407 Disc Loss: 0.165858 Q Losses: [0.0169404, 0.077474147]\n",
      "epoch:2 batch_done:35 Gen Loss: 9.69696 Disc Loss: 0.0556199 Q Losses: [0.021487452, 0.075665846]\n",
      "epoch:2 batch_done:36 Gen Loss: 10.6501 Disc Loss: 0.11888 Q Losses: [0.0091988239, 0.077904142]\n",
      "epoch:2 batch_done:37 Gen Loss: 14.153 Disc Loss: 0.0470694 Q Losses: [0.0078274347, 0.073479861]\n",
      "epoch:2 batch_done:38 Gen Loss: 10.7148 Disc Loss: 0.165964 Q Losses: [0.0064605009, 0.074065015]\n",
      "epoch:2 batch_done:39 Gen Loss: 14.8841 Disc Loss: 0.0145251 Q Losses: [0.01314861, 0.085812911]\n",
      "epoch:2 batch_done:40 Gen Loss: 11.3297 Disc Loss: 0.0304002 Q Losses: [0.012705953, 0.079493999]\n",
      "epoch:2 batch_done:41 Gen Loss: 17.2057 Disc Loss: 0.0541156 Q Losses: [0.0085333558, 0.076376952]\n",
      "epoch:2 batch_done:42 Gen Loss: 5.37728 Disc Loss: 0.0222534 Q Losses: [0.0093002925, 0.082227424]\n",
      "epoch:2 batch_done:43 Gen Loss: 16.8047 Disc Loss: 0.00327631 Q Losses: [0.012777949, 0.076054513]\n",
      "epoch:2 batch_done:44 Gen Loss: 5.25665 Disc Loss: 0.034409 Q Losses: [0.010061545, 0.071546055]\n",
      "epoch:2 batch_done:45 Gen Loss: 13.5329 Disc Loss: 0.017361 Q Losses: [0.010386208, 0.076019749]\n",
      "epoch:2 batch_done:46 Gen Loss: 25.987 Disc Loss: 0.673152 Q Losses: [0.0069721527, 0.073392071]\n",
      "epoch:2 batch_done:47 Gen Loss: 32.3178 Disc Loss: 1.75016 Q Losses: [0.0064305444, 0.086271949]\n",
      "epoch:2 batch_done:48 Gen Loss: 24.1256 Disc Loss: 0.326548 Q Losses: [0.0068858401, 0.066681266]\n",
      "epoch:2 batch_done:49 Gen Loss: 25.0545 Disc Loss: 0.00500201 Q Losses: [0.028616253, 0.073791347]\n",
      "epoch:2 batch_done:50 Gen Loss: 29.2019 Disc Loss: 0.000422551 Q Losses: [0.011874769, 0.077725075]\n",
      "epoch:2 batch_done:51 Gen Loss: 20.4823 Disc Loss: 0.00547077 Q Losses: [0.012242486, 0.073488951]\n",
      "epoch:2 batch_done:52 Gen Loss: 24.486 Disc Loss: 0.00396074 Q Losses: [0.0083272364, 0.075744666]\n",
      "epoch:2 batch_done:53 Gen Loss: 14.7092 Disc Loss: 0.000103333 Q Losses: [0.014311055, 0.071265541]\n",
      "epoch:2 batch_done:54 Gen Loss: 13.9753 Disc Loss: 0.000182962 Q Losses: [0.01634565, 0.079986945]\n",
      "epoch:2 batch_done:55 Gen Loss: 14.9755 Disc Loss: 1.13212e-05 Q Losses: [0.012174388, 0.090312518]\n",
      "epoch:2 batch_done:56 Gen Loss: 5.5597 Disc Loss: 0.00631511 Q Losses: [0.013002166, 0.08792749]\n",
      "epoch:2 batch_done:57 Gen Loss: 5.14848 Disc Loss: 0.0627994 Q Losses: [0.0092094215, 0.06924165]\n",
      "epoch:2 batch_done:58 Gen Loss: 18.2845 Disc Loss: 0.00747905 Q Losses: [0.010110191, 0.083820641]\n",
      "epoch:2 batch_done:59 Gen Loss: 5.93317 Disc Loss: 0.00764518 Q Losses: [0.0094956048, 0.083227672]\n",
      "epoch:2 batch_done:60 Gen Loss: 20.9359 Disc Loss: 0.00625029 Q Losses: [0.015561868, 0.075318158]\n",
      "epoch:2 batch_done:61 Gen Loss: 13.9055 Disc Loss: 0.000412504 Q Losses: [0.01077627, 0.083427109]\n",
      "epoch:2 batch_done:62 Gen Loss: 12.2087 Disc Loss: 0.150809 Q Losses: [0.011038346, 0.079552799]\n",
      "epoch:2 batch_done:63 Gen Loss: 16.8535 Disc Loss: 0.00257945 Q Losses: [0.014224869, 0.070228271]\n",
      "epoch:2 batch_done:64 Gen Loss: 17.691 Disc Loss: 0.0730481 Q Losses: [0.01224809, 0.074451454]\n",
      "epoch:2 batch_done:65 Gen Loss: 8.64871 Disc Loss: 0.139406 Q Losses: [0.013370562, 0.071058132]\n",
      "epoch:2 batch_done:66 Gen Loss: 6.04802 Disc Loss: 0.0879135 Q Losses: [0.009841877, 0.086491138]\n",
      "epoch:2 batch_done:67 Gen Loss: 5.55515 Disc Loss: 0.138143 Q Losses: [0.013082387, 0.080372944]\n",
      "epoch:2 batch_done:68 Gen Loss: 5.42229 Disc Loss: 0.0107922 Q Losses: [0.012115151, 0.079809569]\n",
      "epoch:2 batch_done:69 Gen Loss: 12.8827 Disc Loss: 0.120641 Q Losses: [0.0094240662, 0.067815095]\n",
      "epoch:2 batch_done:70 Gen Loss: 15.866 Disc Loss: 0.034154 Q Losses: [0.0089255217, 0.075043485]\n",
      "epoch:2 batch_done:71 Gen Loss: 11.942 Disc Loss: 0.067061 Q Losses: [0.010333014, 0.076719999]\n",
      "epoch:2 batch_done:72 Gen Loss: 11.9599 Disc Loss: 0.00687849 Q Losses: [0.015364395, 0.073003002]\n",
      "epoch:2 batch_done:73 Gen Loss: 10.3919 Disc Loss: 0.00224959 Q Losses: [0.011334826, 0.08025077]\n",
      "epoch:2 batch_done:74 Gen Loss: 8.58764 Disc Loss: 0.00444785 Q Losses: [0.011290737, 0.076084629]\n",
      "epoch:2 batch_done:75 Gen Loss: 6.53558 Disc Loss: 0.00323245 Q Losses: [0.014747089, 0.065772593]\n",
      "epoch:2 batch_done:76 Gen Loss: 5.01741 Disc Loss: 0.0287971 Q Losses: [0.017582197, 0.073206209]\n",
      "epoch:2 batch_done:77 Gen Loss: 5.34006 Disc Loss: 0.0264594 Q Losses: [0.013933369, 0.068779483]\n",
      "epoch:2 batch_done:78 Gen Loss: 6.96548 Disc Loss: 0.0819861 Q Losses: [0.009816411, 0.075922295]\n",
      "epoch:2 batch_done:79 Gen Loss: 7.06678 Disc Loss: 0.0832898 Q Losses: [0.012147196, 0.08099097]\n",
      "epoch:2 batch_done:80 Gen Loss: 18.036 Disc Loss: 0.277576 Q Losses: [0.014271305, 0.064219132]\n",
      "epoch:2 batch_done:81 Gen Loss: 16.8262 Disc Loss: 0.784797 Q Losses: [0.0094996598, 0.082338214]\n",
      "epoch:2 batch_done:82 Gen Loss: 15.3575 Disc Loss: 0.191936 Q Losses: [0.020826925, 0.068882234]\n",
      "epoch:2 batch_done:83 Gen Loss: 18.2498 Disc Loss: 0.00805401 Q Losses: [0.01073141, 0.10398699]\n",
      "epoch:2 batch_done:84 Gen Loss: 6.58538 Disc Loss: 0.00248023 Q Losses: [0.011276255, 0.065224707]\n",
      "epoch:2 batch_done:85 Gen Loss: 6.44327 Disc Loss: 0.00146767 Q Losses: [0.007692473, 0.064563259]\n",
      "epoch:2 batch_done:86 Gen Loss: 18.6767 Disc Loss: 0.00152246 Q Losses: [0.014381992, 0.085977875]\n",
      "epoch:2 batch_done:87 Gen Loss: 5.76723 Disc Loss: 0.00404067 Q Losses: [0.013196877, 0.07776925]\n",
      "epoch:2 batch_done:88 Gen Loss: 19.6729 Disc Loss: 0.00022959 Q Losses: [0.0075495504, 0.07414563]\n",
      "epoch:2 batch_done:89 Gen Loss: 5.51374 Disc Loss: 0.0230643 Q Losses: [0.012952841, 0.070642345]\n",
      "epoch:2 batch_done:90 Gen Loss: 19.2109 Disc Loss: 2.93407e-05 Q Losses: [0.016154058, 0.066580221]\n",
      "epoch:2 batch_done:91 Gen Loss: 11.8288 Disc Loss: 0.0260531 Q Losses: [0.011834919, 0.066891328]\n",
      "epoch:2 batch_done:92 Gen Loss: 9.13264 Disc Loss: 0.000486548 Q Losses: [0.018141411, 0.074117079]\n",
      "epoch:2 batch_done:93 Gen Loss: 9.60823 Disc Loss: 0.000425521 Q Losses: [0.015382124, 0.063164264]\n",
      "epoch:2 batch_done:94 Gen Loss: 5.88201 Disc Loss: 0.00499206 Q Losses: [0.00807775, 0.062748797]\n",
      "epoch:2 batch_done:95 Gen Loss: 5.74593 Disc Loss: 0.00874709 Q Losses: [0.010635756, 0.06117171]\n",
      "epoch:2 batch_done:96 Gen Loss: 7.86258 Disc Loss: 0.000667316 Q Losses: [0.0073091974, 0.068043143]\n",
      "epoch:2 batch_done:97 Gen Loss: 6.52636 Disc Loss: 0.0495151 Q Losses: [0.012517437, 0.06700848]\n",
      "epoch:2 batch_done:98 Gen Loss: 8.58656 Disc Loss: 0.00195684 Q Losses: [0.011153548, 0.058510553]\n",
      "epoch:2 batch_done:99 Gen Loss: 7.9843 Disc Loss: 0.00229666 Q Losses: [0.0092247324, 0.062138535]\n",
      "epoch:2 batch_done:100 Gen Loss: 7.12996 Disc Loss: 0.00213736 Q Losses: [0.011615358, 0.072064877]\n",
      "epoch:2 batch_done:101 Gen Loss: 8.61532 Disc Loss: 0.00785313 Q Losses: [0.00961826, 0.062471561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:102 Gen Loss: 5.8159 Disc Loss: 0.0100949 Q Losses: [0.01013051, 0.065195926]\n",
      "epoch:2 batch_done:103 Gen Loss: 5.6651 Disc Loss: 0.0353024 Q Losses: [0.011847334, 0.056050096]\n",
      "epoch:2 batch_done:104 Gen Loss: 6.19319 Disc Loss: 0.0258813 Q Losses: [0.0096182656, 0.074596427]\n",
      "epoch:2 batch_done:105 Gen Loss: 9.29597 Disc Loss: 0.00256331 Q Losses: [0.0060238559, 0.064074159]\n",
      "epoch:2 batch_done:106 Gen Loss: 6.19791 Disc Loss: 0.0447447 Q Losses: [0.014490042, 0.061295748]\n",
      "epoch:2 batch_done:107 Gen Loss: 6.96555 Disc Loss: 0.0152945 Q Losses: [0.011128375, 0.061587501]\n",
      "epoch:2 batch_done:108 Gen Loss: 9.30985 Disc Loss: 0.0432671 Q Losses: [0.0062037827, 0.067384407]\n",
      "epoch:2 batch_done:109 Gen Loss: 8.74903 Disc Loss: 0.00234937 Q Losses: [0.010520373, 0.061301179]\n",
      "epoch:2 batch_done:110 Gen Loss: 5.28264 Disc Loss: 0.0231897 Q Losses: [0.012834067, 0.069800392]\n",
      "epoch:2 batch_done:111 Gen Loss: 5.66935 Disc Loss: 0.024177 Q Losses: [0.0096957982, 0.060726587]\n",
      "epoch:2 batch_done:112 Gen Loss: 5.73281 Disc Loss: 0.0136359 Q Losses: [0.011083027, 0.086017087]\n",
      "epoch:2 batch_done:113 Gen Loss: 9.59947 Disc Loss: 0.0191845 Q Losses: [0.0097944187, 0.061790798]\n",
      "epoch:2 batch_done:114 Gen Loss: 6.29134 Disc Loss: 0.00396768 Q Losses: [0.013413563, 0.076101109]\n",
      "epoch:2 batch_done:115 Gen Loss: 9.88905 Disc Loss: 0.000941628 Q Losses: [0.010135187, 0.059956908]\n",
      "epoch:2 batch_done:116 Gen Loss: 25.2091 Disc Loss: 0.481735 Q Losses: [0.011560147, 0.070242323]\n",
      "epoch:2 batch_done:117 Gen Loss: 23.545 Disc Loss: 1.58966 Q Losses: [0.015379785, 0.057260856]\n",
      "epoch:2 batch_done:118 Gen Loss: 20.9222 Disc Loss: 0.0453347 Q Losses: [0.0096732089, 0.077893965]\n",
      "epoch:2 batch_done:119 Gen Loss: 21.7182 Disc Loss: 0.00129209 Q Losses: [0.0097724106, 0.062403239]\n",
      "epoch:2 batch_done:120 Gen Loss: 21.005 Disc Loss: 4.02773e-05 Q Losses: [0.013810095, 0.053216606]\n",
      "epoch:2 batch_done:121 Gen Loss: 20.8621 Disc Loss: 0.000919173 Q Losses: [0.0094119869, 0.064397238]\n",
      "epoch:2 batch_done:122 Gen Loss: 13.3444 Disc Loss: 0.038643 Q Losses: [0.011599801, 0.065252222]\n",
      "epoch:2 batch_done:123 Gen Loss: 12.1197 Disc Loss: 0.00862773 Q Losses: [0.010322548, 0.05948104]\n",
      "epoch:2 batch_done:124 Gen Loss: 11.4697 Disc Loss: 1.57471e-05 Q Losses: [0.011564272, 0.073745444]\n",
      "epoch:2 batch_done:125 Gen Loss: 11.3076 Disc Loss: 9.14103e-05 Q Losses: [0.012562528, 0.071143836]\n",
      "epoch:2 batch_done:126 Gen Loss: 5.36806 Disc Loss: 0.0116181 Q Losses: [0.025540698, 0.064608119]\n",
      "epoch:2 batch_done:127 Gen Loss: 11.2383 Disc Loss: 0.118694 Q Losses: [0.013171297, 0.068176419]\n",
      "epoch:2 batch_done:128 Gen Loss: 17.2156 Disc Loss: 0.00138489 Q Losses: [0.0073579066, 0.073273674]\n",
      "epoch:2 batch_done:129 Gen Loss: 18.9383 Disc Loss: 0.0670059 Q Losses: [0.010778779, 0.065147065]\n",
      "epoch:2 batch_done:130 Gen Loss: 15.1839 Disc Loss: 0.000158127 Q Losses: [0.012228355, 0.067174055]\n",
      "epoch:2 batch_done:131 Gen Loss: 12.1501 Disc Loss: 0.0414689 Q Losses: [0.022904748, 0.063775964]\n",
      "epoch:2 batch_done:132 Gen Loss: 10.6533 Disc Loss: 0.00524708 Q Losses: [0.012924279, 0.064128973]\n",
      "epoch:2 batch_done:133 Gen Loss: 12.3037 Disc Loss: 3.33838e-05 Q Losses: [0.0080778701, 0.063800238]\n",
      "epoch:2 batch_done:134 Gen Loss: 7.89953 Disc Loss: 0.00214546 Q Losses: [0.014895714, 0.064121649]\n",
      "epoch:2 batch_done:135 Gen Loss: 8.45104 Disc Loss: 0.00403343 Q Losses: [0.020884344, 0.076997004]\n",
      "epoch:2 batch_done:136 Gen Loss: 8.44632 Disc Loss: 0.0011599 Q Losses: [0.016401296, 0.06668815]\n",
      "epoch:2 batch_done:137 Gen Loss: 11.6417 Disc Loss: 0.00147786 Q Losses: [0.023303509, 0.063774325]\n",
      "epoch:2 batch_done:138 Gen Loss: 5.47887 Disc Loss: 0.0279667 Q Losses: [0.0077015655, 0.065594375]\n",
      "epoch:2 batch_done:139 Gen Loss: 7.84518 Disc Loss: 0.00569418 Q Losses: [0.011429965, 0.061882339]\n",
      "epoch:2 batch_done:140 Gen Loss: 10.2154 Disc Loss: 0.000496521 Q Losses: [0.01708637, 0.059615538]\n",
      "epoch:2 batch_done:141 Gen Loss: 5.79327 Disc Loss: 0.0325842 Q Losses: [0.013679096, 0.065061226]\n",
      "epoch:2 batch_done:142 Gen Loss: 6.3489 Disc Loss: 0.0248605 Q Losses: [0.011440124, 0.06019941]\n",
      "epoch:2 batch_done:143 Gen Loss: 5.92249 Disc Loss: 0.0179597 Q Losses: [0.021283098, 0.051942099]\n",
      "epoch:2 batch_done:144 Gen Loss: 7.7065 Disc Loss: 0.0178406 Q Losses: [0.012455935, 0.0624553]\n",
      "epoch:2 batch_done:145 Gen Loss: 10.1147 Disc Loss: 0.055388 Q Losses: [0.015129179, 0.059996136]\n",
      "epoch:2 batch_done:146 Gen Loss: 5.56476 Disc Loss: 0.0428946 Q Losses: [0.011336391, 0.0573068]\n",
      "epoch:2 batch_done:147 Gen Loss: 10.4469 Disc Loss: 0.00735967 Q Losses: [0.020228386, 0.053887784]\n",
      "epoch:2 batch_done:148 Gen Loss: 6.52913 Disc Loss: 0.00417174 Q Losses: [0.010235427, 0.055411302]\n",
      "epoch:2 batch_done:149 Gen Loss: 6.02927 Disc Loss: 0.00599191 Q Losses: [0.020119248, 0.053567432]\n",
      "epoch:2 batch_done:150 Gen Loss: 7.7572 Disc Loss: 0.00563846 Q Losses: [0.03478932, 0.054543406]\n",
      "epoch:2 batch_done:151 Gen Loss: 5.25076 Disc Loss: 0.0331573 Q Losses: [0.0069857831, 0.074450575]\n",
      "epoch:2 batch_done:152 Gen Loss: 5.80166 Disc Loss: 0.0443542 Q Losses: [0.018784266, 0.058122028]\n",
      "epoch:2 batch_done:153 Gen Loss: 7.06367 Disc Loss: 0.00377892 Q Losses: [0.00732505, 0.079990871]\n",
      "epoch:2 batch_done:154 Gen Loss: 6.71113 Disc Loss: 0.0446195 Q Losses: [0.028844107, 0.058945537]\n",
      "epoch:2 batch_done:155 Gen Loss: 6.48153 Disc Loss: 0.14558 Q Losses: [0.0099566774, 0.059629597]\n",
      "epoch:2 batch_done:156 Gen Loss: 4.84196 Disc Loss: 0.0156084 Q Losses: [0.021414855, 0.065461025]\n",
      "epoch:2 batch_done:157 Gen Loss: 33.3302 Disc Loss: 0.705875 Q Losses: [0.021074928, 0.060691804]\n",
      "epoch:2 batch_done:158 Gen Loss: 27.9688 Disc Loss: 2.60138 Q Losses: [0.0079984842, 0.059409842]\n",
      "epoch:2 batch_done:159 Gen Loss: 17.4477 Disc Loss: 0.573786 Q Losses: [0.013646902, 0.071269207]\n",
      "epoch:2 batch_done:160 Gen Loss: 9.48704 Disc Loss: 0.0315362 Q Losses: [0.012745369, 0.061429411]\n",
      "epoch:2 batch_done:161 Gen Loss: 4.54862 Disc Loss: 0.00110259 Q Losses: [0.014172012, 0.065639734]\n",
      "epoch:2 batch_done:162 Gen Loss: 5.16544 Disc Loss: 0.0413219 Q Losses: [0.010962091, 0.055085272]\n",
      "epoch:2 batch_done:163 Gen Loss: 7.53289 Disc Loss: 0.040084 Q Losses: [0.019114457, 0.073935136]\n",
      "epoch:2 batch_done:164 Gen Loss: 9.72104 Disc Loss: 0.00060642 Q Losses: [0.010150325, 0.062229291]\n",
      "epoch:2 batch_done:165 Gen Loss: 10.1883 Disc Loss: 0.00151673 Q Losses: [0.0079825558, 0.055199828]\n",
      "epoch:2 batch_done:166 Gen Loss: 9.00078 Disc Loss: 0.000829189 Q Losses: [0.0096728234, 0.061609793]\n",
      "epoch:2 batch_done:167 Gen Loss: 8.69985 Disc Loss: 0.0164455 Q Losses: [0.017806241, 0.070821628]\n",
      "epoch:2 batch_done:168 Gen Loss: 10.139 Disc Loss: 0.00666 Q Losses: [0.014027966, 0.062502429]\n",
      "epoch:2 batch_done:169 Gen Loss: 7.97785 Disc Loss: 0.00186091 Q Losses: [0.014286954, 0.057968587]\n",
      "epoch:2 batch_done:170 Gen Loss: 8.91086 Disc Loss: 0.0761714 Q Losses: [0.010616854, 0.062090836]\n",
      "epoch:2 batch_done:171 Gen Loss: 7.98032 Disc Loss: 0.0851799 Q Losses: [0.016040634, 0.055181134]\n",
      "epoch:2 batch_done:172 Gen Loss: 5.66722 Disc Loss: 0.0122201 Q Losses: [0.013049735, 0.049325898]\n",
      "epoch:2 batch_done:173 Gen Loss: 4.40762 Disc Loss: 0.184294 Q Losses: [0.018425897, 0.060222715]\n",
      "epoch:2 batch_done:174 Gen Loss: 5.44727 Disc Loss: 0.0193333 Q Losses: [0.017276298, 0.052654907]\n",
      "epoch:2 batch_done:175 Gen Loss: 10.9342 Disc Loss: 0.00575472 Q Losses: [0.013575183, 0.061122641]\n",
      "epoch:2 batch_done:176 Gen Loss: 6.35505 Disc Loss: 0.00334929 Q Losses: [0.011113537, 0.05115607]\n",
      "epoch:2 batch_done:177 Gen Loss: 14.3047 Disc Loss: 0.101641 Q Losses: [0.0064891865, 0.057386529]\n",
      "epoch:2 batch_done:178 Gen Loss: 13.7748 Disc Loss: 0.126672 Q Losses: [0.0084044905, 0.054187682]\n",
      "epoch:2 batch_done:179 Gen Loss: 11.9812 Disc Loss: 0.0135644 Q Losses: [0.0084828101, 0.056037039]\n",
      "epoch:2 batch_done:180 Gen Loss: 12.3789 Disc Loss: 0.101798 Q Losses: [0.013641272, 0.063970916]\n",
      "epoch:2 batch_done:181 Gen Loss: 11.4955 Disc Loss: 0.0060976 Q Losses: [0.0078057405, 0.06129697]\n",
      "epoch:2 batch_done:182 Gen Loss: 13.6087 Disc Loss: 0.0266925 Q Losses: [0.019882763, 0.063000903]\n",
      "epoch:2 batch_done:183 Gen Loss: 8.48769 Disc Loss: 0.00451741 Q Losses: [0.0083991801, 0.04812843]\n",
      "epoch:2 batch_done:184 Gen Loss: 5.1693 Disc Loss: 0.0131184 Q Losses: [0.01166082, 0.050634351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:185 Gen Loss: 11.9527 Disc Loss: 0.228394 Q Losses: [0.015870797, 0.050781239]\n",
      "epoch:2 batch_done:186 Gen Loss: 14.7004 Disc Loss: 0.00187173 Q Losses: [0.0079510882, 0.056459434]\n",
      "epoch:2 batch_done:187 Gen Loss: 17.6581 Disc Loss: 0.0306061 Q Losses: [0.014765312, 0.051025137]\n",
      "epoch:2 batch_done:188 Gen Loss: 13.34 Disc Loss: 0.0381761 Q Losses: [0.009813481, 0.055838518]\n",
      "epoch:2 batch_done:189 Gen Loss: 11.1989 Disc Loss: 0.0499833 Q Losses: [0.011255065, 0.060946025]\n",
      "epoch:2 batch_done:190 Gen Loss: 11.964 Disc Loss: 0.000992725 Q Losses: [0.011048739, 0.055656102]\n",
      "epoch:2 batch_done:191 Gen Loss: 14.6706 Disc Loss: 0.000226905 Q Losses: [0.0087483265, 0.047263965]\n",
      "epoch:2 batch_done:192 Gen Loss: 6.99915 Disc Loss: 0.0579903 Q Losses: [0.013312509, 0.052115008]\n",
      "epoch:2 batch_done:193 Gen Loss: 8.11141 Disc Loss: 0.00275051 Q Losses: [0.017880423, 0.050412752]\n",
      "epoch:2 batch_done:194 Gen Loss: 15.9983 Disc Loss: 0.00753713 Q Losses: [0.0086251982, 0.056513321]\n",
      "epoch:2 batch_done:195 Gen Loss: 6.94524 Disc Loss: 0.003732 Q Losses: [0.0079954397, 0.061286092]\n",
      "epoch:2 batch_done:196 Gen Loss: 6.62291 Disc Loss: 0.016293 Q Losses: [0.015971929, 0.052718066]\n",
      "epoch:2 batch_done:197 Gen Loss: 10.4219 Disc Loss: 0.0152948 Q Losses: [0.0087072533, 0.052438676]\n",
      "epoch:2 batch_done:198 Gen Loss: 15.2891 Disc Loss: 0.000476376 Q Losses: [0.010991876, 0.056331515]\n",
      "epoch:2 batch_done:199 Gen Loss: 6.22577 Disc Loss: 0.0478562 Q Losses: [0.0049659023, 0.05896455]\n",
      "epoch:2 batch_done:200 Gen Loss: 8.56649 Disc Loss: 0.065533 Q Losses: [0.010959576, 0.046014816]\n",
      "epoch:2 batch_done:201 Gen Loss: 18.3169 Disc Loss: 0.0106549 Q Losses: [0.0080922376, 0.047604248]\n",
      "epoch:2 batch_done:202 Gen Loss: 6.21951 Disc Loss: 0.00801267 Q Losses: [0.01791222, 0.051749371]\n",
      "epoch:2 batch_done:203 Gen Loss: 6.49266 Disc Loss: 0.00221352 Q Losses: [0.010826331, 0.052904814]\n",
      "epoch:2 batch_done:204 Gen Loss: 16.7132 Disc Loss: 0.00108136 Q Losses: [0.021355998, 0.054783866]\n",
      "epoch:2 batch_done:205 Gen Loss: 9.1868 Disc Loss: 0.106995 Q Losses: [0.011694003, 0.046711743]\n",
      "epoch:2 batch_done:206 Gen Loss: 19.259 Disc Loss: 0.0149546 Q Losses: [0.015396788, 0.051233452]\n",
      "epoch:2 batch_done:207 Gen Loss: 11.9104 Disc Loss: 0.0183731 Q Losses: [0.0089755636, 0.057911251]\n",
      "epoch:3 batch_done:1 Gen Loss: 17.9209 Disc Loss: 0.0560924 Q Losses: [0.0099778548, 0.055660196]\n",
      "epoch:3 batch_done:2 Gen Loss: 10.428 Disc Loss: 0.0131035 Q Losses: [0.0091116298, 0.06056124]\n",
      "epoch:3 batch_done:3 Gen Loss: 9.8123 Disc Loss: 0.00332046 Q Losses: [0.013318031, 0.049479645]\n",
      "epoch:3 batch_done:4 Gen Loss: 14.9074 Disc Loss: 0.00167943 Q Losses: [0.0089356638, 0.048910338]\n",
      "epoch:3 batch_done:5 Gen Loss: 9.46095 Disc Loss: 0.000273206 Q Losses: [0.010081451, 0.05143065]\n",
      "epoch:3 batch_done:6 Gen Loss: 6.28118 Disc Loss: 0.0023693 Q Losses: [0.0086122928, 0.057134226]\n",
      "epoch:3 batch_done:7 Gen Loss: 7.56188 Disc Loss: 0.000983645 Q Losses: [0.0088835247, 0.051095825]\n",
      "epoch:3 batch_done:8 Gen Loss: 16.8245 Disc Loss: 0.213378 Q Losses: [0.011409923, 0.04998377]\n",
      "epoch:3 batch_done:9 Gen Loss: 19.2406 Disc Loss: 0.102254 Q Losses: [0.0082942173, 0.054189041]\n",
      "epoch:3 batch_done:10 Gen Loss: 17.7903 Disc Loss: 0.112023 Q Losses: [0.0065527107, 0.049093977]\n",
      "epoch:3 batch_done:11 Gen Loss: 21.2616 Disc Loss: 0.00355226 Q Losses: [0.011169201, 0.055501476]\n",
      "epoch:3 batch_done:12 Gen Loss: 24.9738 Disc Loss: 0.00263636 Q Losses: [0.010133711, 0.054245003]\n",
      "epoch:3 batch_done:13 Gen Loss: 14.5552 Disc Loss: 0.00274623 Q Losses: [0.0064430921, 0.05243133]\n",
      "epoch:3 batch_done:14 Gen Loss: 21.4519 Disc Loss: 0.000436051 Q Losses: [0.0095748426, 0.063563257]\n",
      "epoch:3 batch_done:15 Gen Loss: 12.7343 Disc Loss: 0.00101965 Q Losses: [0.011120614, 0.052167527]\n",
      "epoch:3 batch_done:16 Gen Loss: 11.2022 Disc Loss: 0.000571278 Q Losses: [0.016926125, 0.048799869]\n",
      "epoch:3 batch_done:17 Gen Loss: 14.0366 Disc Loss: 0.000550209 Q Losses: [0.01573465, 0.055689603]\n",
      "epoch:3 batch_done:18 Gen Loss: 8.12836 Disc Loss: 0.00126804 Q Losses: [0.015356198, 0.049138092]\n",
      "epoch:3 batch_done:19 Gen Loss: 9.09064 Disc Loss: 0.00249701 Q Losses: [0.010406596, 0.050256062]\n",
      "epoch:3 batch_done:20 Gen Loss: 8.88244 Disc Loss: 0.000891967 Q Losses: [0.021323893, 0.057073254]\n",
      "epoch:3 batch_done:21 Gen Loss: 7.72025 Disc Loss: 0.00988643 Q Losses: [0.011355361, 0.06263414]\n",
      "epoch:3 batch_done:22 Gen Loss: 16.8149 Disc Loss: 0.000673403 Q Losses: [0.018226855, 0.046673041]\n",
      "epoch:3 batch_done:23 Gen Loss: 6.46523 Disc Loss: 0.0267303 Q Losses: [0.012307331, 0.047825068]\n",
      "epoch:3 batch_done:24 Gen Loss: 6.57389 Disc Loss: 0.00230738 Q Losses: [0.02008871, 0.046189494]\n",
      "epoch:3 batch_done:25 Gen Loss: 10.7373 Disc Loss: 0.0215334 Q Losses: [0.017326113, 0.050390314]\n",
      "epoch:3 batch_done:26 Gen Loss: 8.3736 Disc Loss: 0.0655448 Q Losses: [0.0097424388, 0.051068854]\n",
      "epoch:3 batch_done:27 Gen Loss: 19.8937 Disc Loss: 0.00956709 Q Losses: [0.0051292479, 0.041473225]\n",
      "epoch:3 batch_done:28 Gen Loss: 10.1944 Disc Loss: 0.0303402 Q Losses: [0.015292282, 0.049058698]\n",
      "epoch:3 batch_done:29 Gen Loss: 16.3048 Disc Loss: 0.00605517 Q Losses: [0.015439535, 0.056855358]\n",
      "epoch:3 batch_done:30 Gen Loss: 6.02899 Disc Loss: 0.00953809 Q Losses: [0.0090527479, 0.043089353]\n",
      "epoch:3 batch_done:31 Gen Loss: 5.94631 Disc Loss: 0.0100027 Q Losses: [0.013567129, 0.055351533]\n",
      "epoch:3 batch_done:32 Gen Loss: 9.21283 Disc Loss: 0.000379474 Q Losses: [0.0096203052, 0.048352621]\n",
      "epoch:3 batch_done:33 Gen Loss: 54.7455 Disc Loss: 3.09324 Q Losses: [0.0095287748, 0.053880818]\n",
      "epoch:3 batch_done:34 Gen Loss: 37.9811 Disc Loss: 4.48282 Q Losses: [0.010171909, 0.047268629]\n",
      "epoch:3 batch_done:35 Gen Loss: 21.4232 Disc Loss: 0.829323 Q Losses: [0.012217853, 0.055113506]\n",
      "epoch:3 batch_done:36 Gen Loss: 10.7204 Disc Loss: 0.103674 Q Losses: [0.013877426, 0.054833338]\n",
      "epoch:3 batch_done:37 Gen Loss: 3.24315 Disc Loss: 0.0347245 Q Losses: [0.016872827, 0.066706225]\n",
      "epoch:3 batch_done:38 Gen Loss: 36.8513 Disc Loss: 3.04466 Q Losses: [0.009809643, 0.065922476]\n",
      "epoch:3 batch_done:39 Gen Loss: 41.2233 Disc Loss: 2.22251 Q Losses: [0.016289044, 0.060394708]\n",
      "epoch:3 batch_done:40 Gen Loss: 32.2817 Disc Loss: 1.09743 Q Losses: [0.014248047, 0.078443766]\n",
      "epoch:3 batch_done:41 Gen Loss: 14.062 Disc Loss: 1.05853 Q Losses: [0.01239544, 0.082480945]\n",
      "epoch:3 batch_done:42 Gen Loss: 2.26736 Disc Loss: 0.0302918 Q Losses: [0.023337094, 0.074895889]\n",
      "epoch:3 batch_done:43 Gen Loss: 37.5634 Disc Loss: 1.95991 Q Losses: [0.023758497, 0.059888303]\n",
      "epoch:3 batch_done:44 Gen Loss: 39.7942 Disc Loss: 3.82205 Q Losses: [0.016327459, 0.077719323]\n",
      "epoch:3 batch_done:45 Gen Loss: 24.0772 Disc Loss: 4.88946 Q Losses: [0.023451436, 0.054115918]\n",
      "epoch:3 batch_done:46 Gen Loss: 11.8057 Disc Loss: 0.619936 Q Losses: [0.0099018253, 0.067185372]\n",
      "epoch:3 batch_done:47 Gen Loss: 4.6983 Disc Loss: 0.0349024 Q Losses: [0.008303185, 0.054799426]\n",
      "epoch:3 batch_done:48 Gen Loss: 5.66862 Disc Loss: 0.186919 Q Losses: [0.0096438443, 0.052979209]\n",
      "epoch:3 batch_done:49 Gen Loss: 10.7589 Disc Loss: 0.189582 Q Losses: [0.012657607, 0.050406352]\n",
      "epoch:3 batch_done:50 Gen Loss: 11.0671 Disc Loss: 0.00647185 Q Losses: [0.011465014, 0.05152522]\n",
      "epoch:3 batch_done:51 Gen Loss: 7.02372 Disc Loss: 0.252692 Q Losses: [0.013090611, 0.049019367]\n",
      "epoch:3 batch_done:52 Gen Loss: 11.628 Disc Loss: 0.340414 Q Losses: [0.012071891, 0.055208735]\n",
      "epoch:3 batch_done:53 Gen Loss: 8.79399 Disc Loss: 0.604189 Q Losses: [0.012384977, 0.052878525]\n",
      "epoch:3 batch_done:54 Gen Loss: 3.64517 Disc Loss: 0.339775 Q Losses: [0.012743514, 0.047536086]\n",
      "epoch:3 batch_done:55 Gen Loss: 15.0632 Disc Loss: 0.348836 Q Losses: [0.0091449041, 0.046497911]\n",
      "epoch:3 batch_done:56 Gen Loss: 14.7085 Disc Loss: 0.320116 Q Losses: [0.009680545, 0.045170769]\n",
      "epoch:3 batch_done:57 Gen Loss: 10.3639 Disc Loss: 0.22191 Q Losses: [0.0069773123, 0.041535776]\n",
      "epoch:3 batch_done:58 Gen Loss: 5.71036 Disc Loss: 0.149716 Q Losses: [0.011572342, 0.041942723]\n",
      "epoch:3 batch_done:59 Gen Loss: 4.62699 Disc Loss: 0.0725534 Q Losses: [0.018999944, 0.037776627]\n",
      "epoch:3 batch_done:60 Gen Loss: 12.0579 Disc Loss: 0.165347 Q Losses: [0.0093110669, 0.045162983]\n",
      "epoch:3 batch_done:61 Gen Loss: 11.4659 Disc Loss: 0.367403 Q Losses: [0.010759319, 0.042257123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 batch_done:62 Gen Loss: 5.62501 Disc Loss: 0.27382 Q Losses: [0.0086430823, 0.039691634]\n",
      "epoch:3 batch_done:63 Gen Loss: 14.8433 Disc Loss: 0.252616 Q Losses: [0.0080079753, 0.035510574]\n",
      "epoch:3 batch_done:64 Gen Loss: 15.7108 Disc Loss: 0.309851 Q Losses: [0.0070596719, 0.037753664]\n",
      "epoch:3 batch_done:65 Gen Loss: 13.7083 Disc Loss: 0.241324 Q Losses: [0.012009991, 0.047778957]\n",
      "epoch:3 batch_done:66 Gen Loss: 9.17846 Disc Loss: 0.163318 Q Losses: [0.010497994, 0.04219304]\n",
      "epoch:3 batch_done:67 Gen Loss: 5.43872 Disc Loss: 0.0307411 Q Losses: [0.005250636, 0.040704161]\n",
      "epoch:3 batch_done:68 Gen Loss: 6.32415 Disc Loss: 0.0676628 Q Losses: [0.0091439746, 0.041990265]\n",
      "epoch:3 batch_done:69 Gen Loss: 8.41848 Disc Loss: 0.0074601 Q Losses: [0.010469358, 0.042811066]\n",
      "epoch:3 batch_done:70 Gen Loss: 6.32707 Disc Loss: 0.0864732 Q Losses: [0.0065669911, 0.035396803]\n",
      "epoch:3 batch_done:71 Gen Loss: 6.80778 Disc Loss: 0.0251988 Q Losses: [0.011933888, 0.047803216]\n",
      "epoch:3 batch_done:72 Gen Loss: 6.36804 Disc Loss: 0.0616437 Q Losses: [0.011426019, 0.039722361]\n",
      "epoch:3 batch_done:73 Gen Loss: 12.1007 Disc Loss: 0.151659 Q Losses: [0.0080638044, 0.041643769]\n",
      "epoch:3 batch_done:74 Gen Loss: 13.9953 Disc Loss: 0.00989625 Q Losses: [0.013875092, 0.042573471]\n",
      "epoch:3 batch_done:75 Gen Loss: 10.3256 Disc Loss: 0.269389 Q Losses: [0.012409896, 0.037218049]\n",
      "epoch:3 batch_done:76 Gen Loss: 7.24259 Disc Loss: 0.00480735 Q Losses: [0.012403561, 0.039425541]\n",
      "epoch:3 batch_done:77 Gen Loss: 4.62861 Disc Loss: 0.0496813 Q Losses: [0.0083177518, 0.046856828]\n",
      "epoch:3 batch_done:78 Gen Loss: 9.54687 Disc Loss: 0.109809 Q Losses: [0.0086100306, 0.045586016]\n",
      "epoch:3 batch_done:79 Gen Loss: 15.6136 Disc Loss: 0.0616221 Q Losses: [0.018422885, 0.043847658]\n",
      "epoch:3 batch_done:80 Gen Loss: 9.66755 Disc Loss: 0.074902 Q Losses: [0.011212602, 0.0480785]\n",
      "epoch:3 batch_done:81 Gen Loss: 17.5193 Disc Loss: 0.00936488 Q Losses: [0.013007494, 0.039855156]\n",
      "epoch:3 batch_done:82 Gen Loss: 9.18854 Disc Loss: 0.174154 Q Losses: [0.007618174, 0.041066535]\n",
      "epoch:3 batch_done:83 Gen Loss: 4.6633 Disc Loss: 0.0224956 Q Losses: [0.019430049, 0.036838118]\n",
      "epoch:3 batch_done:84 Gen Loss: 22.9154 Disc Loss: 0.424839 Q Losses: [0.0088180834, 0.036712661]\n",
      "epoch:3 batch_done:85 Gen Loss: 26.3463 Disc Loss: 0.305289 Q Losses: [0.0090726651, 0.033769157]\n",
      "epoch:3 batch_done:86 Gen Loss: 24.4683 Disc Loss: 0.28464 Q Losses: [0.0090022283, 0.044666089]\n",
      "epoch:3 batch_done:87 Gen Loss: 22.104 Disc Loss: 0.0377954 Q Losses: [0.013247368, 0.035295833]\n",
      "epoch:3 batch_done:88 Gen Loss: 19.8894 Disc Loss: 0.00806638 Q Losses: [0.0074899993, 0.043239087]\n",
      "epoch:3 batch_done:89 Gen Loss: 16.6231 Disc Loss: 0.00170402 Q Losses: [0.010622973, 0.048711695]\n",
      "epoch:3 batch_done:90 Gen Loss: 15.9929 Disc Loss: 0.000276865 Q Losses: [0.0056529506, 0.039599396]\n",
      "epoch:3 batch_done:91 Gen Loss: 17.7969 Disc Loss: 0.000931698 Q Losses: [0.011415003, 0.042963415]\n",
      "epoch:3 batch_done:92 Gen Loss: 13.6157 Disc Loss: 0.000137323 Q Losses: [0.012879364, 0.041865773]\n",
      "epoch:3 batch_done:93 Gen Loss: 9.11788 Disc Loss: 0.000872254 Q Losses: [0.01293261, 0.046828642]\n",
      "epoch:3 batch_done:94 Gen Loss: 9.75609 Disc Loss: 0.000293177 Q Losses: [0.0094237588, 0.041446835]\n",
      "epoch:3 batch_done:95 Gen Loss: 7.55803 Disc Loss: 0.000766554 Q Losses: [0.0062382086, 0.041052189]\n",
      "epoch:3 batch_done:96 Gen Loss: 8.74473 Disc Loss: 0.00047733 Q Losses: [0.012052473, 0.039503708]\n",
      "epoch:3 batch_done:97 Gen Loss: 18.1763 Disc Loss: 0.177162 Q Losses: [0.020698309, 0.036547258]\n",
      "epoch:3 batch_done:98 Gen Loss: 21.2748 Disc Loss: 0.119238 Q Losses: [0.0087249856, 0.038686682]\n",
      "epoch:3 batch_done:99 Gen Loss: 14.7718 Disc Loss: 0.425884 Q Losses: [0.0097176665, 0.041384391]\n",
      "epoch:3 batch_done:100 Gen Loss: 14.0702 Disc Loss: 0.0169158 Q Losses: [0.0066035502, 0.040746965]\n",
      "epoch:3 batch_done:101 Gen Loss: 6.79851 Disc Loss: 0.000837081 Q Losses: [0.0053595686, 0.036635064]\n",
      "epoch:3 batch_done:102 Gen Loss: 9.11328 Disc Loss: 0.0817052 Q Losses: [0.0091753732, 0.03412243]\n",
      "epoch:3 batch_done:103 Gen Loss: 15.5523 Disc Loss: 0.000349785 Q Losses: [0.0094217304, 0.03745003]\n",
      "epoch:3 batch_done:104 Gen Loss: 15.1916 Disc Loss: 0.00173387 Q Losses: [0.0098335817, 0.029675506]\n",
      "epoch:3 batch_done:105 Gen Loss: 7.16815 Disc Loss: 0.00363734 Q Losses: [0.0091275852, 0.041298375]\n",
      "epoch:3 batch_done:106 Gen Loss: 7.26545 Disc Loss: 0.0394814 Q Losses: [0.0073509538, 0.033216748]\n",
      "epoch:3 batch_done:107 Gen Loss: 13.5226 Disc Loss: 0.00095199 Q Losses: [0.0083680013, 0.060073018]\n",
      "epoch:3 batch_done:108 Gen Loss: 6.4033 Disc Loss: 0.032764 Q Losses: [0.010091318, 0.043187045]\n",
      "epoch:3 batch_done:109 Gen Loss: 8.37825 Disc Loss: 0.00184143 Q Losses: [0.008696313, 0.042661242]\n",
      "epoch:3 batch_done:110 Gen Loss: 6.0205 Disc Loss: 0.0477369 Q Losses: [0.0092720538, 0.047242291]\n",
      "epoch:3 batch_done:111 Gen Loss: 43.801 Disc Loss: 0.839833 Q Losses: [0.013606358, 0.047434703]\n",
      "epoch:3 batch_done:112 Gen Loss: 18.1646 Disc Loss: 4.58397 Q Losses: [0.011808215, 0.070166506]\n",
      "epoch:3 batch_done:113 Gen Loss: 6.01286 Disc Loss: 0.0373122 Q Losses: [0.010734633, 0.064680412]\n",
      "epoch:3 batch_done:114 Gen Loss: 1.0652 Disc Loss: 0.015118 Q Losses: [0.014773804, 0.071681589]\n",
      "epoch:3 batch_done:115 Gen Loss: 3.73534 Disc Loss: 0.019455 Q Losses: [0.04095326, 0.044162933]\n",
      "epoch:3 batch_done:116 Gen Loss: 8.54175 Disc Loss: 0.0662448 Q Losses: [0.016001899, 0.051150411]\n",
      "epoch:3 batch_done:117 Gen Loss: 12.7506 Disc Loss: 0.124525 Q Losses: [0.0089614904, 0.055428095]\n",
      "epoch:3 batch_done:118 Gen Loss: 9.56141 Disc Loss: 0.0468838 Q Losses: [0.013252315, 0.046372741]\n",
      "epoch:3 batch_done:119 Gen Loss: 32.2335 Disc Loss: 0.27008 Q Losses: [0.012016224, 0.057345301]\n",
      "epoch:3 batch_done:120 Gen Loss: 31.1295 Disc Loss: 1.01646 Q Losses: [0.010850976, 0.040185269]\n",
      "epoch:3 batch_done:121 Gen Loss: 23.1949 Disc Loss: 0.047145 Q Losses: [0.011601466, 0.038342811]\n",
      "epoch:3 batch_done:122 Gen Loss: 15.4561 Disc Loss: 9.38151e-05 Q Losses: [0.0077452422, 0.053279687]\n",
      "epoch:3 batch_done:123 Gen Loss: 10.5036 Disc Loss: 1.19592e-05 Q Losses: [0.008736616, 0.038771618]\n",
      "epoch:3 batch_done:124 Gen Loss: 5.86371 Disc Loss: 0.00210965 Q Losses: [0.019950157, 0.043826338]\n",
      "epoch:3 batch_done:125 Gen Loss: 27.7681 Disc Loss: 0.498485 Q Losses: [0.011203749, 0.036719941]\n",
      "epoch:3 batch_done:126 Gen Loss: 32.8952 Disc Loss: 0.166114 Q Losses: [0.012169773, 0.045922428]\n",
      "epoch:3 batch_done:127 Gen Loss: 32.418 Disc Loss: 0.273986 Q Losses: [0.011770525, 0.043381281]\n",
      "epoch:3 batch_done:128 Gen Loss: 29.799 Disc Loss: 0.320389 Q Losses: [0.0067007151, 0.036038272]\n",
      "epoch:3 batch_done:129 Gen Loss: 21.3411 Disc Loss: 0.0855529 Q Losses: [0.006940675, 0.03433108]\n",
      "epoch:3 batch_done:130 Gen Loss: 20.4097 Disc Loss: 0.0172745 Q Losses: [0.0072992095, 0.042241205]\n",
      "epoch:3 batch_done:131 Gen Loss: 19.1844 Disc Loss: 0.00484379 Q Losses: [0.028423153, 0.037641123]\n",
      "epoch:3 batch_done:132 Gen Loss: 10.939 Disc Loss: 0.0131827 Q Losses: [0.01368718, 0.036552571]\n",
      "epoch:3 batch_done:133 Gen Loss: 10.5142 Disc Loss: 0.000204702 Q Losses: [0.0093245599, 0.051755641]\n",
      "epoch:3 batch_done:134 Gen Loss: 9.45606 Disc Loss: 7.83642e-05 Q Losses: [0.0089397337, 0.042398699]\n",
      "epoch:3 batch_done:135 Gen Loss: 5.90205 Disc Loss: 0.00365993 Q Losses: [0.0096212905, 0.036682669]\n",
      "epoch:3 batch_done:136 Gen Loss: 8.07239 Disc Loss: 0.000394318 Q Losses: [0.0077546784, 0.042068876]\n",
      "epoch:3 batch_done:137 Gen Loss: 10.1843 Disc Loss: 0.000195121 Q Losses: [0.01063116, 0.036891378]\n",
      "epoch:3 batch_done:138 Gen Loss: 8.14231 Disc Loss: 0.0518076 Q Losses: [0.0076112556, 0.036406722]\n",
      "epoch:3 batch_done:139 Gen Loss: 8.20251 Disc Loss: 0.0050357 Q Losses: [0.022241071, 0.037385006]\n",
      "epoch:3 batch_done:140 Gen Loss: 6.91048 Disc Loss: 0.00851806 Q Losses: [0.0070809075, 0.036845136]\n",
      "epoch:3 batch_done:141 Gen Loss: 7.63625 Disc Loss: 0.00160184 Q Losses: [0.0098002423, 0.034841627]\n",
      "epoch:3 batch_done:142 Gen Loss: 12.0172 Disc Loss: 0.0483664 Q Losses: [0.0087734442, 0.033506334]\n",
      "epoch:3 batch_done:143 Gen Loss: 8.84574 Disc Loss: 0.120877 Q Losses: [0.007048639, 0.03257136]\n",
      "epoch:3 batch_done:144 Gen Loss: 11.7669 Disc Loss: 0.101521 Q Losses: [0.027620405, 0.02903777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 batch_done:145 Gen Loss: 10.1189 Disc Loss: 0.00381403 Q Losses: [0.008281948, 0.031827882]\n",
      "epoch:3 batch_done:146 Gen Loss: 14.6841 Disc Loss: 0.153676 Q Losses: [0.015236048, 0.037748612]\n",
      "epoch:3 batch_done:147 Gen Loss: 20.5401 Disc Loss: 0.0395078 Q Losses: [0.0099226441, 0.03950201]\n",
      "epoch:3 batch_done:148 Gen Loss: 22.2075 Disc Loss: 0.169447 Q Losses: [0.0073650721, 0.033146583]\n",
      "epoch:3 batch_done:149 Gen Loss: 10.6249 Disc Loss: 0.0196982 Q Losses: [0.0074986434, 0.034083709]\n",
      "epoch:3 batch_done:150 Gen Loss: 7.19462 Disc Loss: 0.0133715 Q Losses: [0.017625976, 0.032672361]\n",
      "epoch:3 batch_done:151 Gen Loss: 17.2251 Disc Loss: 0.000315674 Q Losses: [0.0058445451, 0.033450987]\n",
      "epoch:3 batch_done:152 Gen Loss: 6.41303 Disc Loss: 0.0472635 Q Losses: [0.0064887339, 0.033678055]\n",
      "epoch:3 batch_done:153 Gen Loss: 11.3638 Disc Loss: 0.000538508 Q Losses: [0.011573849, 0.035820551]\n",
      "epoch:3 batch_done:154 Gen Loss: 21.7654 Disc Loss: 0.0105384 Q Losses: [0.0081355069, 0.031830676]\n",
      "epoch:3 batch_done:155 Gen Loss: 6.09628 Disc Loss: 0.0353257 Q Losses: [0.0079344139, 0.037884884]\n",
      "epoch:3 batch_done:156 Gen Loss: 19.29 Disc Loss: 0.010924 Q Losses: [0.0089334557, 0.033724084]\n",
      "epoch:3 batch_done:157 Gen Loss: 5.77189 Disc Loss: 0.00501632 Q Losses: [0.0098077506, 0.030800667]\n",
      "epoch:3 batch_done:158 Gen Loss: 8.34085 Disc Loss: 0.0193904 Q Losses: [0.011170918, 0.033160906]\n",
      "epoch:3 batch_done:159 Gen Loss: 53.4961 Disc Loss: 5.62023 Q Losses: [0.0071383594, 0.042262089]\n",
      "epoch:3 batch_done:160 Gen Loss: 47.1547 Disc Loss: 10.7832 Q Losses: [0.012527943, 0.033902828]\n",
      "epoch:3 batch_done:161 Gen Loss: 35.7328 Disc Loss: 1.7892 Q Losses: [0.0091641974, 0.026733143]\n",
      "epoch:3 batch_done:162 Gen Loss: 27.1912 Disc Loss: 0.138397 Q Losses: [0.0076342165, 0.032486208]\n",
      "epoch:3 batch_done:163 Gen Loss: 19.8994 Disc Loss: 0.0134043 Q Losses: [0.0057739322, 0.032608323]\n",
      "epoch:3 batch_done:164 Gen Loss: 13.1219 Disc Loss: 2.28522e-05 Q Losses: [0.011598336, 0.032762785]\n",
      "epoch:3 batch_done:165 Gen Loss: 6.7384 Disc Loss: 0.000915528 Q Losses: [0.0065730722, 0.043189172]\n",
      "epoch:3 batch_done:166 Gen Loss: 18.7755 Disc Loss: 0.341372 Q Losses: [0.012808425, 0.034124352]\n",
      "epoch:3 batch_done:167 Gen Loss: 22.7665 Disc Loss: 0.000668771 Q Losses: [0.0069479137, 0.030623546]\n",
      "epoch:3 batch_done:168 Gen Loss: 20.8229 Disc Loss: 0.16388 Q Losses: [0.016604602, 0.039064381]\n",
      "epoch:3 batch_done:169 Gen Loss: 16.2283 Disc Loss: 0.737327 Q Losses: [0.010231657, 0.042636]\n",
      "epoch:3 batch_done:170 Gen Loss: 11.1557 Disc Loss: 0.165605 Q Losses: [0.01074245, 0.030472253]\n",
      "epoch:3 batch_done:171 Gen Loss: 5.79191 Disc Loss: 0.106616 Q Losses: [0.010584144, 0.038640235]\n",
      "epoch:3 batch_done:172 Gen Loss: 4.58408 Disc Loss: 0.0278724 Q Losses: [0.0066942954, 0.031189004]\n",
      "epoch:3 batch_done:173 Gen Loss: 5.83318 Disc Loss: 0.0153119 Q Losses: [0.008648593, 0.031383097]\n",
      "epoch:3 batch_done:174 Gen Loss: 7.07416 Disc Loss: 0.0175646 Q Losses: [0.010357717, 0.034267679]\n",
      "epoch:3 batch_done:175 Gen Loss: 6.43553 Disc Loss: 0.010399 Q Losses: [0.0076551228, 0.035535052]\n",
      "epoch:3 batch_done:176 Gen Loss: 21.1993 Disc Loss: 0.286796 Q Losses: [0.0081139244, 0.03755217]\n",
      "epoch:3 batch_done:177 Gen Loss: 23.7477 Disc Loss: 0.223096 Q Losses: [0.0095119383, 0.02917527]\n",
      "epoch:3 batch_done:178 Gen Loss: 17.3152 Disc Loss: 0.713813 Q Losses: [0.0080305133, 0.034695387]\n",
      "epoch:3 batch_done:179 Gen Loss: 12.9791 Disc Loss: 0.0378603 Q Losses: [0.0091407737, 0.032788448]\n",
      "epoch:3 batch_done:180 Gen Loss: 14.8056 Disc Loss: 0.0851423 Q Losses: [0.015113938, 0.036880895]\n",
      "epoch:3 batch_done:181 Gen Loss: 7.25534 Disc Loss: 0.297519 Q Losses: [0.010734572, 0.032086633]\n",
      "epoch:3 batch_done:182 Gen Loss: 5.88386 Disc Loss: 0.00271948 Q Losses: [0.011080028, 0.037778787]\n",
      "epoch:3 batch_done:183 Gen Loss: 9.13312 Disc Loss: 0.00630648 Q Losses: [0.0098487157, 0.034344584]\n",
      "epoch:3 batch_done:184 Gen Loss: 27.1405 Disc Loss: 0.551305 Q Losses: [0.012108412, 0.033003755]\n",
      "epoch:3 batch_done:185 Gen Loss: 26.61 Disc Loss: 0.0612711 Q Losses: [0.012815166, 0.032564104]\n",
      "epoch:3 batch_done:186 Gen Loss: 22.3945 Disc Loss: 0.262475 Q Losses: [0.010995456, 0.052090392]\n",
      "epoch:3 batch_done:187 Gen Loss: 15.821 Disc Loss: 0.206198 Q Losses: [0.010058822, 0.029705623]\n",
      "epoch:3 batch_done:188 Gen Loss: 11.2994 Disc Loss: 0.055079 Q Losses: [0.008004345, 0.040208787]\n",
      "epoch:3 batch_done:189 Gen Loss: 8.29716 Disc Loss: 0.0649141 Q Losses: [0.008219365, 0.032827221]\n",
      "epoch:3 batch_done:190 Gen Loss: 5.34887 Disc Loss: 0.0507687 Q Losses: [0.0094798915, 0.034446232]\n",
      "epoch:3 batch_done:191 Gen Loss: 4.40068 Disc Loss: 0.00892372 Q Losses: [0.010982479, 0.032232713]\n",
      "epoch:3 batch_done:192 Gen Loss: 4.53165 Disc Loss: 0.0492216 Q Losses: [0.012562875, 0.026141765]\n",
      "epoch:3 batch_done:193 Gen Loss: 5.54682 Disc Loss: 0.0184721 Q Losses: [0.0075194417, 0.032692987]\n",
      "epoch:3 batch_done:194 Gen Loss: 9.11944 Disc Loss: 0.001587 Q Losses: [0.015545793, 0.031397603]\n",
      "epoch:3 batch_done:195 Gen Loss: 10.4403 Disc Loss: 0.00973175 Q Losses: [0.006638594, 0.036991835]\n",
      "epoch:3 batch_done:196 Gen Loss: 5.17308 Disc Loss: 0.0559284 Q Losses: [0.010241869, 0.036437593]\n",
      "epoch:3 batch_done:197 Gen Loss: 4.67925 Disc Loss: 0.0349032 Q Losses: [0.016012525, 0.025412438]\n",
      "epoch:3 batch_done:198 Gen Loss: 4.94071 Disc Loss: 0.0442185 Q Losses: [0.011571908, 0.033989284]\n",
      "epoch:3 batch_done:199 Gen Loss: 8.27106 Disc Loss: 0.0778512 Q Losses: [0.0084864367, 0.030935043]\n",
      "epoch:3 batch_done:200 Gen Loss: 8.73679 Disc Loss: 0.0104462 Q Losses: [0.010740839, 0.037743136]\n",
      "epoch:3 batch_done:201 Gen Loss: 6.66785 Disc Loss: 0.0433894 Q Losses: [0.0081865918, 0.025495663]\n",
      "epoch:3 batch_done:202 Gen Loss: 10.2143 Disc Loss: 0.132576 Q Losses: [0.0081509706, 0.03755179]\n",
      "epoch:3 batch_done:203 Gen Loss: 7.75753 Disc Loss: 0.0239664 Q Losses: [0.0076839318, 0.032883435]\n",
      "epoch:3 batch_done:204 Gen Loss: 8.33213 Disc Loss: 0.109795 Q Losses: [0.016044915, 0.029550418]\n",
      "epoch:3 batch_done:205 Gen Loss: 8.02405 Disc Loss: 0.102706 Q Losses: [0.0082769757, 0.03687828]\n",
      "epoch:3 batch_done:206 Gen Loss: 9.09967 Disc Loss: 0.00581444 Q Losses: [0.010742044, 0.032503136]\n",
      "epoch:3 batch_done:207 Gen Loss: 10.426 Disc Loss: 0.0890014 Q Losses: [0.01076285, 0.029512398]\n",
      "epoch:4 batch_done:1 Gen Loss: 27.2082 Disc Loss: 0.573529 Q Losses: [0.0076107183, 0.028971884]\n",
      "epoch:4 batch_done:2 Gen Loss: 33.9958 Disc Loss: 0.406928 Q Losses: [0.0090664178, 0.029383764]\n",
      "epoch:4 batch_done:3 Gen Loss: 28.9267 Disc Loss: 0.581106 Q Losses: [0.0076370914, 0.035164289]\n",
      "epoch:4 batch_done:4 Gen Loss: 20.9945 Disc Loss: 0.20506 Q Losses: [0.00997754, 0.026199549]\n",
      "epoch:4 batch_done:5 Gen Loss: 14.5348 Disc Loss: 0.0281379 Q Losses: [0.015849609, 0.033604607]\n",
      "epoch:4 batch_done:6 Gen Loss: 12.4258 Disc Loss: 0.00194483 Q Losses: [0.010270594, 0.02729835]\n",
      "epoch:4 batch_done:7 Gen Loss: 9.70131 Disc Loss: 9.41647e-05 Q Losses: [0.010136612, 0.040345028]\n",
      "epoch:4 batch_done:8 Gen Loss: 42.7317 Disc Loss: 3.01821 Q Losses: [0.01347285, 0.054921962]\n",
      "epoch:4 batch_done:9 Gen Loss: 34.1049 Disc Loss: 0.37209 Q Losses: [0.0088101495, 0.041363999]\n",
      "epoch:4 batch_done:10 Gen Loss: 20.6085 Disc Loss: 0.494589 Q Losses: [0.0073751993, 0.036477238]\n",
      "epoch:4 batch_done:11 Gen Loss: 1.95705 Disc Loss: 0.491642 Q Losses: [0.0079554468, 0.042614665]\n",
      "epoch:4 batch_done:12 Gen Loss: 32.3286 Disc Loss: 10.107 Q Losses: [0.013201173, 0.040826678]\n",
      "epoch:4 batch_done:13 Gen Loss: 32.8009 Disc Loss: 1.32055 Q Losses: [0.014589645, 0.043539792]\n",
      "epoch:4 batch_done:14 Gen Loss: 24.3237 Disc Loss: 2.34884 Q Losses: [0.011837635, 0.041018993]\n",
      "epoch:4 batch_done:15 Gen Loss: 17.4497 Disc Loss: 0.732853 Q Losses: [0.015238786, 0.040884271]\n",
      "epoch:4 batch_done:16 Gen Loss: 13.7377 Disc Loss: 0.206284 Q Losses: [0.014171895, 0.037988968]\n",
      "epoch:4 batch_done:17 Gen Loss: 9.71401 Disc Loss: 0.0590928 Q Losses: [0.017487742, 0.036923312]\n",
      "epoch:4 batch_done:18 Gen Loss: 5.78214 Disc Loss: 0.00130117 Q Losses: [0.011087606, 0.041405089]\n",
      "epoch:4 batch_done:19 Gen Loss: 7.78124 Disc Loss: 0.000842477 Q Losses: [0.0096515175, 0.049551271]\n",
      "epoch:4 batch_done:20 Gen Loss: 7.69468 Disc Loss: 0.0130275 Q Losses: [0.0093089901, 0.034476105]\n",
      "epoch:4 batch_done:21 Gen Loss: 7.09933 Disc Loss: 0.0105143 Q Losses: [0.0098142158, 0.036909577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:22 Gen Loss: 6.26471 Disc Loss: 0.00379931 Q Losses: [0.0072436971, 0.042035162]\n",
      "epoch:4 batch_done:23 Gen Loss: 6.1891 Disc Loss: 0.0107797 Q Losses: [0.0066287089, 0.028982677]\n",
      "epoch:4 batch_done:24 Gen Loss: 4.85538 Disc Loss: 0.0214235 Q Losses: [0.0089161191, 0.027370987]\n",
      "epoch:4 batch_done:25 Gen Loss: 6.57682 Disc Loss: 0.0671516 Q Losses: [0.0081976224, 0.031595312]\n",
      "epoch:4 batch_done:26 Gen Loss: 7.1897 Disc Loss: 0.0107012 Q Losses: [0.008650789, 0.024879728]\n",
      "epoch:4 batch_done:27 Gen Loss: 5.67843 Disc Loss: 0.0739666 Q Losses: [0.0091564944, 0.031599529]\n",
      "epoch:4 batch_done:28 Gen Loss: 5.49468 Disc Loss: 0.0247113 Q Losses: [0.0056099109, 0.026507141]\n",
      "epoch:4 batch_done:29 Gen Loss: 5.47572 Disc Loss: 0.0502316 Q Losses: [0.0067852102, 0.030885722]\n",
      "epoch:4 batch_done:30 Gen Loss: 8.45423 Disc Loss: 0.132317 Q Losses: [0.0086971018, 0.025754515]\n",
      "epoch:4 batch_done:31 Gen Loss: 8.10141 Disc Loss: 0.0899049 Q Losses: [0.0042376635, 0.026386293]\n",
      "epoch:4 batch_done:32 Gen Loss: 6.04813 Disc Loss: 0.0494338 Q Losses: [0.0087340828, 0.034683377]\n",
      "epoch:4 batch_done:33 Gen Loss: 4.10559 Disc Loss: 0.128877 Q Losses: [0.0103499, 0.032192815]\n",
      "epoch:4 batch_done:34 Gen Loss: 6.13313 Disc Loss: 0.171333 Q Losses: [0.0081270197, 0.023457345]\n",
      "epoch:4 batch_done:35 Gen Loss: 14.0235 Disc Loss: 0.264514 Q Losses: [0.0068392288, 0.029337818]\n",
      "epoch:4 batch_done:36 Gen Loss: 12.3051 Disc Loss: 0.545374 Q Losses: [0.0066687521, 0.024578288]\n",
      "epoch:4 batch_done:37 Gen Loss: 7.59434 Disc Loss: 0.29367 Q Losses: [0.0065991981, 0.028039785]\n",
      "epoch:4 batch_done:38 Gen Loss: 3.30094 Disc Loss: 0.15286 Q Losses: [0.011148001, 0.028251424]\n",
      "epoch:4 batch_done:39 Gen Loss: 26.5816 Disc Loss: 0.970444 Q Losses: [0.0084072715, 0.028279981]\n",
      "epoch:4 batch_done:40 Gen Loss: 26.6599 Disc Loss: 1.78348 Q Losses: [0.0073430841, 0.025638018]\n",
      "epoch:4 batch_done:41 Gen Loss: 20.9098 Disc Loss: 0.858565 Q Losses: [0.01416027, 0.028143056]\n",
      "epoch:4 batch_done:42 Gen Loss: 16.2563 Disc Loss: 0.188251 Q Losses: [0.0063632294, 0.026156686]\n",
      "epoch:4 batch_done:43 Gen Loss: 8.34732 Disc Loss: 0.0137785 Q Losses: [0.0079271141, 0.029650826]\n",
      "epoch:4 batch_done:44 Gen Loss: 21.082 Disc Loss: 0.593131 Q Losses: [0.0065643019, 0.028243268]\n",
      "epoch:4 batch_done:45 Gen Loss: 22.3805 Disc Loss: 0.172968 Q Losses: [0.0064830109, 0.027418818]\n",
      "epoch:4 batch_done:46 Gen Loss: 15.7233 Disc Loss: 0.620134 Q Losses: [0.0074821962, 0.027324824]\n",
      "epoch:4 batch_done:47 Gen Loss: 8.81771 Disc Loss: 0.253283 Q Losses: [0.0049921498, 0.031732399]\n",
      "epoch:4 batch_done:48 Gen Loss: 4.41804 Disc Loss: 0.0236274 Q Losses: [0.0074231951, 0.038212307]\n",
      "epoch:4 batch_done:49 Gen Loss: 5.39086 Disc Loss: 0.0272715 Q Losses: [0.0085258, 0.027451791]\n",
      "epoch:4 batch_done:50 Gen Loss: 17.3221 Disc Loss: 0.324478 Q Losses: [0.0053897751, 0.025426496]\n",
      "epoch:4 batch_done:51 Gen Loss: 17.9552 Disc Loss: 0.38494 Q Losses: [0.0097898897, 0.028747389]\n",
      "epoch:4 batch_done:52 Gen Loss: 15.687 Disc Loss: 0.475641 Q Losses: [0.0081837047, 0.024386518]\n",
      "epoch:4 batch_done:53 Gen Loss: 13.3444 Disc Loss: 0.0393534 Q Losses: [0.0076280502, 0.027489323]\n",
      "epoch:4 batch_done:54 Gen Loss: 9.49313 Disc Loss: 0.0509154 Q Losses: [0.0097078132, 0.026589541]\n",
      "epoch:4 batch_done:55 Gen Loss: 9.0357 Disc Loss: 0.0157869 Q Losses: [0.0066944151, 0.038378313]\n",
      "epoch:4 batch_done:56 Gen Loss: 4.89609 Disc Loss: 0.0285209 Q Losses: [0.0081883892, 0.026250582]\n",
      "epoch:4 batch_done:57 Gen Loss: 5.85987 Disc Loss: 0.0138018 Q Losses: [0.0067999354, 0.039625928]\n",
      "epoch:4 batch_done:58 Gen Loss: 12.3729 Disc Loss: 0.135165 Q Losses: [0.010642389, 0.024467435]\n",
      "epoch:4 batch_done:59 Gen Loss: 12.8481 Disc Loss: 0.0994709 Q Losses: [0.010635436, 0.023890715]\n",
      "epoch:4 batch_done:60 Gen Loss: 10.4039 Disc Loss: 0.109082 Q Losses: [0.009188991, 0.02607353]\n",
      "epoch:4 batch_done:61 Gen Loss: 10.0943 Disc Loss: 0.00720641 Q Losses: [0.0068156752, 0.025671747]\n",
      "epoch:4 batch_done:62 Gen Loss: 11.9361 Disc Loss: 0.0220056 Q Losses: [0.0063297627, 0.028796259]\n",
      "epoch:4 batch_done:63 Gen Loss: 8.9511 Disc Loss: 0.000499599 Q Losses: [0.0094690816, 0.031439375]\n",
      "epoch:4 batch_done:64 Gen Loss: 9.43669 Disc Loss: 0.0367952 Q Losses: [0.0073380363, 0.021026812]\n",
      "epoch:4 batch_done:65 Gen Loss: 7.78058 Disc Loss: 0.0214547 Q Losses: [0.0095432596, 0.027681794]\n",
      "epoch:4 batch_done:66 Gen Loss: 5.11088 Disc Loss: 0.0258145 Q Losses: [0.025333431, 0.021566339]\n",
      "epoch:4 batch_done:67 Gen Loss: 10.0253 Disc Loss: 0.00150912 Q Losses: [0.022158207, 0.024151994]\n",
      "epoch:4 batch_done:68 Gen Loss: 6.83895 Disc Loss: 0.0531628 Q Losses: [0.0073967935, 0.032545809]\n",
      "epoch:4 batch_done:69 Gen Loss: 7.59383 Disc Loss: 0.0119177 Q Losses: [0.013225107, 0.026631963]\n",
      "epoch:4 batch_done:70 Gen Loss: 12.8887 Disc Loss: 0.00616537 Q Losses: [0.0057368916, 0.024200741]\n",
      "epoch:4 batch_done:71 Gen Loss: 7.01895 Disc Loss: 0.0360037 Q Losses: [0.014694487, 0.026712837]\n",
      "epoch:4 batch_done:72 Gen Loss: 6.71446 Disc Loss: 0.0303254 Q Losses: [0.0056490963, 0.027934425]\n",
      "epoch:4 batch_done:73 Gen Loss: 7.08337 Disc Loss: 0.00166332 Q Losses: [0.011998052, 0.033505529]\n",
      "epoch:4 batch_done:74 Gen Loss: 22.4128 Disc Loss: 0.362533 Q Losses: [0.0099651096, 0.021925773]\n",
      "epoch:4 batch_done:75 Gen Loss: 22.0205 Disc Loss: 1.10419 Q Losses: [0.0057803872, 0.023152316]\n",
      "epoch:4 batch_done:76 Gen Loss: 18.3204 Disc Loss: 0.0563088 Q Losses: [0.0058129826, 0.027884094]\n",
      "epoch:4 batch_done:77 Gen Loss: 15.1941 Disc Loss: 0.00350922 Q Losses: [0.006774263, 0.024173992]\n",
      "epoch:4 batch_done:78 Gen Loss: 18.1582 Disc Loss: 0.00023301 Q Losses: [0.008031914, 0.022299813]\n",
      "epoch:4 batch_done:79 Gen Loss: 18.8266 Disc Loss: 0.0030429 Q Losses: [0.010287114, 0.020369701]\n",
      "epoch:4 batch_done:80 Gen Loss: 16.7794 Disc Loss: 0.000378667 Q Losses: [0.0092162509, 0.021804366]\n",
      "epoch:4 batch_done:81 Gen Loss: 11.3511 Disc Loss: 0.000174344 Q Losses: [0.0058985241, 0.02473186]\n",
      "epoch:4 batch_done:82 Gen Loss: 15.6215 Disc Loss: 0.000202111 Q Losses: [0.0084635746, 0.020766178]\n",
      "epoch:4 batch_done:83 Gen Loss: 10.6268 Disc Loss: 0.00169921 Q Losses: [0.01047313, 0.02680284]\n",
      "epoch:4 batch_done:84 Gen Loss: 13.9844 Disc Loss: 0.00189049 Q Losses: [0.0086243507, 0.021166787]\n",
      "epoch:4 batch_done:85 Gen Loss: 6.10753 Disc Loss: 0.00831308 Q Losses: [0.0097144078, 0.024241792]\n",
      "epoch:4 batch_done:86 Gen Loss: 5.5543 Disc Loss: 0.00743484 Q Losses: [0.005195098, 0.020357465]\n",
      "epoch:4 batch_done:87 Gen Loss: 9.16116 Disc Loss: 0.00124387 Q Losses: [0.0055489624, 0.02360661]\n",
      "epoch:4 batch_done:88 Gen Loss: 6.84697 Disc Loss: 0.0528769 Q Losses: [0.0084300656, 0.023992598]\n",
      "epoch:4 batch_done:89 Gen Loss: 8.43205 Disc Loss: 0.0148753 Q Losses: [0.006479139, 0.01946133]\n",
      "epoch:4 batch_done:90 Gen Loss: 49.1217 Disc Loss: 1.71272 Q Losses: [0.0064509492, 0.028565424]\n",
      "epoch:4 batch_done:91 Gen Loss: 38.5875 Disc Loss: 10.8168 Q Losses: [0.0081781801, 0.027626418]\n",
      "epoch:4 batch_done:92 Gen Loss: 18.0943 Disc Loss: 2.72428 Q Losses: [0.0073552486, 0.020745622]\n",
      "epoch:4 batch_done:93 Gen Loss: 4.99184 Disc Loss: 0.0153759 Q Losses: [0.0060963109, 0.030514617]\n",
      "epoch:4 batch_done:94 Gen Loss: 8.24743 Disc Loss: 0.242271 Q Losses: [0.0064611984, 0.021207843]\n",
      "epoch:4 batch_done:95 Gen Loss: 9.10014 Disc Loss: 0.00260708 Q Losses: [0.0067763501, 0.026132472]\n",
      "epoch:4 batch_done:96 Gen Loss: 7.37104 Disc Loss: 0.00343334 Q Losses: [0.0071721761, 0.022791356]\n",
      "epoch:4 batch_done:97 Gen Loss: 6.41037 Disc Loss: 0.00428275 Q Losses: [0.0064453529, 0.024081912]\n",
      "epoch:4 batch_done:98 Gen Loss: 6.12678 Disc Loss: 0.00434642 Q Losses: [0.0088161612, 0.025686249]\n",
      "epoch:4 batch_done:99 Gen Loss: 5.50894 Disc Loss: 0.0078226 Q Losses: [0.0063207736, 0.021258185]\n",
      "epoch:4 batch_done:100 Gen Loss: 5.44999 Disc Loss: 0.0486775 Q Losses: [0.0052231783, 0.024823228]\n",
      "epoch:4 batch_done:101 Gen Loss: 5.89351 Disc Loss: 0.0251211 Q Losses: [0.0070078084, 0.022869099]\n",
      "epoch:4 batch_done:102 Gen Loss: 7.94045 Disc Loss: 0.114818 Q Losses: [0.0080753006, 0.02112955]\n",
      "epoch:4 batch_done:103 Gen Loss: 8.89056 Disc Loss: 0.00428735 Q Losses: [0.0093474947, 0.029071204]\n",
      "epoch:4 batch_done:104 Gen Loss: 10.1261 Disc Loss: 0.0404336 Q Losses: [0.0090223122, 0.022235341]\n",
      "epoch:4 batch_done:105 Gen Loss: 8.55964 Disc Loss: 0.151257 Q Losses: [0.0077001471, 0.020417711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:106 Gen Loss: 4.64468 Disc Loss: 0.0592246 Q Losses: [0.006409415, 0.020172615]\n",
      "epoch:4 batch_done:107 Gen Loss: 12.4956 Disc Loss: 0.344116 Q Losses: [0.0082426127, 0.02177649]\n",
      "epoch:4 batch_done:108 Gen Loss: 14.3908 Disc Loss: 0.124184 Q Losses: [0.0087971222, 0.023603451]\n",
      "epoch:4 batch_done:109 Gen Loss: 13.0204 Disc Loss: 0.174378 Q Losses: [0.0070643481, 0.029917914]\n",
      "epoch:4 batch_done:110 Gen Loss: 11.4547 Disc Loss: 0.0665171 Q Losses: [0.0062586553, 0.027931962]\n",
      "epoch:4 batch_done:111 Gen Loss: 10.6961 Disc Loss: 0.0595775 Q Losses: [0.0071246219, 0.021377485]\n",
      "epoch:4 batch_done:112 Gen Loss: 11.2186 Disc Loss: 0.0354818 Q Losses: [0.01636263, 0.018577788]\n",
      "epoch:4 batch_done:113 Gen Loss: 4.53108 Disc Loss: 0.039273 Q Losses: [0.0067509916, 0.023276228]\n",
      "epoch:4 batch_done:114 Gen Loss: 5.85836 Disc Loss: 0.0548324 Q Losses: [0.012218082, 0.025908841]\n",
      "epoch:4 batch_done:115 Gen Loss: 9.41592 Disc Loss: 0.0021208 Q Losses: [0.0086152125, 0.019769223]\n",
      "epoch:4 batch_done:116 Gen Loss: 8.9683 Disc Loss: 0.147247 Q Losses: [0.010312079, 0.025216345]\n",
      "epoch:4 batch_done:117 Gen Loss: 11.4174 Disc Loss: 0.0708589 Q Losses: [0.0073629785, 0.026320208]\n",
      "epoch:4 batch_done:118 Gen Loss: 6.71201 Disc Loss: 0.0884887 Q Losses: [0.0059050228, 0.025144946]\n",
      "epoch:4 batch_done:119 Gen Loss: 5.10812 Disc Loss: 0.0679261 Q Losses: [0.013331115, 0.023352383]\n",
      "epoch:4 batch_done:120 Gen Loss: 6.30651 Disc Loss: 0.0456655 Q Losses: [0.012859801, 0.029559856]\n",
      "epoch:4 batch_done:121 Gen Loss: 37.5863 Disc Loss: 5.75977 Q Losses: [0.021581979, 0.022895917]\n",
      "epoch:4 batch_done:122 Gen Loss: 46.6594 Disc Loss: 1.68631 Q Losses: [0.0089586247, 0.024747923]\n",
      "epoch:4 batch_done:123 Gen Loss: 41.3414 Disc Loss: 1.93654 Q Losses: [0.010584988, 0.026767056]\n",
      "epoch:4 batch_done:124 Gen Loss: 34.6812 Disc Loss: 0.311512 Q Losses: [0.0092161577, 0.025748581]\n",
      "epoch:4 batch_done:125 Gen Loss: 28.1155 Disc Loss: 0.00361422 Q Losses: [0.0092671318, 0.024251573]\n",
      "epoch:4 batch_done:126 Gen Loss: 24.3827 Disc Loss: 0.0908246 Q Losses: [0.014119737, 0.02877393]\n",
      "epoch:4 batch_done:127 Gen Loss: 21.6999 Disc Loss: 0.0527661 Q Losses: [0.011506585, 0.02538627]\n",
      "epoch:4 batch_done:128 Gen Loss: 19.3201 Disc Loss: 0.00155471 Q Losses: [0.010534924, 0.024177384]\n",
      "epoch:4 batch_done:129 Gen Loss: 18.4626 Disc Loss: 0.00934998 Q Losses: [0.011092674, 0.024763566]\n",
      "epoch:4 batch_done:130 Gen Loss: 16.3089 Disc Loss: 0.0161605 Q Losses: [0.011303609, 0.024196044]\n",
      "epoch:4 batch_done:131 Gen Loss: 11.5708 Disc Loss: 0.0366817 Q Losses: [0.0071339319, 0.021544479]\n",
      "epoch:4 batch_done:132 Gen Loss: 9.11011 Disc Loss: 0.00369085 Q Losses: [0.0063034859, 0.028560102]\n",
      "epoch:4 batch_done:133 Gen Loss: 7.68901 Disc Loss: 0.000571578 Q Losses: [0.012595597, 0.023572456]\n",
      "epoch:4 batch_done:134 Gen Loss: 5.45271 Disc Loss: 0.0461972 Q Losses: [0.0093706194, 0.030514777]\n",
      "epoch:4 batch_done:135 Gen Loss: 7.20899 Disc Loss: 0.0673886 Q Losses: [0.0088759912, 0.028049022]\n",
      "epoch:4 batch_done:136 Gen Loss: 8.10934 Disc Loss: 0.00917822 Q Losses: [0.0088554043, 0.023451161]\n",
      "epoch:4 batch_done:137 Gen Loss: 15.0625 Disc Loss: 0.053985 Q Losses: [0.0065098591, 0.033487458]\n",
      "epoch:4 batch_done:138 Gen Loss: 6.54813 Disc Loss: 0.0609133 Q Losses: [0.010277162, 0.021296337]\n",
      "epoch:4 batch_done:139 Gen Loss: 4.88069 Disc Loss: 0.0172485 Q Losses: [0.012820741, 0.030321129]\n",
      "epoch:4 batch_done:140 Gen Loss: 6.88622 Disc Loss: 0.00154189 Q Losses: [0.007862444, 0.022740427]\n",
      "epoch:4 batch_done:141 Gen Loss: 9.05499 Disc Loss: 0.000899523 Q Losses: [0.0064518061, 0.025834605]\n",
      "epoch:4 batch_done:142 Gen Loss: 6.94457 Disc Loss: 0.074725 Q Losses: [0.0092366831, 0.022007383]\n",
      "epoch:4 batch_done:143 Gen Loss: 7.68065 Disc Loss: 0.00722867 Q Losses: [0.012643937, 0.01858695]\n",
      "epoch:4 batch_done:144 Gen Loss: 6.08922 Disc Loss: 0.0233338 Q Losses: [0.0089594331, 0.019371051]\n",
      "epoch:4 batch_done:145 Gen Loss: 5.93022 Disc Loss: 0.0273553 Q Losses: [0.012552123, 0.026998498]\n",
      "epoch:4 batch_done:146 Gen Loss: 13.5086 Disc Loss: 0.0149064 Q Losses: [0.0095151067, 0.022138137]\n",
      "epoch:4 batch_done:147 Gen Loss: 5.90606 Disc Loss: 0.00714527 Q Losses: [0.0083396845, 0.026000215]\n",
      "epoch:4 batch_done:148 Gen Loss: 7.89138 Disc Loss: 0.0592726 Q Losses: [0.0077903578, 0.025534749]\n",
      "epoch:4 batch_done:149 Gen Loss: 5.22063 Disc Loss: 0.00747361 Q Losses: [0.012138477, 0.019031662]\n",
      "epoch:4 batch_done:150 Gen Loss: 8.41498 Disc Loss: 0.0987167 Q Losses: [0.011002727, 0.01930864]\n",
      "epoch:4 batch_done:151 Gen Loss: 10.4186 Disc Loss: 0.0239992 Q Losses: [0.0090373484, 0.020480486]\n",
      "epoch:4 batch_done:152 Gen Loss: 11.378 Disc Loss: 0.00403598 Q Losses: [0.010130759, 0.017861564]\n",
      "epoch:4 batch_done:153 Gen Loss: 12.537 Disc Loss: 0.0151091 Q Losses: [0.005960146, 0.021595482]\n",
      "epoch:4 batch_done:154 Gen Loss: 9.23443 Disc Loss: 0.0127241 Q Losses: [0.0093434034, 0.022765337]\n",
      "epoch:4 batch_done:155 Gen Loss: 3.67678 Disc Loss: 0.174137 Q Losses: [0.0096672997, 0.022201275]\n",
      "epoch:4 batch_done:156 Gen Loss: 4.8173 Disc Loss: 0.00852034 Q Losses: [0.0084710252, 0.023979144]\n",
      "epoch:4 batch_done:157 Gen Loss: 6.34918 Disc Loss: 0.00224104 Q Losses: [0.026111981, 0.022655189]\n",
      "epoch:4 batch_done:158 Gen Loss: 11.7675 Disc Loss: 0.159868 Q Losses: [0.017356364, 0.019776732]\n",
      "epoch:4 batch_done:159 Gen Loss: 13.0021 Disc Loss: 0.00379683 Q Losses: [0.007161078, 0.020194862]\n",
      "epoch:4 batch_done:160 Gen Loss: 17.0664 Disc Loss: 0.0565159 Q Losses: [0.0092180874, 0.022393674]\n",
      "epoch:4 batch_done:161 Gen Loss: 15.7365 Disc Loss: 0.0470879 Q Losses: [0.008546574, 0.020989724]\n",
      "epoch:4 batch_done:162 Gen Loss: 19.3998 Disc Loss: 0.0364008 Q Losses: [0.0079417853, 0.019856697]\n",
      "epoch:4 batch_done:163 Gen Loss: 10.2032 Disc Loss: 0.0726865 Q Losses: [0.018625924, 0.023567352]\n",
      "epoch:4 batch_done:164 Gen Loss: 8.79866 Disc Loss: 0.00129603 Q Losses: [0.014582905, 0.021186015]\n",
      "epoch:4 batch_done:165 Gen Loss: 15.2809 Disc Loss: 0.0188174 Q Losses: [0.01364472, 0.01979531]\n",
      "epoch:4 batch_done:166 Gen Loss: 5.23422 Disc Loss: 0.0282312 Q Losses: [0.008356167, 0.018520579]\n",
      "epoch:4 batch_done:167 Gen Loss: 4.63453 Disc Loss: 0.110487 Q Losses: [0.0077639548, 0.031564612]\n",
      "epoch:4 batch_done:168 Gen Loss: 13.1008 Disc Loss: 0.00297047 Q Losses: [0.0063580954, 0.023777487]\n",
      "epoch:4 batch_done:169 Gen Loss: 8.03864 Disc Loss: 0.0762955 Q Losses: [0.0072787963, 0.019902082]\n",
      "epoch:4 batch_done:170 Gen Loss: 7.90796 Disc Loss: 0.0108837 Q Losses: [0.0070820507, 0.021281201]\n",
      "epoch:4 batch_done:171 Gen Loss: 20.8005 Disc Loss: 0.00448185 Q Losses: [0.0089063905, 0.028717294]\n",
      "epoch:4 batch_done:172 Gen Loss: 12.8513 Disc Loss: 0.0102582 Q Losses: [0.0070338519, 0.034478556]\n",
      "epoch:4 batch_done:173 Gen Loss: 10.3562 Disc Loss: 0.00961192 Q Losses: [0.016055347, 0.018497676]\n",
      "epoch:4 batch_done:174 Gen Loss: 13.4231 Disc Loss: 0.0299043 Q Losses: [0.0052607078, 0.032759175]\n",
      "epoch:4 batch_done:175 Gen Loss: 11.8688 Disc Loss: 0.0216057 Q Losses: [0.01561632, 0.032061592]\n",
      "epoch:4 batch_done:176 Gen Loss: 5.12012 Disc Loss: 0.0120866 Q Losses: [0.0097168833, 0.033073381]\n",
      "epoch:4 batch_done:177 Gen Loss: 8.96642 Disc Loss: 0.0156096 Q Losses: [0.022499725, 0.023663463]\n",
      "epoch:4 batch_done:178 Gen Loss: 8.36839 Disc Loss: 0.00116233 Q Losses: [0.0058370717, 0.024408944]\n",
      "epoch:4 batch_done:179 Gen Loss: 6.55392 Disc Loss: 0.00406328 Q Losses: [0.0108713, 0.023031268]\n",
      "epoch:4 batch_done:180 Gen Loss: 24.3462 Disc Loss: 0.392473 Q Losses: [0.011136401, 0.020190965]\n",
      "epoch:4 batch_done:181 Gen Loss: 23.9591 Disc Loss: 0.414703 Q Losses: [0.012316307, 0.02164679]\n",
      "epoch:4 batch_done:182 Gen Loss: 22.5888 Disc Loss: 0.390315 Q Losses: [0.0087093618, 0.026507653]\n",
      "epoch:4 batch_done:183 Gen Loss: 27.8007 Disc Loss: 0.0569143 Q Losses: [0.0078191878, 0.022063904]\n",
      "epoch:4 batch_done:184 Gen Loss: 22.2814 Disc Loss: 0.0303964 Q Losses: [0.0098902248, 0.021857554]\n",
      "epoch:4 batch_done:185 Gen Loss: 20.492 Disc Loss: 0.00200271 Q Losses: [0.0092069414, 0.021096053]\n",
      "epoch:4 batch_done:186 Gen Loss: 18.1508 Disc Loss: 0.00013408 Q Losses: [0.011305838, 0.026036879]\n",
      "epoch:4 batch_done:187 Gen Loss: 14.0099 Disc Loss: 0.000189082 Q Losses: [0.010865221, 0.020359976]\n",
      "epoch:4 batch_done:188 Gen Loss: 23.955 Disc Loss: 0.010655 Q Losses: [0.011146873, 0.025355976]\n",
      "epoch:4 batch_done:189 Gen Loss: 11.4369 Disc Loss: 0.000425328 Q Losses: [0.0092502311, 0.025569849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:190 Gen Loss: 14.1882 Disc Loss: 0.00114923 Q Losses: [0.0072842003, 0.030296015]\n",
      "epoch:4 batch_done:191 Gen Loss: 7.32849 Disc Loss: 0.00644756 Q Losses: [0.015848555, 0.018708177]\n",
      "epoch:4 batch_done:192 Gen Loss: 5.55143 Disc Loss: 0.00785002 Q Losses: [0.012518085, 0.021049762]\n",
      "epoch:4 batch_done:193 Gen Loss: 17.0048 Disc Loss: 0.000262301 Q Losses: [0.013659278, 0.025638953]\n",
      "epoch:4 batch_done:194 Gen Loss: 11.1402 Disc Loss: 0.000330064 Q Losses: [0.0084552737, 0.029059652]\n",
      "epoch:4 batch_done:195 Gen Loss: 41.2341 Disc Loss: 1.15853 Q Losses: [0.014554327, 0.023249779]\n",
      "epoch:4 batch_done:196 Gen Loss: 27.5601 Disc Loss: 6.02713 Q Losses: [0.0080598556, 0.02349899]\n",
      "epoch:4 batch_done:197 Gen Loss: 16.0435 Disc Loss: 0.000999968 Q Losses: [0.011094503, 0.022088736]\n",
      "epoch:4 batch_done:198 Gen Loss: 6.97094 Disc Loss: 0.000299432 Q Losses: [0.0087363385, 0.025117852]\n",
      "epoch:4 batch_done:199 Gen Loss: 6.05637 Disc Loss: 0.0138502 Q Losses: [0.012839132, 0.021784056]\n",
      "epoch:4 batch_done:200 Gen Loss: 40.3525 Disc Loss: 0.980464 Q Losses: [0.0088424031, 0.02380731]\n",
      "epoch:4 batch_done:201 Gen Loss: 37.075 Disc Loss: 3.41075 Q Losses: [0.0099038668, 0.029052319]\n",
      "epoch:4 batch_done:202 Gen Loss: 25.7253 Disc Loss: 0.660208 Q Losses: [0.0097476104, 0.024984606]\n",
      "epoch:4 batch_done:203 Gen Loss: 12.9801 Disc Loss: 0.188665 Q Losses: [0.0077796839, 0.02795006]\n",
      "epoch:4 batch_done:204 Gen Loss: 6.17243 Disc Loss: 0.00128495 Q Losses: [0.0077346177, 0.023200739]\n",
      "epoch:4 batch_done:205 Gen Loss: 5.30754 Disc Loss: 0.00154582 Q Losses: [0.0090024192, 0.0286962]\n",
      "epoch:4 batch_done:206 Gen Loss: 32.4808 Disc Loss: 2.02395 Q Losses: [0.011307335, 0.0215319]\n",
      "epoch:4 batch_done:207 Gen Loss: 36.1678 Disc Loss: 0.190255 Q Losses: [0.01709261, 0.022580434]\n",
      "epoch:5 batch_done:1 Gen Loss: 31.173 Disc Loss: 0.811527 Q Losses: [0.0091509297, 0.022395501]\n",
      "epoch:5 batch_done:2 Gen Loss: 24.1449 Disc Loss: 0.328748 Q Losses: [0.014618378, 0.024624228]\n",
      "epoch:5 batch_done:3 Gen Loss: 20.5642 Disc Loss: 0.0125074 Q Losses: [0.0097751282, 0.02062377]\n",
      "epoch:5 batch_done:4 Gen Loss: 19.0585 Disc Loss: 0.00623632 Q Losses: [0.0064909579, 0.026629087]\n",
      "epoch:5 batch_done:5 Gen Loss: 11.3384 Disc Loss: 0.000504626 Q Losses: [0.010285892, 0.024825964]\n",
      "epoch:5 batch_done:6 Gen Loss: 11.7419 Disc Loss: 0.00541866 Q Losses: [0.0078266058, 0.020391025]\n",
      "epoch:5 batch_done:7 Gen Loss: 7.20774 Disc Loss: 0.037647 Q Losses: [0.0066279545, 0.03165105]\n",
      "epoch:5 batch_done:8 Gen Loss: 4.72581 Disc Loss: 0.0130189 Q Losses: [0.0087889936, 0.024759213]\n",
      "epoch:5 batch_done:9 Gen Loss: 12.4823 Disc Loss: 0.210603 Q Losses: [0.010244273, 0.024546603]\n",
      "epoch:5 batch_done:10 Gen Loss: 15.4931 Disc Loss: 0.0708753 Q Losses: [0.005771657, 0.020734088]\n",
      "epoch:5 batch_done:11 Gen Loss: 9.58548 Disc Loss: 0.455434 Q Losses: [0.0065124314, 0.022533525]\n",
      "epoch:5 batch_done:12 Gen Loss: 4.00775 Disc Loss: 0.109079 Q Losses: [0.0057008788, 0.021566741]\n",
      "epoch:5 batch_done:13 Gen Loss: 19.9395 Disc Loss: 0.482238 Q Losses: [0.0058566006, 0.019231476]\n",
      "epoch:5 batch_done:14 Gen Loss: 17.2972 Disc Loss: 1.13809 Q Losses: [0.0076205628, 0.021000441]\n",
      "epoch:5 batch_done:15 Gen Loss: 11.4429 Disc Loss: 0.415803 Q Losses: [0.024593133, 0.019087849]\n",
      "epoch:5 batch_done:16 Gen Loss: 7.97265 Disc Loss: 0.00230362 Q Losses: [0.007901419, 0.022400474]\n",
      "epoch:5 batch_done:17 Gen Loss: 7.42885 Disc Loss: 0.00926363 Q Losses: [0.0066213086, 0.01885034]\n",
      "epoch:5 batch_done:18 Gen Loss: 4.84799 Disc Loss: 0.0475156 Q Losses: [0.0091272034, 0.031286195]\n",
      "epoch:5 batch_done:19 Gen Loss: 9.38634 Disc Loss: 0.0171669 Q Losses: [0.014616691, 0.020565731]\n",
      "epoch:5 batch_done:20 Gen Loss: 5.25787 Disc Loss: 0.0297697 Q Losses: [0.0074672531, 0.021449583]\n",
      "epoch:5 batch_done:21 Gen Loss: 4.37568 Disc Loss: 0.0567715 Q Losses: [0.0055798097, 0.022792265]\n",
      "epoch:5 batch_done:22 Gen Loss: 7.03288 Disc Loss: 0.0865569 Q Losses: [0.0063580908, 0.018705014]\n",
      "epoch:5 batch_done:23 Gen Loss: 7.99687 Disc Loss: 0.0161543 Q Losses: [0.0036394401, 0.01797488]\n",
      "epoch:5 batch_done:24 Gen Loss: 8.1146 Disc Loss: 0.0161018 Q Losses: [0.0065171691, 0.015605632]\n",
      "epoch:5 batch_done:25 Gen Loss: 9.10144 Disc Loss: 0.0236251 Q Losses: [0.0063496558, 0.016154133]\n",
      "epoch:5 batch_done:26 Gen Loss: 10.724 Disc Loss: 0.0355655 Q Losses: [0.005175957, 0.017882995]\n",
      "epoch:5 batch_done:27 Gen Loss: 5.66934 Disc Loss: 0.0176181 Q Losses: [0.0074269827, 0.02379263]\n",
      "epoch:5 batch_done:28 Gen Loss: 5.79666 Disc Loss: 0.0119575 Q Losses: [0.0056829192, 0.023055525]\n",
      "epoch:5 batch_done:29 Gen Loss: 4.99007 Disc Loss: 0.0178286 Q Losses: [0.0074886051, 0.025532633]\n",
      "epoch:5 batch_done:30 Gen Loss: 12.3076 Disc Loss: 0.158422 Q Losses: [0.0097969742, 0.019103838]\n",
      "epoch:5 batch_done:31 Gen Loss: 13.1873 Disc Loss: 0.0521457 Q Losses: [0.0065221032, 0.024586305]\n",
      "epoch:5 batch_done:32 Gen Loss: 12.5628 Disc Loss: 0.193269 Q Losses: [0.007722463, 0.017265528]\n",
      "epoch:5 batch_done:33 Gen Loss: 10.953 Disc Loss: 0.0545303 Q Losses: [0.0053661913, 0.01905508]\n",
      "epoch:5 batch_done:34 Gen Loss: 12.4385 Disc Loss: 0.0200659 Q Losses: [0.0061669182, 0.01896378]\n",
      "epoch:5 batch_done:35 Gen Loss: 5.88795 Disc Loss: 0.00655855 Q Losses: [0.014930969, 0.021082468]\n",
      "epoch:5 batch_done:36 Gen Loss: 7.30103 Disc Loss: 0.0164005 Q Losses: [0.0059893634, 0.021582749]\n",
      "epoch:5 batch_done:37 Gen Loss: 5.59875 Disc Loss: 0.010124 Q Losses: [0.0059931632, 0.01551012]\n",
      "epoch:5 batch_done:38 Gen Loss: 4.81752 Disc Loss: 0.0416714 Q Losses: [0.0059390292, 0.0218012]\n",
      "epoch:5 batch_done:39 Gen Loss: 14.9823 Disc Loss: 0.214597 Q Losses: [0.005233367, 0.017248208]\n",
      "epoch:5 batch_done:40 Gen Loss: 18.3587 Disc Loss: 0.043966 Q Losses: [0.011989878, 0.017681491]\n",
      "epoch:5 batch_done:41 Gen Loss: 13.4811 Disc Loss: 0.262406 Q Losses: [0.0081699975, 0.019388061]\n",
      "epoch:5 batch_done:42 Gen Loss: 10.8742 Disc Loss: 0.0176652 Q Losses: [0.0082761683, 0.018534523]\n",
      "epoch:5 batch_done:43 Gen Loss: 17.3858 Disc Loss: 0.00518976 Q Losses: [0.0076324507, 0.017530043]\n",
      "epoch:5 batch_done:44 Gen Loss: 6.7792 Disc Loss: 0.0679716 Q Losses: [0.0078896806, 0.017058194]\n",
      "epoch:5 batch_done:45 Gen Loss: 11.7832 Disc Loss: 7.31226e-05 Q Losses: [0.0051841452, 0.016458284]\n",
      "epoch:5 batch_done:46 Gen Loss: 5.18368 Disc Loss: 0.00816259 Q Losses: [0.016934324, 0.017298911]\n",
      "epoch:5 batch_done:47 Gen Loss: 7.06043 Disc Loss: 0.000950884 Q Losses: [0.0044860938, 0.015594447]\n",
      "epoch:5 batch_done:48 Gen Loss: 5.29266 Disc Loss: 0.0277505 Q Losses: [0.0069116596, 0.019035563]\n",
      "epoch:5 batch_done:49 Gen Loss: 7.38028 Disc Loss: 0.00363857 Q Losses: [0.017126225, 0.019236462]\n",
      "epoch:5 batch_done:50 Gen Loss: 7.69711 Disc Loss: 0.00159302 Q Losses: [0.0099167898, 0.018174659]\n",
      "epoch:5 batch_done:51 Gen Loss: 6.64114 Disc Loss: 0.00187055 Q Losses: [0.016557544, 0.016567782]\n",
      "epoch:5 batch_done:52 Gen Loss: 6.86024 Disc Loss: 0.0061489 Q Losses: [0.0061600371, 0.01806174]\n",
      "epoch:5 batch_done:53 Gen Loss: 5.54169 Disc Loss: 0.0147279 Q Losses: [0.0066287392, 0.016311977]\n",
      "epoch:5 batch_done:54 Gen Loss: 6.54477 Disc Loss: 0.00192517 Q Losses: [0.011084647, 0.020046961]\n",
      "epoch:5 batch_done:55 Gen Loss: 16.0912 Disc Loss: 0.319258 Q Losses: [0.0070045856, 0.014711326]\n",
      "epoch:5 batch_done:56 Gen Loss: 17.1165 Disc Loss: 0.513413 Q Losses: [0.007827281, 0.015648564]\n",
      "epoch:5 batch_done:57 Gen Loss: 18.4506 Disc Loss: 0.0329143 Q Losses: [0.01276, 0.019213684]\n",
      "epoch:5 batch_done:58 Gen Loss: 11.0258 Disc Loss: 0.00171825 Q Losses: [0.0092025651, 0.020906571]\n",
      "epoch:5 batch_done:59 Gen Loss: 15.0826 Disc Loss: 0.00986426 Q Losses: [0.012123819, 0.01970415]\n",
      "epoch:5 batch_done:60 Gen Loss: 16.8475 Disc Loss: 0.0225595 Q Losses: [0.0069609783, 0.017142881]\n",
      "epoch:5 batch_done:61 Gen Loss: 4.22374 Disc Loss: 0.0775205 Q Losses: [0.007797054, 0.016099855]\n",
      "epoch:5 batch_done:62 Gen Loss: 5.81796 Disc Loss: 0.00483461 Q Losses: [0.0080846353, 0.022430463]\n",
      "epoch:5 batch_done:63 Gen Loss: 15.8349 Disc Loss: 0.00106306 Q Losses: [0.014320871, 0.017104004]\n",
      "epoch:5 batch_done:64 Gen Loss: 8.77552 Disc Loss: 0.000308955 Q Losses: [0.0081456704, 0.017402034]\n",
      "epoch:5 batch_done:65 Gen Loss: 12.3069 Disc Loss: 0.000758067 Q Losses: [0.0077571594, 0.017612349]\n",
      "epoch:5 batch_done:66 Gen Loss: 5.46056 Disc Loss: 0.00752519 Q Losses: [0.0088997819, 0.021128761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch_done:67 Gen Loss: 10.7284 Disc Loss: 0.000418258 Q Losses: [0.0070585464, 0.022392686]\n",
      "epoch:5 batch_done:68 Gen Loss: 11.1711 Disc Loss: 0.000110244 Q Losses: [0.0079097664, 0.01861465]\n",
      "epoch:5 batch_done:69 Gen Loss: 33.6264 Disc Loss: 1.03201 Q Losses: [0.0095928814, 0.022923693]\n",
      "epoch:5 batch_done:70 Gen Loss: 32.9075 Disc Loss: 3.54354 Q Losses: [0.0089104958, 0.024429444]\n",
      "epoch:5 batch_done:71 Gen Loss: 23.3923 Disc Loss: 0.698875 Q Losses: [0.008554019, 0.016343875]\n",
      "epoch:5 batch_done:72 Gen Loss: 18.4787 Disc Loss: 0.00350632 Q Losses: [0.0086695077, 0.024966707]\n",
      "epoch:5 batch_done:73 Gen Loss: 15.5251 Disc Loss: 0.00167653 Q Losses: [0.0098306518, 0.017991552]\n",
      "epoch:5 batch_done:74 Gen Loss: 10.274 Disc Loss: 4.5432e-05 Q Losses: [0.0095196888, 0.016138412]\n",
      "epoch:5 batch_done:75 Gen Loss: 9.59063 Disc Loss: 4.20976e-05 Q Losses: [0.008524444, 0.021509619]\n",
      "epoch:5 batch_done:76 Gen Loss: 8.53181 Disc Loss: 0.000157807 Q Losses: [0.0067726364, 0.021294991]\n",
      "epoch:5 batch_done:77 Gen Loss: 7.18928 Disc Loss: 0.000704253 Q Losses: [0.0068255779, 0.015302523]\n",
      "epoch:5 batch_done:78 Gen Loss: 5.81499 Disc Loss: 0.00363695 Q Losses: [0.0086194687, 0.017022481]\n",
      "epoch:5 batch_done:79 Gen Loss: 47.05 Disc Loss: 2.90848 Q Losses: [0.0081963632, 0.020411015]\n",
      "epoch:5 batch_done:80 Gen Loss: 42.7379 Disc Loss: 4.40916 Q Losses: [0.0079990681, 0.022852451]\n",
      "epoch:5 batch_done:81 Gen Loss: 26.0706 Disc Loss: 1.83171 Q Losses: [0.010496438, 0.025199987]\n",
      "epoch:5 batch_done:82 Gen Loss: 22.3242 Disc Loss: 0.213914 Q Losses: [0.0066664191, 0.022774756]\n",
      "epoch:5 batch_done:83 Gen Loss: 8.55711 Disc Loss: 0.0254866 Q Losses: [0.0093920901, 0.024776138]\n",
      "epoch:5 batch_done:84 Gen Loss: 7.06618 Disc Loss: 0.000204219 Q Losses: [0.010668691, 0.02209813]\n",
      "epoch:5 batch_done:85 Gen Loss: 5.50789 Disc Loss: 0.00140917 Q Losses: [0.010177839, 0.022538686]\n",
      "epoch:5 batch_done:86 Gen Loss: 7.14852 Disc Loss: 0.000699022 Q Losses: [0.010972006, 0.025118742]\n",
      "epoch:5 batch_done:87 Gen Loss: 5.06683 Disc Loss: 0.0245675 Q Losses: [0.018972248, 0.026983814]\n",
      "epoch:5 batch_done:88 Gen Loss: 6.28605 Disc Loss: 0.0128418 Q Losses: [0.0098011196, 0.022356849]\n",
      "epoch:5 batch_done:89 Gen Loss: 6.00744 Disc Loss: 0.0061123 Q Losses: [0.0099899955, 0.022573512]\n",
      "epoch:5 batch_done:90 Gen Loss: 7.88189 Disc Loss: 0.00607045 Q Losses: [0.0094420537, 0.01949325]\n",
      "epoch:5 batch_done:91 Gen Loss: 7.37322 Disc Loss: 0.00181922 Q Losses: [0.0098491674, 0.024571169]\n",
      "epoch:5 batch_done:92 Gen Loss: 6.59719 Disc Loss: 0.0167271 Q Losses: [0.011623291, 0.018050548]\n",
      "epoch:5 batch_done:93 Gen Loss: 6.18991 Disc Loss: 0.0554921 Q Losses: [0.012173969, 0.021659058]\n",
      "epoch:5 batch_done:94 Gen Loss: 7.3063 Disc Loss: 0.0037334 Q Losses: [0.015277704, 0.016735211]\n",
      "epoch:5 batch_done:95 Gen Loss: 5.27818 Disc Loss: 0.0581113 Q Losses: [0.010406447, 0.019331286]\n",
      "epoch:5 batch_done:96 Gen Loss: 4.67626 Disc Loss: 0.0527503 Q Losses: [0.011348058, 0.022553947]\n",
      "epoch:5 batch_done:97 Gen Loss: 5.08818 Disc Loss: 0.0267727 Q Losses: [0.010855678, 0.016239416]\n",
      "epoch:5 batch_done:98 Gen Loss: 5.88011 Disc Loss: 0.0147554 Q Losses: [0.0076943235, 0.019456891]\n",
      "epoch:5 batch_done:99 Gen Loss: 5.46514 Disc Loss: 0.0498759 Q Losses: [0.010771358, 0.025411993]\n",
      "epoch:5 batch_done:100 Gen Loss: 4.49331 Disc Loss: 0.120986 Q Losses: [0.0076511302, 0.021582879]\n",
      "epoch:5 batch_done:101 Gen Loss: 5.36893 Disc Loss: 0.0208849 Q Losses: [0.009547404, 0.021089029]\n",
      "epoch:5 batch_done:102 Gen Loss: 6.5874 Disc Loss: 0.00697783 Q Losses: [0.010202346, 0.020870656]\n",
      "epoch:5 batch_done:103 Gen Loss: 8.18129 Disc Loss: 0.0505568 Q Losses: [0.014411081, 0.021307021]\n",
      "epoch:5 batch_done:104 Gen Loss: 6.26695 Disc Loss: 0.050298 Q Losses: [0.0075100884, 0.027794493]\n",
      "epoch:5 batch_done:105 Gen Loss: 7.96628 Disc Loss: 0.00203925 Q Losses: [0.0099083297, 0.019825682]\n",
      "epoch:5 batch_done:106 Gen Loss: 6.2596 Disc Loss: 0.00591547 Q Losses: [0.0058598798, 0.016416194]\n",
      "epoch:5 batch_done:107 Gen Loss: 8.3073 Disc Loss: 0.00532033 Q Losses: [0.0057290494, 0.019221563]\n",
      "epoch:5 batch_done:108 Gen Loss: 6.34587 Disc Loss: 0.00515981 Q Losses: [0.0076197158, 0.020831298]\n",
      "epoch:5 batch_done:109 Gen Loss: 5.98836 Disc Loss: 0.00621745 Q Losses: [0.0096250363, 0.012853531]\n",
      "epoch:5 batch_done:110 Gen Loss: 5.75024 Disc Loss: 0.00746222 Q Losses: [0.024379091, 0.017583121]\n",
      "epoch:5 batch_done:111 Gen Loss: 7.28226 Disc Loss: 0.0142387 Q Losses: [0.0051886365, 0.01752243]\n",
      "epoch:5 batch_done:112 Gen Loss: 6.16907 Disc Loss: 0.00273635 Q Losses: [0.0063104965, 0.016159859]\n",
      "epoch:5 batch_done:113 Gen Loss: 7.72856 Disc Loss: 0.000604993 Q Losses: [0.0066718743, 0.019019589]\n",
      "epoch:5 batch_done:114 Gen Loss: 5.95025 Disc Loss: 0.0393196 Q Losses: [0.0065070395, 0.015110002]\n",
      "epoch:5 batch_done:115 Gen Loss: 11.0298 Disc Loss: 0.133197 Q Losses: [0.0049234931, 0.019133965]\n",
      "epoch:5 batch_done:116 Gen Loss: 15.7098 Disc Loss: 0.132821 Q Losses: [0.0077082906, 0.01680419]\n",
      "epoch:5 batch_done:117 Gen Loss: 13.106 Disc Loss: 0.0267305 Q Losses: [0.0084903929, 0.016299315]\n",
      "epoch:5 batch_done:118 Gen Loss: 20.7763 Disc Loss: 0.00324803 Q Losses: [0.0094888322, 0.01782456]\n",
      "epoch:5 batch_done:119 Gen Loss: 11.6928 Disc Loss: 0.00253468 Q Losses: [0.0073073069, 0.020353537]\n",
      "epoch:5 batch_done:120 Gen Loss: 22.1272 Disc Loss: 0.00487923 Q Losses: [0.0079805236, 0.015681509]\n",
      "epoch:5 batch_done:121 Gen Loss: 15.4571 Disc Loss: 0.0834258 Q Losses: [0.0075922981, 0.019683037]\n",
      "epoch:5 batch_done:122 Gen Loss: 10.5761 Disc Loss: 0.00308848 Q Losses: [0.0066371094, 0.01508439]\n",
      "epoch:5 batch_done:123 Gen Loss: 5.62555 Disc Loss: 0.00681693 Q Losses: [0.0081278458, 0.016831525]\n",
      "epoch:5 batch_done:124 Gen Loss: 8.73561 Disc Loss: 0.00665039 Q Losses: [0.0094978772, 0.014620747]\n",
      "epoch:5 batch_done:125 Gen Loss: 18.0813 Disc Loss: 0.00199095 Q Losses: [0.013419669, 0.015688794]\n",
      "epoch:5 batch_done:126 Gen Loss: 8.1513 Disc Loss: 0.00451924 Q Losses: [0.0070100594, 0.016245421]\n",
      "epoch:5 batch_done:127 Gen Loss: 11.8046 Disc Loss: 0.00547434 Q Losses: [0.011127353, 0.022533257]\n",
      "epoch:5 batch_done:128 Gen Loss: 5.37354 Disc Loss: 0.00755819 Q Losses: [0.0089729922, 0.016715772]\n",
      "epoch:5 batch_done:129 Gen Loss: 8.18058 Disc Loss: 0.101492 Q Losses: [0.0054989289, 0.015429302]\n",
      "epoch:5 batch_done:130 Gen Loss: 17.4091 Disc Loss: 0.0354494 Q Losses: [0.0050335033, 0.01847434]\n",
      "epoch:5 batch_done:131 Gen Loss: 7.51898 Disc Loss: 0.0051937 Q Losses: [0.006585273, 0.014177693]\n",
      "epoch:5 batch_done:132 Gen Loss: 12.7209 Disc Loss: 0.0655863 Q Losses: [0.0080934661, 0.023316659]\n",
      "epoch:5 batch_done:133 Gen Loss: 13.1816 Disc Loss: 0.0263658 Q Losses: [0.0071609206, 0.016778931]\n",
      "epoch:5 batch_done:134 Gen Loss: 11.0167 Disc Loss: 0.000943288 Q Losses: [0.0092042964, 0.017290194]\n",
      "epoch:5 batch_done:135 Gen Loss: 5.67513 Disc Loss: 0.00599399 Q Losses: [0.004367677, 0.014671628]\n",
      "epoch:5 batch_done:136 Gen Loss: 11.5493 Disc Loss: 0.00260167 Q Losses: [0.013067652, 0.020396519]\n",
      "epoch:5 batch_done:137 Gen Loss: 6.94195 Disc Loss: 0.0106241 Q Losses: [0.0078071612, 0.026756994]\n",
      "epoch:5 batch_done:138 Gen Loss: 7.43695 Disc Loss: 0.00208043 Q Losses: [0.0084615042, 0.017900154]\n",
      "epoch:5 batch_done:139 Gen Loss: 7.85441 Disc Loss: 0.00155411 Q Losses: [0.015016058, 0.018475629]\n",
      "epoch:5 batch_done:140 Gen Loss: 24.0331 Disc Loss: 0.573518 Q Losses: [0.0096193459, 0.015615398]\n",
      "epoch:5 batch_done:141 Gen Loss: 27.6296 Disc Loss: 0.451976 Q Losses: [0.0081435386, 0.014842699]\n",
      "epoch:5 batch_done:142 Gen Loss: 29.8214 Disc Loss: 0.13912 Q Losses: [0.012940961, 0.020546064]\n",
      "epoch:5 batch_done:143 Gen Loss: 27.412 Disc Loss: 0.131398 Q Losses: [0.01188492, 0.015481804]\n",
      "epoch:5 batch_done:144 Gen Loss: 25.597 Disc Loss: 0.00712859 Q Losses: [0.010891377, 0.016215658]\n",
      "epoch:5 batch_done:145 Gen Loss: 29.8221 Disc Loss: 0.006849 Q Losses: [0.018224798, 0.022971429]\n",
      "epoch:5 batch_done:146 Gen Loss: 28.301 Disc Loss: 0.0096032 Q Losses: [0.0078130355, 0.014840848]\n",
      "epoch:5 batch_done:147 Gen Loss: 23.5945 Disc Loss: 0.000495142 Q Losses: [0.017147223, 0.023319179]\n",
      "epoch:5 batch_done:148 Gen Loss: 22.1012 Disc Loss: 0.000594476 Q Losses: [0.0088226227, 0.016721312]\n",
      "epoch:5 batch_done:149 Gen Loss: 19.978 Disc Loss: 0.00125796 Q Losses: [0.0094800368, 0.013806644]\n",
      "epoch:5 batch_done:150 Gen Loss: 19.1334 Disc Loss: 0.000985192 Q Losses: [0.0096819857, 0.016399596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch_done:151 Gen Loss: 18.6571 Disc Loss: 7.70906e-05 Q Losses: [0.0089096017, 0.017280258]\n",
      "epoch:5 batch_done:152 Gen Loss: 17.6117 Disc Loss: 0.000368803 Q Losses: [0.022459351, 0.017278425]\n",
      "epoch:5 batch_done:153 Gen Loss: 11.2412 Disc Loss: 0.00178248 Q Losses: [0.02024081, 0.014898357]\n",
      "epoch:5 batch_done:154 Gen Loss: 7.95676 Disc Loss: 0.000972864 Q Losses: [0.012150699, 0.022870794]\n",
      "epoch:5 batch_done:155 Gen Loss: 6.67118 Disc Loss: 0.00184025 Q Losses: [0.0071106604, 0.01720465]\n",
      "epoch:5 batch_done:156 Gen Loss: 17.7143 Disc Loss: 0.158132 Q Losses: [0.013271067, 0.020209052]\n",
      "epoch:5 batch_done:157 Gen Loss: 20.3259 Disc Loss: 0.00357964 Q Losses: [0.010067506, 0.015798647]\n",
      "epoch:5 batch_done:158 Gen Loss: 18.5565 Disc Loss: 0.177106 Q Losses: [0.0077495328, 0.014244552]\n",
      "epoch:5 batch_done:159 Gen Loss: 15.8084 Disc Loss: 0.00279902 Q Losses: [0.011301321, 0.018419372]\n",
      "epoch:5 batch_done:160 Gen Loss: 11.9564 Disc Loss: 0.00883101 Q Losses: [0.010102484, 0.017989887]\n",
      "epoch:5 batch_done:161 Gen Loss: 6.31141 Disc Loss: 0.00310947 Q Losses: [0.0077503901, 0.01793135]\n",
      "epoch:5 batch_done:162 Gen Loss: 8.03483 Disc Loss: 0.00381525 Q Losses: [0.007565836, 0.014839479]\n",
      "epoch:5 batch_done:163 Gen Loss: 21.5161 Disc Loss: 0.0211651 Q Losses: [0.0080922991, 0.016413771]\n",
      "epoch:5 batch_done:164 Gen Loss: 11.598 Disc Loss: 2.95509e-05 Q Losses: [0.011150738, 0.020774137]\n",
      "epoch:5 batch_done:165 Gen Loss: 8.59674 Disc Loss: 0.0394136 Q Losses: [0.0067953318, 0.027698535]\n",
      "epoch:5 batch_done:166 Gen Loss: 12.0928 Disc Loss: 0.000161811 Q Losses: [0.0092340587, 0.015222693]\n",
      "epoch:5 batch_done:167 Gen Loss: 7.40553 Disc Loss: 0.00448166 Q Losses: [0.012756215, 0.017049551]\n",
      "epoch:5 batch_done:168 Gen Loss: 6.79376 Disc Loss: 0.00601893 Q Losses: [0.015751921, 0.017900079]\n",
      "epoch:5 batch_done:169 Gen Loss: 15.7961 Disc Loss: 0.000698416 Q Losses: [0.0095688784, 0.014631074]\n",
      "epoch:5 batch_done:170 Gen Loss: 6.16384 Disc Loss: 0.00510948 Q Losses: [0.012949276, 0.018873658]\n",
      "epoch:5 batch_done:171 Gen Loss: 20.9791 Disc Loss: 0.0363649 Q Losses: [0.0075150593, 0.021325618]\n",
      "epoch:5 batch_done:172 Gen Loss: 14.6545 Disc Loss: 0.00205576 Q Losses: [0.0069033662, 0.017956324]\n",
      "epoch:5 batch_done:173 Gen Loss: 5.90784 Disc Loss: 0.104701 Q Losses: [0.0050060451, 0.018646404]\n",
      "epoch:5 batch_done:174 Gen Loss: 18.9978 Disc Loss: 0.00157497 Q Losses: [0.012413938, 0.018357903]\n",
      "epoch:5 batch_done:175 Gen Loss: 7.24146 Disc Loss: 0.00269026 Q Losses: [0.007006079, 0.015921393]\n",
      "epoch:5 batch_done:176 Gen Loss: 12.0928 Disc Loss: 0.0123335 Q Losses: [0.0084387623, 0.015107314]\n",
      "epoch:5 batch_done:177 Gen Loss: 9.31633 Disc Loss: 0.00593865 Q Losses: [0.011620432, 0.025202515]\n",
      "epoch:5 batch_done:178 Gen Loss: 6.18372 Disc Loss: 0.0127074 Q Losses: [0.0096560717, 0.024431579]\n",
      "epoch:5 batch_done:179 Gen Loss: 9.87214 Disc Loss: 0.0481995 Q Losses: [0.011341601, 0.016625099]\n",
      "epoch:5 batch_done:180 Gen Loss: 5.38938 Disc Loss: 0.00796417 Q Losses: [0.0085594738, 0.016141517]\n",
      "epoch:5 batch_done:181 Gen Loss: 15.1664 Disc Loss: 0.000263657 Q Losses: [0.0072833672, 0.023563707]\n",
      "epoch:5 batch_done:182 Gen Loss: 11.846 Disc Loss: 0.111655 Q Losses: [0.0063768588, 0.017805794]\n",
      "epoch:5 batch_done:183 Gen Loss: 26.69 Disc Loss: 0.0169548 Q Losses: [0.011640011, 0.020766761]\n",
      "epoch:5 batch_done:184 Gen Loss: 14.8218 Disc Loss: 0.00323864 Q Losses: [0.0094002001, 0.015369903]\n",
      "epoch:5 batch_done:185 Gen Loss: 26.2326 Disc Loss: 0.00444663 Q Losses: [0.014211223, 0.014715491]\n",
      "epoch:5 batch_done:186 Gen Loss: 13.0592 Disc Loss: 0.0134327 Q Losses: [0.016508685, 0.019046027]\n",
      "epoch:5 batch_done:187 Gen Loss: 25.6127 Disc Loss: 0.0100737 Q Losses: [0.010252612, 0.021117531]\n",
      "epoch:5 batch_done:188 Gen Loss: 14.9429 Disc Loss: 0.00117198 Q Losses: [0.010192048, 0.017494071]\n",
      "epoch:5 batch_done:189 Gen Loss: 17.7463 Disc Loss: 0.00339942 Q Losses: [0.0088225435, 0.013760107]\n",
      "epoch:5 batch_done:190 Gen Loss: 15.8418 Disc Loss: 0.0080505 Q Losses: [0.009855995, 0.016464233]\n",
      "epoch:5 batch_done:191 Gen Loss: 19.9905 Disc Loss: 0.0422991 Q Losses: [0.0079566259, 0.015972681]\n",
      "epoch:5 batch_done:192 Gen Loss: 6.86247 Disc Loss: 0.00682127 Q Losses: [0.0076504638, 0.014271461]\n",
      "epoch:5 batch_done:193 Gen Loss: 7.15963 Disc Loss: 0.00235196 Q Losses: [0.015567792, 0.012124706]\n",
      "epoch:5 batch_done:194 Gen Loss: 18.5737 Disc Loss: 0.0230164 Q Losses: [0.01279678, 0.015478872]\n",
      "epoch:5 batch_done:195 Gen Loss: 8.79468 Disc Loss: 0.0815535 Q Losses: [0.0087364465, 0.016438523]\n",
      "epoch:5 batch_done:196 Gen Loss: 25.865 Disc Loss: 0.00221335 Q Losses: [0.0078226086, 0.015445912]\n",
      "epoch:5 batch_done:197 Gen Loss: 20.2886 Disc Loss: 0.00853648 Q Losses: [0.0089415088, 0.018730242]\n",
      "epoch:5 batch_done:198 Gen Loss: 16.5384 Disc Loss: 0.0127677 Q Losses: [0.0055137575, 0.012187245]\n",
      "epoch:5 batch_done:199 Gen Loss: 9.69951 Disc Loss: 0.0243263 Q Losses: [0.0062622307, 0.016799565]\n",
      "epoch:5 batch_done:200 Gen Loss: 23.3926 Disc Loss: 0.00127154 Q Losses: [0.0075898436, 0.01271418]\n",
      "epoch:5 batch_done:201 Gen Loss: 8.89198 Disc Loss: 0.000872651 Q Losses: [0.0087187579, 0.014422898]\n",
      "epoch:5 batch_done:202 Gen Loss: 26.6828 Disc Loss: 0.00136879 Q Losses: [0.0069861389, 0.016382836]\n",
      "epoch:5 batch_done:203 Gen Loss: 19.9561 Disc Loss: 0.000222712 Q Losses: [0.010120293, 0.013627967]\n",
      "epoch:5 batch_done:204 Gen Loss: 12.8221 Disc Loss: 0.000711373 Q Losses: [0.011581705, 0.015815897]\n",
      "epoch:5 batch_done:205 Gen Loss: 21.3851 Disc Loss: 4.39054e-05 Q Losses: [0.02359172, 0.021129403]\n",
      "epoch:5 batch_done:206 Gen Loss: 15.5167 Disc Loss: 0.000217396 Q Losses: [0.0077158739, 0.021445584]\n",
      "epoch:5 batch_done:207 Gen Loss: 7.15657 Disc Loss: 0.000927079 Q Losses: [0.0098173711, 0.018977173]\n",
      "epoch:6 batch_done:1 Gen Loss: 14.2339 Disc Loss: 0.000734062 Q Losses: [0.0064124502, 0.016539868]\n",
      "epoch:6 batch_done:2 Gen Loss: 9.66672 Disc Loss: 0.000243917 Q Losses: [0.012908953, 0.026699033]\n",
      "epoch:6 batch_done:3 Gen Loss: 19.8107 Disc Loss: 0.000279547 Q Losses: [0.013092002, 0.016491922]\n",
      "epoch:6 batch_done:4 Gen Loss: 10.2772 Disc Loss: 0.000881519 Q Losses: [0.0063392143, 0.016687676]\n",
      "epoch:6 batch_done:5 Gen Loss: 13.167 Disc Loss: 0.00115776 Q Losses: [0.0094175637, 0.019815514]\n",
      "epoch:6 batch_done:6 Gen Loss: 5.81971 Disc Loss: 0.0142694 Q Losses: [0.0075935861, 0.012837276]\n",
      "epoch:6 batch_done:7 Gen Loss: 5.63231 Disc Loss: 0.0138876 Q Losses: [0.0061267521, 0.024379412]\n",
      "epoch:6 batch_done:8 Gen Loss: 6.12626 Disc Loss: 0.0184426 Q Losses: [0.012740894, 0.022802517]\n",
      "epoch:6 batch_done:9 Gen Loss: 26.539 Disc Loss: 0.000280249 Q Losses: [0.0099871717, 0.01435579]\n",
      "epoch:6 batch_done:10 Gen Loss: 20.2517 Disc Loss: 0.000843524 Q Losses: [0.0078305323, 0.019894464]\n",
      "epoch:6 batch_done:11 Gen Loss: 16.6487 Disc Loss: 0.168493 Q Losses: [0.0058706645, 0.014311548]\n",
      "epoch:6 batch_done:12 Gen Loss: 30.902 Disc Loss: 0.00316446 Q Losses: [0.015487165, 0.018655617]\n",
      "epoch:6 batch_done:13 Gen Loss: 15.2166 Disc Loss: 0.0889523 Q Losses: [0.0090947896, 0.016005453]\n",
      "epoch:6 batch_done:14 Gen Loss: 18.5816 Disc Loss: 0.330483 Q Losses: [0.0099991988, 0.019032581]\n",
      "epoch:6 batch_done:15 Gen Loss: 30.7662 Disc Loss: 6.24658e-05 Q Losses: [0.011603598, 0.015136169]\n",
      "epoch:6 batch_done:16 Gen Loss: 14.5428 Disc Loss: 1.07631e-05 Q Losses: [0.0082892627, 0.016949382]\n",
      "epoch:6 batch_done:17 Gen Loss: 28.6421 Disc Loss: 4.80252e-05 Q Losses: [0.0067913095, 0.024404086]\n",
      "epoch:6 batch_done:18 Gen Loss: 18.1739 Disc Loss: 2.12906e-06 Q Losses: [0.007104014, 0.016025681]\n",
      "epoch:6 batch_done:19 Gen Loss: 16.1076 Disc Loss: 5.16076e-06 Q Losses: [0.0068756458, 0.017662263]\n",
      "epoch:6 batch_done:20 Gen Loss: 14.1833 Disc Loss: 1.0049e-06 Q Losses: [0.0062629585, 0.017713787]\n",
      "epoch:6 batch_done:21 Gen Loss: 12.9413 Disc Loss: 2.92156e-06 Q Losses: [0.014809264, 0.02574221]\n",
      "epoch:6 batch_done:22 Gen Loss: 11.9306 Disc Loss: 9.85183e-06 Q Losses: [0.007075476, 0.015873797]\n",
      "epoch:6 batch_done:23 Gen Loss: 6.41677 Disc Loss: 0.00221363 Q Losses: [0.0082364529, 0.016000386]\n",
      "epoch:6 batch_done:24 Gen Loss: 11.7841 Disc Loss: 0.000133136 Q Losses: [0.013711389, 0.020169057]\n",
      "epoch:6 batch_done:25 Gen Loss: 11.8981 Disc Loss: 8.19847e-06 Q Losses: [0.0092773531, 0.020763634]\n",
      "epoch:6 batch_done:26 Gen Loss: 11.4427 Disc Loss: 1.14367e-05 Q Losses: [0.0058143358, 0.013785124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:27 Gen Loss: 20.8809 Disc Loss: 0.169596 Q Losses: [0.0091792783, 0.014805271]\n",
      "epoch:6 batch_done:28 Gen Loss: 28.4012 Disc Loss: 0.00115071 Q Losses: [0.0070272391, 0.013470104]\n",
      "epoch:6 batch_done:29 Gen Loss: 21.3171 Disc Loss: 0.00327242 Q Losses: [0.0098392479, 0.019479007]\n",
      "epoch:6 batch_done:30 Gen Loss: 23.3647 Disc Loss: 0.0138101 Q Losses: [0.015591197, 0.015171735]\n",
      "epoch:6 batch_done:31 Gen Loss: 36.1607 Disc Loss: 0.00664897 Q Losses: [0.0162823, 0.020702878]\n",
      "epoch:6 batch_done:32 Gen Loss: 31.4337 Disc Loss: 0.00157622 Q Losses: [0.010356611, 0.016294857]\n",
      "epoch:6 batch_done:33 Gen Loss: 19.6747 Disc Loss: 0.000385167 Q Losses: [0.011044564, 0.013436876]\n",
      "epoch:6 batch_done:34 Gen Loss: 33.543 Disc Loss: 0.000962505 Q Losses: [0.0071325917, 0.016135804]\n",
      "epoch:6 batch_done:35 Gen Loss: 18.1787 Disc Loss: 6.28607e-05 Q Losses: [0.015656609, 0.020227108]\n",
      "epoch:6 batch_done:36 Gen Loss: 29.8171 Disc Loss: 0.000302522 Q Losses: [0.0076234061, 0.014175062]\n",
      "epoch:6 batch_done:37 Gen Loss: 19.3579 Disc Loss: 0.000647094 Q Losses: [0.0073848213, 0.014742303]\n",
      "epoch:6 batch_done:38 Gen Loss: 30.6878 Disc Loss: 0.000619229 Q Losses: [0.014835697, 0.025629451]\n",
      "epoch:6 batch_done:39 Gen Loss: 27.5226 Disc Loss: 0.00090219 Q Losses: [0.0067366464, 0.015917335]\n",
      "epoch:6 batch_done:40 Gen Loss: 18.5878 Disc Loss: 0.00162173 Q Losses: [0.01596535, 0.016721804]\n",
      "epoch:6 batch_done:41 Gen Loss: 25.5027 Disc Loss: 0.00041352 Q Losses: [0.01140141, 0.020123709]\n",
      "epoch:6 batch_done:42 Gen Loss: 15.1879 Disc Loss: 0.000289997 Q Losses: [0.0072011491, 0.015683172]\n",
      "epoch:6 batch_done:43 Gen Loss: 29.7756 Disc Loss: 0.000576622 Q Losses: [0.0077161477, 0.01544288]\n",
      "epoch:6 batch_done:44 Gen Loss: 26.8257 Disc Loss: 0.000993474 Q Losses: [0.0064979508, 0.016130077]\n",
      "epoch:6 batch_done:45 Gen Loss: 18.5617 Disc Loss: 0.000324171 Q Losses: [0.022988293, 0.0132451]\n",
      "epoch:6 batch_done:46 Gen Loss: 28.6608 Disc Loss: 0.000534831 Q Losses: [0.0073778937, 0.016796285]\n",
      "epoch:6 batch_done:47 Gen Loss: 21.3623 Disc Loss: 0.00109173 Q Losses: [0.013331492, 0.014385409]\n",
      "epoch:6 batch_done:48 Gen Loss: 26.3349 Disc Loss: 0.0251267 Q Losses: [0.0084626265, 0.01348806]\n",
      "epoch:6 batch_done:49 Gen Loss: 18.5779 Disc Loss: 0.00434335 Q Losses: [0.012104478, 0.014109546]\n",
      "epoch:6 batch_done:50 Gen Loss: 19.029 Disc Loss: 0.00118179 Q Losses: [0.025949882, 0.012057178]\n",
      "epoch:6 batch_done:51 Gen Loss: 23.4572 Disc Loss: 0.0428963 Q Losses: [0.0077793431, 0.016650911]\n",
      "epoch:6 batch_done:52 Gen Loss: 18.4069 Disc Loss: 0.000148269 Q Losses: [0.008877757, 0.013708036]\n",
      "epoch:6 batch_done:53 Gen Loss: 21.037 Disc Loss: 0.000139653 Q Losses: [0.01313914, 0.022298487]\n",
      "epoch:6 batch_done:54 Gen Loss: 13.1993 Disc Loss: 6.57616e-06 Q Losses: [0.0067011751, 0.017588984]\n",
      "epoch:6 batch_done:55 Gen Loss: 11.7245 Disc Loss: 2.4224e-05 Q Losses: [0.0072625061, 0.020382425]\n",
      "epoch:6 batch_done:56 Gen Loss: 8.51687 Disc Loss: 0.0346942 Q Losses: [0.01053786, 0.015557068]\n",
      "epoch:6 batch_done:57 Gen Loss: 15.5505 Disc Loss: 8.30809e-05 Q Losses: [0.0057238061, 0.013489487]\n",
      "epoch:6 batch_done:58 Gen Loss: 16.6836 Disc Loss: 0.000247318 Q Losses: [0.0076875612, 0.02330086]\n",
      "epoch:6 batch_done:59 Gen Loss: 27.0748 Disc Loss: 4.12191e-05 Q Losses: [0.0063641458, 0.016262256]\n",
      "epoch:6 batch_done:60 Gen Loss: 19.6217 Disc Loss: 2.37136e-05 Q Losses: [0.0088307839, 0.017695315]\n",
      "epoch:6 batch_done:61 Gen Loss: 20.6165 Disc Loss: 0.000358429 Q Losses: [0.0065331748, 0.016193509]\n",
      "epoch:6 batch_done:62 Gen Loss: 11.4914 Disc Loss: 0.000569885 Q Losses: [0.0079848627, 0.014130145]\n",
      "epoch:6 batch_done:63 Gen Loss: 13.1935 Disc Loss: 4.72698e-05 Q Losses: [0.0073668151, 0.012316581]\n",
      "epoch:6 batch_done:64 Gen Loss: 14.3575 Disc Loss: 0.000309011 Q Losses: [0.010740165, 0.016898388]\n",
      "epoch:6 batch_done:65 Gen Loss: 25.9274 Disc Loss: 0.00380964 Q Losses: [0.0058574164, 0.012047172]\n",
      "epoch:6 batch_done:66 Gen Loss: 15.0816 Disc Loss: 0.000165128 Q Losses: [0.0063336794, 0.016251234]\n",
      "epoch:6 batch_done:67 Gen Loss: 21.5012 Disc Loss: 0.00519965 Q Losses: [0.0087951487, 0.015282996]\n",
      "epoch:6 batch_done:68 Gen Loss: 9.35583 Disc Loss: 0.00208189 Q Losses: [0.0096224044, 0.015122283]\n",
      "epoch:6 batch_done:69 Gen Loss: 23.8692 Disc Loss: 0.000865181 Q Losses: [0.0058914139, 0.013239061]\n",
      "epoch:6 batch_done:70 Gen Loss: 14.2421 Disc Loss: 0.0390482 Q Losses: [0.0062246593, 0.018067233]\n",
      "epoch:6 batch_done:71 Gen Loss: 28.3231 Disc Loss: 0.000670368 Q Losses: [0.0079945289, 0.014234794]\n",
      "epoch:6 batch_done:72 Gen Loss: 24.4273 Disc Loss: 0.000175334 Q Losses: [0.015391431, 0.016452964]\n",
      "epoch:6 batch_done:73 Gen Loss: 11.2886 Disc Loss: 0.00200836 Q Losses: [0.019173663, 0.015473037]\n",
      "epoch:6 batch_done:74 Gen Loss: 23.5829 Disc Loss: 0.000371039 Q Losses: [0.0069390293, 0.011111322]\n",
      "epoch:6 batch_done:75 Gen Loss: 12.3307 Disc Loss: 0.00183118 Q Losses: [0.0088244956, 0.015813693]\n",
      "epoch:6 batch_done:76 Gen Loss: 25.5571 Disc Loss: 0.000201307 Q Losses: [0.013617616, 0.011164091]\n",
      "epoch:6 batch_done:77 Gen Loss: 13.7528 Disc Loss: 0.00010621 Q Losses: [0.011048758, 0.013843138]\n",
      "epoch:6 batch_done:78 Gen Loss: 23.345 Disc Loss: 0.00265462 Q Losses: [0.018948397, 0.01338613]\n",
      "epoch:6 batch_done:79 Gen Loss: 9.13485 Disc Loss: 0.00419092 Q Losses: [0.0070902151, 0.013299794]\n",
      "epoch:6 batch_done:80 Gen Loss: 17.7822 Disc Loss: 0.00899774 Q Losses: [0.019484775, 0.013559709]\n",
      "epoch:6 batch_done:81 Gen Loss: 11.7272 Disc Loss: 0.00635052 Q Losses: [0.011478069, 0.012959702]\n",
      "epoch:6 batch_done:82 Gen Loss: 5.86837 Disc Loss: 0.0212302 Q Losses: [0.013927299, 0.011889986]\n",
      "epoch:6 batch_done:83 Gen Loss: 8.11998 Disc Loss: 0.00384882 Q Losses: [0.010314139, 0.013980189]\n",
      "epoch:6 batch_done:84 Gen Loss: 8.47654 Disc Loss: 0.00280939 Q Losses: [0.017299743, 0.018347623]\n",
      "epoch:6 batch_done:85 Gen Loss: 17.4459 Disc Loss: 0.000865529 Q Losses: [0.010427581, 0.011726627]\n",
      "epoch:6 batch_done:86 Gen Loss: 10.0109 Disc Loss: 0.0609927 Q Losses: [0.01110506, 0.017075563]\n",
      "epoch:6 batch_done:87 Gen Loss: 30.1136 Disc Loss: 0.0136043 Q Losses: [0.0062641529, 0.012237173]\n",
      "epoch:6 batch_done:88 Gen Loss: 25.6982 Disc Loss: 0.0032241 Q Losses: [0.0066465936, 0.01129135]\n",
      "epoch:6 batch_done:89 Gen Loss: 13.5795 Disc Loss: 0.0123627 Q Losses: [0.011618413, 0.024323368]\n",
      "epoch:6 batch_done:90 Gen Loss: 24.5781 Disc Loss: 0.00183135 Q Losses: [0.016760044, 0.014000796]\n",
      "epoch:6 batch_done:91 Gen Loss: 9.73212 Disc Loss: 0.0271375 Q Losses: [0.0068787942, 0.015427539]\n",
      "epoch:6 batch_done:92 Gen Loss: 23.3364 Disc Loss: 0.000933879 Q Losses: [0.0074096164, 0.012175261]\n",
      "epoch:6 batch_done:93 Gen Loss: 9.21793 Disc Loss: 0.00323288 Q Losses: [0.0070816805, 0.015384981]\n",
      "epoch:6 batch_done:94 Gen Loss: 25.9874 Disc Loss: 0.000491293 Q Losses: [0.0043758107, 0.020702414]\n",
      "epoch:6 batch_done:95 Gen Loss: 21.1894 Disc Loss: 0.000204432 Q Losses: [0.011719337, 0.013318575]\n",
      "epoch:6 batch_done:96 Gen Loss: 11.6282 Disc Loss: 4.34601e-05 Q Losses: [0.0057665808, 0.012622412]\n",
      "epoch:6 batch_done:97 Gen Loss: 11.7022 Disc Loss: 0.000143125 Q Losses: [0.0064522838, 0.012922637]\n",
      "epoch:6 batch_done:98 Gen Loss: 10.0292 Disc Loss: 9.01555e-05 Q Losses: [0.0092549045, 0.016041912]\n",
      "epoch:6 batch_done:99 Gen Loss: 9.89244 Disc Loss: 0.00331481 Q Losses: [0.0095216949, 0.011849685]\n",
      "epoch:6 batch_done:100 Gen Loss: 16.6046 Disc Loss: 0.00301754 Q Losses: [0.0063201794, 0.01379911]\n",
      "epoch:6 batch_done:101 Gen Loss: 9.54516 Disc Loss: 0.000261105 Q Losses: [0.0087792985, 0.016892403]\n",
      "epoch:6 batch_done:102 Gen Loss: 22.3305 Disc Loss: 0.00272446 Q Losses: [0.018162563, 0.01348695]\n",
      "epoch:6 batch_done:103 Gen Loss: 14.6958 Disc Loss: 0.000873571 Q Losses: [0.0079084188, 0.010447529]\n",
      "epoch:6 batch_done:104 Gen Loss: 11.8936 Disc Loss: 0.000549455 Q Losses: [0.021474399, 0.018158456]\n",
      "epoch:6 batch_done:105 Gen Loss: 15.1199 Disc Loss: 0.00268276 Q Losses: [0.0078266934, 0.014285363]\n",
      "epoch:6 batch_done:106 Gen Loss: 7.73684 Disc Loss: 0.00826054 Q Losses: [0.0067000077, 0.01838926]\n",
      "epoch:6 batch_done:107 Gen Loss: 13.6201 Disc Loss: 2.22831e-05 Q Losses: [0.0083806487, 0.011521579]\n",
      "epoch:6 batch_done:108 Gen Loss: 11.9119 Disc Loss: 0.0732535 Q Losses: [0.010949384, 0.013056219]\n",
      "epoch:6 batch_done:109 Gen Loss: 25.0256 Disc Loss: 0.108347 Q Losses: [0.0090279151, 0.011744982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:110 Gen Loss: 19.5299 Disc Loss: 0.00694119 Q Losses: [0.0062118806, 0.013389417]\n",
      "epoch:6 batch_done:111 Gen Loss: 24.783 Disc Loss: 0.000527894 Q Losses: [0.011539421, 0.015027828]\n",
      "epoch:6 batch_done:112 Gen Loss: 14.2361 Disc Loss: 0.00121467 Q Losses: [0.0068207192, 0.015974063]\n",
      "epoch:6 batch_done:113 Gen Loss: 25.6876 Disc Loss: 0.00105179 Q Losses: [0.0097743748, 0.011462979]\n",
      "epoch:6 batch_done:114 Gen Loss: 17.7473 Disc Loss: 0.000433854 Q Losses: [0.0063518877, 0.01107472]\n",
      "epoch:6 batch_done:115 Gen Loss: 24.3146 Disc Loss: 0.00108251 Q Losses: [0.010410724, 0.013560856]\n",
      "epoch:6 batch_done:116 Gen Loss: 16.1431 Disc Loss: 0.00189867 Q Losses: [0.011734481, 0.013385389]\n",
      "epoch:6 batch_done:117 Gen Loss: 23.0791 Disc Loss: 0.000144033 Q Losses: [0.0097660702, 0.010876402]\n",
      "epoch:6 batch_done:118 Gen Loss: 8.27739 Disc Loss: 0.0202152 Q Losses: [0.007673813, 0.011054657]\n",
      "epoch:6 batch_done:119 Gen Loss: 11.8957 Disc Loss: 0.000132795 Q Losses: [0.0056970716, 0.012117146]\n",
      "epoch:6 batch_done:120 Gen Loss: 25.2527 Disc Loss: 1.92442e-05 Q Losses: [0.0074325921, 0.011880672]\n",
      "epoch:6 batch_done:121 Gen Loss: 18.9514 Disc Loss: 0.000659075 Q Losses: [0.0076365024, 0.012298655]\n",
      "epoch:6 batch_done:122 Gen Loss: 14.2955 Disc Loss: 0.000675381 Q Losses: [0.0069492329, 0.012978692]\n",
      "epoch:6 batch_done:123 Gen Loss: 10.6141 Disc Loss: 0.000482833 Q Losses: [0.011252366, 0.0097744009]\n",
      "epoch:6 batch_done:124 Gen Loss: 20.3466 Disc Loss: 4.22666e-05 Q Losses: [0.014290843, 0.012611217]\n",
      "epoch:6 batch_done:125 Gen Loss: 6.90453 Disc Loss: 0.0026046 Q Losses: [0.0095287589, 0.018223412]\n",
      "epoch:6 batch_done:126 Gen Loss: 14.8198 Disc Loss: 2.19382e-05 Q Losses: [0.0081598256, 0.013193734]\n",
      "epoch:6 batch_done:127 Gen Loss: 23.891 Disc Loss: 0.000175708 Q Losses: [0.007744601, 0.013015041]\n",
      "epoch:6 batch_done:128 Gen Loss: 12.4886 Disc Loss: 0.000116079 Q Losses: [0.0075505176, 0.012966895]\n",
      "epoch:6 batch_done:129 Gen Loss: 22.9754 Disc Loss: 9.47803e-05 Q Losses: [0.006034784, 0.01653371]\n",
      "epoch:6 batch_done:130 Gen Loss: 11.5623 Disc Loss: 0.000400761 Q Losses: [0.01061108, 0.011890501]\n",
      "epoch:6 batch_done:131 Gen Loss: 19.9886 Disc Loss: 9.59202e-05 Q Losses: [0.007474646, 0.012463425]\n",
      "epoch:6 batch_done:132 Gen Loss: 9.75366 Disc Loss: 0.00239659 Q Losses: [0.0090306466, 0.017784171]\n",
      "epoch:6 batch_done:133 Gen Loss: 13.4937 Disc Loss: 5.61563e-05 Q Losses: [0.0094736004, 0.013543607]\n",
      "epoch:6 batch_done:134 Gen Loss: 12.0384 Disc Loss: 9.76625e-05 Q Losses: [0.0072153248, 0.01717166]\n",
      "epoch:6 batch_done:135 Gen Loss: 9.87152 Disc Loss: 0.000591351 Q Losses: [0.0095953755, 0.015375075]\n",
      "epoch:6 batch_done:136 Gen Loss: 9.16509 Disc Loss: 0.000374581 Q Losses: [0.013233109, 0.012874253]\n",
      "epoch:6 batch_done:137 Gen Loss: 16.9492 Disc Loss: 0.000576913 Q Losses: [0.0075607891, 0.013161503]\n",
      "epoch:6 batch_done:138 Gen Loss: 11.7192 Disc Loss: 0.00264035 Q Losses: [0.0064015416, 0.01745541]\n",
      "epoch:6 batch_done:139 Gen Loss: 14.751 Disc Loss: 0.000407722 Q Losses: [0.021374634, 0.019078679]\n",
      "epoch:6 batch_done:140 Gen Loss: 6.11676 Disc Loss: 0.0133514 Q Losses: [0.0080914795, 0.016753081]\n",
      "epoch:6 batch_done:141 Gen Loss: 8.33874 Disc Loss: 0.000966925 Q Losses: [0.0079133455, 0.01381929]\n",
      "epoch:6 batch_done:142 Gen Loss: 19.1539 Disc Loss: 0.000165686 Q Losses: [0.0065492298, 0.017435687]\n",
      "epoch:6 batch_done:143 Gen Loss: 6.53536 Disc Loss: 0.0026834 Q Losses: [0.0086590806, 0.016285842]\n",
      "epoch:6 batch_done:144 Gen Loss: 21.1629 Disc Loss: 0.00477828 Q Losses: [0.011205949, 0.0144463]\n",
      "epoch:6 batch_done:145 Gen Loss: 5.28621 Disc Loss: 0.0282692 Q Losses: [0.0070731025, 0.016900538]\n",
      "epoch:6 batch_done:146 Gen Loss: 18.155 Disc Loss: 0.00190356 Q Losses: [0.0088698026, 0.013543272]\n",
      "epoch:6 batch_done:147 Gen Loss: 11.3065 Disc Loss: 0.000381966 Q Losses: [0.0071024029, 0.017967965]\n",
      "epoch:6 batch_done:148 Gen Loss: 16.3453 Disc Loss: 3.77957e-05 Q Losses: [0.0094089024, 0.017602641]\n",
      "epoch:6 batch_done:149 Gen Loss: 31.1903 Disc Loss: 0.297123 Q Losses: [0.010668527, 0.016028613]\n",
      "epoch:6 batch_done:150 Gen Loss: 35.5492 Disc Loss: 0.753596 Q Losses: [0.006789736, 0.015696071]\n",
      "epoch:6 batch_done:151 Gen Loss: 32.972 Disc Loss: 0.000504639 Q Losses: [0.0093345214, 0.011665655]\n",
      "epoch:6 batch_done:152 Gen Loss: 32.6173 Disc Loss: 1.79191e-05 Q Losses: [0.0082186144, 0.018158149]\n",
      "epoch:6 batch_done:153 Gen Loss: 31.9896 Disc Loss: 5.40167e-08 Q Losses: [0.016906563, 0.011920874]\n",
      "epoch:6 batch_done:154 Gen Loss: 32.9866 Disc Loss: 9.22031e-07 Q Losses: [0.010918064, 0.023796137]\n",
      "epoch:6 batch_done:155 Gen Loss: 32.3368 Disc Loss: 2.10182e-05 Q Losses: [0.010647049, 0.014898496]\n",
      "epoch:6 batch_done:156 Gen Loss: 35.0896 Disc Loss: 2.86848e-07 Q Losses: [0.0099547431, 0.017887959]\n",
      "epoch:6 batch_done:157 Gen Loss: 30.1038 Disc Loss: 1.06171e-07 Q Losses: [0.0087471306, 0.033359125]\n",
      "epoch:6 batch_done:158 Gen Loss: 43.2449 Disc Loss: 1.69501e-07 Q Losses: [0.0066660526, 0.032962587]\n",
      "epoch:6 batch_done:159 Gen Loss: 44.0557 Disc Loss: 1.67638e-08 Q Losses: [0.01143941, 0.019061927]\n",
      "epoch:6 batch_done:160 Gen Loss: 40.6025 Disc Loss: 6.2378e-06 Q Losses: [0.017979991, 0.045826547]\n",
      "epoch:6 batch_done:161 Gen Loss: 36.239 Disc Loss: 2.68222e-07 Q Losses: [0.0058373986, 0.056208275]\n",
      "epoch:6 batch_done:162 Gen Loss: 35.5784 Disc Loss: 7.33115e-05 Q Losses: [0.014783699, 0.072650969]\n",
      "epoch:6 batch_done:163 Gen Loss: 35.2171 Disc Loss: 4.37616e-05 Q Losses: [0.0086522028, 0.10846011]\n",
      "epoch:6 batch_done:164 Gen Loss: 32.4279 Disc Loss: 0.000601175 Q Losses: [0.012381259, 0.099717379]\n",
      "epoch:6 batch_done:165 Gen Loss: 28.6114 Disc Loss: 1.71217e-05 Q Losses: [0.014986073, 0.094535589]\n",
      "epoch:6 batch_done:166 Gen Loss: 17.7964 Disc Loss: 0.000467836 Q Losses: [0.016973257, 0.27932942]\n",
      "epoch:6 batch_done:167 Gen Loss: 28.247 Disc Loss: 0.128725 Q Losses: [0.01958387, 0.50200391]\n",
      "epoch:6 batch_done:168 Gen Loss: 40.857 Disc Loss: 0.536101 Q Losses: [0.021789953, 0.2758916]\n",
      "epoch:6 batch_done:169 Gen Loss: 38.9359 Disc Loss: 0.0457865 Q Losses: [0.051603246, 1.1317613]\n",
      "epoch:6 batch_done:170 Gen Loss: 22.8015 Disc Loss: 0.000787491 Q Losses: [0.042025521, 0.52139258]\n",
      "epoch:6 batch_done:171 Gen Loss: 20.2056 Disc Loss: 0.000162294 Q Losses: [0.042509966, 0.41645029]\n",
      "epoch:6 batch_done:172 Gen Loss: 24.1844 Disc Loss: 0.00229783 Q Losses: [0.079609036, 0.60816276]\n",
      "epoch:6 batch_done:173 Gen Loss: 31.2432 Disc Loss: 0.00250671 Q Losses: [0.055889003, 0.38302469]\n",
      "epoch:6 batch_done:174 Gen Loss: 29.2628 Disc Loss: 0.0447786 Q Losses: [0.053082708, 0.25949919]\n",
      "epoch:6 batch_done:175 Gen Loss: 25.7428 Disc Loss: 0.00344662 Q Losses: [0.063093841, 0.32781339]\n",
      "epoch:6 batch_done:176 Gen Loss: 24.5234 Disc Loss: 0.000464512 Q Losses: [0.13049263, 0.45370358]\n",
      "epoch:6 batch_done:177 Gen Loss: 21.3387 Disc Loss: 8.44311e-05 Q Losses: [0.058730185, 0.23245397]\n",
      "epoch:6 batch_done:178 Gen Loss: 19.5632 Disc Loss: 0.000201484 Q Losses: [0.058209684, 0.19747213]\n",
      "epoch:6 batch_done:179 Gen Loss: 18.7259 Disc Loss: 0.000103447 Q Losses: [0.036135204, 0.37703907]\n",
      "epoch:6 batch_done:180 Gen Loss: 16.114 Disc Loss: 0.000185154 Q Losses: [0.048493542, 0.18907301]\n",
      "epoch:6 batch_done:181 Gen Loss: 15.9309 Disc Loss: 4.4987e-05 Q Losses: [0.029338434, 0.099614479]\n",
      "epoch:6 batch_done:182 Gen Loss: 15.9431 Disc Loss: 0.000457049 Q Losses: [0.019877598, 0.061817054]\n",
      "epoch:6 batch_done:183 Gen Loss: 16.7136 Disc Loss: 1.9384e-05 Q Losses: [0.017121851, 0.083610162]\n",
      "epoch:6 batch_done:184 Gen Loss: 16.8381 Disc Loss: 6.52884e-06 Q Losses: [0.014822662, 0.086765766]\n",
      "epoch:6 batch_done:185 Gen Loss: 16.66 Disc Loss: 3.04546e-06 Q Losses: [0.014327537, 0.07448554]\n",
      "epoch:6 batch_done:186 Gen Loss: 16.0282 Disc Loss: 7.69198e-05 Q Losses: [0.018436128, 0.03393811]\n",
      "epoch:6 batch_done:187 Gen Loss: 15.4835 Disc Loss: 4.32463e-05 Q Losses: [0.013093788, 0.061748832]\n",
      "epoch:6 batch_done:188 Gen Loss: 15.3429 Disc Loss: 6.20639e-05 Q Losses: [0.017843362, 0.046900354]\n",
      "epoch:6 batch_done:189 Gen Loss: 15.4162 Disc Loss: 2.06541e-05 Q Losses: [0.020302346, 0.026541382]\n",
      "epoch:6 batch_done:190 Gen Loss: 15.5869 Disc Loss: 0.000146911 Q Losses: [0.010583533, 0.034862537]\n",
      "epoch:6 batch_done:191 Gen Loss: 15.4502 Disc Loss: 9.61129e-05 Q Losses: [0.016331187, 0.024462614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:192 Gen Loss: 15.3596 Disc Loss: 2.12431e-05 Q Losses: [0.012021819, 0.028521679]\n",
      "epoch:6 batch_done:193 Gen Loss: 15.307 Disc Loss: 3.66482e-05 Q Losses: [0.010595437, 0.021354977]\n",
      "epoch:6 batch_done:194 Gen Loss: 15.2304 Disc Loss: 7.06331e-06 Q Losses: [0.0095149567, 0.016482197]\n",
      "epoch:6 batch_done:195 Gen Loss: 15.0093 Disc Loss: 2.34844e-05 Q Losses: [0.0080573857, 0.014978097]\n",
      "epoch:6 batch_done:196 Gen Loss: 14.7249 Disc Loss: 8.6459e-05 Q Losses: [0.0083470158, 0.018919827]\n",
      "epoch:6 batch_done:197 Gen Loss: 14.4535 Disc Loss: 0.00027148 Q Losses: [0.023725335, 0.015988763]\n",
      "epoch:6 batch_done:198 Gen Loss: 14.3385 Disc Loss: 1.13059e-05 Q Losses: [0.0067144851, 0.016774001]\n",
      "epoch:6 batch_done:199 Gen Loss: 14.1597 Disc Loss: 8.62071e-06 Q Losses: [0.0055303704, 0.01853594]\n",
      "epoch:6 batch_done:200 Gen Loss: 13.9855 Disc Loss: 0.000504237 Q Losses: [0.0055361488, 0.013492475]\n",
      "epoch:6 batch_done:201 Gen Loss: 13.8717 Disc Loss: 2.72111e-05 Q Losses: [0.0088728545, 0.015410458]\n",
      "epoch:6 batch_done:202 Gen Loss: 13.8078 Disc Loss: 2.21238e-05 Q Losses: [0.0059312736, 0.013636936]\n",
      "epoch:6 batch_done:203 Gen Loss: 13.7412 Disc Loss: 8.82635e-06 Q Losses: [0.007100529, 0.012158761]\n",
      "epoch:6 batch_done:204 Gen Loss: 13.7053 Disc Loss: 2.82745e-05 Q Losses: [0.0048698322, 0.01580064]\n",
      "epoch:6 batch_done:205 Gen Loss: 13.6964 Disc Loss: 1.71646e-05 Q Losses: [0.010322994, 0.011825182]\n",
      "epoch:6 batch_done:206 Gen Loss: 13.6987 Disc Loss: 2.55955e-05 Q Losses: [0.0062111644, 0.011908555]\n",
      "epoch:6 batch_done:207 Gen Loss: 13.6362 Disc Loss: 2.08041e-05 Q Losses: [0.010637352, 0.014056507]\n",
      "epoch:7 batch_done:1 Gen Loss: 13.6275 Disc Loss: 1.19301e-05 Q Losses: [0.007209945, 0.010385914]\n",
      "epoch:7 batch_done:2 Gen Loss: 13.5365 Disc Loss: 2.20429e-05 Q Losses: [0.0049297716, 0.011434827]\n",
      "epoch:7 batch_done:3 Gen Loss: 13.4926 Disc Loss: 2.45803e-05 Q Losses: [0.0046659969, 0.010328112]\n",
      "epoch:7 batch_done:4 Gen Loss: 13.4159 Disc Loss: 1.1612e-05 Q Losses: [0.0052782902, 0.0092678471]\n",
      "epoch:7 batch_done:5 Gen Loss: 13.2865 Disc Loss: 5.82589e-05 Q Losses: [0.005927104, 0.017588522]\n",
      "epoch:7 batch_done:6 Gen Loss: 13.21 Disc Loss: 7.07813e-06 Q Losses: [0.0038981135, 0.0085197343]\n",
      "epoch:7 batch_done:7 Gen Loss: 13.114 Disc Loss: 9.13835e-06 Q Losses: [0.0071390858, 0.0083940849]\n",
      "epoch:7 batch_done:8 Gen Loss: 13.0508 Disc Loss: 0.000342722 Q Losses: [0.0074066352, 0.0094695333]\n",
      "epoch:7 batch_done:9 Gen Loss: 13.0209 Disc Loss: 1.20262e-05 Q Losses: [0.0095676444, 0.011039805]\n",
      "epoch:7 batch_done:10 Gen Loss: 12.9423 Disc Loss: 3.11375e-05 Q Losses: [0.0058421995, 0.009349267]\n",
      "epoch:7 batch_done:11 Gen Loss: 12.808 Disc Loss: 0.000307723 Q Losses: [0.0056102951, 0.0088070007]\n",
      "epoch:7 batch_done:12 Gen Loss: 12.7046 Disc Loss: 2.01499e-05 Q Losses: [0.022015784, 0.0098312162]\n",
      "epoch:7 batch_done:13 Gen Loss: 12.6054 Disc Loss: 1.21926e-05 Q Losses: [0.007335701, 0.010645999]\n",
      "epoch:7 batch_done:14 Gen Loss: 12.5248 Disc Loss: 5.21263e-06 Q Losses: [0.0056268247, 0.012432007]\n",
      "epoch:7 batch_done:15 Gen Loss: 12.4269 Disc Loss: 7.89962e-05 Q Losses: [0.0076676635, 0.011161646]\n",
      "epoch:7 batch_done:16 Gen Loss: 12.3585 Disc Loss: 0.000107314 Q Losses: [0.01134176, 0.0093512684]\n",
      "epoch:7 batch_done:17 Gen Loss: 12.2801 Disc Loss: 8.94026e-05 Q Losses: [0.0063512693, 0.0076003661]\n",
      "epoch:7 batch_done:18 Gen Loss: 12.3014 Disc Loss: 1.77172e-05 Q Losses: [0.0078293476, 0.0072695906]\n",
      "epoch:7 batch_done:19 Gen Loss: 12.316 Disc Loss: 0.000519331 Q Losses: [0.005768856, 0.01029055]\n",
      "epoch:7 batch_done:20 Gen Loss: 12.3157 Disc Loss: 1.92457e-05 Q Losses: [0.0049903532, 0.007620547]\n",
      "epoch:7 batch_done:21 Gen Loss: 12.2629 Disc Loss: 1.74594e-05 Q Losses: [0.010587843, 0.0085703358]\n",
      "epoch:7 batch_done:22 Gen Loss: 12.1548 Disc Loss: 5.13962e-05 Q Losses: [0.0059449682, 0.0088167051]\n",
      "epoch:7 batch_done:23 Gen Loss: 12.0669 Disc Loss: 5.30877e-05 Q Losses: [0.014441168, 0.0088414568]\n",
      "epoch:7 batch_done:24 Gen Loss: 12.0018 Disc Loss: 0.000324376 Q Losses: [0.0053050239, 0.015584737]\n",
      "epoch:7 batch_done:25 Gen Loss: 11.9034 Disc Loss: 2.21018e-05 Q Losses: [0.0065681115, 0.0079460824]\n",
      "epoch:7 batch_done:26 Gen Loss: 11.8684 Disc Loss: 2.60068e-05 Q Losses: [0.0074991947, 0.0092396941]\n",
      "epoch:7 batch_done:27 Gen Loss: 11.8272 Disc Loss: 6.29859e-05 Q Losses: [0.0072125485, 0.012512079]\n",
      "epoch:7 batch_done:28 Gen Loss: 11.7748 Disc Loss: 1.1664e-05 Q Losses: [0.010184012, 0.0078189671]\n",
      "epoch:7 batch_done:29 Gen Loss: 11.6773 Disc Loss: 2.35412e-05 Q Losses: [0.010162728, 0.0082488786]\n",
      "epoch:7 batch_done:30 Gen Loss: 11.7198 Disc Loss: 1.0036e-05 Q Losses: [0.007229351, 0.0065848916]\n",
      "epoch:7 batch_done:31 Gen Loss: 11.7136 Disc Loss: 2.47338e-05 Q Losses: [0.010385629, 0.0076304069]\n",
      "epoch:7 batch_done:32 Gen Loss: 11.6489 Disc Loss: 5.51247e-05 Q Losses: [0.0063504707, 0.0072967671]\n",
      "epoch:7 batch_done:33 Gen Loss: 11.6392 Disc Loss: 3.14386e-05 Q Losses: [0.010819158, 0.0082476642]\n",
      "epoch:7 batch_done:34 Gen Loss: 11.6146 Disc Loss: 3.27601e-05 Q Losses: [0.0055992766, 0.01185295]\n",
      "epoch:7 batch_done:35 Gen Loss: 11.5834 Disc Loss: 4.17404e-05 Q Losses: [0.008178331, 0.0067984075]\n",
      "epoch:7 batch_done:36 Gen Loss: 11.5644 Disc Loss: 1.87489e-05 Q Losses: [0.0048519606, 0.0084002409]\n",
      "epoch:7 batch_done:37 Gen Loss: 11.5142 Disc Loss: 3.75425e-05 Q Losses: [0.0044294414, 0.0099964514]\n",
      "epoch:7 batch_done:38 Gen Loss: 11.4759 Disc Loss: 2.17004e-05 Q Losses: [0.0052943062, 0.0078981314]\n",
      "epoch:7 batch_done:39 Gen Loss: 11.4373 Disc Loss: 3.12798e-05 Q Losses: [0.0086808717, 0.0099183237]\n",
      "epoch:7 batch_done:40 Gen Loss: 11.4049 Disc Loss: 2.2314e-05 Q Losses: [0.011327362, 0.0079278573]\n",
      "epoch:7 batch_done:41 Gen Loss: 11.3543 Disc Loss: 1.57349e-05 Q Losses: [0.0056993514, 0.0074331728]\n",
      "epoch:7 batch_done:42 Gen Loss: 11.3171 Disc Loss: 2.26646e-05 Q Losses: [0.0059048529, 0.0078811301]\n",
      "epoch:7 batch_done:43 Gen Loss: 11.3023 Disc Loss: 1.89506e-05 Q Losses: [0.0063281138, 0.0082928203]\n",
      "epoch:7 batch_done:44 Gen Loss: 11.2529 Disc Loss: 1.66308e-05 Q Losses: [0.0073189065, 0.0080759274]\n",
      "epoch:7 batch_done:45 Gen Loss: 11.2058 Disc Loss: 0.00072087 Q Losses: [0.0073473603, 0.0063926168]\n",
      "epoch:7 batch_done:46 Gen Loss: 11.1803 Disc Loss: 3.52967e-05 Q Losses: [0.0058050603, 0.0080075227]\n",
      "epoch:7 batch_done:47 Gen Loss: 11.1539 Disc Loss: 1.75118e-05 Q Losses: [0.012416046, 0.0077474732]\n",
      "epoch:7 batch_done:48 Gen Loss: 11.1991 Disc Loss: 1.96177e-05 Q Losses: [0.0075766318, 0.0064295372]\n",
      "epoch:7 batch_done:49 Gen Loss: 11.1311 Disc Loss: 1.60282e-05 Q Losses: [0.0031324453, 0.0074653863]\n",
      "epoch:7 batch_done:50 Gen Loss: 11.0748 Disc Loss: 3.59344e-05 Q Losses: [0.0046398011, 0.008119653]\n",
      "epoch:7 batch_done:51 Gen Loss: 10.978 Disc Loss: 1.86406e-05 Q Losses: [0.0096702892, 0.0074003553]\n",
      "epoch:7 batch_done:52 Gen Loss: 10.8884 Disc Loss: 5.91299e-05 Q Losses: [0.0050401781, 0.0096551208]\n",
      "epoch:7 batch_done:53 Gen Loss: 10.7969 Disc Loss: 5.26719e-05 Q Losses: [0.0049141226, 0.0072312346]\n",
      "epoch:7 batch_done:54 Gen Loss: 10.7632 Disc Loss: 2.89294e-05 Q Losses: [0.005011952, 0.0065884003]\n",
      "epoch:7 batch_done:55 Gen Loss: 10.6679 Disc Loss: 3.92065e-05 Q Losses: [0.0036837161, 0.0089237597]\n",
      "epoch:7 batch_done:56 Gen Loss: 10.594 Disc Loss: 2.84206e-05 Q Losses: [0.0067827655, 0.0063067982]\n",
      "epoch:7 batch_done:57 Gen Loss: 10.5561 Disc Loss: 2.88183e-05 Q Losses: [0.0037444739, 0.0066709477]\n",
      "epoch:7 batch_done:58 Gen Loss: 10.4936 Disc Loss: 3.81357e-05 Q Losses: [0.0048475023, 0.0087869205]\n",
      "epoch:7 batch_done:59 Gen Loss: 10.3809 Disc Loss: 4.11385e-05 Q Losses: [0.0050137453, 0.0062207794]\n",
      "epoch:7 batch_done:60 Gen Loss: 10.3043 Disc Loss: 6.77997e-05 Q Losses: [0.0046969908, 0.0073366361]\n",
      "epoch:7 batch_done:61 Gen Loss: 10.2042 Disc Loss: 0.000112894 Q Losses: [0.0068722507, 0.0080027748]\n",
      "epoch:7 batch_done:62 Gen Loss: 10.0709 Disc Loss: 0.000121536 Q Losses: [0.011618199, 0.0093999114]\n",
      "epoch:7 batch_done:63 Gen Loss: 10.0021 Disc Loss: 4.9107e-05 Q Losses: [0.010540891, 0.0076079625]\n",
      "epoch:7 batch_done:64 Gen Loss: 9.98154 Disc Loss: 5.48519e-05 Q Losses: [0.0054759076, 0.0062242304]\n",
      "epoch:7 batch_done:65 Gen Loss: 9.95159 Disc Loss: 0.000156995 Q Losses: [0.0044446597, 0.0061134915]\n",
      "epoch:7 batch_done:66 Gen Loss: 9.89951 Disc Loss: 5.74474e-05 Q Losses: [0.0037508034, 0.0063827327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 batch_done:67 Gen Loss: 9.84517 Disc Loss: 0.000238508 Q Losses: [0.0068626278, 0.0063351826]\n",
      "epoch:7 batch_done:68 Gen Loss: 9.78147 Disc Loss: 7.16148e-05 Q Losses: [0.005202522, 0.0082481969]\n",
      "epoch:7 batch_done:69 Gen Loss: 9.75527 Disc Loss: 6.35377e-05 Q Losses: [0.011097489, 0.0074630082]\n",
      "epoch:7 batch_done:70 Gen Loss: 9.76019 Disc Loss: 0.000131315 Q Losses: [0.0058580497, 0.0069540548]\n",
      "epoch:7 batch_done:71 Gen Loss: 9.71064 Disc Loss: 6.80645e-05 Q Losses: [0.0057282206, 0.0075373133]\n",
      "epoch:7 batch_done:72 Gen Loss: 9.66518 Disc Loss: 8.303e-05 Q Losses: [0.0044521289, 0.0078263227]\n",
      "epoch:7 batch_done:73 Gen Loss: 9.68813 Disc Loss: 9.28717e-05 Q Losses: [0.013772243, 0.0069144182]\n",
      "epoch:7 batch_done:74 Gen Loss: 9.65138 Disc Loss: 7.75801e-05 Q Losses: [0.0067031793, 0.0096285772]\n",
      "epoch:7 batch_done:75 Gen Loss: 9.60277 Disc Loss: 0.000108686 Q Losses: [0.0092224628, 0.0068503181]\n",
      "epoch:7 batch_done:76 Gen Loss: 9.57758 Disc Loss: 0.000105338 Q Losses: [0.012590955, 0.0075538149]\n",
      "epoch:7 batch_done:77 Gen Loss: 9.56516 Disc Loss: 8.15439e-05 Q Losses: [0.0058494327, 0.0091938544]\n",
      "epoch:7 batch_done:78 Gen Loss: 9.54088 Disc Loss: 7.63936e-05 Q Losses: [0.005666221, 0.0064870426]\n",
      "epoch:7 batch_done:79 Gen Loss: 9.51146 Disc Loss: 8.08726e-05 Q Losses: [0.004327252, 0.0096560298]\n",
      "epoch:7 batch_done:80 Gen Loss: 9.44119 Disc Loss: 9.6579e-05 Q Losses: [0.0073540444, 0.0066389302]\n",
      "epoch:7 batch_done:81 Gen Loss: 9.34187 Disc Loss: 9.98847e-05 Q Losses: [0.0083630215, 0.0062787891]\n",
      "epoch:7 batch_done:82 Gen Loss: 9.23447 Disc Loss: 0.000108902 Q Losses: [0.0078638261, 0.014543828]\n",
      "epoch:7 batch_done:83 Gen Loss: 9.17302 Disc Loss: 0.000113553 Q Losses: [0.0029791864, 0.0074882749]\n",
      "epoch:7 batch_done:84 Gen Loss: 9.13936 Disc Loss: 0.000121263 Q Losses: [0.0038138297, 0.0054925526]\n",
      "epoch:7 batch_done:85 Gen Loss: 9.07848 Disc Loss: 0.000142281 Q Losses: [0.0046885265, 0.0065637627]\n",
      "epoch:7 batch_done:86 Gen Loss: 9.05331 Disc Loss: 0.000159474 Q Losses: [0.0093682138, 0.0071701081]\n",
      "epoch:7 batch_done:87 Gen Loss: 8.99819 Disc Loss: 0.000147411 Q Losses: [0.0094851097, 0.0083378982]\n",
      "epoch:7 batch_done:88 Gen Loss: 8.98661 Disc Loss: 0.000131317 Q Losses: [0.0088793226, 0.0068214275]\n",
      "epoch:7 batch_done:89 Gen Loss: 8.97151 Disc Loss: 0.000136474 Q Losses: [0.0038315076, 0.0075662183]\n",
      "epoch:7 batch_done:90 Gen Loss: 8.86497 Disc Loss: 0.00014857 Q Losses: [0.0036749195, 0.0080523752]\n",
      "epoch:7 batch_done:91 Gen Loss: 8.77977 Disc Loss: 0.000160733 Q Losses: [0.007615943, 0.009112332]\n",
      "epoch:7 batch_done:92 Gen Loss: 8.72288 Disc Loss: 0.000171417 Q Losses: [0.0035217258, 0.0074924869]\n",
      "epoch:7 batch_done:93 Gen Loss: 8.71374 Disc Loss: 0.000195312 Q Losses: [0.0048677274, 0.0064908983]\n",
      "epoch:7 batch_done:94 Gen Loss: 8.69596 Disc Loss: 0.000241653 Q Losses: [0.0062106084, 0.0065219086]\n",
      "epoch:7 batch_done:95 Gen Loss: 8.60845 Disc Loss: 0.000220193 Q Losses: [0.0059804404, 0.0072287535]\n",
      "epoch:7 batch_done:96 Gen Loss: 8.49582 Disc Loss: 0.00022431 Q Losses: [0.020922251, 0.0082120784]\n",
      "epoch:7 batch_done:97 Gen Loss: 8.41551 Disc Loss: 0.000237265 Q Losses: [0.0036503705, 0.0052635996]\n",
      "epoch:7 batch_done:98 Gen Loss: 8.36733 Disc Loss: 0.000249803 Q Losses: [0.0127114, 0.0061484021]\n",
      "epoch:7 batch_done:99 Gen Loss: 8.33555 Disc Loss: 0.000262288 Q Losses: [0.0096744373, 0.0084177162]\n",
      "epoch:7 batch_done:100 Gen Loss: 8.31135 Disc Loss: 0.00027478 Q Losses: [0.0058356128, 0.0097432323]\n",
      "epoch:7 batch_done:101 Gen Loss: 8.37275 Disc Loss: 0.000303279 Q Losses: [0.0043352814, 0.0068707042]\n",
      "epoch:7 batch_done:102 Gen Loss: 8.37974 Disc Loss: 0.000246495 Q Losses: [0.0066594677, 0.0069744289]\n",
      "epoch:7 batch_done:103 Gen Loss: 8.35965 Disc Loss: 0.000252472 Q Losses: [0.005161548, 0.0055748923]\n",
      "epoch:7 batch_done:104 Gen Loss: 8.29626 Disc Loss: 0.000270482 Q Losses: [0.0050130049, 0.0059255762]\n",
      "epoch:7 batch_done:105 Gen Loss: 8.28426 Disc Loss: 0.000273792 Q Losses: [0.012559189, 0.0058892686]\n",
      "epoch:7 batch_done:106 Gen Loss: 8.27528 Disc Loss: 0.000276282 Q Losses: [0.0040286873, 0.0078237131]\n",
      "epoch:7 batch_done:107 Gen Loss: 8.21791 Disc Loss: 0.000291059 Q Losses: [0.0078954268, 0.006514186]\n",
      "epoch:7 batch_done:108 Gen Loss: 8.14206 Disc Loss: 0.000332087 Q Losses: [0.013118599, 0.0065029031]\n",
      "epoch:7 batch_done:109 Gen Loss: 8.10116 Disc Loss: 0.00034213 Q Losses: [0.0045414404, 0.0057649706]\n",
      "epoch:7 batch_done:110 Gen Loss: 8.00552 Disc Loss: 0.00037083 Q Losses: [0.013874916, 0.0067489883]\n",
      "epoch:7 batch_done:111 Gen Loss: 7.86924 Disc Loss: 0.00049551 Q Losses: [0.0082937013, 0.0059023658]\n",
      "epoch:7 batch_done:112 Gen Loss: 7.7036 Disc Loss: 0.000505954 Q Losses: [0.0057328101, 0.0075774221]\n",
      "epoch:7 batch_done:113 Gen Loss: 7.68816 Disc Loss: 0.000517496 Q Losses: [0.0083689429, 0.0075548869]\n",
      "epoch:7 batch_done:114 Gen Loss: 7.75081 Disc Loss: 0.000538349 Q Losses: [0.0072195698, 0.008098972]\n",
      "epoch:7 batch_done:115 Gen Loss: 7.70566 Disc Loss: 0.000531137 Q Losses: [0.0044109821, 0.0065784464]\n",
      "epoch:7 batch_done:116 Gen Loss: 7.61184 Disc Loss: 0.000656973 Q Losses: [0.007533249, 0.0091611715]\n",
      "epoch:7 batch_done:117 Gen Loss: 7.44301 Disc Loss: 0.000761795 Q Losses: [0.005005626, 0.0076909643]\n",
      "epoch:7 batch_done:118 Gen Loss: 7.34996 Disc Loss: 0.000771861 Q Losses: [0.022077644, 0.0080222283]\n",
      "epoch:7 batch_done:119 Gen Loss: 7.36522 Disc Loss: 0.000787686 Q Losses: [0.010243556, 0.0064312667]\n",
      "epoch:7 batch_done:120 Gen Loss: 7.29307 Disc Loss: 0.000832731 Q Losses: [0.0104744, 0.011511798]\n",
      "epoch:7 batch_done:121 Gen Loss: 7.21092 Disc Loss: 0.000953398 Q Losses: [0.011200356, 0.0061490824]\n",
      "epoch:7 batch_done:122 Gen Loss: 7.04711 Disc Loss: 0.0011031 Q Losses: [0.0056826719, 0.0094179576]\n",
      "epoch:7 batch_done:123 Gen Loss: 6.95923 Disc Loss: 0.00159314 Q Losses: [0.0075585912, 0.0072907102]\n",
      "epoch:7 batch_done:124 Gen Loss: 6.90559 Disc Loss: 0.00132847 Q Losses: [0.0047186729, 0.0069194101]\n",
      "epoch:7 batch_done:125 Gen Loss: 6.84196 Disc Loss: 0.00148143 Q Losses: [0.0065000178, 0.0059855543]\n",
      "epoch:7 batch_done:126 Gen Loss: 6.75627 Disc Loss: 0.00172067 Q Losses: [0.0046460684, 0.0060493834]\n",
      "epoch:7 batch_done:127 Gen Loss: 6.6733 Disc Loss: 0.00191827 Q Losses: [0.007761301, 0.005515744]\n",
      "epoch:7 batch_done:128 Gen Loss: 6.56095 Disc Loss: 0.00227977 Q Losses: [0.0078379009, 0.0088415118]\n",
      "epoch:7 batch_done:129 Gen Loss: 6.6266 Disc Loss: 0.00219106 Q Losses: [0.0086034173, 0.0068452973]\n",
      "epoch:7 batch_done:130 Gen Loss: 6.64908 Disc Loss: 0.00217399 Q Losses: [0.0051479107, 0.0070091365]\n",
      "epoch:7 batch_done:131 Gen Loss: 6.6381 Disc Loss: 0.00222101 Q Losses: [0.0033669369, 0.0080109509]\n",
      "epoch:7 batch_done:132 Gen Loss: 6.62804 Disc Loss: 0.0022359 Q Losses: [0.0054485546, 0.0099435784]\n",
      "epoch:7 batch_done:133 Gen Loss: 6.63654 Disc Loss: 0.00224381 Q Losses: [0.0080282167, 0.0074949814]\n",
      "epoch:7 batch_done:134 Gen Loss: 6.59563 Disc Loss: 0.00239721 Q Losses: [0.0043030796, 0.0062929108]\n",
      "epoch:7 batch_done:135 Gen Loss: 6.60463 Disc Loss: 0.00248452 Q Losses: [0.010046971, 0.006686647]\n",
      "epoch:7 batch_done:136 Gen Loss: 6.65257 Disc Loss: 0.00223829 Q Losses: [0.0088671483, 0.0089783873]\n",
      "epoch:7 batch_done:137 Gen Loss: 6.62364 Disc Loss: 0.00229076 Q Losses: [0.0049177743, 0.0065525663]\n",
      "epoch:7 batch_done:138 Gen Loss: 6.57806 Disc Loss: 0.00257439 Q Losses: [0.0062550502, 0.0068595516]\n",
      "epoch:7 batch_done:139 Gen Loss: 6.5552 Disc Loss: 0.00262794 Q Losses: [0.012221698, 0.0073209484]\n",
      "epoch:7 batch_done:140 Gen Loss: 6.55356 Disc Loss: 0.0026622 Q Losses: [0.0097044064, 0.0067417687]\n",
      "epoch:7 batch_done:141 Gen Loss: 6.60702 Disc Loss: 0.00267704 Q Losses: [0.0057542305, 0.0086030718]\n",
      "epoch:7 batch_done:142 Gen Loss: 6.63101 Disc Loss: 0.0024512 Q Losses: [0.0070824679, 0.0073239375]\n",
      "epoch:7 batch_done:143 Gen Loss: 6.50386 Disc Loss: 0.00288213 Q Losses: [0.0050200061, 0.0063505722]\n",
      "epoch:7 batch_done:144 Gen Loss: 6.59547 Disc Loss: 0.00262377 Q Losses: [0.0045156158, 0.0085427659]\n",
      "epoch:7 batch_done:145 Gen Loss: 6.61824 Disc Loss: 0.00254239 Q Losses: [0.005362723, 0.011232095]\n",
      "epoch:7 batch_done:146 Gen Loss: 6.73248 Disc Loss: 0.00225782 Q Losses: [0.0072546741, 0.0062864134]\n",
      "epoch:7 batch_done:147 Gen Loss: 6.79422 Disc Loss: 0.0019347 Q Losses: [0.0055596917, 0.0090044839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 batch_done:148 Gen Loss: 6.78346 Disc Loss: 0.00199421 Q Losses: [0.0048539583, 0.0062896502]\n",
      "epoch:7 batch_done:149 Gen Loss: 6.798 Disc Loss: 0.00182072 Q Losses: [0.010427753, 0.0095965229]\n",
      "epoch:7 batch_done:150 Gen Loss: 6.79184 Disc Loss: 0.00220812 Q Losses: [0.0050212652, 0.0083686486]\n",
      "epoch:7 batch_done:151 Gen Loss: 6.75534 Disc Loss: 0.00207436 Q Losses: [0.0087338863, 0.0083979797]\n",
      "epoch:7 batch_done:152 Gen Loss: 6.82672 Disc Loss: 0.0041268 Q Losses: [0.010659983, 0.0079375654]\n",
      "epoch:7 batch_done:153 Gen Loss: 6.71214 Disc Loss: 0.0019179 Q Losses: [0.01458524, 0.0066361921]\n",
      "epoch:7 batch_done:154 Gen Loss: 6.63898 Disc Loss: 0.00220245 Q Losses: [0.0046471385, 0.0089547001]\n",
      "epoch:7 batch_done:155 Gen Loss: 6.48862 Disc Loss: 0.00276246 Q Losses: [0.0074716425, 0.0079967864]\n",
      "epoch:7 batch_done:156 Gen Loss: 6.49513 Disc Loss: 0.00279766 Q Losses: [0.0096279019, 0.013574003]\n",
      "epoch:7 batch_done:157 Gen Loss: 6.34142 Disc Loss: 0.00387292 Q Losses: [0.005763608, 0.014524531]\n",
      "epoch:7 batch_done:158 Gen Loss: 6.44342 Disc Loss: 0.00408748 Q Losses: [0.0069825845, 0.0084417816]\n",
      "epoch:7 batch_done:159 Gen Loss: 6.60174 Disc Loss: 0.00311536 Q Losses: [0.0068263053, 0.0071060569]\n",
      "epoch:7 batch_done:160 Gen Loss: 6.82627 Disc Loss: 0.0022344 Q Losses: [0.0044903462, 0.012099965]\n",
      "epoch:7 batch_done:161 Gen Loss: 7.0661 Disc Loss: 0.00493807 Q Losses: [0.0055063255, 0.011016225]\n",
      "epoch:7 batch_done:162 Gen Loss: 6.88785 Disc Loss: 0.00214384 Q Losses: [0.006856042, 0.0067622722]\n",
      "epoch:7 batch_done:163 Gen Loss: 6.67805 Disc Loss: 0.00206046 Q Losses: [0.0092068985, 0.007060538]\n",
      "epoch:7 batch_done:164 Gen Loss: 6.51909 Disc Loss: 0.00266416 Q Losses: [0.0065743704, 0.009928789]\n",
      "epoch:7 batch_done:165 Gen Loss: 6.48698 Disc Loss: 0.0050203 Q Losses: [0.0051922719, 0.010844548]\n",
      "epoch:7 batch_done:166 Gen Loss: 6.41967 Disc Loss: 0.00355807 Q Losses: [0.0054203095, 0.0077585923]\n",
      "epoch:7 batch_done:167 Gen Loss: 6.57438 Disc Loss: 0.00334087 Q Losses: [0.0059657022, 0.009568993]\n",
      "epoch:7 batch_done:168 Gen Loss: 6.73289 Disc Loss: 0.00304268 Q Losses: [0.0054372717, 0.0064729797]\n",
      "epoch:7 batch_done:169 Gen Loss: 6.89237 Disc Loss: 0.00232447 Q Losses: [0.0076545347, 0.0066374587]\n",
      "epoch:7 batch_done:170 Gen Loss: 6.94331 Disc Loss: 0.00194658 Q Losses: [0.0066508232, 0.0071282042]\n",
      "epoch:7 batch_done:171 Gen Loss: 6.9822 Disc Loss: 0.00442878 Q Losses: [0.005600892, 0.0072942767]\n",
      "epoch:7 batch_done:172 Gen Loss: 7.14583 Disc Loss: 0.00141018 Q Losses: [0.0053131748, 0.007505089]\n",
      "epoch:7 batch_done:173 Gen Loss: 7.0065 Disc Loss: 0.00199652 Q Losses: [0.010825362, 0.0074152355]\n",
      "epoch:7 batch_done:174 Gen Loss: 6.92688 Disc Loss: 0.00184784 Q Losses: [0.0052248081, 0.0083810398]\n",
      "epoch:7 batch_done:175 Gen Loss: 6.90904 Disc Loss: 0.00252768 Q Losses: [0.0064505395, 0.0092300605]\n",
      "epoch:7 batch_done:176 Gen Loss: 6.92813 Disc Loss: 0.00294175 Q Losses: [0.0066030668, 0.0078545455]\n",
      "epoch:7 batch_done:177 Gen Loss: 7.13543 Disc Loss: 0.00145433 Q Losses: [0.0063257604, 0.010552431]\n",
      "epoch:7 batch_done:178 Gen Loss: 7.13439 Disc Loss: 0.00303646 Q Losses: [0.0061434419, 0.0067877942]\n",
      "epoch:7 batch_done:179 Gen Loss: 6.98106 Disc Loss: 0.00133581 Q Losses: [0.0071041603, 0.0081202518]\n",
      "epoch:7 batch_done:180 Gen Loss: 7.18147 Disc Loss: 0.00111648 Q Losses: [0.0051770266, 0.0075450619]\n",
      "epoch:7 batch_done:181 Gen Loss: 7.12735 Disc Loss: 0.00246554 Q Losses: [0.0050426703, 0.0071047056]\n",
      "epoch:7 batch_done:182 Gen Loss: 7.12109 Disc Loss: 0.00194031 Q Losses: [0.0040010791, 0.0079289246]\n",
      "epoch:7 batch_done:183 Gen Loss: 7.08882 Disc Loss: 0.00156013 Q Losses: [0.0040073548, 0.00664675]\n",
      "epoch:7 batch_done:184 Gen Loss: 7.01533 Disc Loss: 0.00321283 Q Losses: [0.0097797466, 0.005985403]\n",
      "epoch:7 batch_done:185 Gen Loss: 6.76558 Disc Loss: 0.00447145 Q Losses: [0.0073048254, 0.0076303896]\n",
      "epoch:7 batch_done:186 Gen Loss: 6.7382 Disc Loss: 0.00175413 Q Losses: [0.0070452783, 0.0061910646]\n",
      "epoch:7 batch_done:187 Gen Loss: 6.77513 Disc Loss: 0.00210975 Q Losses: [0.0046170009, 0.0078556277]\n",
      "epoch:7 batch_done:188 Gen Loss: 6.86973 Disc Loss: 0.00168654 Q Losses: [0.0077723269, 0.0069455202]\n",
      "epoch:7 batch_done:189 Gen Loss: 6.9824 Disc Loss: 0.00147246 Q Losses: [0.0096664662, 0.0053111878]\n",
      "epoch:7 batch_done:190 Gen Loss: 7.09282 Disc Loss: 0.00134176 Q Losses: [0.013545493, 0.0052205781]\n",
      "epoch:7 batch_done:191 Gen Loss: 7.15338 Disc Loss: 0.00178393 Q Losses: [0.0081870025, 0.0064613628]\n",
      "epoch:7 batch_done:192 Gen Loss: 7.11407 Disc Loss: 0.0026973 Q Losses: [0.0066938559, 0.008551443]\n",
      "epoch:7 batch_done:193 Gen Loss: 7.15085 Disc Loss: 0.00216578 Q Losses: [0.0058492571, 0.0079236086]\n",
      "epoch:7 batch_done:194 Gen Loss: 7.06196 Disc Loss: 0.00176322 Q Losses: [0.0092935432, 0.0081687886]\n",
      "epoch:7 batch_done:195 Gen Loss: 7.0293 Disc Loss: 0.00136981 Q Losses: [0.0057634916, 0.0070386026]\n",
      "epoch:7 batch_done:196 Gen Loss: 6.99221 Disc Loss: 0.00146485 Q Losses: [0.0088812523, 0.010086935]\n",
      "epoch:7 batch_done:197 Gen Loss: 7.13681 Disc Loss: 0.00139972 Q Losses: [0.0056515243, 0.0062126797]\n",
      "epoch:7 batch_done:198 Gen Loss: 7.04011 Disc Loss: 0.0015976 Q Losses: [0.0087091522, 0.007577979]\n",
      "epoch:7 batch_done:199 Gen Loss: 6.97892 Disc Loss: 0.0014709 Q Losses: [0.0044030538, 0.0088551696]\n",
      "epoch:7 batch_done:200 Gen Loss: 7.09677 Disc Loss: 0.00138753 Q Losses: [0.0075045493, 0.0060824063]\n",
      "epoch:7 batch_done:201 Gen Loss: 7.07006 Disc Loss: 0.00153327 Q Losses: [0.0074464967, 0.0064982809]\n",
      "epoch:7 batch_done:202 Gen Loss: 6.89123 Disc Loss: 0.00157747 Q Losses: [0.0070596063, 0.0066546882]\n",
      "epoch:7 batch_done:203 Gen Loss: 6.77299 Disc Loss: 0.00224138 Q Losses: [0.010544447, 0.008331839]\n",
      "epoch:7 batch_done:204 Gen Loss: 6.88361 Disc Loss: 0.00184973 Q Losses: [0.0046393448, 0.0072469874]\n",
      "epoch:7 batch_done:205 Gen Loss: 6.88816 Disc Loss: 0.00221332 Q Losses: [0.0074681295, 0.0097678313]\n",
      "epoch:7 batch_done:206 Gen Loss: 6.8623 Disc Loss: 0.00227427 Q Losses: [0.0071838098, 0.0070341914]\n",
      "epoch:7 batch_done:207 Gen Loss: 6.57518 Disc Loss: 0.00536059 Q Losses: [0.011329364, 0.0092715677]\n",
      "epoch:8 batch_done:1 Gen Loss: 6.63952 Disc Loss: 0.00261789 Q Losses: [0.0043008481, 0.0096810739]\n",
      "epoch:8 batch_done:2 Gen Loss: 6.7441 Disc Loss: 0.00244914 Q Losses: [0.0093947127, 0.0092699919]\n",
      "epoch:8 batch_done:3 Gen Loss: 7.01663 Disc Loss: 0.00167256 Q Losses: [0.012645815, 0.010581323]\n",
      "epoch:8 batch_done:4 Gen Loss: 7.26884 Disc Loss: 0.00115725 Q Losses: [0.0072090602, 0.0066266395]\n",
      "epoch:8 batch_done:5 Gen Loss: 7.44963 Disc Loss: 0.00351796 Q Losses: [0.014880035, 0.0057954965]\n",
      "epoch:8 batch_done:6 Gen Loss: 7.30224 Disc Loss: 0.00111763 Q Losses: [0.0058727553, 0.0064881579]\n",
      "epoch:8 batch_done:7 Gen Loss: 7.10026 Disc Loss: 0.00141793 Q Losses: [0.0074909469, 0.0082202964]\n",
      "epoch:8 batch_done:8 Gen Loss: 7.01495 Disc Loss: 0.00179664 Q Losses: [0.0041701947, 0.0069099749]\n",
      "epoch:8 batch_done:9 Gen Loss: 6.96271 Disc Loss: 0.00224397 Q Losses: [0.026769465, 0.007626792]\n",
      "epoch:8 batch_done:10 Gen Loss: 7.17749 Disc Loss: 0.00298299 Q Losses: [0.0077953869, 0.0075126253]\n",
      "epoch:8 batch_done:11 Gen Loss: 7.04238 Disc Loss: 0.00124354 Q Losses: [0.0054343552, 0.0084999446]\n",
      "epoch:8 batch_done:12 Gen Loss: 7.1227 Disc Loss: 0.00173089 Q Losses: [0.0063447985, 0.0074356245]\n",
      "epoch:8 batch_done:13 Gen Loss: 7.01346 Disc Loss: 0.00168088 Q Losses: [0.0082335593, 0.0068817311]\n",
      "epoch:8 batch_done:14 Gen Loss: 6.98239 Disc Loss: 0.0019002 Q Losses: [0.0079424866, 0.0065521253]\n",
      "epoch:8 batch_done:15 Gen Loss: 6.99623 Disc Loss: 0.0014822 Q Losses: [0.010198507, 0.0078721708]\n",
      "epoch:8 batch_done:16 Gen Loss: 7.11558 Disc Loss: 0.00132412 Q Losses: [0.016486259, 0.0082671829]\n",
      "epoch:8 batch_done:17 Gen Loss: 7.32465 Disc Loss: 0.00112896 Q Losses: [0.01453645, 0.0082594529]\n",
      "epoch:8 batch_done:18 Gen Loss: 7.96401 Disc Loss: 0.000535338 Q Losses: [0.0098211914, 0.0070884582]\n",
      "epoch:8 batch_done:19 Gen Loss: 7.80492 Disc Loss: 0.000769578 Q Losses: [0.0078699887, 0.0074509932]\n",
      "epoch:8 batch_done:20 Gen Loss: 7.08191 Disc Loss: 0.00128213 Q Losses: [0.0084962472, 0.006570701]\n",
      "epoch:8 batch_done:21 Gen Loss: 7.02774 Disc Loss: 0.00176478 Q Losses: [0.0067446292, 0.006500395]\n",
      "epoch:8 batch_done:22 Gen Loss: 6.97137 Disc Loss: 0.00152717 Q Losses: [0.0065140883, 0.0088649951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:23 Gen Loss: 6.86714 Disc Loss: 0.00249779 Q Losses: [0.0073753488, 0.0082580503]\n",
      "epoch:8 batch_done:24 Gen Loss: 7.06267 Disc Loss: 0.00188649 Q Losses: [0.005019566, 0.0064837425]\n",
      "epoch:8 batch_done:25 Gen Loss: 6.92333 Disc Loss: 0.00165422 Q Losses: [0.0052534826, 0.0064046723]\n",
      "epoch:8 batch_done:26 Gen Loss: 6.93505 Disc Loss: 0.00173746 Q Losses: [0.0068221167, 0.0067293867]\n",
      "epoch:8 batch_done:27 Gen Loss: 7.09853 Disc Loss: 0.00142644 Q Losses: [0.0060605332, 0.0072180964]\n",
      "epoch:8 batch_done:28 Gen Loss: 7.28082 Disc Loss: 0.00135339 Q Losses: [0.0055463724, 0.0060213935]\n",
      "epoch:8 batch_done:29 Gen Loss: 7.75184 Disc Loss: 0.000625524 Q Losses: [0.0062375031, 0.0075720837]\n",
      "epoch:8 batch_done:30 Gen Loss: 7.96133 Disc Loss: 0.00049578 Q Losses: [0.0085636023, 0.0091410633]\n",
      "epoch:8 batch_done:31 Gen Loss: 7.69148 Disc Loss: 0.000739533 Q Losses: [0.0055930587, 0.005411061]\n",
      "epoch:8 batch_done:32 Gen Loss: 7.56531 Disc Loss: 0.000664991 Q Losses: [0.010311676, 0.0075785825]\n",
      "epoch:8 batch_done:33 Gen Loss: 7.69627 Disc Loss: 0.000700322 Q Losses: [0.0057505555, 0.0088160206]\n",
      "epoch:8 batch_done:34 Gen Loss: 7.45231 Disc Loss: 0.00122482 Q Losses: [0.0060357591, 0.0062175412]\n",
      "epoch:8 batch_done:35 Gen Loss: 7.40101 Disc Loss: 0.000838457 Q Losses: [0.010309584, 0.0067243888]\n",
      "epoch:8 batch_done:36 Gen Loss: 7.79768 Disc Loss: 0.000818378 Q Losses: [0.0046846489, 0.0049500838]\n",
      "epoch:8 batch_done:37 Gen Loss: 7.78047 Disc Loss: 0.000700772 Q Losses: [0.011305314, 0.0070146476]\n",
      "epoch:8 batch_done:38 Gen Loss: 7.33837 Disc Loss: 0.000966245 Q Losses: [0.0089676902, 0.0058203796]\n",
      "epoch:8 batch_done:39 Gen Loss: 7.6328 Disc Loss: 0.00094054 Q Losses: [0.0092519354, 0.0064470442]\n",
      "epoch:8 batch_done:40 Gen Loss: 7.43029 Disc Loss: 0.00119049 Q Losses: [0.0048500705, 0.0057230862]\n",
      "epoch:8 batch_done:41 Gen Loss: 7.46029 Disc Loss: 0.00191357 Q Losses: [0.0048131002, 0.0063770078]\n",
      "epoch:8 batch_done:42 Gen Loss: 7.08766 Disc Loss: 0.00207559 Q Losses: [0.0062352424, 0.0093603078]\n",
      "epoch:8 batch_done:43 Gen Loss: 7.09405 Disc Loss: 0.00130512 Q Losses: [0.0048563457, 0.007929258]\n",
      "epoch:8 batch_done:44 Gen Loss: 7.30556 Disc Loss: 0.000985868 Q Losses: [0.017323187, 0.007001685]\n",
      "epoch:8 batch_done:45 Gen Loss: 8.0495 Disc Loss: 0.000540543 Q Losses: [0.0063024899, 0.0063197603]\n",
      "epoch:8 batch_done:46 Gen Loss: 6.86636 Disc Loss: 0.0229482 Q Losses: [0.0077551869, 0.009280758]\n",
      "epoch:8 batch_done:47 Gen Loss: 6.55183 Disc Loss: 0.00149442 Q Losses: [0.0088617839, 0.0094743203]\n",
      "epoch:8 batch_done:48 Gen Loss: 6.60098 Disc Loss: 0.00191203 Q Losses: [0.021225302, 0.0056278151]\n",
      "epoch:8 batch_done:49 Gen Loss: 7.13195 Disc Loss: 0.00121606 Q Losses: [0.0064647682, 0.0071840901]\n",
      "epoch:8 batch_done:50 Gen Loss: 7.78805 Disc Loss: 0.000554925 Q Losses: [0.0048592766, 0.0089909099]\n",
      "epoch:8 batch_done:51 Gen Loss: 8.62671 Disc Loss: 0.000235028 Q Losses: [0.0069673122, 0.007367623]\n",
      "epoch:8 batch_done:52 Gen Loss: 8.23655 Disc Loss: 0.000763967 Q Losses: [0.0053591072, 0.0081754085]\n",
      "epoch:8 batch_done:53 Gen Loss: 7.59138 Disc Loss: 0.000647095 Q Losses: [0.0084197531, 0.0085029257]\n",
      "epoch:8 batch_done:54 Gen Loss: 7.41586 Disc Loss: 0.000763207 Q Losses: [0.0061437064, 0.0079524713]\n",
      "epoch:8 batch_done:55 Gen Loss: 7.1972 Disc Loss: 0.00101868 Q Losses: [0.0058728503, 0.0063194092]\n",
      "epoch:8 batch_done:56 Gen Loss: 7.30931 Disc Loss: 0.000921044 Q Losses: [0.014246047, 0.0089729521]\n",
      "epoch:8 batch_done:57 Gen Loss: 7.52766 Disc Loss: 0.000724309 Q Losses: [0.020393198, 0.0095579755]\n",
      "epoch:8 batch_done:58 Gen Loss: 7.80767 Disc Loss: 0.000524445 Q Losses: [0.0075161513, 0.0088095078]\n",
      "epoch:8 batch_done:59 Gen Loss: 9.5033 Disc Loss: 9.13992e-05 Q Losses: [0.0058933818, 0.0085203657]\n",
      "epoch:8 batch_done:60 Gen Loss: 8.13418 Disc Loss: 0.000354363 Q Losses: [0.0082907267, 0.0061611421]\n",
      "epoch:8 batch_done:61 Gen Loss: 7.81498 Disc Loss: 0.000484414 Q Losses: [0.0067664483, 0.0067617083]\n",
      "epoch:8 batch_done:62 Gen Loss: 7.79212 Disc Loss: 0.000495189 Q Losses: [0.014511274, 0.0082110772]\n",
      "epoch:8 batch_done:63 Gen Loss: 8.12536 Disc Loss: 0.000382729 Q Losses: [0.007214779, 0.0085991714]\n",
      "epoch:8 batch_done:64 Gen Loss: 8.31526 Disc Loss: 0.000326203 Q Losses: [0.0058385618, 0.0072734323]\n",
      "epoch:8 batch_done:65 Gen Loss: 7.78584 Disc Loss: 0.000532652 Q Losses: [0.0060212426, 0.0064630401]\n",
      "epoch:8 batch_done:66 Gen Loss: 7.60152 Disc Loss: 0.000786035 Q Losses: [0.0067211082, 0.006489709]\n",
      "epoch:8 batch_done:67 Gen Loss: 7.81946 Disc Loss: 0.000503766 Q Losses: [0.0302258, 0.0081219887]\n",
      "epoch:8 batch_done:68 Gen Loss: 8.4812 Disc Loss: 0.000342944 Q Losses: [0.017343054, 0.0066779642]\n",
      "epoch:8 batch_done:69 Gen Loss: 9.02273 Disc Loss: 0.000150952 Q Losses: [0.0079411715, 0.0071289227]\n",
      "epoch:8 batch_done:70 Gen Loss: 8.70026 Disc Loss: 0.000208498 Q Losses: [0.0066397013, 0.0072796037]\n",
      "epoch:8 batch_done:71 Gen Loss: 8.28636 Disc Loss: 0.000323321 Q Losses: [0.0080518294, 0.0083600748]\n",
      "epoch:8 batch_done:72 Gen Loss: 8.02719 Disc Loss: 0.000546119 Q Losses: [0.0091362372, 0.0073621487]\n",
      "epoch:8 batch_done:73 Gen Loss: 7.89617 Disc Loss: 0.000629485 Q Losses: [0.0050434624, 0.00640475]\n",
      "epoch:8 batch_done:74 Gen Loss: 8.02471 Disc Loss: 0.000494629 Q Losses: [0.012465794, 0.0061430135]\n",
      "epoch:8 batch_done:75 Gen Loss: 8.27784 Disc Loss: 0.000745162 Q Losses: [0.01042104, 0.0059398869]\n",
      "epoch:8 batch_done:76 Gen Loss: 8.09467 Disc Loss: 0.000371937 Q Losses: [0.013156246, 0.0064942604]\n",
      "epoch:8 batch_done:77 Gen Loss: 8.16028 Disc Loss: 0.000573898 Q Losses: [0.0048488583, 0.0073256264]\n",
      "epoch:8 batch_done:78 Gen Loss: 8.24594 Disc Loss: 0.00034544 Q Losses: [0.0059897341, 0.009045084]\n",
      "epoch:8 batch_done:79 Gen Loss: 8.35501 Disc Loss: 0.000396463 Q Losses: [0.003869704, 0.0073263263]\n",
      "epoch:8 batch_done:80 Gen Loss: 8.67264 Disc Loss: 0.000350132 Q Losses: [0.0070320428, 0.0066018524]\n",
      "epoch:8 batch_done:81 Gen Loss: 9.70671 Disc Loss: 0.000451448 Q Losses: [0.0069845286, 0.0067854542]\n",
      "epoch:8 batch_done:82 Gen Loss: 9.0935 Disc Loss: 0.000169427 Q Losses: [0.010736914, 0.008066291]\n",
      "epoch:8 batch_done:83 Gen Loss: 8.16665 Disc Loss: 0.000425104 Q Losses: [0.0082876645, 0.0060143936]\n",
      "epoch:8 batch_done:84 Gen Loss: 7.66075 Disc Loss: 0.000722218 Q Losses: [0.010035908, 0.0050023114]\n",
      "epoch:8 batch_done:85 Gen Loss: 7.7036 Disc Loss: 0.000566685 Q Losses: [0.0064369706, 0.0071783205]\n",
      "epoch:8 batch_done:86 Gen Loss: 7.76077 Disc Loss: 0.000868762 Q Losses: [0.0074614477, 0.0057625873]\n",
      "epoch:8 batch_done:87 Gen Loss: 7.80508 Disc Loss: 0.000621877 Q Losses: [0.0061036274, 0.0089966003]\n",
      "epoch:8 batch_done:88 Gen Loss: 7.97921 Disc Loss: 0.000439067 Q Losses: [0.0060553197, 0.011469269]\n",
      "epoch:8 batch_done:89 Gen Loss: 8.01549 Disc Loss: 0.000548229 Q Losses: [0.010719685, 0.0061789858]\n",
      "epoch:8 batch_done:90 Gen Loss: 8.20671 Disc Loss: 0.00037571 Q Losses: [0.0036853515, 0.0090293568]\n",
      "epoch:8 batch_done:91 Gen Loss: 9.15087 Disc Loss: 0.000181766 Q Losses: [0.0083630765, 0.0080852304]\n",
      "epoch:8 batch_done:92 Gen Loss: 9.09053 Disc Loss: 0.000141294 Q Losses: [0.0070924489, 0.0076310257]\n",
      "epoch:8 batch_done:93 Gen Loss: 9.82542 Disc Loss: 0.000104104 Q Losses: [0.0051943175, 0.0079923011]\n",
      "epoch:8 batch_done:94 Gen Loss: 8.02381 Disc Loss: 0.000469332 Q Losses: [0.0083559267, 0.0055700806]\n",
      "epoch:8 batch_done:95 Gen Loss: 7.77293 Disc Loss: 0.000715574 Q Losses: [0.0087073538, 0.0082629714]\n",
      "epoch:8 batch_done:96 Gen Loss: 7.66505 Disc Loss: 0.000767727 Q Losses: [0.0050738854, 0.0056120753]\n",
      "epoch:8 batch_done:97 Gen Loss: 7.63212 Disc Loss: 0.00063737 Q Losses: [0.0081561636, 0.0070589017]\n",
      "epoch:8 batch_done:98 Gen Loss: 8.24949 Disc Loss: 0.000544881 Q Losses: [0.0043220399, 0.007039303]\n",
      "epoch:8 batch_done:99 Gen Loss: 8.91793 Disc Loss: 0.000278907 Q Losses: [0.0061947629, 0.0066732601]\n",
      "epoch:8 batch_done:100 Gen Loss: 8.41677 Disc Loss: 0.000347406 Q Losses: [0.010535938, 0.0069349529]\n",
      "epoch:8 batch_done:101 Gen Loss: 8.54581 Disc Loss: 0.000314387 Q Losses: [0.0061907303, 0.008391114]\n",
      "epoch:8 batch_done:102 Gen Loss: 6.91137 Disc Loss: 0.00136887 Q Losses: [0.0051506418, 0.0083033629]\n",
      "epoch:8 batch_done:103 Gen Loss: 7.11752 Disc Loss: 0.00118382 Q Losses: [0.0066297818, 0.0082805157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:104 Gen Loss: 7.25713 Disc Loss: 0.0010799 Q Losses: [0.0054446827, 0.0067939525]\n",
      "epoch:8 batch_done:105 Gen Loss: 7.75319 Disc Loss: 0.000622322 Q Losses: [0.0075615854, 0.0085721053]\n",
      "epoch:8 batch_done:106 Gen Loss: 8.23835 Disc Loss: 0.000340576 Q Losses: [0.0087881153, 0.0069152452]\n",
      "epoch:8 batch_done:107 Gen Loss: 8.36509 Disc Loss: 0.000296591 Q Losses: [0.0056427261, 0.0066111935]\n",
      "epoch:8 batch_done:108 Gen Loss: 8.05029 Disc Loss: 0.000451739 Q Losses: [0.0047481824, 0.0074281916]\n",
      "epoch:8 batch_done:109 Gen Loss: 9.4066 Disc Loss: 0.000254266 Q Losses: [0.0053429105, 0.0076785041]\n",
      "epoch:8 batch_done:110 Gen Loss: 9.59822 Disc Loss: 0.000311631 Q Losses: [0.0070568477, 0.0081174392]\n",
      "epoch:8 batch_done:111 Gen Loss: 7.57251 Disc Loss: 0.00069513 Q Losses: [0.0099301431, 0.008305382]\n",
      "epoch:8 batch_done:112 Gen Loss: 7.5636 Disc Loss: 0.00064924 Q Losses: [0.0078685274, 0.0070020072]\n",
      "epoch:8 batch_done:113 Gen Loss: 7.38863 Disc Loss: 0.000928954 Q Losses: [0.013134047, 0.0073636081]\n",
      "epoch:8 batch_done:114 Gen Loss: 7.23359 Disc Loss: 0.0010686 Q Losses: [0.0047053793, 0.0074264565]\n",
      "epoch:8 batch_done:115 Gen Loss: 7.5531 Disc Loss: 0.00078946 Q Losses: [0.0080064628, 0.0056565539]\n",
      "epoch:8 batch_done:116 Gen Loss: 8.78802 Disc Loss: 0.000212227 Q Losses: [0.0063049421, 0.0066227405]\n",
      "epoch:8 batch_done:117 Gen Loss: 8.88522 Disc Loss: 0.00117433 Q Losses: [0.0053349836, 0.0080969287]\n",
      "epoch:8 batch_done:118 Gen Loss: 7.94364 Disc Loss: 0.000446801 Q Losses: [0.0049716281, 0.0066068545]\n",
      "epoch:8 batch_done:119 Gen Loss: 8.05706 Disc Loss: 0.000863748 Q Losses: [0.0055685043, 0.0069473335]\n",
      "epoch:8 batch_done:120 Gen Loss: 8.34512 Disc Loss: 0.000376877 Q Losses: [0.0073069199, 0.0067496463]\n",
      "epoch:8 batch_done:121 Gen Loss: 9.36098 Disc Loss: 0.00018918 Q Losses: [0.0061601531, 0.0061418219]\n",
      "epoch:8 batch_done:122 Gen Loss: 7.99313 Disc Loss: 0.000497634 Q Losses: [0.0064554736, 0.0071898312]\n",
      "epoch:8 batch_done:123 Gen Loss: 7.78857 Disc Loss: 0.000592608 Q Losses: [0.0041612163, 0.0060319742]\n",
      "epoch:8 batch_done:124 Gen Loss: 7.95846 Disc Loss: 0.000464879 Q Losses: [0.0059240833, 0.0062890118]\n",
      "epoch:8 batch_done:125 Gen Loss: 8.36489 Disc Loss: 0.000417369 Q Losses: [0.0081412271, 0.0089898948]\n",
      "epoch:8 batch_done:126 Gen Loss: 8.95301 Disc Loss: 0.000194196 Q Losses: [0.0072226771, 0.011842208]\n",
      "epoch:8 batch_done:127 Gen Loss: 8.34262 Disc Loss: 0.000609459 Q Losses: [0.0066239852, 0.0069083292]\n",
      "epoch:8 batch_done:128 Gen Loss: 8.44148 Disc Loss: 0.000295116 Q Losses: [0.007777452, 0.0076337568]\n",
      "epoch:8 batch_done:129 Gen Loss: 7.92301 Disc Loss: 0.000545308 Q Losses: [0.01162705, 0.0077065635]\n",
      "epoch:8 batch_done:130 Gen Loss: 7.90304 Disc Loss: 0.000509586 Q Losses: [0.0053108195, 0.0094420584]\n",
      "epoch:8 batch_done:131 Gen Loss: 7.5029 Disc Loss: 0.000870976 Q Losses: [0.0068577211, 0.0085071754]\n",
      "epoch:8 batch_done:132 Gen Loss: 8.03407 Disc Loss: 0.000485848 Q Losses: [0.0097440686, 0.013161245]\n",
      "epoch:8 batch_done:133 Gen Loss: 7.68824 Disc Loss: 0.000731384 Q Losses: [0.0060581444, 0.006153699]\n",
      "epoch:8 batch_done:134 Gen Loss: 8.56434 Disc Loss: 0.000355506 Q Losses: [0.0053039603, 0.0068280883]\n",
      "epoch:8 batch_done:135 Gen Loss: 7.88469 Disc Loss: 0.000499721 Q Losses: [0.0058422992, 0.0088971471]\n",
      "epoch:8 batch_done:136 Gen Loss: 7.41976 Disc Loss: 0.000880886 Q Losses: [0.0072822208, 0.010849778]\n",
      "epoch:8 batch_done:137 Gen Loss: 7.36118 Disc Loss: 0.000887437 Q Losses: [0.0062192399, 0.0079980604]\n",
      "epoch:8 batch_done:138 Gen Loss: 7.79986 Disc Loss: 0.00069337 Q Losses: [0.0075980863, 0.0072851749]\n",
      "epoch:8 batch_done:139 Gen Loss: 7.32229 Disc Loss: 0.00104034 Q Losses: [0.0095401453, 0.0085163713]\n",
      "epoch:8 batch_done:140 Gen Loss: 7.27026 Disc Loss: 0.00106125 Q Losses: [0.0065674474, 0.0063899281]\n",
      "epoch:8 batch_done:141 Gen Loss: 7.28405 Disc Loss: 0.00109649 Q Losses: [0.0052286088, 0.0081212074]\n",
      "epoch:8 batch_done:142 Gen Loss: 8.51151 Disc Loss: 0.000284902 Q Losses: [0.0074423337, 0.0067477645]\n",
      "epoch:8 batch_done:143 Gen Loss: 11.0166 Disc Loss: 0.000217016 Q Losses: [0.0084037296, 0.0083288327]\n",
      "epoch:8 batch_done:144 Gen Loss: 7.57117 Disc Loss: 0.000714803 Q Losses: [0.0085085034, 0.0087007703]\n",
      "epoch:8 batch_done:145 Gen Loss: 8.24698 Disc Loss: 0.000333732 Q Losses: [0.0061881365, 0.0085694585]\n",
      "epoch:8 batch_done:146 Gen Loss: 8.44812 Disc Loss: 0.000369026 Q Losses: [0.03194005, 0.0060698837]\n",
      "epoch:8 batch_done:147 Gen Loss: 7.83347 Disc Loss: 0.000554188 Q Losses: [0.0070591667, 0.0085552065]\n",
      "epoch:8 batch_done:148 Gen Loss: 7.46864 Disc Loss: 0.000927851 Q Losses: [0.014013851, 0.0065817507]\n",
      "epoch:8 batch_done:149 Gen Loss: 7.78922 Disc Loss: 0.000740606 Q Losses: [0.0070977258, 0.0060414458]\n",
      "epoch:8 batch_done:150 Gen Loss: 8.37108 Disc Loss: 0.000538929 Q Losses: [0.0054672444, 0.0075938674]\n",
      "epoch:8 batch_done:151 Gen Loss: 8.65218 Disc Loss: 0.000580084 Q Losses: [0.015870782, 0.011067122]\n",
      "epoch:8 batch_done:152 Gen Loss: 8.41024 Disc Loss: 0.00050008 Q Losses: [0.012112657, 0.00550758]\n",
      "epoch:8 batch_done:153 Gen Loss: 8.74238 Disc Loss: 0.00119403 Q Losses: [0.010690084, 0.0079806643]\n",
      "epoch:8 batch_done:154 Gen Loss: 9.45601 Disc Loss: 0.00244986 Q Losses: [0.0070828786, 0.0068990216]\n",
      "epoch:8 batch_done:155 Gen Loss: 7.70336 Disc Loss: 0.00330234 Q Losses: [0.0067211171, 0.010925456]\n",
      "epoch:8 batch_done:156 Gen Loss: 7.82226 Disc Loss: 0.000775263 Q Losses: [0.0085948715, 0.0070897243]\n",
      "epoch:8 batch_done:157 Gen Loss: 8.44263 Disc Loss: 0.00238014 Q Losses: [0.010285321, 0.0073421467]\n",
      "epoch:8 batch_done:158 Gen Loss: 7.95118 Disc Loss: 0.000631107 Q Losses: [0.010393219, 0.008651251]\n",
      "epoch:8 batch_done:159 Gen Loss: 8.25621 Disc Loss: 0.000297916 Q Losses: [0.0079822056, 0.007846267]\n",
      "epoch:8 batch_done:160 Gen Loss: 9.59417 Disc Loss: 0.000127578 Q Losses: [0.0070144404, 0.0075370818]\n",
      "epoch:8 batch_done:161 Gen Loss: 10.3025 Disc Loss: 0.000288807 Q Losses: [0.0064375931, 0.0073674275]\n",
      "epoch:8 batch_done:162 Gen Loss: 8.16871 Disc Loss: 0.000509314 Q Losses: [0.010772259, 0.010355017]\n",
      "epoch:8 batch_done:163 Gen Loss: 8.07075 Disc Loss: 0.000395076 Q Losses: [0.0074274875, 0.0059598889]\n",
      "epoch:8 batch_done:164 Gen Loss: 7.88785 Disc Loss: 0.000561366 Q Losses: [0.0071312478, 0.0074295774]\n",
      "epoch:8 batch_done:165 Gen Loss: 8.08659 Disc Loss: 0.000422838 Q Losses: [0.0057314709, 0.0071749515]\n",
      "epoch:8 batch_done:166 Gen Loss: 7.93859 Disc Loss: 0.000459669 Q Losses: [0.0095859412, 0.0080458289]\n",
      "epoch:8 batch_done:167 Gen Loss: 7.92365 Disc Loss: 0.000478905 Q Losses: [0.0050650104, 0.0079740947]\n",
      "epoch:8 batch_done:168 Gen Loss: 8.03985 Disc Loss: 0.000711978 Q Losses: [0.0072166757, 0.0088441297]\n",
      "epoch:8 batch_done:169 Gen Loss: 7.53985 Disc Loss: 0.000891574 Q Losses: [0.0046050712, 0.0064705098]\n",
      "epoch:8 batch_done:170 Gen Loss: 7.45609 Disc Loss: 0.000829385 Q Losses: [0.0067682005, 0.010633629]\n",
      "epoch:8 batch_done:171 Gen Loss: 7.92464 Disc Loss: 0.000677642 Q Losses: [0.0065734033, 0.0070904987]\n",
      "epoch:8 batch_done:172 Gen Loss: 10.6274 Disc Loss: 0.000113715 Q Losses: [0.006881292, 0.0096358843]\n",
      "epoch:8 batch_done:173 Gen Loss: 8.50159 Disc Loss: 0.000302115 Q Losses: [0.0056911758, 0.010259619]\n",
      "epoch:8 batch_done:174 Gen Loss: 7.75275 Disc Loss: 0.000691144 Q Losses: [0.0099124899, 0.0080301799]\n",
      "epoch:8 batch_done:175 Gen Loss: 7.54279 Disc Loss: 0.000726887 Q Losses: [0.0048340564, 0.0066211419]\n",
      "epoch:8 batch_done:176 Gen Loss: 7.2694 Disc Loss: 0.00106648 Q Losses: [0.016593091, 0.0062906155]\n",
      "epoch:8 batch_done:177 Gen Loss: 7.59844 Disc Loss: 0.000752863 Q Losses: [0.0062018409, 0.005862698]\n",
      "epoch:8 batch_done:178 Gen Loss: 7.90369 Disc Loss: 0.000584349 Q Losses: [0.0068340586, 0.0076163202]\n",
      "epoch:8 batch_done:179 Gen Loss: 9.08444 Disc Loss: 0.000218445 Q Losses: [0.0056259939, 0.0083136223]\n",
      "epoch:8 batch_done:180 Gen Loss: 8.02306 Disc Loss: 0.000827243 Q Losses: [0.0096542099, 0.014785104]\n",
      "epoch:8 batch_done:181 Gen Loss: 7.5034 Disc Loss: 0.00075865 Q Losses: [0.0069743246, 0.0079098605]\n",
      "epoch:8 batch_done:182 Gen Loss: 6.90648 Disc Loss: 0.00174941 Q Losses: [0.0071386122, 0.01151154]\n",
      "epoch:8 batch_done:183 Gen Loss: 7.25629 Disc Loss: 0.00138579 Q Losses: [0.0059854677, 0.010000093]\n",
      "epoch:8 batch_done:184 Gen Loss: 7.55897 Disc Loss: 0.00102251 Q Losses: [0.0059715766, 0.012358025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:185 Gen Loss: 7.6607 Disc Loss: 0.000800186 Q Losses: [0.013981825, 0.011799883]\n",
      "epoch:8 batch_done:186 Gen Loss: 8.59928 Disc Loss: 0.00103706 Q Losses: [0.0099817198, 0.012473162]\n",
      "epoch:8 batch_done:187 Gen Loss: 11.4 Disc Loss: 9.02317e-05 Q Losses: [0.0075219106, 0.015124094]\n",
      "epoch:8 batch_done:188 Gen Loss: 15.5718 Disc Loss: 6.40622e-05 Q Losses: [0.0084066149, 0.018002136]\n",
      "epoch:8 batch_done:189 Gen Loss: 12.429 Disc Loss: 0.000270943 Q Losses: [0.0074464115, 0.011461616]\n",
      "epoch:8 batch_done:190 Gen Loss: 9.84611 Disc Loss: 0.000102487 Q Losses: [0.0059577245, 0.0097381203]\n",
      "epoch:8 batch_done:191 Gen Loss: 9.30604 Disc Loss: 0.000112291 Q Losses: [0.008295048, 0.010370598]\n",
      "epoch:8 batch_done:192 Gen Loss: 8.85952 Disc Loss: 0.000225288 Q Losses: [0.010607991, 0.006708255]\n",
      "epoch:8 batch_done:193 Gen Loss: 8.80742 Disc Loss: 0.000197578 Q Losses: [0.0076492294, 0.011880966]\n",
      "epoch:8 batch_done:194 Gen Loss: 8.89131 Disc Loss: 0.0002077 Q Losses: [0.0078092865, 0.012270552]\n",
      "epoch:8 batch_done:195 Gen Loss: 8.52594 Disc Loss: 0.00026803 Q Losses: [0.0059184423, 0.0070836237]\n",
      "epoch:8 batch_done:196 Gen Loss: 8.25014 Disc Loss: 0.000331629 Q Losses: [0.0049247914, 0.0077353115]\n",
      "epoch:8 batch_done:197 Gen Loss: 8.89315 Disc Loss: 0.000238717 Q Losses: [0.0075948578, 0.0070733298]\n",
      "epoch:8 batch_done:198 Gen Loss: 9.13608 Disc Loss: 0.000227199 Q Losses: [0.0070366147, 0.0074583497]\n",
      "epoch:8 batch_done:199 Gen Loss: 7.86069 Disc Loss: 0.000569005 Q Losses: [0.0072874306, 0.011218451]\n",
      "epoch:8 batch_done:200 Gen Loss: 6.97097 Disc Loss: 0.00181209 Q Losses: [0.013497047, 0.0071062474]\n",
      "epoch:8 batch_done:201 Gen Loss: 7.14627 Disc Loss: 0.00131325 Q Losses: [0.0094397888, 0.0088447984]\n",
      "epoch:8 batch_done:202 Gen Loss: 7.88875 Disc Loss: 0.000614363 Q Losses: [0.0060887486, 0.018808506]\n",
      "epoch:8 batch_done:203 Gen Loss: 8.08873 Disc Loss: 0.000650063 Q Losses: [0.0074351747, 0.0073281606]\n",
      "epoch:8 batch_done:204 Gen Loss: 8.5105 Disc Loss: 0.000362454 Q Losses: [0.012222935, 0.013959156]\n",
      "epoch:8 batch_done:205 Gen Loss: 7.90005 Disc Loss: 0.000558066 Q Losses: [0.0058838278, 0.026748812]\n",
      "epoch:8 batch_done:206 Gen Loss: 7.33033 Disc Loss: 0.00101282 Q Losses: [0.0060486207, 0.026029162]\n",
      "epoch:8 batch_done:207 Gen Loss: 8.56237 Disc Loss: 0.000309558 Q Losses: [0.0081108045, 0.017793203]\n",
      "epoch:9 batch_done:1 Gen Loss: 6.96967 Disc Loss: 0.00152978 Q Losses: [0.0099991383, 0.026137125]\n",
      "epoch:9 batch_done:2 Gen Loss: 6.79103 Disc Loss: 0.00262359 Q Losses: [0.007380865, 0.036556084]\n",
      "epoch:9 batch_done:3 Gen Loss: 7.51031 Disc Loss: 0.0010591 Q Losses: [0.0082245376, 0.029471138]\n",
      "epoch:9 batch_done:4 Gen Loss: 7.97342 Disc Loss: 0.000564712 Q Losses: [0.008828775, 0.036354315]\n",
      "epoch:9 batch_done:5 Gen Loss: 9.15537 Disc Loss: 0.000196858 Q Losses: [0.014716086, 0.026703969]\n",
      "epoch:9 batch_done:6 Gen Loss: 13.307 Disc Loss: 4.80148e-05 Q Losses: [0.0075759115, 0.028600723]\n",
      "epoch:9 batch_done:7 Gen Loss: 10.4891 Disc Loss: 7.56245e-05 Q Losses: [0.013526385, 0.027284326]\n",
      "epoch:9 batch_done:8 Gen Loss: 10.435 Disc Loss: 0.000100173 Q Losses: [0.0085547045, 0.033046763]\n",
      "epoch:9 batch_done:9 Gen Loss: 9.60608 Disc Loss: 8.22668e-05 Q Losses: [0.0070360405, 0.013440849]\n",
      "epoch:9 batch_done:10 Gen Loss: 8.8305 Disc Loss: 0.000188828 Q Losses: [0.0072497851, 0.015353584]\n",
      "epoch:9 batch_done:11 Gen Loss: 8.43681 Disc Loss: 0.000250169 Q Losses: [0.0075184754, 0.024954326]\n",
      "epoch:9 batch_done:12 Gen Loss: 7.35454 Disc Loss: 0.000816486 Q Losses: [0.0071586175, 0.018276708]\n",
      "epoch:9 batch_done:13 Gen Loss: 7.28082 Disc Loss: 0.00100087 Q Losses: [0.0060034916, 0.018921731]\n",
      "epoch:9 batch_done:14 Gen Loss: 7.03821 Disc Loss: 0.00148392 Q Losses: [0.006911898, 0.019184273]\n",
      "epoch:9 batch_done:15 Gen Loss: 7.29625 Disc Loss: 0.00116797 Q Losses: [0.0081778904, 0.016185848]\n",
      "epoch:9 batch_done:16 Gen Loss: 7.87299 Disc Loss: 0.000585507 Q Losses: [0.010916049, 0.0095675979]\n",
      "epoch:9 batch_done:17 Gen Loss: 8.96189 Disc Loss: 0.000168738 Q Losses: [0.0072698891, 0.0097801303]\n",
      "epoch:9 batch_done:18 Gen Loss: 11.0067 Disc Loss: 8.3096e-05 Q Losses: [0.0062076477, 0.01219718]\n",
      "epoch:9 batch_done:19 Gen Loss: 8.04737 Disc Loss: 0.000389582 Q Losses: [0.02436243, 0.011768559]\n",
      "epoch:9 batch_done:20 Gen Loss: 7.74264 Disc Loss: 0.000548802 Q Losses: [0.0061163204, 0.0086814975]\n",
      "epoch:9 batch_done:21 Gen Loss: 7.55555 Disc Loss: 0.00070653 Q Losses: [0.018563595, 0.011326388]\n",
      "epoch:9 batch_done:22 Gen Loss: 7.70214 Disc Loss: 0.000632397 Q Losses: [0.01220522, 0.010501721]\n",
      "epoch:9 batch_done:23 Gen Loss: 9.13187 Disc Loss: 0.00016753 Q Losses: [0.0066008358, 0.01019142]\n",
      "epoch:9 batch_done:24 Gen Loss: 8.69458 Disc Loss: 0.000226421 Q Losses: [0.0059622284, 0.012671253]\n",
      "epoch:9 batch_done:25 Gen Loss: 7.58466 Disc Loss: 0.000671253 Q Losses: [0.0093331812, 0.0081840809]\n",
      "epoch:9 batch_done:26 Gen Loss: 7.35116 Disc Loss: 0.000932867 Q Losses: [0.0083957883, 0.0097925095]\n",
      "epoch:9 batch_done:27 Gen Loss: 7.52803 Disc Loss: 0.000804392 Q Losses: [0.012505126, 0.0074122241]\n",
      "epoch:9 batch_done:28 Gen Loss: 8.10038 Disc Loss: 0.000442829 Q Losses: [0.012180706, 0.0074035157]\n",
      "epoch:9 batch_done:29 Gen Loss: 9.91413 Disc Loss: 0.000214592 Q Losses: [0.0064399117, 0.0087897126]\n",
      "epoch:9 batch_done:30 Gen Loss: 12.3075 Disc Loss: 3.87022e-05 Q Losses: [0.0075591616, 0.01203721]\n",
      "epoch:9 batch_done:31 Gen Loss: 9.16341 Disc Loss: 0.00024389 Q Losses: [0.0089678206, 0.010454432]\n",
      "epoch:9 batch_done:32 Gen Loss: 9.1545 Disc Loss: 0.00022352 Q Losses: [0.0055963648, 0.008285718]\n",
      "epoch:9 batch_done:33 Gen Loss: 8.53272 Disc Loss: 0.000360047 Q Losses: [0.0055517657, 0.0063882172]\n",
      "epoch:9 batch_done:34 Gen Loss: 7.32024 Disc Loss: 0.00101837 Q Losses: [0.01111434, 0.0066973902]\n",
      "epoch:9 batch_done:35 Gen Loss: 7.15375 Disc Loss: 0.00132278 Q Losses: [0.0069341878, 0.0068693226]\n",
      "epoch:9 batch_done:36 Gen Loss: 7.63241 Disc Loss: 0.000732653 Q Losses: [0.0042344676, 0.0075729131]\n",
      "epoch:9 batch_done:37 Gen Loss: 8.93494 Disc Loss: 0.000432241 Q Losses: [0.008881826, 0.0091470554]\n",
      "epoch:9 batch_done:38 Gen Loss: 8.66484 Disc Loss: 0.00033002 Q Losses: [0.00780473, 0.0054514813]\n",
      "epoch:9 batch_done:39 Gen Loss: 7.39905 Disc Loss: 0.000875017 Q Losses: [0.0061511812, 0.010190859]\n",
      "epoch:9 batch_done:40 Gen Loss: 6.94993 Disc Loss: 0.00187107 Q Losses: [0.0088840928, 0.0054484038]\n",
      "epoch:9 batch_done:41 Gen Loss: 7.37606 Disc Loss: 0.00156663 Q Losses: [0.0082338396, 0.0069377935]\n",
      "epoch:9 batch_done:42 Gen Loss: 8.0763 Disc Loss: 0.000489734 Q Losses: [0.0090859588, 0.006493526]\n",
      "epoch:9 batch_done:43 Gen Loss: 8.68131 Disc Loss: 0.000277306 Q Losses: [0.0056528198, 0.0053413715]\n",
      "epoch:9 batch_done:44 Gen Loss: 7.96649 Disc Loss: 0.000768475 Q Losses: [0.0075955456, 0.00590753]\n",
      "epoch:9 batch_done:45 Gen Loss: 7.79735 Disc Loss: 0.000665405 Q Losses: [0.012532724, 0.0080551393]\n",
      "epoch:9 batch_done:46 Gen Loss: 7.50987 Disc Loss: 0.000904281 Q Losses: [0.015382176, 0.0065188585]\n",
      "epoch:9 batch_done:47 Gen Loss: 7.96844 Disc Loss: 0.000944719 Q Losses: [0.006347144, 0.006472358]\n",
      "epoch:9 batch_done:48 Gen Loss: 8.68546 Disc Loss: 0.000481147 Q Losses: [0.0062448257, 0.0071665738]\n",
      "epoch:9 batch_done:49 Gen Loss: 11.2661 Disc Loss: 0.0025574 Q Losses: [0.0073161609, 0.0053761052]\n",
      "epoch:9 batch_done:50 Gen Loss: 7.50956 Disc Loss: 0.000756095 Q Losses: [0.0098573025, 0.0064117815]\n",
      "epoch:9 batch_done:51 Gen Loss: 6.67354 Disc Loss: 0.00295856 Q Losses: [0.0051406706, 0.0064719575]\n",
      "epoch:9 batch_done:52 Gen Loss: 7.37459 Disc Loss: 0.00143552 Q Losses: [0.0072571822, 0.0056346003]\n",
      "epoch:9 batch_done:53 Gen Loss: 7.21364 Disc Loss: 0.00182315 Q Losses: [0.0077566598, 0.0058797975]\n",
      "epoch:9 batch_done:54 Gen Loss: 8.07397 Disc Loss: 0.000819361 Q Losses: [0.0094846487, 0.0060217408]\n",
      "epoch:9 batch_done:55 Gen Loss: 9.20522 Disc Loss: 0.00039698 Q Losses: [0.019977119, 0.0053294571]\n",
      "epoch:9 batch_done:56 Gen Loss: 9.27318 Disc Loss: 0.00180858 Q Losses: [0.0074215643, 0.0065024374]\n",
      "epoch:9 batch_done:57 Gen Loss: 8.05223 Disc Loss: 0.000510793 Q Losses: [0.011876043, 0.0062801111]\n",
      "epoch:9 batch_done:58 Gen Loss: 8.24096 Disc Loss: 0.000612147 Q Losses: [0.0051226364, 0.0069853263]\n",
      "epoch:9 batch_done:59 Gen Loss: 8.9155 Disc Loss: 0.000614225 Q Losses: [0.0084622595, 0.006645408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 batch_done:60 Gen Loss: 8.76384 Disc Loss: 0.000329831 Q Losses: [0.012756507, 0.0050327587]\n",
      "epoch:9 batch_done:61 Gen Loss: 7.03563 Disc Loss: 0.0014441 Q Losses: [0.0055086724, 0.0084803142]\n",
      "epoch:9 batch_done:62 Gen Loss: 6.90544 Disc Loss: 0.00240659 Q Losses: [0.0098870378, 0.0067510186]\n",
      "epoch:9 batch_done:63 Gen Loss: 7.49807 Disc Loss: 0.00136565 Q Losses: [0.0058409162, 0.0068575349]\n",
      "epoch:9 batch_done:64 Gen Loss: 8.11221 Disc Loss: 0.00266336 Q Losses: [0.011380978, 0.0063553606]\n",
      "epoch:9 batch_done:65 Gen Loss: 9.41036 Disc Loss: 0.000735784 Q Losses: [0.0060763434, 0.0058759255]\n",
      "epoch:9 batch_done:66 Gen Loss: 9.81499 Disc Loss: 0.000265409 Q Losses: [0.0057485066, 0.006650717]\n",
      "epoch:9 batch_done:67 Gen Loss: 7.26172 Disc Loss: 0.00146402 Q Losses: [0.006883217, 0.0068705878]\n",
      "epoch:9 batch_done:68 Gen Loss: 6.79523 Disc Loss: 0.00267526 Q Losses: [0.0084398985, 0.0077645257]\n",
      "epoch:9 batch_done:69 Gen Loss: 7.56604 Disc Loss: 0.00106489 Q Losses: [0.0066315606, 0.0082837306]\n",
      "epoch:9 batch_done:70 Gen Loss: 8.27801 Disc Loss: 0.00110345 Q Losses: [0.0063598761, 0.0057085925]\n",
      "epoch:9 batch_done:71 Gen Loss: 8.92562 Disc Loss: 0.000692592 Q Losses: [0.007164632, 0.0071941782]\n",
      "epoch:9 batch_done:72 Gen Loss: 7.04951 Disc Loss: 0.00175157 Q Losses: [0.0050556664, 0.0079383682]\n",
      "epoch:9 batch_done:73 Gen Loss: 6.82912 Disc Loss: 0.00342571 Q Losses: [0.0051292623, 0.0065899687]\n",
      "epoch:9 batch_done:74 Gen Loss: 6.9375 Disc Loss: 0.0058188 Q Losses: [0.013449026, 0.0071893823]\n",
      "epoch:9 batch_done:75 Gen Loss: 7.23291 Disc Loss: 0.00231211 Q Losses: [0.0097332094, 0.0069451164]\n",
      "epoch:9 batch_done:76 Gen Loss: 8.18868 Disc Loss: 0.000867839 Q Losses: [0.0050782571, 0.0061797802]\n",
      "epoch:9 batch_done:77 Gen Loss: 9.72303 Disc Loss: 0.00216924 Q Losses: [0.015569191, 0.0093792453]\n",
      "epoch:9 batch_done:78 Gen Loss: 8.80359 Disc Loss: 0.00143842 Q Losses: [0.010187211, 0.007269633]\n",
      "epoch:9 batch_done:79 Gen Loss: 8.87376 Disc Loss: 0.00222582 Q Losses: [0.008240615, 0.0048878733]\n",
      "epoch:9 batch_done:80 Gen Loss: 10.8847 Disc Loss: 0.000484806 Q Losses: [0.010049501, 0.0078594461]\n",
      "epoch:9 batch_done:81 Gen Loss: 5.80427 Disc Loss: 0.0095242 Q Losses: [0.013237705, 0.0068489136]\n",
      "epoch:9 batch_done:82 Gen Loss: 29.4595 Disc Loss: 0.104055 Q Losses: [0.0079685384, 0.0094064809]\n",
      "epoch:9 batch_done:83 Gen Loss: 6.15989 Disc Loss: 1.0928 Q Losses: [0.0092228688, 0.0049392805]\n",
      "epoch:9 batch_done:84 Gen Loss: 0.00864873 Disc Loss: 0.011611 Q Losses: [0.0069615552, 0.0058673359]\n",
      "epoch:9 batch_done:85 Gen Loss: 75.8125 Disc Loss: 5.97194 Q Losses: [0.01205728, 0.011419183]\n",
      "epoch:9 batch_done:86 Gen Loss: 64.6798 Disc Loss: 5.03006 Q Losses: [0.014529442, 0.01664819]\n",
      "epoch:9 batch_done:87 Gen Loss: 46.8272 Disc Loss: 1.35619 Q Losses: [0.014545448, 0.021051254]\n",
      "epoch:9 batch_done:88 Gen Loss: 34.9063 Disc Loss: 0.273828 Q Losses: [0.022946339, 0.024164494]\n",
      "epoch:9 batch_done:89 Gen Loss: 24.31 Disc Loss: 0.041521 Q Losses: [0.048455052, 0.031335503]\n",
      "epoch:9 batch_done:90 Gen Loss: 17.45 Disc Loss: 0.000110597 Q Losses: [0.034039028, 0.030287627]\n",
      "epoch:9 batch_done:91 Gen Loss: 12.8504 Disc Loss: 3.13787e-05 Q Losses: [0.026297092, 0.033193558]\n",
      "epoch:9 batch_done:92 Gen Loss: 9.68676 Disc Loss: 4.23416e-05 Q Losses: [0.014356642, 0.03257703]\n",
      "epoch:9 batch_done:93 Gen Loss: 5.74852 Disc Loss: 0.0117802 Q Losses: [0.01055682, 0.052128144]\n",
      "epoch:9 batch_done:94 Gen Loss: 6.51955 Disc Loss: 0.0050472 Q Losses: [0.016023874, 0.12577379]\n",
      "epoch:9 batch_done:95 Gen Loss: 15.0539 Disc Loss: 9.64855e-07 Q Losses: [0.022992827, 0.23554265]\n",
      "epoch:9 batch_done:96 Gen Loss: 17.506 Disc Loss: 9.35893e-05 Q Losses: [0.01918773, 0.49819061]\n",
      "epoch:9 batch_done:97 Gen Loss: 22.0966 Disc Loss: 0.000383213 Q Losses: [0.034715209, 0.73183107]\n",
      "epoch:9 batch_done:98 Gen Loss: 24.4888 Disc Loss: 0.0547439 Q Losses: [0.071862318, 0.48991376]\n",
      "epoch:9 batch_done:99 Gen Loss: 26.0389 Disc Loss: 0.104376 Q Losses: [0.052420188, 0.67868787]\n",
      "epoch:9 batch_done:100 Gen Loss: 25.2528 Disc Loss: 0.149863 Q Losses: [0.062393863, 0.44685614]\n",
      "epoch:9 batch_done:101 Gen Loss: 25.4736 Disc Loss: 0.0935533 Q Losses: [0.073228151, 0.61658752]\n",
      "epoch:9 batch_done:102 Gen Loss: 24.5613 Disc Loss: 0.00277984 Q Losses: [0.15552841, 1.1973069]\n",
      "epoch:9 batch_done:103 Gen Loss: 19.9474 Disc Loss: 3.44225e-05 Q Losses: [0.062721536, 0.46613735]\n",
      "epoch:9 batch_done:104 Gen Loss: 11.9096 Disc Loss: 3.01876e-05 Q Losses: [0.040900711, 0.34542635]\n",
      "epoch:9 batch_done:105 Gen Loss: 8.17739 Disc Loss: 0.000257039 Q Losses: [0.03187554, 0.27746719]\n",
      "epoch:9 batch_done:106 Gen Loss: 6.75926 Disc Loss: 0.00131598 Q Losses: [0.048906632, 0.23087074]\n",
      "epoch:9 batch_done:107 Gen Loss: 5.95495 Disc Loss: 0.00600719 Q Losses: [0.052299827, 0.1443654]\n",
      "epoch:9 batch_done:108 Gen Loss: 6.30919 Disc Loss: 0.00724074 Q Losses: [0.053823851, 0.16157949]\n",
      "epoch:9 batch_done:109 Gen Loss: 5.78939 Disc Loss: 0.0674336 Q Losses: [0.029631821, 0.053907543]\n",
      "epoch:9 batch_done:110 Gen Loss: 6.37261 Disc Loss: 0.0056705 Q Losses: [0.036039375, 0.11352076]\n",
      "epoch:9 batch_done:111 Gen Loss: 6.42254 Disc Loss: 0.00877252 Q Losses: [0.019028516, 0.047128484]\n",
      "epoch:9 batch_done:112 Gen Loss: 6.71094 Disc Loss: 0.00656974 Q Losses: [0.019645005, 0.064965725]\n",
      "epoch:9 batch_done:113 Gen Loss: 6.68155 Disc Loss: 0.00785202 Q Losses: [0.019463956, 0.10105399]\n",
      "epoch:9 batch_done:114 Gen Loss: 6.79511 Disc Loss: 0.00353175 Q Losses: [0.010789905, 0.023771901]\n",
      "epoch:9 batch_done:115 Gen Loss: 6.70968 Disc Loss: 0.00424311 Q Losses: [0.03054972, 0.032104392]\n",
      "epoch:9 batch_done:116 Gen Loss: 6.49856 Disc Loss: 0.00412365 Q Losses: [0.011181433, 0.031824753]\n",
      "epoch:9 batch_done:117 Gen Loss: 6.40301 Disc Loss: 0.00696669 Q Losses: [0.012796866, 0.012079043]\n",
      "epoch:9 batch_done:118 Gen Loss: 6.48992 Disc Loss: 0.00392914 Q Losses: [0.01074059, 0.028818734]\n",
      "epoch:9 batch_done:119 Gen Loss: 6.50886 Disc Loss: 0.00664675 Q Losses: [0.0093650613, 0.025686312]\n",
      "epoch:9 batch_done:120 Gen Loss: 6.53688 Disc Loss: 0.00366022 Q Losses: [0.0064778491, 0.016173851]\n",
      "epoch:9 batch_done:121 Gen Loss: 6.58978 Disc Loss: 0.00365624 Q Losses: [0.007185909, 0.028608184]\n",
      "epoch:9 batch_done:122 Gen Loss: 6.62012 Disc Loss: 0.00309481 Q Losses: [0.0092462162, 0.033468448]\n",
      "epoch:9 batch_done:123 Gen Loss: 6.50695 Disc Loss: 0.00373934 Q Losses: [0.009713443, 0.013558489]\n",
      "epoch:9 batch_done:124 Gen Loss: 6.57077 Disc Loss: 0.00370848 Q Losses: [0.0068565207, 0.024765458]\n",
      "epoch:9 batch_done:125 Gen Loss: 6.63183 Disc Loss: 0.00350953 Q Losses: [0.007824285, 0.020894177]\n",
      "epoch:9 batch_done:126 Gen Loss: 6.78948 Disc Loss: 0.0025474 Q Losses: [0.0079844929, 0.016820781]\n",
      "epoch:9 batch_done:127 Gen Loss: 6.84913 Disc Loss: 0.00216315 Q Losses: [0.013470066, 0.011660536]\n",
      "epoch:9 batch_done:128 Gen Loss: 6.79855 Disc Loss: 0.00301479 Q Losses: [0.006649185, 0.011930483]\n",
      "epoch:9 batch_done:129 Gen Loss: 6.6427 Disc Loss: 0.00525815 Q Losses: [0.0072337445, 0.0090464447]\n",
      "epoch:9 batch_done:130 Gen Loss: 6.55618 Disc Loss: 0.00266477 Q Losses: [0.011261182, 0.013621168]\n",
      "epoch:9 batch_done:131 Gen Loss: 6.44991 Disc Loss: 0.00371685 Q Losses: [0.0047678896, 0.010242531]\n",
      "epoch:9 batch_done:132 Gen Loss: 6.5464 Disc Loss: 0.00347185 Q Losses: [0.0044639325, 0.012307856]\n",
      "epoch:9 batch_done:133 Gen Loss: 6.64577 Disc Loss: 0.00413491 Q Losses: [0.0057938751, 0.0093313931]\n",
      "epoch:9 batch_done:134 Gen Loss: 6.72876 Disc Loss: 0.00260074 Q Losses: [0.0068302425, 0.0081111211]\n",
      "epoch:9 batch_done:135 Gen Loss: 6.61892 Disc Loss: 0.00710499 Q Losses: [0.007701213, 0.0079603922]\n",
      "epoch:9 batch_done:136 Gen Loss: 6.54894 Disc Loss: 0.00304714 Q Losses: [0.00826695, 0.0078217285]\n",
      "epoch:9 batch_done:137 Gen Loss: 6.44719 Disc Loss: 0.00728523 Q Losses: [0.011679757, 0.012128311]\n",
      "epoch:9 batch_done:138 Gen Loss: 6.29357 Disc Loss: 0.00911741 Q Losses: [0.008415157, 0.010689078]\n",
      "epoch:9 batch_done:139 Gen Loss: 5.99563 Disc Loss: 0.0214114 Q Losses: [0.0049094623, 0.0082421927]\n",
      "epoch:9 batch_done:140 Gen Loss: 6.26132 Disc Loss: 0.00563167 Q Losses: [0.0069196681, 0.0091961641]\n",
      "epoch:9 batch_done:141 Gen Loss: 6.57874 Disc Loss: 0.00500797 Q Losses: [0.0082325339, 0.0075287698]\n",
      "epoch:9 batch_done:142 Gen Loss: 6.73956 Disc Loss: 0.00443586 Q Losses: [0.0073466245, 0.0074607288]\n",
      "epoch:9 batch_done:143 Gen Loss: 6.7538 Disc Loss: 0.00428541 Q Losses: [0.0082719363, 0.0071352469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 batch_done:144 Gen Loss: 6.69885 Disc Loss: 0.00423681 Q Losses: [0.012956385, 0.0065321019]\n",
      "epoch:9 batch_done:145 Gen Loss: 6.58842 Disc Loss: 0.0072379 Q Losses: [0.0036479416, 0.0071142204]\n",
      "epoch:9 batch_done:146 Gen Loss: 6.09906 Disc Loss: 0.0222944 Q Losses: [0.012888747, 0.009341348]\n",
      "epoch:9 batch_done:147 Gen Loss: 6.63526 Disc Loss: 0.0111494 Q Losses: [0.0058408715, 0.0059065805]\n",
      "epoch:9 batch_done:148 Gen Loss: 7.06554 Disc Loss: 0.00511508 Q Losses: [0.005088876, 0.0065969154]\n",
      "epoch:9 batch_done:149 Gen Loss: 7.22314 Disc Loss: 0.0028828 Q Losses: [0.0099478662, 0.0085976701]\n",
      "epoch:9 batch_done:150 Gen Loss: 7.03544 Disc Loss: 0.00246766 Q Losses: [0.0091863722, 0.0095257852]\n",
      "epoch:9 batch_done:151 Gen Loss: 6.91186 Disc Loss: 0.0024317 Q Losses: [0.0083575975, 0.0071993242]\n",
      "epoch:9 batch_done:152 Gen Loss: 6.77817 Disc Loss: 0.00281906 Q Losses: [0.0071761594, 0.0059642959]\n",
      "epoch:9 batch_done:153 Gen Loss: 6.60217 Disc Loss: 0.00408549 Q Losses: [0.013569463, 0.0067917877]\n",
      "epoch:9 batch_done:154 Gen Loss: 6.55202 Disc Loss: 0.0071482 Q Losses: [0.0092716925, 0.0072443602]\n",
      "epoch:9 batch_done:155 Gen Loss: 6.80568 Disc Loss: 0.00798111 Q Losses: [0.0069081183, 0.0079422174]\n",
      "epoch:9 batch_done:156 Gen Loss: 7.11858 Disc Loss: 0.00460932 Q Losses: [0.0053786822, 0.0076621631]\n",
      "epoch:9 batch_done:157 Gen Loss: 7.28459 Disc Loss: 0.00245404 Q Losses: [0.0053852028, 0.007496573]\n",
      "epoch:9 batch_done:158 Gen Loss: 7.236 Disc Loss: 0.00191193 Q Losses: [0.0046434989, 0.0066483039]\n",
      "epoch:9 batch_done:159 Gen Loss: 6.83197 Disc Loss: 0.00736869 Q Losses: [0.0064001195, 0.0067445803]\n",
      "epoch:9 batch_done:160 Gen Loss: 6.59563 Disc Loss: 0.00362098 Q Losses: [0.012472639, 0.0080347825]\n",
      "epoch:9 batch_done:161 Gen Loss: 6.52263 Disc Loss: 0.0102253 Q Losses: [0.0083187278, 0.0077982354]\n",
      "epoch:9 batch_done:162 Gen Loss: 7.09987 Disc Loss: 0.00935026 Q Losses: [0.0075852699, 0.0098124705]\n",
      "epoch:9 batch_done:163 Gen Loss: 7.77144 Disc Loss: 0.0155233 Q Losses: [0.0073101157, 0.0069019673]\n",
      "epoch:9 batch_done:164 Gen Loss: 7.80597 Disc Loss: 0.00763048 Q Losses: [0.012135874, 0.0091994554]\n",
      "epoch:9 batch_done:165 Gen Loss: 7.54626 Disc Loss: 0.00376553 Q Losses: [0.0080599096, 0.0064204438]\n",
      "epoch:9 batch_done:166 Gen Loss: 6.80299 Disc Loss: 0.031652 Q Losses: [0.014798012, 0.0065302663]\n",
      "epoch:9 batch_done:167 Gen Loss: 6.65874 Disc Loss: 0.00680605 Q Losses: [0.010329754, 0.0075635924]\n",
      "epoch:9 batch_done:168 Gen Loss: 7.19696 Disc Loss: 0.00969804 Q Losses: [0.008525284, 0.0074636806]\n",
      "epoch:9 batch_done:169 Gen Loss: 7.65608 Disc Loss: 0.00320533 Q Losses: [0.010415258, 0.009681222]\n",
      "epoch:9 batch_done:170 Gen Loss: 7.28448 Disc Loss: 0.00421365 Q Losses: [0.0062053418, 0.016145347]\n",
      "epoch:9 batch_done:171 Gen Loss: 6.96764 Disc Loss: 0.00358665 Q Losses: [0.015126605, 0.0058564246]\n",
      "epoch:9 batch_done:172 Gen Loss: 6.92712 Disc Loss: 0.00733794 Q Losses: [0.018157925, 0.0066136913]\n",
      "epoch:9 batch_done:173 Gen Loss: 7.17253 Disc Loss: 0.00674183 Q Losses: [0.013961094, 0.0093228482]\n",
      "epoch:9 batch_done:174 Gen Loss: 7.26023 Disc Loss: 0.00854995 Q Losses: [0.0062401742, 0.0058200555]\n",
      "epoch:9 batch_done:175 Gen Loss: 6.8151 Disc Loss: 0.067161 Q Losses: [0.0091433814, 0.0066073379]\n",
      "epoch:9 batch_done:176 Gen Loss: 7.51192 Disc Loss: 0.0164776 Q Losses: [0.0066160318, 0.008671023]\n",
      "epoch:9 batch_done:177 Gen Loss: 7.6682 Disc Loss: 0.00772281 Q Losses: [0.0082962327, 0.0059417216]\n",
      "epoch:9 batch_done:178 Gen Loss: 7.21275 Disc Loss: 0.00841185 Q Losses: [0.0067449044, 0.0099737756]\n",
      "epoch:9 batch_done:179 Gen Loss: 6.62419 Disc Loss: 0.0259072 Q Losses: [0.011233436, 0.0071774088]\n",
      "epoch:9 batch_done:180 Gen Loss: 7.22967 Disc Loss: 0.00964359 Q Losses: [0.011256782, 0.0094450694]\n",
      "epoch:9 batch_done:181 Gen Loss: 7.54094 Disc Loss: 0.00436425 Q Losses: [0.0089280196, 0.0061115241]\n",
      "epoch:9 batch_done:182 Gen Loss: 7.57544 Disc Loss: 0.00267719 Q Losses: [0.0077957008, 0.0056219986]\n",
      "epoch:9 batch_done:183 Gen Loss: 7.37112 Disc Loss: 0.0021185 Q Losses: [0.0093868766, 0.0055478881]\n",
      "epoch:9 batch_done:184 Gen Loss: 6.89799 Disc Loss: 0.00547999 Q Losses: [0.0083089955, 0.0058346987]\n",
      "epoch:9 batch_done:185 Gen Loss: 6.77171 Disc Loss: 0.00399501 Q Losses: [0.0079667717, 0.0055584302]\n",
      "epoch:9 batch_done:186 Gen Loss: 6.04382 Disc Loss: 0.0357991 Q Losses: [0.012855593, 0.0052254656]\n",
      "epoch:9 batch_done:187 Gen Loss: 6.8092 Disc Loss: 0.0108525 Q Losses: [0.0082804142, 0.005401311]\n",
      "epoch:9 batch_done:188 Gen Loss: 7.41145 Disc Loss: 0.00489306 Q Losses: [0.0079910327, 0.006628165]\n",
      "epoch:9 batch_done:189 Gen Loss: 7.57429 Disc Loss: 0.00243572 Q Losses: [0.0070927138, 0.0049742125]\n",
      "epoch:9 batch_done:190 Gen Loss: 7.49031 Disc Loss: 0.00169853 Q Losses: [0.012802579, 0.0076936251]\n"
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "# blow up after 81800\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/Session#run\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "c_val = 10\n",
    "\n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 1000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        train_data_filenames=permutation(train_data_filenames) # mini-batch\n",
    "        data_left = len(train_data_filenames)\n",
    "        batch_counter = 0\n",
    "        while data_left>0:\n",
    "            batch_size_to_train = min(batch_size, data_left)          \n",
    "            \n",
    "            zs = np.random.uniform(-1.0,1.0,size=[batch_size_to_train,z_size]).astype(np.float32) #Generate a random z batch\n",
    "            #print(\"zs shape:\",zs.shape)\n",
    "            \n",
    "            #lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "            lcat = np.random.randint(0,c_val,[batch_size_to_train,len(categorical_list)]) #Generate random c batch\n",
    "            \n",
    "            lcont = np.random.uniform(-1,1,[batch_size_to_train,number_continuous]) #\n",
    "\n",
    "            #xs = read_train_data_random_batch(train_data_filenames, batchsize=batch_size_to_train)\n",
    "            xs = read_train_data_mini_batch(train_data_filenames, batch_counter*batch_size, batch_size_to_train)\n",
    "            \n",
    "            _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "            _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "            _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "\n",
    "            data_left = data_left - batch_size_to_train\n",
    "            batch_counter +=1\n",
    "            if batch_counter%1 == 0 or data_left == 0:\n",
    "                z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32)\n",
    "                lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "                a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "                b = np.reshape(a,[c_val*c_val,1])\n",
    "                lcont_sample = b\n",
    "                samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample})\n",
    "                if not os.path.exists(sample_directory):\n",
    "                    os.makedirs(sample_directory)\n",
    "                save_images(np.reshape(samples[0:100],[100,32,32,3]),[10,10],sample_directory+'/fig'\\\n",
    "                            +str(i)+'_'+str(batch_counter)+'.png')\n",
    "                \n",
    "                print (\"epoch:\"+str(i)+\" batch_done:\"+str(batch_counter) \\\n",
    "                       +\" Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            \n",
    "             \n",
    "        \"\"\"\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "            #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "            latent_fixed = np.ones((c_val*c_val,1))\n",
    "            lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "            \n",
    "            #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "            #b = np.reshape(a,[100,1])\n",
    "            b = np.reshape(a,[c_val*c_val,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            #\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "            save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig'+str(i)+'.png')\n",
    "        \"\"\"\n",
    "        \n",
    "        if i % 10 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model on \", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\"\"\"\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "c_val = 10\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "    #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #lcat_sample = np.reshape(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]),[100,1])\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "    #print(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]))\n",
    "    #latent_fixed = np.ones((c_val*c_val,1))*50\n",
    "    latent_fixed = np.zeros((c_val*c_val,1))\n",
    "    #lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "    # good shape\n",
    "    lcat_sample = np.hstack([lcat_sample,latent_fixed])\n",
    "            \n",
    "    #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #a = a = np.ones((c_val*c_val,1))*-0.5\n",
    "    #a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #b = np.reshape(a,[100,1])\n",
    "    b = np.reshape(a,[c_val*c_val,1])\n",
    "    #c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)\n",
    "    c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)+8\n",
    "    #angle\n",
    "    lcont_sample = np.hstack([b,c])\n",
    "    # width\n",
    "    #lcont_sample = np.hstack([c,b])\n",
    "    \n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test_4'+'.png')\n",
    "    save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig_test_13'+'.png')\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35tf]",
   "language": "python",
   "name": "conda-env-dlpy35tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
