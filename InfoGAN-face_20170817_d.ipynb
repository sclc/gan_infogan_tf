{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN CeleA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import os, glob, cv2, math, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.misc\n",
    "import scipy\n",
    "#from PIL import Image\n",
    "\n",
    "np.random.seed(1)\n",
    "#plt.ion()   # interactive mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load CeleA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html\n",
    "#img_rows, img_cols = 100, 100\n",
    "#img_rows, img_cols = 64, 64\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "def get_im(path):\n",
    "\n",
    "    #img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.imread(path)\n",
    "    #img = plt.imread(path)\n",
    "    #resized = img\n",
    "    resized = cv2.resize(img, (img_cols, img_rows))\n",
    "\n",
    "    return resized\n",
    "\n",
    "def read_train_data_fullname(path):\n",
    "\n",
    "    \n",
    "    files = glob.glob(path)\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_fullname_lfw(path):\n",
    "\n",
    "    root = path\n",
    "    all_folders = os.listdir(root)\n",
    "    path = []\n",
    "    for afolder in all_folders:\n",
    "        path.append(root+\"/\"+afolder+\"/*.jpg\")\n",
    "    #print(path)\n",
    "    files = []\n",
    "    for apath in path:\n",
    "        templist = glob.glob(apath)\n",
    "        for afile in templist:\n",
    "            files.append(afile)\n",
    "\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_batch(filenames, batchsize=5):\n",
    "    \n",
    "    end = min(len(filenames), batchsize)\n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[:end]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        #normalization\n",
    "        #img -= np.mean(img)\n",
    "        #img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    filenames = filenames[end:]\n",
    "    \n",
    "    return train_data, filenames\n",
    "\n",
    "def read_train_data_mini_batch(filenames, startpoint, batchsize=5):\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[startpoint:startpoint+batchsize]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #print(type(img))        \n",
    "        #normalization\n",
    "        #img -= np.mean(img)\n",
    "        #img /= np.std(img)\n",
    "        #print(img.shape)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "#random mini-batch\n",
    "def read_train_data_random_batch(filenames, batchsize=5):\n",
    "    fullsize=len(filenames)\n",
    "    #http://qiita.com/hamukazu/items/ec1b4659df00f0ce43b1\n",
    "    idset = np.random.randint(0, high=fullsize, size=batchsize)\n",
    "    #print(idset) \n",
    "    train_data = []\n",
    "    \n",
    "    for fid in idset:\n",
    "        fl = filenames[fid]\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        #img[:,:,1] = 0\n",
    "        #img[:,:,2] = 0\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #img = np.reshape(img,[img.shape[0],img.shape[1],1])\n",
    "        # normalization\n",
    "        # https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn\n",
    "        img -= np.mean(img)\n",
    "        img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    \n",
    "    return train_data\n",
    "# must be full path, ~/... not ok\n",
    "# train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data,train_data_filenames = read_train_data_batch(train_data_filenames)\n",
    "\n",
    "#random mini-batch\n",
    "#train_data = read_train_data_random_batch(train_data_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(train_data),len(train_data_filenames))\n",
    "#print(len(train_data[0][1]))\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # need this transpose if there is transpose in read_train_data_xx calls\n",
    "    #inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #inp = std * inp + mean\n",
    "    #plt.imshow(inp,cmap=\"Purples\",interpolation = 'bicubic')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.01)  # pause a bit so that plots are updated\n",
    "\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "#train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "#print(len(train_data_filenames))\n",
    "#print(train_data_filenames[3])\n",
    "#train_data = read_train_data_mini_batch(train_data_filenames,0)  \n",
    "#print(train_data[0].shape)\n",
    "#print(train_data[43][:,:,0:2].shape)\n",
    "\n",
    "#imshow(train_data[2][:,:,0])\n",
    "#imshow(train_data[2][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge_color(images, size))\n",
    "    #return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return images\n",
    "    #return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def merge_color(images, size):\n",
    "    h, w, c = images.shape[1], images.shape[2],images.shape[3]\n",
    "    img = np.zeros((h * size[0], w * size[1],c))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image[:,:,:]\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/depth_to_space\n",
    "# http://qiita.com/tadOne/items/48302a399dcad44c69c8   Tensorflow - padding = VALID/SAMEの違いについて\n",
    "#     so 3 tf.depth_to_space(genX,2) gives 4x2^3 = 32\n",
    "# \n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*448,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,448])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=256,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=128,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=64,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=3,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,64,[4,4],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,128,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,256,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    #print(\"d_out\"+str(d_out))\n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/split\n",
    "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "# https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "# https://www.tensorflow.org/api_docs/python/tf/trainable_variables\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "# https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "# z_lat: one_hot_size + z_size + number_continuous = 10+64+2=76\n",
    "# g_loss def is interesting, my understanding: \n",
    "#        if Dg is the probablity to be told as feak data, then 1-Dg is the probabily of suceessfully cheating, \n",
    "#        so we cal KL(Dg/(1-Dg)), and readuce_mean works as sampling proceduce\n",
    "# \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "z_size = 128 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "#categorical_list = [10]*10 # Each entry in this list defines a categorical variable of a specific size.\n",
    "categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "# categorical_list = [10,10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 1 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "#print(\"latent_cat_in:\", latent_cat_in)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "#print(\"latent_cat_list: \",latent_cat_list)\n",
    "if number_continuous>0:\n",
    "    latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    #print(latent_cat_list[idx])\n",
    "    #print(latent_oh),  woundn't print anything in sess.run()\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "#print(\"1st z_lats: \", z_lats )\n",
    "z_lats.append(z_in)\n",
    "#print(\"2nd z_lats: \", z_lats )\n",
    "if number_continuous>0:\n",
    "    z_lats.append(latent_cont_in)\n",
    "#print(\"3rd z_lats: \", z_lats )\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "#print(\"z_lat: \", z_lat )\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "#print (Gz.shape)\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "#g_loss = -tf.reduce_mean(tf.log((Dg/(1.-Dg)))) #KL Divergence optimizer\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) \n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    #print (\"latent_var: \", latent_var)\n",
    "    #print (\"tf.log(QgCat[idx]): \",tf.log(QgCat[idx]))\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "#print (len(tvars))\n",
    "#for i in tvars:\n",
    "#    print(i)\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.4)\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "#\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:1 Gen Loss: 7.74446 Disc Loss: 1.76523 Q Losses: [0.21422461, 2.3141761]\n",
      "epoch:0 batch_done:2 Gen Loss: 9.55653 Disc Loss: 0.341702 Q Losses: [0.1500565, 2.3029916]\n",
      "epoch:0 batch_done:3 Gen Loss: 12.6012 Disc Loss: 0.313274 Q Losses: [0.17423716, 2.3220661]\n",
      "epoch:0 batch_done:4 Gen Loss: 0.00561882 Disc Loss: 2.93934 Q Losses: [0.1517016, 2.2868762]\n",
      "epoch:0 batch_done:5 Gen Loss: 4.84678 Disc Loss: 8.12348 Q Losses: [0.12837598, 2.2783859]\n",
      "epoch:0 batch_done:6 Gen Loss: 6.71776 Disc Loss: 0.685103 Q Losses: [0.13120843, 2.2631927]\n",
      "epoch:0 batch_done:7 Gen Loss: 1.74364 Disc Loss: 0.820542 Q Losses: [0.11917087, 2.2742789]\n",
      "epoch:0 batch_done:8 Gen Loss: 9.83898 Disc Loss: 1.53457 Q Losses: [0.09216439, 2.2650433]\n",
      "epoch:0 batch_done:9 Gen Loss: 8.10418 Disc Loss: 0.842016 Q Losses: [0.13054526, 2.2629499]\n",
      "epoch:0 batch_done:10 Gen Loss: 2.73808 Disc Loss: 0.35335 Q Losses: [0.13370244, 2.2205045]\n",
      "epoch:0 batch_done:11 Gen Loss: 11.0294 Disc Loss: 2.99439 Q Losses: [0.11867183, 2.25313]\n",
      "epoch:0 batch_done:12 Gen Loss: 9.07112 Disc Loss: 0.71874 Q Losses: [0.08022052, 2.2142277]\n",
      "epoch:0 batch_done:13 Gen Loss: 5.42095 Disc Loss: 0.614302 Q Losses: [0.13266882, 2.1931143]\n",
      "epoch:0 batch_done:14 Gen Loss: 13.788 Disc Loss: 1.54587 Q Losses: [0.094902612, 2.2344751]\n",
      "epoch:0 batch_done:15 Gen Loss: 10.4348 Disc Loss: 1.10727 Q Losses: [0.0991247, 2.2298114]\n",
      "epoch:0 batch_done:16 Gen Loss: 8.91971 Disc Loss: 0.509733 Q Losses: [0.079653606, 2.1971345]\n",
      "epoch:0 batch_done:17 Gen Loss: 7.35005 Disc Loss: 0.42997 Q Losses: [0.076496683, 2.1743002]\n",
      "epoch:0 batch_done:18 Gen Loss: 13.3904 Disc Loss: 0.769877 Q Losses: [0.074954763, 2.2233338]\n",
      "epoch:0 batch_done:19 Gen Loss: 0.954441 Disc Loss: 2.37126 Q Losses: [0.078788966, 2.2280781]\n",
      "epoch:0 batch_done:20 Gen Loss: 13.0059 Disc Loss: 3.82354 Q Losses: [0.081181921, 2.2050104]\n",
      "epoch:0 batch_done:21 Gen Loss: 12.228 Disc Loss: 1.22501 Q Losses: [0.070218541, 2.2064397]\n",
      "epoch:0 batch_done:22 Gen Loss: 6.04949 Disc Loss: 0.468465 Q Losses: [0.053279031, 2.1519508]\n",
      "epoch:0 batch_done:23 Gen Loss: 7.63149 Disc Loss: 0.629388 Q Losses: [0.050404064, 2.1306603]\n",
      "epoch:0 batch_done:24 Gen Loss: 6.40902 Disc Loss: 0.424295 Q Losses: [0.048429899, 2.158246]\n",
      "epoch:0 batch_done:25 Gen Loss: 6.55951 Disc Loss: 0.421724 Q Losses: [0.05376659, 2.1212938]\n",
      "epoch:0 batch_done:26 Gen Loss: 9.29841 Disc Loss: 0.412933 Q Losses: [0.053032197, 2.1319094]\n",
      "epoch:0 batch_done:27 Gen Loss: 6.27422 Disc Loss: 0.414612 Q Losses: [0.058449782, 2.1138113]\n",
      "epoch:0 batch_done:28 Gen Loss: 19.7164 Disc Loss: 2.15476 Q Losses: [0.061249122, 2.0921206]\n",
      "epoch:0 batch_done:29 Gen Loss: 16.188 Disc Loss: 3.03456 Q Losses: [0.054968975, 2.0749002]\n",
      "epoch:0 batch_done:30 Gen Loss: 7.21071 Disc Loss: 0.0441931 Q Losses: [0.060318317, 2.1217384]\n",
      "epoch:0 batch_done:31 Gen Loss: 18.0108 Disc Loss: 1.9313 Q Losses: [0.064018168, 2.0619302]\n",
      "epoch:0 batch_done:32 Gen Loss: 14.4628 Disc Loss: 0.375167 Q Losses: [0.056628358, 2.054539]\n",
      "epoch:0 batch_done:33 Gen Loss: 3.4442 Disc Loss: 1.1335 Q Losses: [0.046981916, 2.0439925]\n",
      "epoch:0 batch_done:34 Gen Loss: 18.647 Disc Loss: 2.19627 Q Losses: [0.03669291, 2.0247083]\n",
      "epoch:0 batch_done:35 Gen Loss: 14.5984 Disc Loss: 1.77847 Q Losses: [0.054612406, 1.951655]\n",
      "epoch:0 batch_done:36 Gen Loss: 6.01519 Disc Loss: 0.116661 Q Losses: [0.043998845, 1.9902503]\n",
      "epoch:0 batch_done:37 Gen Loss: 9.0042 Disc Loss: 0.320069 Q Losses: [0.056327306, 2.0042787]\n",
      "epoch:0 batch_done:38 Gen Loss: 7.57255 Disc Loss: 0.0515378 Q Losses: [0.05041524, 1.9360054]\n",
      "epoch:0 batch_done:39 Gen Loss: 5.2077 Disc Loss: 0.210833 Q Losses: [0.043535791, 1.9345107]\n",
      "epoch:0 batch_done:40 Gen Loss: 9.62363 Disc Loss: 0.419042 Q Losses: [0.053597648, 1.8982131]\n",
      "epoch:0 batch_done:41 Gen Loss: 7.5504 Disc Loss: 0.287343 Q Losses: [0.044068165, 1.866215]\n",
      "epoch:0 batch_done:42 Gen Loss: 5.47711 Disc Loss: 0.181658 Q Losses: [0.052968487, 1.911533]\n",
      "epoch:0 batch_done:43 Gen Loss: 9.94572 Disc Loss: 0.395807 Q Losses: [0.046238218, 1.8335055]\n",
      "epoch:0 batch_done:44 Gen Loss: 8.44131 Disc Loss: 0.29715 Q Losses: [0.055316266, 1.8144125]\n",
      "epoch:0 batch_done:45 Gen Loss: 4.86163 Disc Loss: 0.115422 Q Losses: [0.049876586, 1.8411845]\n",
      "epoch:0 batch_done:46 Gen Loss: 9.39746 Disc Loss: 0.195609 Q Losses: [0.051197421, 1.7334601]\n",
      "epoch:0 batch_done:47 Gen Loss: 8.77452 Disc Loss: 0.102173 Q Losses: [0.045974299, 1.7365489]\n",
      "epoch:0 batch_done:48 Gen Loss: 5.14778 Disc Loss: 0.112435 Q Losses: [0.041634127, 1.7027193]\n",
      "epoch:0 batch_done:49 Gen Loss: 9.24698 Disc Loss: 0.209117 Q Losses: [0.056564901, 1.6808189]\n",
      "epoch:0 batch_done:50 Gen Loss: 7.9834 Disc Loss: 0.1216 Q Losses: [0.05576849, 1.6649337]\n",
      "epoch:0 batch_done:51 Gen Loss: 5.25254 Disc Loss: 0.0798268 Q Losses: [0.059995756, 1.6934801]\n",
      "epoch:0 batch_done:52 Gen Loss: 12.5358 Disc Loss: 0.222649 Q Losses: [0.044298515, 1.614351]\n",
      "epoch:0 batch_done:53 Gen Loss: 10.8792 Disc Loss: 0.206574 Q Losses: [0.036769208, 1.598892]\n",
      "epoch:0 batch_done:54 Gen Loss: 7.36782 Disc Loss: 0.0463487 Q Losses: [0.056696009, 1.5804698]\n",
      "epoch:0 batch_done:55 Gen Loss: 4.69467 Disc Loss: 0.023232 Q Losses: [0.048826285, 1.5659447]\n",
      "epoch:0 batch_done:56 Gen Loss: 8.70614 Disc Loss: 0.0958036 Q Losses: [0.054761581, 1.5501395]\n",
      "epoch:0 batch_done:57 Gen Loss: 6.72433 Disc Loss: 0.202295 Q Losses: [0.0372473, 1.4674616]\n",
      "epoch:0 batch_done:58 Gen Loss: 4.45948 Disc Loss: 0.0182858 Q Losses: [0.042952251, 1.4162984]\n",
      "epoch:0 batch_done:59 Gen Loss: 11.8962 Disc Loss: 0.176321 Q Losses: [0.057545416, 1.4134247]\n",
      "epoch:0 batch_done:60 Gen Loss: 9.2237 Disc Loss: 0.24487 Q Losses: [0.042023227, 1.3372492]\n",
      "epoch:0 batch_done:61 Gen Loss: 5.52442 Disc Loss: 0.0180361 Q Losses: [0.044252869, 1.3154297]\n",
      "epoch:0 batch_done:62 Gen Loss: 5.67071 Disc Loss: 0.03151 Q Losses: [0.060697395, 1.3051934]\n",
      "epoch:0 batch_done:63 Gen Loss: 7.54466 Disc Loss: 0.0374427 Q Losses: [0.043480448, 1.288018]\n",
      "epoch:0 batch_done:64 Gen Loss: 7.99696 Disc Loss: 0.00950222 Q Losses: [0.044719856, 1.2964842]\n",
      "epoch:0 batch_done:65 Gen Loss: 6.68801 Disc Loss: 0.0122219 Q Losses: [0.065298937, 1.2761829]\n",
      "epoch:0 batch_done:66 Gen Loss: 4.64231 Disc Loss: 0.0508669 Q Losses: [0.062616251, 1.256097]\n",
      "epoch:0 batch_done:67 Gen Loss: 7.51549 Disc Loss: 0.0654832 Q Losses: [0.046146207, 1.2123995]\n",
      "epoch:0 batch_done:68 Gen Loss: 7.86387 Disc Loss: 0.0178228 Q Losses: [0.046076834, 1.1972213]\n",
      "epoch:0 batch_done:69 Gen Loss: 6.44093 Disc Loss: 0.0255316 Q Losses: [0.051409952, 1.2073555]\n",
      "epoch:0 batch_done:70 Gen Loss: 4.82758 Disc Loss: 0.0412373 Q Losses: [0.062539056, 1.1843454]\n",
      "epoch:0 batch_done:71 Gen Loss: 6.50828 Disc Loss: 0.0430092 Q Losses: [0.046727292, 1.1601861]\n",
      "epoch:0 batch_done:72 Gen Loss: 4.91166 Disc Loss: 0.0757625 Q Losses: [0.06041934, 1.1769626]\n",
      "epoch:0 batch_done:73 Gen Loss: 7.89057 Disc Loss: 0.0640833 Q Losses: [0.071874097, 1.1184152]\n",
      "epoch:0 batch_done:74 Gen Loss: 6.54426 Disc Loss: 0.0682838 Q Losses: [0.039137192, 1.1038549]\n",
      "epoch:0 batch_done:75 Gen Loss: 5.07811 Disc Loss: 0.0190795 Q Losses: [0.06069956, 1.1020925]\n",
      "epoch:0 batch_done:76 Gen Loss: 7.73127 Disc Loss: 0.0407948 Q Losses: [0.067994304, 1.0884372]\n",
      "epoch:0 batch_done:77 Gen Loss: 7.35954 Disc Loss: 0.0242336 Q Losses: [0.061456915, 1.0381913]\n",
      "epoch:0 batch_done:78 Gen Loss: 6.03568 Disc Loss: 0.0177932 Q Losses: [0.058629759, 1.0018809]\n",
      "epoch:0 batch_done:79 Gen Loss: 4.10271 Disc Loss: 0.0802999 Q Losses: [0.048826009, 1.0111624]\n",
      "epoch:0 batch_done:80 Gen Loss: 7.27892 Disc Loss: 0.0358771 Q Losses: [0.066070542, 1.0605769]\n",
      "epoch:0 batch_done:81 Gen Loss: 5.69752 Disc Loss: 0.0910435 Q Losses: [0.04400938, 1.0002749]\n",
      "epoch:0 batch_done:82 Gen Loss: 5.01474 Disc Loss: 0.0115576 Q Losses: [0.05580695, 0.98969913]\n",
      "epoch:0 batch_done:83 Gen Loss: 5.54033 Disc Loss: 0.0183358 Q Losses: [0.070483863, 0.94074464]\n",
      "epoch:0 batch_done:84 Gen Loss: 6.18436 Disc Loss: 0.010385 Q Losses: [0.043547176, 0.96029997]\n",
      "epoch:0 batch_done:85 Gen Loss: 6.20143 Disc Loss: 0.0116294 Q Losses: [0.046957783, 0.94165081]\n",
      "epoch:0 batch_done:86 Gen Loss: 5.49238 Disc Loss: 0.0230701 Q Losses: [0.052716073, 0.93386483]\n",
      "epoch:0 batch_done:87 Gen Loss: 5.67714 Disc Loss: 0.017926 Q Losses: [0.063597374, 0.91491693]\n",
      "epoch:0 batch_done:88 Gen Loss: 6.86084 Disc Loss: 0.0305691 Q Losses: [0.057144921, 0.88658881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:89 Gen Loss: 6.7349 Disc Loss: 0.0194738 Q Losses: [0.053753067, 0.89239991]\n",
      "epoch:0 batch_done:90 Gen Loss: 5.97347 Disc Loss: 0.0288662 Q Losses: [0.049722239, 0.86780715]\n",
      "epoch:0 batch_done:91 Gen Loss: 6.63178 Disc Loss: 0.0157772 Q Losses: [0.061414968, 0.89548206]\n",
      "epoch:0 batch_done:92 Gen Loss: 6.82764 Disc Loss: 0.00973621 Q Losses: [0.066409752, 0.8635416]\n",
      "epoch:0 batch_done:93 Gen Loss: 6.34347 Disc Loss: 0.0139117 Q Losses: [0.051662244, 0.8470754]\n",
      "epoch:0 batch_done:94 Gen Loss: 6.07166 Disc Loss: 0.0109578 Q Losses: [0.055166345, 0.85676813]\n",
      "epoch:0 batch_done:95 Gen Loss: 5.50397 Disc Loss: 0.0136194 Q Losses: [0.048959114, 0.82888246]\n",
      "epoch:0 batch_done:96 Gen Loss: 5.67049 Disc Loss: 0.0147191 Q Losses: [0.065827608, 0.82724422]\n",
      "epoch:0 batch_done:97 Gen Loss: 6.90934 Disc Loss: 0.0217528 Q Losses: [0.049925599, 0.81955725]\n",
      "epoch:0 batch_done:98 Gen Loss: 7.64283 Disc Loss: 0.0152874 Q Losses: [0.06489557, 0.80858171]\n",
      "epoch:0 batch_done:99 Gen Loss: 7.14041 Disc Loss: 0.0156446 Q Losses: [0.059300236, 0.78404671]\n",
      "epoch:0 batch_done:100 Gen Loss: 6.53056 Disc Loss: 0.0171686 Q Losses: [0.043040872, 0.75470048]\n",
      "epoch:0 batch_done:101 Gen Loss: 5.65248 Disc Loss: 0.0162142 Q Losses: [0.059952989, 0.76156354]\n",
      "epoch:0 batch_done:102 Gen Loss: 4.64158 Disc Loss: 0.0303585 Q Losses: [0.045395833, 0.74920464]\n",
      "epoch:0 batch_done:103 Gen Loss: 12.378 Disc Loss: 0.0790916 Q Losses: [0.058934476, 0.75949466]\n",
      "epoch:0 batch_done:104 Gen Loss: 11.1086 Disc Loss: 0.0563846 Q Losses: [0.050490506, 0.73127532]\n",
      "epoch:0 batch_done:105 Gen Loss: 8.99566 Disc Loss: 0.00659153 Q Losses: [0.044001598, 0.75503767]\n",
      "epoch:0 batch_done:106 Gen Loss: 6.70835 Disc Loss: 0.00488473 Q Losses: [0.076674514, 0.74063194]\n",
      "epoch:0 batch_done:107 Gen Loss: 6.05075 Disc Loss: 0.00453636 Q Losses: [0.04449895, 0.71223384]\n",
      "epoch:0 batch_done:108 Gen Loss: 6.14536 Disc Loss: 0.0065799 Q Losses: [0.065061122, 0.74083471]\n",
      "epoch:0 batch_done:109 Gen Loss: 6.35234 Disc Loss: 0.00931705 Q Losses: [0.042261727, 0.71880877]\n",
      "epoch:0 batch_done:110 Gen Loss: 7.11319 Disc Loss: 0.0122485 Q Losses: [0.05746365, 0.69911879]\n",
      "epoch:0 batch_done:111 Gen Loss: 8.94741 Disc Loss: 0.00492065 Q Losses: [0.050498635, 0.71664613]\n",
      "epoch:0 batch_done:112 Gen Loss: 8.67151 Disc Loss: 0.00474529 Q Losses: [0.044377513, 0.70165062]\n",
      "epoch:0 batch_done:113 Gen Loss: 5.79104 Disc Loss: 0.0116435 Q Losses: [0.056216456, 0.68222117]\n",
      "epoch:0 batch_done:114 Gen Loss: 6.06186 Disc Loss: 0.013628 Q Losses: [0.067947082, 0.7015537]\n",
      "epoch:0 batch_done:115 Gen Loss: 6.89337 Disc Loss: 0.0128559 Q Losses: [0.04822414, 0.65949661]\n",
      "epoch:0 batch_done:116 Gen Loss: 7.21804 Disc Loss: 0.0153317 Q Losses: [0.044556364, 0.67661345]\n",
      "epoch:0 batch_done:117 Gen Loss: 6.54397 Disc Loss: 0.00780907 Q Losses: [0.058780588, 0.62935656]\n",
      "epoch:0 batch_done:118 Gen Loss: 6.57667 Disc Loss: 0.00457617 Q Losses: [0.046502154, 0.6345582]\n",
      "epoch:0 batch_done:119 Gen Loss: 6.89753 Disc Loss: 0.0223452 Q Losses: [0.040898561, 0.66558838]\n",
      "epoch:0 batch_done:120 Gen Loss: 7.39833 Disc Loss: 0.00898433 Q Losses: [0.050032463, 0.6635794]\n",
      "epoch:0 batch_done:121 Gen Loss: 6.91356 Disc Loss: 0.00565479 Q Losses: [0.043473855, 0.60307121]\n",
      "epoch:0 batch_done:122 Gen Loss: 7.98378 Disc Loss: 0.00425726 Q Losses: [0.053892858, 0.62936181]\n",
      "epoch:0 batch_done:123 Gen Loss: 8.21062 Disc Loss: 0.0028084 Q Losses: [0.048848756, 0.63430315]\n",
      "epoch:0 batch_done:124 Gen Loss: 8.2775 Disc Loss: 0.00719001 Q Losses: [0.044372387, 0.60700327]\n",
      "epoch:0 batch_done:125 Gen Loss: 8.11227 Disc Loss: 0.0268183 Q Losses: [0.05426589, 0.56551218]\n",
      "epoch:0 batch_done:126 Gen Loss: 7.86926 Disc Loss: 0.0165176 Q Losses: [0.057689071, 0.58528471]\n",
      "epoch:0 batch_done:127 Gen Loss: 5.91659 Disc Loss: 0.0164156 Q Losses: [0.036365978, 0.57422245]\n",
      "epoch:0 batch_done:128 Gen Loss: 5.747 Disc Loss: 0.00519518 Q Losses: [0.047097981, 0.55559289]\n",
      "epoch:0 batch_done:129 Gen Loss: 6.08031 Disc Loss: 0.00774897 Q Losses: [0.050196435, 0.57710195]\n",
      "epoch:0 batch_done:130 Gen Loss: 9.34503 Disc Loss: 0.000911673 Q Losses: [0.036093622, 0.58038628]\n",
      "epoch:0 batch_done:131 Gen Loss: 6.15474 Disc Loss: 0.00900428 Q Losses: [0.05778278, 0.55762839]\n",
      "epoch:0 batch_done:132 Gen Loss: 7.49165 Disc Loss: 0.00265601 Q Losses: [0.047418658, 0.56188715]\n",
      "epoch:0 batch_done:133 Gen Loss: 7.24993 Disc Loss: 0.0082611 Q Losses: [0.050688516, 0.54134083]\n",
      "epoch:0 batch_done:134 Gen Loss: 6.61955 Disc Loss: 0.00331484 Q Losses: [0.039251581, 0.54253495]\n",
      "epoch:0 batch_done:135 Gen Loss: 6.17595 Disc Loss: 0.00420413 Q Losses: [0.044000335, 0.52317768]\n",
      "epoch:0 batch_done:136 Gen Loss: 6.40451 Disc Loss: 0.00925906 Q Losses: [0.046852548, 0.53967905]\n",
      "epoch:0 batch_done:137 Gen Loss: 7.00961 Disc Loss: 0.00908624 Q Losses: [0.03391115, 0.51886195]\n",
      "epoch:0 batch_done:138 Gen Loss: 7.33362 Disc Loss: 0.00914828 Q Losses: [0.056535266, 0.50796789]\n",
      "epoch:0 batch_done:139 Gen Loss: 5.29596 Disc Loss: 0.0174336 Q Losses: [0.036105853, 0.52673954]\n",
      "epoch:0 batch_done:140 Gen Loss: 5.88556 Disc Loss: 0.00414738 Q Losses: [0.052188434, 0.48891541]\n",
      "epoch:0 batch_done:141 Gen Loss: 6.71761 Disc Loss: 0.00972877 Q Losses: [0.041671164, 0.51211047]\n",
      "epoch:0 batch_done:142 Gen Loss: 6.72302 Disc Loss: 0.0136267 Q Losses: [0.037155166, 0.51225811]\n",
      "epoch:0 batch_done:143 Gen Loss: 7.93728 Disc Loss: 0.014443 Q Losses: [0.04996679, 0.48499107]\n",
      "epoch:0 batch_done:144 Gen Loss: 8.09423 Disc Loss: 0.0085857 Q Losses: [0.057481937, 0.47084776]\n",
      "epoch:0 batch_done:145 Gen Loss: 6.47209 Disc Loss: 0.0143587 Q Losses: [0.036057532, 0.47624099]\n",
      "epoch:0 batch_done:146 Gen Loss: 6.53429 Disc Loss: 0.00319146 Q Losses: [0.028306726, 0.47726262]\n",
      "epoch:0 batch_done:147 Gen Loss: 7.11981 Disc Loss: 0.00246423 Q Losses: [0.02718411, 0.48490214]\n",
      "epoch:0 batch_done:148 Gen Loss: 6.82306 Disc Loss: 0.0018443 Q Losses: [0.053374253, 0.45462018]\n",
      "epoch:0 batch_done:149 Gen Loss: 7.5044 Disc Loss: 0.0175317 Q Losses: [0.042799786, 0.50875783]\n",
      "epoch:0 batch_done:150 Gen Loss: 7.94867 Disc Loss: 0.0105101 Q Losses: [0.053964473, 0.45448682]\n",
      "epoch:0 batch_done:151 Gen Loss: 7.54486 Disc Loss: 0.00226387 Q Losses: [0.036735155, 0.45456815]\n",
      "epoch:0 batch_done:152 Gen Loss: 6.75427 Disc Loss: 0.00547147 Q Losses: [0.031705409, 0.44632506]\n",
      "epoch:0 batch_done:153 Gen Loss: 6.31767 Disc Loss: 0.00423126 Q Losses: [0.047193091, 0.45561004]\n",
      "epoch:0 batch_done:154 Gen Loss: 6.39114 Disc Loss: 0.00625854 Q Losses: [0.041204542, 0.43421015]\n",
      "epoch:0 batch_done:155 Gen Loss: 6.53324 Disc Loss: 0.0050561 Q Losses: [0.03637315, 0.44574177]\n",
      "epoch:0 batch_done:156 Gen Loss: 7.02398 Disc Loss: 0.00677223 Q Losses: [0.053356655, 0.44792271]\n",
      "epoch:0 batch_done:157 Gen Loss: 7.63656 Disc Loss: 0.019026 Q Losses: [0.035078786, 0.43253753]\n",
      "epoch:0 batch_done:158 Gen Loss: 8.78309 Disc Loss: 0.00490094 Q Losses: [0.045874067, 0.41667557]\n",
      "epoch:0 batch_done:159 Gen Loss: 6.5731 Disc Loss: 0.0194921 Q Losses: [0.038440783, 0.41885695]\n",
      "epoch:0 batch_done:160 Gen Loss: 6.10854 Disc Loss: 0.00395748 Q Losses: [0.042281009, 0.40842989]\n",
      "epoch:0 batch_done:161 Gen Loss: 6.98887 Disc Loss: 0.0130848 Q Losses: [0.035459552, 0.43190575]\n",
      "epoch:0 batch_done:162 Gen Loss: 8.48791 Disc Loss: 0.00172866 Q Losses: [0.032968275, 0.41305894]\n",
      "epoch:0 batch_done:163 Gen Loss: 8.02609 Disc Loss: 0.00147144 Q Losses: [0.042183392, 0.40980631]\n",
      "epoch:0 batch_done:164 Gen Loss: 7.64413 Disc Loss: 0.00730618 Q Losses: [0.026649639, 0.40570527]\n",
      "epoch:0 batch_done:165 Gen Loss: 6.24054 Disc Loss: 0.00311983 Q Losses: [0.047574174, 0.40444687]\n",
      "epoch:0 batch_done:166 Gen Loss: 6.55371 Disc Loss: 0.00400562 Q Losses: [0.030180149, 0.41053396]\n",
      "epoch:0 batch_done:167 Gen Loss: 6.61636 Disc Loss: 0.0049867 Q Losses: [0.029439326, 0.40601987]\n",
      "epoch:0 batch_done:168 Gen Loss: 6.91288 Disc Loss: 0.00467789 Q Losses: [0.036925554, 0.39738202]\n",
      "epoch:0 batch_done:169 Gen Loss: 7.03058 Disc Loss: 0.00356737 Q Losses: [0.030981507, 0.38088971]\n",
      "epoch:0 batch_done:170 Gen Loss: 6.84443 Disc Loss: 0.00631544 Q Losses: [0.037373811, 0.39109203]\n",
      "epoch:0 batch_done:171 Gen Loss: 6.21196 Disc Loss: 0.00603537 Q Losses: [0.029177144, 0.38111129]\n",
      "epoch:0 batch_done:172 Gen Loss: 6.46576 Disc Loss: 0.00610344 Q Losses: [0.038256545, 0.37815839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:173 Gen Loss: 7.35984 Disc Loss: 0.0144048 Q Losses: [0.040627327, 0.37627664]\n",
      "epoch:0 batch_done:174 Gen Loss: 7.50647 Disc Loss: 0.00751522 Q Losses: [0.039026313, 0.39565244]\n",
      "epoch:0 batch_done:175 Gen Loss: 7.07023 Disc Loss: 0.00622706 Q Losses: [0.032066096, 0.37574685]\n",
      "epoch:0 batch_done:176 Gen Loss: 5.89771 Disc Loss: 0.00651128 Q Losses: [0.043428622, 0.36005372]\n",
      "epoch:0 batch_done:177 Gen Loss: 6.76559 Disc Loss: 0.00781545 Q Losses: [0.029346906, 0.36572695]\n",
      "epoch:0 batch_done:178 Gen Loss: 7.34245 Disc Loss: 0.00458954 Q Losses: [0.033302169, 0.37619054]\n",
      "epoch:0 batch_done:179 Gen Loss: 7.75165 Disc Loss: 0.00309449 Q Losses: [0.040099978, 0.36192822]\n",
      "epoch:0 batch_done:180 Gen Loss: 8.22525 Disc Loss: 0.00186831 Q Losses: [0.039652787, 0.37560213]\n",
      "epoch:0 batch_done:181 Gen Loss: 7.50628 Disc Loss: 0.00340773 Q Losses: [0.032970425, 0.3642157]\n",
      "epoch:0 batch_done:182 Gen Loss: 6.4106 Disc Loss: 0.00731912 Q Losses: [0.034819309, 0.34256989]\n",
      "epoch:0 batch_done:183 Gen Loss: 6.95675 Disc Loss: 0.00772497 Q Losses: [0.028426854, 0.32344404]\n",
      "epoch:0 batch_done:184 Gen Loss: 4.61306 Disc Loss: 0.0196361 Q Losses: [0.035231035, 0.35500115]\n",
      "epoch:0 batch_done:185 Gen Loss: 8.72006 Disc Loss: 0.0223903 Q Losses: [0.03086981, 0.32548499]\n",
      "epoch:0 batch_done:186 Gen Loss: 8.4259 Disc Loss: 0.0112652 Q Losses: [0.030004386, 0.33905244]\n",
      "epoch:0 batch_done:187 Gen Loss: 6.83058 Disc Loss: 0.00970249 Q Losses: [0.031369969, 0.34503165]\n",
      "epoch:0 batch_done:188 Gen Loss: 6.44079 Disc Loss: 0.00187197 Q Losses: [0.041501224, 0.31723264]\n",
      "epoch:0 batch_done:189 Gen Loss: 7.13251 Disc Loss: 0.00108822 Q Losses: [0.034138836, 0.34209794]\n",
      "epoch:0 batch_done:190 Gen Loss: 6.57565 Disc Loss: 0.00357023 Q Losses: [0.027848491, 0.34084597]\n",
      "epoch:0 batch_done:191 Gen Loss: 7.49916 Disc Loss: 0.00205781 Q Losses: [0.034797259, 0.32635307]\n",
      "epoch:0 batch_done:192 Gen Loss: 7.62618 Disc Loss: 0.0119984 Q Losses: [0.032138318, 0.32832938]\n",
      "epoch:0 batch_done:193 Gen Loss: 6.41215 Disc Loss: 0.0109957 Q Losses: [0.036486633, 0.31305784]\n",
      "epoch:0 batch_done:194 Gen Loss: 7.3071 Disc Loss: 0.0127547 Q Losses: [0.033151306, 0.32050484]\n",
      "epoch:0 batch_done:195 Gen Loss: 7.60451 Disc Loss: 0.00550992 Q Losses: [0.034177717, 0.33684373]\n",
      "epoch:0 batch_done:196 Gen Loss: 6.94644 Disc Loss: 0.00686033 Q Losses: [0.034656525, 0.33125573]\n",
      "epoch:0 batch_done:197 Gen Loss: 7.19945 Disc Loss: 0.00187894 Q Losses: [0.027779892, 0.30906433]\n",
      "epoch:0 batch_done:198 Gen Loss: 8.23322 Disc Loss: 0.00172337 Q Losses: [0.026621517, 0.31042475]\n",
      "epoch:0 batch_done:199 Gen Loss: 8.83499 Disc Loss: 0.000716265 Q Losses: [0.032312863, 0.31632906]\n",
      "epoch:0 batch_done:200 Gen Loss: 6.14652 Disc Loss: 0.00631546 Q Losses: [0.037620641, 0.32305047]\n",
      "epoch:0 batch_done:201 Gen Loss: 7.45635 Disc Loss: 0.0151332 Q Losses: [0.029855162, 0.31446019]\n",
      "epoch:0 batch_done:202 Gen Loss: 6.40406 Disc Loss: 0.0110374 Q Losses: [0.022102568, 0.30073246]\n",
      "epoch:0 batch_done:203 Gen Loss: 6.71097 Disc Loss: 0.00288404 Q Losses: [0.027468454, 0.28748581]\n",
      "epoch:0 batch_done:204 Gen Loss: 6.86277 Disc Loss: 0.00252939 Q Losses: [0.025798842, 0.28020501]\n",
      "epoch:0 batch_done:205 Gen Loss: 8.07958 Disc Loss: 0.0102252 Q Losses: [0.027182123, 0.29402658]\n",
      "epoch:0 batch_done:206 Gen Loss: 6.37471 Disc Loss: 0.00167719 Q Losses: [0.032434713, 0.30517307]\n",
      "epoch:0 batch_done:207 Gen Loss: 6.77228 Disc Loss: 0.00224966 Q Losses: [0.029657714, 0.28269917]\n",
      "epoch:1 batch_done:1 Gen Loss: 12.8818 Disc Loss: 0.0269687 Q Losses: [0.027924336, 0.29230756]\n",
      "epoch:1 batch_done:2 Gen Loss: 7.28653 Disc Loss: 0.050249 Q Losses: [0.027162766, 0.29162312]\n",
      "epoch:1 batch_done:3 Gen Loss: 4.43153 Disc Loss: 0.00427773 Q Losses: [0.018835075, 0.28756675]\n",
      "epoch:1 batch_done:4 Gen Loss: 8.48878 Disc Loss: 0.0126237 Q Losses: [0.027880028, 0.27755231]\n",
      "epoch:1 batch_done:5 Gen Loss: 10.5175 Disc Loss: 0.000282823 Q Losses: [0.036048606, 0.2995958]\n",
      "epoch:1 batch_done:6 Gen Loss: 11.1528 Disc Loss: 0.000186253 Q Losses: [0.027856268, 0.27604613]\n",
      "epoch:1 batch_done:7 Gen Loss: 8.30272 Disc Loss: 0.000555712 Q Losses: [0.02466492, 0.27473134]\n",
      "epoch:1 batch_done:8 Gen Loss: 8.0836 Disc Loss: 0.0006929 Q Losses: [0.027951427, 0.28119802]\n",
      "epoch:1 batch_done:9 Gen Loss: 9.44689 Disc Loss: 0.000999511 Q Losses: [0.026904872, 0.28256908]\n",
      "epoch:1 batch_done:10 Gen Loss: 12.062 Disc Loss: 0.000876793 Q Losses: [0.034381907, 0.25677687]\n",
      "epoch:1 batch_done:11 Gen Loss: 7.3224 Disc Loss: 0.00123861 Q Losses: [0.021919791, 0.27027208]\n",
      "epoch:1 batch_done:12 Gen Loss: 7.62969 Disc Loss: 0.00233872 Q Losses: [0.02823372, 0.27877963]\n",
      "epoch:1 batch_done:13 Gen Loss: 8.68541 Disc Loss: 0.00184487 Q Losses: [0.028351177, 0.27150786]\n",
      "epoch:1 batch_done:14 Gen Loss: 6.52382 Disc Loss: 0.00343725 Q Losses: [0.033571444, 0.29042906]\n",
      "epoch:1 batch_done:15 Gen Loss: 6.70893 Disc Loss: 0.00557625 Q Losses: [0.029616235, 0.26625773]\n",
      "epoch:1 batch_done:16 Gen Loss: 8.51111 Disc Loss: 0.00116478 Q Losses: [0.027238008, 0.2801339]\n",
      "epoch:1 batch_done:17 Gen Loss: 8.45827 Disc Loss: 0.00208356 Q Losses: [0.020337366, 0.26692569]\n",
      "epoch:1 batch_done:18 Gen Loss: 6.21377 Disc Loss: 0.00412426 Q Losses: [0.018721815, 0.25537273]\n",
      "epoch:1 batch_done:19 Gen Loss: 6.657 Disc Loss: 0.00256934 Q Losses: [0.023323383, 0.25472319]\n",
      "epoch:1 batch_done:20 Gen Loss: 7.14329 Disc Loss: 0.00678587 Q Losses: [0.02718224, 0.23452345]\n",
      "epoch:1 batch_done:21 Gen Loss: 8.62001 Disc Loss: 0.00183392 Q Losses: [0.023446623, 0.24753812]\n",
      "epoch:1 batch_done:22 Gen Loss: 7.44649 Disc Loss: 0.00282296 Q Losses: [0.021251719, 0.2712763]\n",
      "epoch:1 batch_done:23 Gen Loss: 7.38681 Disc Loss: 0.0025569 Q Losses: [0.028959861, 0.25683671]\n",
      "epoch:1 batch_done:24 Gen Loss: 5.99956 Disc Loss: 0.00524774 Q Losses: [0.036705948, 0.23540863]\n",
      "epoch:1 batch_done:25 Gen Loss: 6.84622 Disc Loss: 0.00168549 Q Losses: [0.026774889, 0.23610267]\n",
      "epoch:1 batch_done:26 Gen Loss: 7.11067 Disc Loss: 0.00459312 Q Losses: [0.018300693, 0.28843415]\n",
      "epoch:1 batch_done:27 Gen Loss: 7.85384 Disc Loss: 0.00835644 Q Losses: [0.018919591, 0.24073075]\n",
      "epoch:1 batch_done:28 Gen Loss: 7.4902 Disc Loss: 0.0047712 Q Losses: [0.029464493, 0.26864806]\n",
      "epoch:1 batch_done:29 Gen Loss: 7.35729 Disc Loss: 0.00339168 Q Losses: [0.025444299, 0.23162378]\n",
      "epoch:1 batch_done:30 Gen Loss: 7.77033 Disc Loss: 0.00151882 Q Losses: [0.023149498, 0.28599477]\n",
      "epoch:1 batch_done:31 Gen Loss: 11.2228 Disc Loss: 0.0041243 Q Losses: [0.032030106, 0.2413069]\n",
      "epoch:1 batch_done:32 Gen Loss: 7.0152 Disc Loss: 0.000963837 Q Losses: [0.021126423, 0.24160749]\n",
      "epoch:1 batch_done:33 Gen Loss: 6.79833 Disc Loss: 0.00190566 Q Losses: [0.027554583, 0.22720993]\n",
      "epoch:1 batch_done:34 Gen Loss: 7.44127 Disc Loss: 0.00110236 Q Losses: [0.0277237, 0.22945869]\n",
      "epoch:1 batch_done:35 Gen Loss: 13.0753 Disc Loss: 0.0232166 Q Losses: [0.02228635, 0.22844215]\n",
      "epoch:1 batch_done:36 Gen Loss: 4.95096 Disc Loss: 0.0457731 Q Losses: [0.023270756, 0.21659367]\n",
      "epoch:1 batch_done:37 Gen Loss: 4.99888 Disc Loss: 0.00925128 Q Losses: [0.023193436, 0.23379965]\n",
      "epoch:1 batch_done:38 Gen Loss: 12.5633 Disc Loss: 0.0201011 Q Losses: [0.019764598, 0.21245787]\n",
      "epoch:1 batch_done:39 Gen Loss: 16.8743 Disc Loss: 0.000485585 Q Losses: [0.028212208, 0.23138206]\n",
      "epoch:1 batch_done:40 Gen Loss: 15.6116 Disc Loss: 0.00159503 Q Losses: [0.026997112, 0.21061766]\n",
      "epoch:1 batch_done:41 Gen Loss: 14.4114 Disc Loss: 0.00324562 Q Losses: [0.02080453, 0.2115708]\n",
      "epoch:1 batch_done:42 Gen Loss: 13.0234 Disc Loss: 0.00290812 Q Losses: [0.023493864, 0.22180706]\n",
      "epoch:1 batch_done:43 Gen Loss: 13.4903 Disc Loss: 0.00226724 Q Losses: [0.022926569, 0.22450127]\n",
      "epoch:1 batch_done:44 Gen Loss: 9.67058 Disc Loss: 0.000322789 Q Losses: [0.017195042, 0.2068886]\n",
      "epoch:1 batch_done:45 Gen Loss: 8.93435 Disc Loss: 0.00016088 Q Losses: [0.026716113, 0.20360351]\n",
      "epoch:1 batch_done:46 Gen Loss: 8.71419 Disc Loss: 0.000241321 Q Losses: [0.021909654, 0.25396365]\n",
      "epoch:1 batch_done:47 Gen Loss: 11.3023 Disc Loss: 0.000131113 Q Losses: [0.018365297, 0.22011086]\n",
      "epoch:1 batch_done:48 Gen Loss: 8.52022 Disc Loss: 0.00043377 Q Losses: [0.021192472, 0.18962029]\n",
      "epoch:1 batch_done:49 Gen Loss: 7.92434 Disc Loss: 0.00069744 Q Losses: [0.027636789, 0.22883701]\n",
      "epoch:1 batch_done:50 Gen Loss: 8.64707 Disc Loss: 0.000486196 Q Losses: [0.02103718, 0.21323317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:51 Gen Loss: 10.0955 Disc Loss: 0.00106203 Q Losses: [0.016798833, 0.19960874]\n",
      "epoch:1 batch_done:52 Gen Loss: 7.81025 Disc Loss: 0.00134414 Q Losses: [0.027156999, 0.198843]\n",
      "epoch:1 batch_done:53 Gen Loss: 7.19889 Disc Loss: 0.00223747 Q Losses: [0.030534353, 0.18901207]\n",
      "epoch:1 batch_done:54 Gen Loss: 6.11602 Disc Loss: 0.00485861 Q Losses: [0.020435795, 0.21045974]\n",
      "epoch:1 batch_done:55 Gen Loss: 6.7622 Disc Loss: 0.00475306 Q Losses: [0.019869868, 0.19274004]\n",
      "epoch:1 batch_done:56 Gen Loss: 4.11114 Disc Loss: 0.0221417 Q Losses: [0.016349107, 0.19144675]\n",
      "epoch:1 batch_done:57 Gen Loss: 13.2151 Disc Loss: 0.0248321 Q Losses: [0.018848419, 0.19859406]\n",
      "epoch:1 batch_done:58 Gen Loss: 12.8083 Disc Loss: 0.0211046 Q Losses: [0.025010642, 0.19081984]\n",
      "epoch:1 batch_done:59 Gen Loss: 12.7208 Disc Loss: 0.000523382 Q Losses: [0.024675444, 0.21270972]\n",
      "epoch:1 batch_done:60 Gen Loss: 11.4898 Disc Loss: 0.0005525 Q Losses: [0.017364506, 0.19549885]\n",
      "epoch:1 batch_done:61 Gen Loss: 9.21793 Disc Loss: 0.000116191 Q Losses: [0.017652374, 0.18385585]\n",
      "epoch:1 batch_done:62 Gen Loss: 9.36964 Disc Loss: 0.000132271 Q Losses: [0.028486218, 0.18813823]\n",
      "epoch:1 batch_done:63 Gen Loss: 8.21825 Disc Loss: 0.000324612 Q Losses: [0.025417557, 0.18506628]\n",
      "epoch:1 batch_done:64 Gen Loss: 7.75712 Disc Loss: 0.000614733 Q Losses: [0.01705336, 0.17578891]\n",
      "epoch:1 batch_done:65 Gen Loss: 7.61182 Disc Loss: 0.000881312 Q Losses: [0.021332346, 0.19554308]\n",
      "epoch:1 batch_done:66 Gen Loss: 7.62735 Disc Loss: 0.0011504 Q Losses: [0.023980893, 0.18974073]\n",
      "epoch:1 batch_done:67 Gen Loss: 8.09286 Disc Loss: 0.000590738 Q Losses: [0.017985726, 0.18545124]\n",
      "epoch:1 batch_done:68 Gen Loss: 8.34916 Disc Loss: 0.000772666 Q Losses: [0.015601277, 0.17352702]\n",
      "epoch:1 batch_done:69 Gen Loss: 9.02344 Disc Loss: 0.000275311 Q Losses: [0.022437088, 0.19308522]\n",
      "epoch:1 batch_done:70 Gen Loss: 10.3244 Disc Loss: 0.000142328 Q Losses: [0.023267269, 0.19831271]\n",
      "epoch:1 batch_done:71 Gen Loss: 7.15555 Disc Loss: 0.00288433 Q Losses: [0.013452072, 0.18416542]\n",
      "epoch:1 batch_done:72 Gen Loss: 8.31035 Disc Loss: 0.0010211 Q Losses: [0.016499974, 0.20121971]\n",
      "epoch:1 batch_done:73 Gen Loss: 10.6367 Disc Loss: 0.000448852 Q Losses: [0.019883798, 0.19719246]\n",
      "epoch:1 batch_done:74 Gen Loss: 7.44559 Disc Loss: 0.00138593 Q Losses: [0.023426699, 0.17798597]\n",
      "epoch:1 batch_done:75 Gen Loss: 7.5406 Disc Loss: 0.00289564 Q Losses: [0.015893498, 0.16616364]\n",
      "epoch:1 batch_done:76 Gen Loss: 8.12232 Disc Loss: 0.00152304 Q Losses: [0.017235547, 0.17572218]\n",
      "epoch:1 batch_done:77 Gen Loss: 7.44712 Disc Loss: 0.00128369 Q Losses: [0.013918377, 0.16970719]\n",
      "epoch:1 batch_done:78 Gen Loss: 7.34243 Disc Loss: 0.00109928 Q Losses: [0.024808871, 0.19606678]\n",
      "epoch:1 batch_done:79 Gen Loss: 7.13103 Disc Loss: 0.00215537 Q Losses: [0.020289786, 0.16307592]\n",
      "epoch:1 batch_done:80 Gen Loss: 8.13431 Disc Loss: 0.000930421 Q Losses: [0.017721485, 0.17903346]\n",
      "epoch:1 batch_done:81 Gen Loss: 7.32473 Disc Loss: 0.0019342 Q Losses: [0.014917628, 0.18261354]\n",
      "epoch:1 batch_done:82 Gen Loss: 7.44944 Disc Loss: 0.00169867 Q Losses: [0.020498917, 0.15886873]\n",
      "epoch:1 batch_done:83 Gen Loss: 7.36456 Disc Loss: 0.00225597 Q Losses: [0.021237269, 0.17326806]\n",
      "epoch:1 batch_done:84 Gen Loss: 7.7546 Disc Loss: 0.00150616 Q Losses: [0.015652727, 0.15700832]\n",
      "epoch:1 batch_done:85 Gen Loss: 8.68289 Disc Loss: 0.00118441 Q Losses: [0.024855457, 0.15626165]\n",
      "epoch:1 batch_done:86 Gen Loss: 9.55171 Disc Loss: 0.00250655 Q Losses: [0.016551232, 0.17300901]\n",
      "epoch:1 batch_done:87 Gen Loss: 6.83561 Disc Loss: 0.00229802 Q Losses: [0.01630304, 0.16293311]\n",
      "epoch:1 batch_done:88 Gen Loss: 7.42827 Disc Loss: 0.00190042 Q Losses: [0.014343414, 0.15971637]\n",
      "epoch:1 batch_done:89 Gen Loss: 7.9746 Disc Loss: 0.00163602 Q Losses: [0.018830389, 0.17252517]\n",
      "epoch:1 batch_done:90 Gen Loss: 7.68479 Disc Loss: 0.00186417 Q Losses: [0.019952824, 0.15936565]\n",
      "epoch:1 batch_done:91 Gen Loss: 7.95625 Disc Loss: 0.00168127 Q Losses: [0.013967059, 0.17106766]\n",
      "epoch:1 batch_done:92 Gen Loss: 8.29704 Disc Loss: 0.00173431 Q Losses: [0.021370197, 0.15956838]\n",
      "epoch:1 batch_done:93 Gen Loss: 7.03626 Disc Loss: 0.00164036 Q Losses: [0.022158507, 0.16059764]\n",
      "epoch:1 batch_done:94 Gen Loss: 6.55048 Disc Loss: 0.00476569 Q Losses: [0.021408603, 0.16485848]\n",
      "epoch:1 batch_done:95 Gen Loss: 7.58743 Disc Loss: 0.00478692 Q Losses: [0.015289959, 0.15860161]\n",
      "epoch:1 batch_done:96 Gen Loss: 9.02644 Disc Loss: 0.0011145 Q Losses: [0.02903631, 0.14727834]\n",
      "epoch:1 batch_done:97 Gen Loss: 8.7349 Disc Loss: 0.000800111 Q Losses: [0.014660412, 0.15741065]\n",
      "epoch:1 batch_done:98 Gen Loss: 7.83058 Disc Loss: 0.00737113 Q Losses: [0.013348758, 0.15884587]\n",
      "epoch:1 batch_done:99 Gen Loss: 6.87258 Disc Loss: 0.000808783 Q Losses: [0.015152605, 0.15359184]\n",
      "epoch:1 batch_done:100 Gen Loss: 6.94733 Disc Loss: 0.00189393 Q Losses: [0.014975401, 0.15350865]\n",
      "epoch:1 batch_done:101 Gen Loss: 7.88623 Disc Loss: 0.00465949 Q Losses: [0.021323908, 0.15177742]\n",
      "epoch:1 batch_done:102 Gen Loss: 9.44633 Disc Loss: 0.000449313 Q Losses: [0.018742854, 0.17762882]\n",
      "epoch:1 batch_done:103 Gen Loss: 8.9019 Disc Loss: 0.000547003 Q Losses: [0.012981048, 0.14430389]\n",
      "epoch:1 batch_done:104 Gen Loss: 9.31546 Disc Loss: 0.000490924 Q Losses: [0.015663795, 0.14955129]\n",
      "epoch:1 batch_done:105 Gen Loss: 8.06563 Disc Loss: 0.00118611 Q Losses: [0.016479991, 0.14275041]\n",
      "epoch:1 batch_done:106 Gen Loss: 8.8577 Disc Loss: 0.00108301 Q Losses: [0.02084022, 0.14251763]\n",
      "epoch:1 batch_done:107 Gen Loss: 7.29933 Disc Loss: 0.00152017 Q Losses: [0.015874894, 0.15966035]\n",
      "epoch:1 batch_done:108 Gen Loss: 7.2541 Disc Loss: 0.00322727 Q Losses: [0.016242798, 0.14944495]\n",
      "epoch:1 batch_done:109 Gen Loss: 7.87532 Disc Loss: 0.00171148 Q Losses: [0.022556789, 0.15343852]\n",
      "epoch:1 batch_done:110 Gen Loss: 8.6733 Disc Loss: 0.00138343 Q Losses: [0.01224854, 0.15287447]\n",
      "epoch:1 batch_done:111 Gen Loss: 8.34871 Disc Loss: 0.00485983 Q Losses: [0.015005201, 0.1466783]\n",
      "epoch:1 batch_done:112 Gen Loss: 7.48277 Disc Loss: 0.000750069 Q Losses: [0.023972787, 0.14703016]\n",
      "epoch:1 batch_done:113 Gen Loss: 7.81579 Disc Loss: 0.000900838 Q Losses: [0.015241541, 0.1373588]\n",
      "epoch:1 batch_done:114 Gen Loss: 9.14708 Disc Loss: 0.000425182 Q Losses: [0.015202358, 0.18798098]\n",
      "epoch:1 batch_done:115 Gen Loss: 7.05805 Disc Loss: 0.0022464 Q Losses: [0.013625298, 0.16177714]\n",
      "epoch:1 batch_done:116 Gen Loss: 7.7705 Disc Loss: 0.0043828 Q Losses: [0.015112545, 0.14295118]\n",
      "epoch:1 batch_done:117 Gen Loss: 8.61373 Disc Loss: 0.00166259 Q Losses: [0.025881216, 0.13290527]\n",
      "epoch:1 batch_done:118 Gen Loss: 9.40983 Disc Loss: 0.00209507 Q Losses: [0.017878123, 0.15169023]\n",
      "epoch:1 batch_done:119 Gen Loss: 8.32344 Disc Loss: 0.00172792 Q Losses: [0.014867168, 0.13734184]\n",
      "epoch:1 batch_done:120 Gen Loss: 8.88077 Disc Loss: 0.00149113 Q Losses: [0.015163442, 0.12881012]\n",
      "epoch:1 batch_done:121 Gen Loss: 6.89258 Disc Loss: 0.0026558 Q Losses: [0.016777541, 0.13875723]\n",
      "epoch:1 batch_done:122 Gen Loss: 7.42097 Disc Loss: 0.00242918 Q Losses: [0.016572874, 0.15888152]\n",
      "epoch:1 batch_done:123 Gen Loss: 8.22879 Disc Loss: 0.00107098 Q Losses: [0.020040777, 0.13517237]\n",
      "epoch:1 batch_done:124 Gen Loss: 10.758 Disc Loss: 0.00101435 Q Losses: [0.021697816, 0.13978386]\n",
      "epoch:1 batch_done:125 Gen Loss: 7.93991 Disc Loss: 0.00102692 Q Losses: [0.018942237, 0.14877433]\n",
      "epoch:1 batch_done:126 Gen Loss: 7.20142 Disc Loss: 0.00186026 Q Losses: [0.018959867, 0.13335556]\n",
      "epoch:1 batch_done:127 Gen Loss: 7.43971 Disc Loss: 0.00209605 Q Losses: [0.01514858, 0.14102554]\n",
      "epoch:1 batch_done:128 Gen Loss: 7.4342 Disc Loss: 0.00422787 Q Losses: [0.016815238, 0.1441845]\n",
      "epoch:1 batch_done:129 Gen Loss: 10.6492 Disc Loss: 0.00193596 Q Losses: [0.013312967, 0.13449185]\n",
      "epoch:1 batch_done:130 Gen Loss: 9.25292 Disc Loss: 0.00061895 Q Losses: [0.018368367, 0.13796106]\n",
      "epoch:1 batch_done:131 Gen Loss: 10.47 Disc Loss: 0.000260701 Q Losses: [0.012677675, 0.14712587]\n",
      "epoch:1 batch_done:132 Gen Loss: 8.84133 Disc Loss: 0.00774964 Q Losses: [0.013586811, 0.13208736]\n",
      "epoch:1 batch_done:133 Gen Loss: 7.84587 Disc Loss: 0.00635539 Q Losses: [0.016595382, 0.14504337]\n",
      "epoch:1 batch_done:134 Gen Loss: 7.71432 Disc Loss: 0.0036029 Q Losses: [0.019121973, 0.13890395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:135 Gen Loss: 7.96727 Disc Loss: 0.000906117 Q Losses: [0.023441916, 0.13309869]\n",
      "epoch:1 batch_done:136 Gen Loss: 10.9062 Disc Loss: 0.000424412 Q Losses: [0.012207425, 0.16570376]\n",
      "epoch:1 batch_done:137 Gen Loss: 12.255 Disc Loss: 0.00140628 Q Losses: [0.015023682, 0.12041267]\n",
      "epoch:1 batch_done:138 Gen Loss: 8.14157 Disc Loss: 0.00641435 Q Losses: [0.021257028, 0.12885933]\n",
      "epoch:1 batch_done:139 Gen Loss: 10.9433 Disc Loss: 0.00279121 Q Losses: [0.012817391, 0.12873968]\n",
      "epoch:1 batch_done:140 Gen Loss: 13.3846 Disc Loss: 0.00147113 Q Losses: [0.01973368, 0.13627192]\n",
      "epoch:1 batch_done:141 Gen Loss: 10.0532 Disc Loss: 0.000958233 Q Losses: [0.014905505, 0.13417849]\n",
      "epoch:1 batch_done:142 Gen Loss: 9.06075 Disc Loss: 0.000852468 Q Losses: [0.017854815, 0.12085256]\n",
      "epoch:1 batch_done:143 Gen Loss: 6.98107 Disc Loss: 0.00309267 Q Losses: [0.019163763, 0.12073275]\n",
      "epoch:1 batch_done:144 Gen Loss: 7.67384 Disc Loss: 0.00057834 Q Losses: [0.014495621, 0.1504655]\n",
      "epoch:1 batch_done:145 Gen Loss: 14.5416 Disc Loss: 0.0194246 Q Losses: [0.02241677, 0.14279628]\n",
      "epoch:1 batch_done:146 Gen Loss: 9.20955 Disc Loss: 0.0381358 Q Losses: [0.013609915, 0.13528278]\n",
      "epoch:1 batch_done:147 Gen Loss: 5.26599 Disc Loss: 0.000375146 Q Losses: [0.01730844, 0.12113164]\n",
      "epoch:1 batch_done:148 Gen Loss: 5.39748 Disc Loss: 0.0011972 Q Losses: [0.013846596, 0.13197535]\n",
      "epoch:1 batch_done:149 Gen Loss: 86.0368 Disc Loss: 0.510369 Q Losses: [0.025951765, 0.15554187]\n",
      "epoch:1 batch_done:150 Gen Loss: 25.3687 Disc Loss: 55.5287 Q Losses: [0.021389918, 0.14652136]\n",
      "epoch:1 batch_done:151 Gen Loss: 0.000118134 Disc Loss: 3.22527 Q Losses: [0.016970256, 0.17906339]\n",
      "epoch:1 batch_done:152 Gen Loss: 24.8509 Disc Loss: 7.26488 Q Losses: [0.015467068, 0.18575427]\n",
      "epoch:1 batch_done:153 Gen Loss: 33.9182 Disc Loss: 0.0029416 Q Losses: [0.024683814, 0.14763322]\n",
      "epoch:1 batch_done:154 Gen Loss: 28.8672 Disc Loss: 0.0828164 Q Losses: [0.01953765, 0.15753247]\n",
      "epoch:1 batch_done:155 Gen Loss: 18.0983 Disc Loss: 0.205039 Q Losses: [0.022872284, 0.16904756]\n",
      "epoch:1 batch_done:156 Gen Loss: 5.85936 Disc Loss: 0.104414 Q Losses: [0.014843682, 0.16875102]\n",
      "epoch:1 batch_done:157 Gen Loss: 8.8104 Disc Loss: 0.673254 Q Losses: [0.017409878, 0.16527483]\n",
      "epoch:1 batch_done:158 Gen Loss: 9.3215 Disc Loss: 0.0305368 Q Losses: [0.019101707, 0.14074543]\n",
      "epoch:1 batch_done:159 Gen Loss: 38.6039 Disc Loss: 1.84451 Q Losses: [0.022194307, 0.15118992]\n",
      "epoch:1 batch_done:160 Gen Loss: 0.98728 Disc Loss: 12.4019 Q Losses: [0.021434294, 0.26872978]\n",
      "epoch:1 batch_done:161 Gen Loss: 0.00480231 Disc Loss: 0.0129399 Q Losses: [0.022945173, 0.20075545]\n",
      "epoch:1 batch_done:162 Gen Loss: 37.6456 Disc Loss: 4.13575 Q Losses: [0.030311594, 0.13423969]\n",
      "epoch:1 batch_done:163 Gen Loss: 45.7017 Disc Loss: 0.418129 Q Losses: [0.022790164, 0.15298802]\n",
      "epoch:1 batch_done:164 Gen Loss: 36.2809 Disc Loss: 1.05377 Q Losses: [0.011722984, 0.13875063]\n",
      "epoch:1 batch_done:165 Gen Loss: 25.2598 Disc Loss: 0.00907038 Q Losses: [0.022097481, 0.1675068]\n",
      "epoch:1 batch_done:166 Gen Loss: 11.3738 Disc Loss: 0.0513897 Q Losses: [0.018674191, 0.15870728]\n",
      "epoch:1 batch_done:167 Gen Loss: 15.2667 Disc Loss: 0.194885 Q Losses: [0.018090241, 0.13246588]\n",
      "epoch:1 batch_done:168 Gen Loss: 11.6799 Disc Loss: 0.0462969 Q Losses: [0.018363331, 0.13772014]\n",
      "epoch:1 batch_done:169 Gen Loss: 30.036 Disc Loss: 0.547726 Q Losses: [0.024979599, 0.13177603]\n",
      "epoch:1 batch_done:170 Gen Loss: 25.3386 Disc Loss: 1.03729 Q Losses: [0.017045774, 0.13894969]\n",
      "epoch:1 batch_done:171 Gen Loss: 15.0025 Disc Loss: 0.0782546 Q Losses: [0.017193438, 0.15964487]\n",
      "epoch:1 batch_done:172 Gen Loss: 4.18121 Disc Loss: 0.0206535 Q Losses: [0.028723102, 0.18839881]\n",
      "epoch:1 batch_done:173 Gen Loss: 10.4569 Disc Loss: 0.117714 Q Losses: [0.030452745, 0.136195]\n",
      "epoch:1 batch_done:174 Gen Loss: 10.8043 Disc Loss: 0.116722 Q Losses: [0.022452693, 0.13131264]\n",
      "epoch:1 batch_done:175 Gen Loss: 14.337 Disc Loss: 0.186897 Q Losses: [0.01707555, 0.12285958]\n",
      "epoch:1 batch_done:176 Gen Loss: 10.2243 Disc Loss: 0.1116 Q Losses: [0.010379531, 0.12891875]\n",
      "epoch:1 batch_done:177 Gen Loss: 44.2124 Disc Loss: 2.1983 Q Losses: [0.019108612, 0.12602177]\n",
      "epoch:1 batch_done:178 Gen Loss: 41.551 Disc Loss: 6.07776 Q Losses: [0.012956647, 0.15423417]\n",
      "epoch:1 batch_done:179 Gen Loss: 36.4715 Disc Loss: 0.028891 Q Losses: [0.02434616, 0.14314511]\n",
      "epoch:1 batch_done:180 Gen Loss: 28.0437 Disc Loss: 0.00354696 Q Losses: [0.013597815, 0.16118203]\n",
      "epoch:1 batch_done:181 Gen Loss: 17.9207 Disc Loss: 0.000128649 Q Losses: [0.017301455, 0.13008715]\n",
      "epoch:1 batch_done:182 Gen Loss: 9.51624 Disc Loss: 8.30833e-05 Q Losses: [0.013264162, 0.15115643]\n",
      "epoch:1 batch_done:183 Gen Loss: 6.38853 Disc Loss: 0.049231 Q Losses: [0.013048653, 0.15320724]\n",
      "epoch:1 batch_done:184 Gen Loss: 12.7038 Disc Loss: 0.139556 Q Losses: [0.018913064, 0.1381613]\n",
      "epoch:1 batch_done:185 Gen Loss: 11.9804 Disc Loss: 0.00178144 Q Losses: [0.018765436, 0.127602]\n",
      "epoch:1 batch_done:186 Gen Loss: 8.28828 Disc Loss: 0.00414227 Q Losses: [0.026900228, 0.11382276]\n",
      "epoch:1 batch_done:187 Gen Loss: 15.1982 Disc Loss: 0.174788 Q Losses: [0.0099141672, 0.12476156]\n",
      "epoch:1 batch_done:188 Gen Loss: 14.9633 Disc Loss: 0.0984049 Q Losses: [0.010721792, 0.10973547]\n",
      "epoch:1 batch_done:189 Gen Loss: 6.89063 Disc Loss: 0.263458 Q Losses: [0.014831375, 0.1287576]\n",
      "epoch:1 batch_done:190 Gen Loss: 42.8053 Disc Loss: 7.4155 Q Losses: [0.016400166, 0.10979129]\n",
      "epoch:1 batch_done:191 Gen Loss: 47.7668 Disc Loss: 2.23505 Q Losses: [0.030924177, 0.11407147]\n",
      "epoch:1 batch_done:192 Gen Loss: 47.3825 Disc Loss: 0.283195 Q Losses: [0.013724445, 0.11940195]\n",
      "epoch:1 batch_done:193 Gen Loss: 43.6781 Disc Loss: 0.02234 Q Losses: [0.010815336, 0.10767075]\n",
      "epoch:1 batch_done:194 Gen Loss: 36.9884 Disc Loss: 0.00117757 Q Losses: [0.012223485, 0.11287487]\n",
      "epoch:1 batch_done:195 Gen Loss: 29.5724 Disc Loss: 0.000389656 Q Losses: [0.013294927, 0.11083286]\n",
      "epoch:1 batch_done:196 Gen Loss: 21.6581 Disc Loss: 0.000508327 Q Losses: [0.0086815199, 0.13568678]\n",
      "epoch:1 batch_done:197 Gen Loss: 14.3526 Disc Loss: 0.000143086 Q Losses: [0.021252409, 0.13263221]\n",
      "epoch:1 batch_done:198 Gen Loss: 10.7017 Disc Loss: 9.84824e-05 Q Losses: [0.017887756, 0.14295581]\n",
      "epoch:1 batch_done:199 Gen Loss: 6.66815 Disc Loss: 0.00170155 Q Losses: [0.017315838, 0.10894519]\n",
      "epoch:1 batch_done:200 Gen Loss: 7.15608 Disc Loss: 0.069668 Q Losses: [0.013785527, 0.10146259]\n",
      "epoch:1 batch_done:201 Gen Loss: 17.4457 Disc Loss: 0.205175 Q Losses: [0.014494116, 0.09544865]\n",
      "epoch:1 batch_done:202 Gen Loss: 16.8508 Disc Loss: 0.0488998 Q Losses: [0.013604637, 0.10241003]\n",
      "epoch:1 batch_done:203 Gen Loss: 8.81692 Disc Loss: 0.0993644 Q Losses: [0.009328085, 0.096571848]\n",
      "epoch:1 batch_done:204 Gen Loss: 45.9875 Disc Loss: 2.52141 Q Losses: [0.011503374, 0.11310752]\n",
      "epoch:1 batch_done:205 Gen Loss: 38.6762 Disc Loss: 17.3943 Q Losses: [0.017863564, 0.10310201]\n",
      "epoch:1 batch_done:206 Gen Loss: 27.8867 Disc Loss: 1.60607 Q Losses: [0.014133004, 0.11927366]\n",
      "epoch:1 batch_done:207 Gen Loss: 21.6306 Disc Loss: 0.000297688 Q Losses: [0.02712504, 0.13588989]\n",
      "epoch:2 batch_done:1 Gen Loss: 11.444 Disc Loss: 2.43077e-06 Q Losses: [0.020320617, 0.20268503]\n",
      "epoch:2 batch_done:2 Gen Loss: 14.9274 Disc Loss: 0.282319 Q Losses: [0.019775402, 0.12259602]\n",
      "epoch:2 batch_done:3 Gen Loss: 12.2227 Disc Loss: 0.00166069 Q Losses: [0.025443353, 0.12484264]\n",
      "epoch:2 batch_done:4 Gen Loss: 9.54241 Disc Loss: 0.095374 Q Losses: [0.017352466, 0.1128796]\n",
      "epoch:2 batch_done:5 Gen Loss: 9.0933 Disc Loss: 0.061663 Q Losses: [0.017601382, 0.11420865]\n",
      "epoch:2 batch_done:6 Gen Loss: 5.94767 Disc Loss: 0.207371 Q Losses: [0.0095277056, 0.091870993]\n",
      "epoch:2 batch_done:7 Gen Loss: 10.4524 Disc Loss: 0.219799 Q Losses: [0.015786111, 0.099232495]\n",
      "epoch:2 batch_done:8 Gen Loss: 7.60861 Disc Loss: 0.330455 Q Losses: [0.010727266, 0.12154843]\n",
      "epoch:2 batch_done:9 Gen Loss: 4.30941 Disc Loss: 0.104422 Q Losses: [0.010161143, 0.097046666]\n",
      "epoch:2 batch_done:10 Gen Loss: 16.5849 Disc Loss: 0.380187 Q Losses: [0.012832005, 0.087218903]\n",
      "epoch:2 batch_done:11 Gen Loss: 16.5681 Disc Loss: 0.365687 Q Losses: [0.018010836, 0.090412967]\n",
      "epoch:2 batch_done:12 Gen Loss: 12.5901 Disc Loss: 0.124458 Q Losses: [0.017493526, 0.09206903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:13 Gen Loss: 8.60486 Disc Loss: 0.0074381 Q Losses: [0.01316572, 0.086165503]\n",
      "epoch:2 batch_done:14 Gen Loss: 4.93172 Disc Loss: 0.0317124 Q Losses: [0.01351304, 0.083716527]\n",
      "epoch:2 batch_done:15 Gen Loss: 9.23339 Disc Loss: 0.167071 Q Losses: [0.010693228, 0.096619591]\n",
      "epoch:2 batch_done:16 Gen Loss: 9.6638 Disc Loss: 0.0246572 Q Losses: [0.012841637, 0.089172177]\n",
      "epoch:2 batch_done:17 Gen Loss: 7.36732 Disc Loss: 0.112044 Q Losses: [0.0072546634, 0.086877346]\n",
      "epoch:2 batch_done:18 Gen Loss: 4.87552 Disc Loss: 0.0385795 Q Losses: [0.033728205, 0.080052279]\n",
      "epoch:2 batch_done:19 Gen Loss: 8.04026 Disc Loss: 0.221687 Q Losses: [0.010978653, 0.081519544]\n",
      "epoch:2 batch_done:20 Gen Loss: 7.6152 Disc Loss: 0.0594848 Q Losses: [0.016226459, 0.087954544]\n",
      "epoch:2 batch_done:21 Gen Loss: 6.01664 Disc Loss: 0.0320434 Q Losses: [0.014003824, 0.08605893]\n",
      "epoch:2 batch_done:22 Gen Loss: 5.54976 Disc Loss: 0.0564164 Q Losses: [0.014115663, 0.09594024]\n",
      "epoch:2 batch_done:23 Gen Loss: 6.80557 Disc Loss: 0.0764535 Q Losses: [0.010697582, 0.08827775]\n",
      "epoch:2 batch_done:24 Gen Loss: 6.19366 Disc Loss: 0.0603346 Q Losses: [0.013281614, 0.082270667]\n",
      "epoch:2 batch_done:25 Gen Loss: 5.66727 Disc Loss: 0.0794249 Q Losses: [0.018400263, 0.084580779]\n",
      "epoch:2 batch_done:26 Gen Loss: 5.74461 Disc Loss: 0.0822394 Q Losses: [0.0067834714, 0.082856476]\n",
      "epoch:2 batch_done:27 Gen Loss: 5.62701 Disc Loss: 0.0560529 Q Losses: [0.010161472, 0.084565744]\n",
      "epoch:2 batch_done:28 Gen Loss: 5.32474 Disc Loss: 0.0432895 Q Losses: [0.015499438, 0.07395833]\n",
      "epoch:2 batch_done:29 Gen Loss: 4.9491 Disc Loss: 0.0468186 Q Losses: [0.01546733, 0.080637977]\n",
      "epoch:2 batch_done:30 Gen Loss: 5.92387 Disc Loss: 0.0476678 Q Losses: [0.010842574, 0.072937511]\n",
      "epoch:2 batch_done:31 Gen Loss: 16.8146 Disc Loss: 0.267283 Q Losses: [0.0073308679, 0.088804737]\n",
      "epoch:2 batch_done:32 Gen Loss: 8.13222 Disc Loss: 1.40729 Q Losses: [0.010481153, 0.091980085]\n",
      "epoch:2 batch_done:33 Gen Loss: 2.27468 Disc Loss: 0.00262492 Q Losses: [0.010661935, 0.083389528]\n",
      "epoch:2 batch_done:34 Gen Loss: 34.1259 Disc Loss: 1.18824 Q Losses: [0.017907236, 0.083059214]\n",
      "epoch:2 batch_done:35 Gen Loss: 34.1866 Disc Loss: 1.82558 Q Losses: [0.017766137, 0.082467616]\n",
      "epoch:2 batch_done:36 Gen Loss: 31.8358 Disc Loss: 0.613613 Q Losses: [0.012774108, 0.079958938]\n",
      "epoch:2 batch_done:37 Gen Loss: 26.3084 Disc Loss: 0.0108919 Q Losses: [0.0078688106, 0.075135924]\n",
      "epoch:2 batch_done:38 Gen Loss: 20.6385 Disc Loss: 0.00379151 Q Losses: [0.01056232, 0.077172443]\n",
      "epoch:2 batch_done:39 Gen Loss: 14.9454 Disc Loss: 0.000867613 Q Losses: [0.01329464, 0.10239679]\n",
      "epoch:2 batch_done:40 Gen Loss: 14.3486 Disc Loss: 0.0575727 Q Losses: [0.016513549, 0.084544823]\n",
      "epoch:2 batch_done:41 Gen Loss: 12.3484 Disc Loss: 0.00274159 Q Losses: [0.0094848573, 0.082969561]\n",
      "epoch:2 batch_done:42 Gen Loss: 9.91514 Disc Loss: 0.000956416 Q Losses: [0.014940733, 0.07390514]\n",
      "epoch:2 batch_done:43 Gen Loss: 6.88079 Disc Loss: 0.0266604 Q Losses: [0.015492605, 0.078310974]\n",
      "epoch:2 batch_done:44 Gen Loss: 5.34234 Disc Loss: 0.0186513 Q Losses: [0.010100896, 0.069670253]\n",
      "epoch:2 batch_done:45 Gen Loss: 15.3991 Disc Loss: 0.192499 Q Losses: [0.0083569679, 0.075045496]\n",
      "epoch:2 batch_done:46 Gen Loss: 16.8186 Disc Loss: 0.0121833 Q Losses: [0.0077633029, 0.076244548]\n",
      "epoch:2 batch_done:47 Gen Loss: 13.3128 Disc Loss: 0.0956966 Q Losses: [0.010000132, 0.091454178]\n",
      "epoch:2 batch_done:48 Gen Loss: 7.57461 Disc Loss: 0.0198675 Q Losses: [0.0092184423, 0.06986206]\n",
      "epoch:2 batch_done:49 Gen Loss: 15.2055 Disc Loss: 0.211121 Q Losses: [0.033878122, 0.077032857]\n",
      "epoch:2 batch_done:50 Gen Loss: 16.148 Disc Loss: 0.0721856 Q Losses: [0.012041658, 0.078999341]\n",
      "epoch:2 batch_done:51 Gen Loss: 13.0853 Disc Loss: 0.165422 Q Losses: [0.012904929, 0.076769508]\n",
      "epoch:2 batch_done:52 Gen Loss: 8.13954 Disc Loss: 0.118217 Q Losses: [0.010340596, 0.077863559]\n",
      "epoch:2 batch_done:53 Gen Loss: 4.7146 Disc Loss: 0.03849 Q Losses: [0.016542286, 0.072714075]\n",
      "epoch:2 batch_done:54 Gen Loss: 7.12679 Disc Loss: 0.00132385 Q Losses: [0.017792255, 0.080508091]\n",
      "epoch:2 batch_done:55 Gen Loss: 7.8805 Disc Loss: 0.000527106 Q Losses: [0.015176185, 0.090104759]\n",
      "epoch:2 batch_done:56 Gen Loss: 5.4491 Disc Loss: 0.00887095 Q Losses: [0.01356971, 0.10264852]\n",
      "epoch:2 batch_done:57 Gen Loss: 7.96855 Disc Loss: 0.00076631 Q Losses: [0.0093552694, 0.076937661]\n",
      "epoch:2 batch_done:58 Gen Loss: 44.3658 Disc Loss: 2.27756 Q Losses: [0.010644425, 0.089435004]\n",
      "epoch:2 batch_done:59 Gen Loss: 35.5565 Disc Loss: 13.5683 Q Losses: [0.013742854, 0.081280537]\n",
      "epoch:2 batch_done:60 Gen Loss: 25.4992 Disc Loss: 0.964608 Q Losses: [0.0148785, 0.079942852]\n",
      "epoch:2 batch_done:61 Gen Loss: 19.4292 Disc Loss: 0.00474188 Q Losses: [0.01152346, 0.089752227]\n",
      "epoch:2 batch_done:62 Gen Loss: 13.9964 Disc Loss: 0.00076697 Q Losses: [0.010426894, 0.083153464]\n",
      "epoch:2 batch_done:63 Gen Loss: 8.7001 Disc Loss: 0.000122442 Q Losses: [0.014054713, 0.073880285]\n",
      "epoch:2 batch_done:64 Gen Loss: 5.21467 Disc Loss: 0.00563699 Q Losses: [0.01200831, 0.085900165]\n",
      "epoch:2 batch_done:65 Gen Loss: 9.81121 Disc Loss: 0.0762308 Q Losses: [0.011710268, 0.082804725]\n",
      "epoch:2 batch_done:66 Gen Loss: 9.34183 Disc Loss: 0.0028926 Q Losses: [0.0094998376, 0.080269605]\n",
      "epoch:2 batch_done:67 Gen Loss: 7.14996 Disc Loss: 0.00561215 Q Losses: [0.011687295, 0.077040479]\n",
      "epoch:2 batch_done:68 Gen Loss: 7.7146 Disc Loss: 0.0487155 Q Losses: [0.0073038968, 0.070517868]\n",
      "epoch:2 batch_done:69 Gen Loss: 7.36907 Disc Loss: 0.0136464 Q Losses: [0.010757219, 0.067698538]\n",
      "epoch:2 batch_done:70 Gen Loss: 6.81089 Disc Loss: 0.00969098 Q Losses: [0.0077605364, 0.076359659]\n",
      "epoch:2 batch_done:71 Gen Loss: 8.10059 Disc Loss: 0.0809733 Q Losses: [0.013566889, 0.071585983]\n",
      "epoch:2 batch_done:72 Gen Loss: 11.2857 Disc Loss: 0.000449477 Q Losses: [0.0092014829, 0.081415139]\n",
      "epoch:2 batch_done:73 Gen Loss: 8.1718 Disc Loss: 0.00258005 Q Losses: [0.0094524361, 0.076171502]\n",
      "epoch:2 batch_done:74 Gen Loss: 5.84709 Disc Loss: 0.0126566 Q Losses: [0.0086162761, 0.068865627]\n",
      "epoch:2 batch_done:75 Gen Loss: 8.33939 Disc Loss: 0.102471 Q Losses: [0.0086173071, 0.065561533]\n",
      "epoch:2 batch_done:76 Gen Loss: 7.62565 Disc Loss: 0.11441 Q Losses: [0.013552718, 0.072426878]\n",
      "epoch:2 batch_done:77 Gen Loss: 11.2007 Disc Loss: 0.080312 Q Losses: [0.010074498, 0.065483145]\n",
      "epoch:2 batch_done:78 Gen Loss: 5.3138 Disc Loss: 0.0128245 Q Losses: [0.0086524943, 0.074882314]\n",
      "epoch:2 batch_done:79 Gen Loss: 6.32538 Disc Loss: 0.128067 Q Losses: [0.012297843, 0.079557471]\n",
      "epoch:2 batch_done:80 Gen Loss: 7.87776 Disc Loss: 0.00543806 Q Losses: [0.010838159, 0.067742281]\n",
      "epoch:2 batch_done:81 Gen Loss: 6.10021 Disc Loss: 0.00979415 Q Losses: [0.007768881, 0.070569433]\n",
      "epoch:2 batch_done:82 Gen Loss: 6.224 Disc Loss: 0.0497927 Q Losses: [0.019592758, 0.07294701]\n",
      "epoch:2 batch_done:83 Gen Loss: 6.95657 Disc Loss: 0.0458741 Q Losses: [0.015219931, 0.10990462]\n",
      "epoch:2 batch_done:84 Gen Loss: 7.58545 Disc Loss: 0.0557815 Q Losses: [0.013053717, 0.071965344]\n",
      "epoch:2 batch_done:85 Gen Loss: 15.245 Disc Loss: 0.0505721 Q Losses: [0.0088800546, 0.068517476]\n",
      "epoch:2 batch_done:86 Gen Loss: 15.9746 Disc Loss: 0.0295846 Q Losses: [0.011346506, 0.078194842]\n",
      "epoch:2 batch_done:87 Gen Loss: 12.5366 Disc Loss: 0.0547124 Q Losses: [0.0088457363, 0.087449089]\n",
      "epoch:2 batch_done:88 Gen Loss: 9.24272 Disc Loss: 0.0120743 Q Losses: [0.0061382353, 0.078805439]\n",
      "epoch:2 batch_done:89 Gen Loss: 5.99977 Disc Loss: 0.00571281 Q Losses: [0.013899732, 0.063745305]\n",
      "epoch:2 batch_done:90 Gen Loss: 5.125 Disc Loss: 0.0159323 Q Losses: [0.015034395, 0.06397973]\n",
      "epoch:2 batch_done:91 Gen Loss: 7.44741 Disc Loss: 0.00629923 Q Losses: [0.0085967211, 0.068591468]\n",
      "epoch:2 batch_done:92 Gen Loss: 12.802 Disc Loss: 0.00277289 Q Losses: [0.021279246, 0.072609439]\n",
      "epoch:2 batch_done:93 Gen Loss: 6.39708 Disc Loss: 0.0441181 Q Losses: [0.012886345, 0.061307345]\n",
      "epoch:2 batch_done:94 Gen Loss: 6.79443 Disc Loss: 0.0114511 Q Losses: [0.010428995, 0.056290545]\n",
      "epoch:2 batch_done:95 Gen Loss: 12.6714 Disc Loss: 0.0160519 Q Losses: [0.0075420663, 0.063908637]\n",
      "epoch:2 batch_done:96 Gen Loss: 5.44801 Disc Loss: 0.0194349 Q Losses: [0.0072723813, 0.066608161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:97 Gen Loss: 7.08027 Disc Loss: 0.00416692 Q Losses: [0.012223825, 0.068501145]\n",
      "epoch:2 batch_done:98 Gen Loss: 7.65591 Disc Loss: 0.00434768 Q Losses: [0.012822092, 0.05663804]\n",
      "epoch:2 batch_done:99 Gen Loss: 5.22762 Disc Loss: 0.0199505 Q Losses: [0.011622287, 0.055559151]\n",
      "epoch:2 batch_done:100 Gen Loss: 7.28497 Disc Loss: 0.071459 Q Losses: [0.013862732, 0.06936571]\n",
      "epoch:2 batch_done:101 Gen Loss: 11.8704 Disc Loss: 0.0260872 Q Losses: [0.007099797, 0.064174458]\n",
      "epoch:2 batch_done:102 Gen Loss: 14.6401 Disc Loss: 0.00336317 Q Losses: [0.0098189823, 0.055347603]\n",
      "epoch:2 batch_done:103 Gen Loss: 7.91396 Disc Loss: 0.0126488 Q Losses: [0.012737341, 0.056186348]\n",
      "epoch:2 batch_done:104 Gen Loss: 7.15292 Disc Loss: 0.00786444 Q Losses: [0.010071231, 0.071318924]\n",
      "epoch:2 batch_done:105 Gen Loss: 13.0264 Disc Loss: 0.00258137 Q Losses: [0.0077961083, 0.060724609]\n",
      "epoch:2 batch_done:106 Gen Loss: 10.8657 Disc Loss: 0.00396019 Q Losses: [0.010128113, 0.059001073]\n",
      "epoch:2 batch_done:107 Gen Loss: 6.09322 Disc Loss: 0.00419217 Q Losses: [0.0098861158, 0.059790999]\n",
      "epoch:2 batch_done:108 Gen Loss: 5.17841 Disc Loss: 0.0176197 Q Losses: [0.008494284, 0.062464561]\n",
      "epoch:2 batch_done:109 Gen Loss: 10.1943 Disc Loss: 0.000795233 Q Losses: [0.0096200472, 0.059739731]\n",
      "epoch:2 batch_done:110 Gen Loss: 6.51171 Disc Loss: 0.0824679 Q Losses: [0.009158236, 0.062809162]\n",
      "epoch:2 batch_done:111 Gen Loss: 12.0509 Disc Loss: 0.006603 Q Losses: [0.0096888971, 0.058441442]\n",
      "epoch:2 batch_done:112 Gen Loss: 11.2317 Disc Loss: 0.00907869 Q Losses: [0.0066426955, 0.071810707]\n",
      "epoch:2 batch_done:113 Gen Loss: 13.7092 Disc Loss: 0.00587941 Q Losses: [0.01160921, 0.05977536]\n",
      "epoch:2 batch_done:114 Gen Loss: 9.84941 Disc Loss: 0.00449516 Q Losses: [0.010504512, 0.069074102]\n",
      "epoch:2 batch_done:115 Gen Loss: 8.97795 Disc Loss: 0.00118599 Q Losses: [0.009481756, 0.059401061]\n",
      "epoch:2 batch_done:116 Gen Loss: 11.9793 Disc Loss: 0.00878549 Q Losses: [0.0095205512, 0.06523598]\n",
      "epoch:2 batch_done:117 Gen Loss: 5.11458 Disc Loss: 0.0130748 Q Losses: [0.012841221, 0.055117786]\n",
      "epoch:2 batch_done:118 Gen Loss: 7.40361 Disc Loss: 0.01977 Q Losses: [0.010925935, 0.077643789]\n",
      "epoch:2 batch_done:119 Gen Loss: 16.023 Disc Loss: 0.000622775 Q Losses: [0.0089856964, 0.055217039]\n",
      "epoch:2 batch_done:120 Gen Loss: 11.4956 Disc Loss: 0.0031841 Q Losses: [0.011394796, 0.050355177]\n",
      "epoch:2 batch_done:121 Gen Loss: 5.64899 Disc Loss: 0.00506586 Q Losses: [0.0091398396, 0.060866173]\n",
      "epoch:2 batch_done:122 Gen Loss: 7.11353 Disc Loss: 0.00332683 Q Losses: [0.010391353, 0.062490754]\n",
      "epoch:2 batch_done:123 Gen Loss: 15.2132 Disc Loss: 0.000493266 Q Losses: [0.010367716, 0.050251395]\n",
      "epoch:2 batch_done:124 Gen Loss: 8.54208 Disc Loss: 0.0012967 Q Losses: [0.010184191, 0.066262439]\n",
      "epoch:2 batch_done:125 Gen Loss: 6.03214 Disc Loss: 0.00369225 Q Losses: [0.012289186, 0.056503884]\n",
      "epoch:2 batch_done:126 Gen Loss: 7.44661 Disc Loss: 0.00442158 Q Losses: [0.024165366, 0.05361177]\n",
      "epoch:2 batch_done:127 Gen Loss: 8.17323 Disc Loss: 0.0799433 Q Losses: [0.0099960584, 0.058519911]\n",
      "epoch:2 batch_done:128 Gen Loss: 11.1939 Disc Loss: 0.00917556 Q Losses: [0.0096587539, 0.056375518]\n",
      "epoch:2 batch_done:129 Gen Loss: 12.7571 Disc Loss: 0.0139906 Q Losses: [0.008749567, 0.05515166]\n",
      "epoch:2 batch_done:130 Gen Loss: 10.9615 Disc Loss: 0.0154907 Q Losses: [0.0096180951, 0.05672013]\n",
      "epoch:2 batch_done:131 Gen Loss: 11.5087 Disc Loss: 0.0126847 Q Losses: [0.016159628, 0.056997925]\n",
      "epoch:2 batch_done:132 Gen Loss: 9.93174 Disc Loss: 0.00993311 Q Losses: [0.013003392, 0.050762076]\n",
      "epoch:2 batch_done:133 Gen Loss: 12.7685 Disc Loss: 0.0033731 Q Losses: [0.0047732084, 0.055763613]\n",
      "epoch:2 batch_done:134 Gen Loss: 10.6391 Disc Loss: 0.00204753 Q Losses: [0.011551352, 0.060128666]\n",
      "epoch:2 batch_done:135 Gen Loss: 11.3293 Disc Loss: 0.00113104 Q Losses: [0.018860344, 0.060428075]\n",
      "epoch:2 batch_done:136 Gen Loss: 11.1238 Disc Loss: 0.00140924 Q Losses: [0.013863411, 0.057023838]\n",
      "epoch:2 batch_done:137 Gen Loss: 5.39727 Disc Loss: 0.0103748 Q Losses: [0.012407513, 0.051431298]\n",
      "epoch:2 batch_done:138 Gen Loss: 6.02174 Disc Loss: 0.0367364 Q Losses: [0.0063680643, 0.053947277]\n",
      "epoch:2 batch_done:139 Gen Loss: 8.15677 Disc Loss: 0.0149748 Q Losses: [0.010316554, 0.047924984]\n",
      "epoch:2 batch_done:140 Gen Loss: 17.8685 Disc Loss: 0.00231421 Q Losses: [0.014978977, 0.051201284]\n",
      "epoch:2 batch_done:141 Gen Loss: 10.5995 Disc Loss: 0.000716668 Q Losses: [0.010860581, 0.054306317]\n",
      "epoch:2 batch_done:142 Gen Loss: 17.0991 Disc Loss: 0.00132832 Q Losses: [0.0052955514, 0.063142955]\n",
      "epoch:2 batch_done:143 Gen Loss: 11.393 Disc Loss: 0.00241416 Q Losses: [0.019050833, 0.051202562]\n",
      "epoch:2 batch_done:144 Gen Loss: 11.2982 Disc Loss: 0.000964958 Q Losses: [0.01193024, 0.051622979]\n",
      "epoch:2 batch_done:145 Gen Loss: 14.4086 Disc Loss: 0.00214938 Q Losses: [0.01471795, 0.049793057]\n",
      "epoch:2 batch_done:146 Gen Loss: 7.9445 Disc Loss: 0.00150835 Q Losses: [0.0097904298, 0.053574044]\n",
      "epoch:2 batch_done:147 Gen Loss: 8.77862 Disc Loss: 0.00140731 Q Losses: [0.010864195, 0.051840574]\n",
      "epoch:2 batch_done:148 Gen Loss: 10.8064 Disc Loss: 0.00168014 Q Losses: [0.0073703928, 0.053359263]\n",
      "epoch:2 batch_done:149 Gen Loss: 9.16902 Disc Loss: 0.00196245 Q Losses: [0.019267794, 0.050630793]\n",
      "epoch:2 batch_done:150 Gen Loss: 9.74793 Disc Loss: 0.00163776 Q Losses: [0.033185534, 0.048204318]\n",
      "epoch:2 batch_done:151 Gen Loss: 5.23547 Disc Loss: 0.0121119 Q Losses: [0.0055257874, 0.060814478]\n",
      "epoch:2 batch_done:152 Gen Loss: 5.89632 Disc Loss: 0.0419713 Q Losses: [0.013764251, 0.054823898]\n",
      "epoch:2 batch_done:153 Gen Loss: 8.41555 Disc Loss: 0.00657588 Q Losses: [0.0081744548, 0.067572258]\n",
      "epoch:2 batch_done:154 Gen Loss: 19.157 Disc Loss: 0.0068439 Q Losses: [0.024055675, 0.055245209]\n",
      "epoch:2 batch_done:155 Gen Loss: 13.1988 Disc Loss: 0.00256126 Q Losses: [0.0063352096, 0.052944466]\n",
      "epoch:2 batch_done:156 Gen Loss: 10.7383 Disc Loss: 0.0102303 Q Losses: [0.015158039, 0.054808155]\n",
      "epoch:2 batch_done:157 Gen Loss: 8.20898 Disc Loss: 0.00164651 Q Losses: [0.01184914, 0.04810743]\n",
      "epoch:2 batch_done:158 Gen Loss: 7.82985 Disc Loss: 0.00069943 Q Losses: [0.0085499212, 0.05208065]\n",
      "epoch:2 batch_done:159 Gen Loss: 8.77855 Disc Loss: 0.00363894 Q Losses: [0.010363182, 0.057739586]\n",
      "epoch:2 batch_done:160 Gen Loss: 14.5987 Disc Loss: 0.00136258 Q Losses: [0.0062640202, 0.050626598]\n",
      "epoch:2 batch_done:161 Gen Loss: 6.25532 Disc Loss: 0.00286014 Q Losses: [0.01274279, 0.051978067]\n",
      "epoch:2 batch_done:162 Gen Loss: 12.8683 Disc Loss: 0.00144034 Q Losses: [0.012716068, 0.047570858]\n",
      "epoch:2 batch_done:163 Gen Loss: 11.4049 Disc Loss: 0.00101858 Q Losses: [0.017026585, 0.055277359]\n",
      "epoch:2 batch_done:164 Gen Loss: 5.25395 Disc Loss: 0.0164123 Q Losses: [0.0094935484, 0.043202423]\n",
      "epoch:2 batch_done:165 Gen Loss: 5.75378 Disc Loss: 0.0128833 Q Losses: [0.0069031678, 0.045028333]\n",
      "epoch:2 batch_done:166 Gen Loss: 12.0699 Disc Loss: 0.00445876 Q Losses: [0.0062491354, 0.049773563]\n",
      "epoch:2 batch_done:167 Gen Loss: 9.65942 Disc Loss: 0.00225929 Q Losses: [0.01602233, 0.056190155]\n",
      "epoch:2 batch_done:168 Gen Loss: 13.6686 Disc Loss: 0.00547447 Q Losses: [0.0087268362, 0.052589312]\n",
      "epoch:2 batch_done:169 Gen Loss: 5.49588 Disc Loss: 0.00826197 Q Losses: [0.0092468746, 0.046404317]\n",
      "epoch:2 batch_done:170 Gen Loss: 6.90938 Disc Loss: 0.00213611 Q Losses: [0.0064368527, 0.056240547]\n",
      "epoch:2 batch_done:171 Gen Loss: 17.3491 Disc Loss: 0.00254956 Q Losses: [0.014220199, 0.047961615]\n",
      "epoch:2 batch_done:172 Gen Loss: 6.89591 Disc Loss: 0.00329646 Q Losses: [0.0087066572, 0.047399856]\n",
      "epoch:2 batch_done:173 Gen Loss: 11.5667 Disc Loss: 0.0011071 Q Losses: [0.015133461, 0.049530137]\n",
      "epoch:2 batch_done:174 Gen Loss: 12.5292 Disc Loss: 0.00359054 Q Losses: [0.0097566685, 0.04614158]\n",
      "epoch:2 batch_done:175 Gen Loss: 5.28105 Disc Loss: 0.0173619 Q Losses: [0.0099194534, 0.053345121]\n",
      "epoch:2 batch_done:176 Gen Loss: 9.6589 Disc Loss: 0.0203283 Q Losses: [0.011109478, 0.041530922]\n",
      "epoch:2 batch_done:177 Gen Loss: 15.1548 Disc Loss: 0.00144794 Q Losses: [0.0057762587, 0.048001736]\n",
      "epoch:2 batch_done:178 Gen Loss: 6.37552 Disc Loss: 0.0029123 Q Losses: [0.0074597942, 0.043054156]\n",
      "epoch:2 batch_done:179 Gen Loss: 13.9692 Disc Loss: 0.00094213 Q Losses: [0.0068448712, 0.052522644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:180 Gen Loss: 9.46952 Disc Loss: 0.00217818 Q Losses: [0.0095276088, 0.058232635]\n",
      "epoch:2 batch_done:181 Gen Loss: 5.33505 Disc Loss: 0.0101676 Q Losses: [0.0096301651, 0.053298008]\n",
      "epoch:2 batch_done:182 Gen Loss: 5.90035 Disc Loss: 0.00660969 Q Losses: [0.019513806, 0.05301138]\n",
      "epoch:2 batch_done:183 Gen Loss: 9.94859 Disc Loss: 0.00126959 Q Losses: [0.0087311929, 0.046147805]\n",
      "epoch:2 batch_done:184 Gen Loss: 8.18049 Disc Loss: 0.00449115 Q Losses: [0.012305673, 0.042926781]\n",
      "epoch:2 batch_done:185 Gen Loss: 14.11 Disc Loss: 0.000382646 Q Losses: [0.014305593, 0.047865421]\n",
      "epoch:2 batch_done:186 Gen Loss: 7.58282 Disc Loss: 0.00334928 Q Losses: [0.0078407032, 0.048127577]\n",
      "epoch:2 batch_done:187 Gen Loss: 17.5573 Disc Loss: 0.000946626 Q Losses: [0.013474505, 0.050279163]\n",
      "epoch:2 batch_done:188 Gen Loss: 5.79136 Disc Loss: 0.00468101 Q Losses: [0.0079051452, 0.044875521]\n",
      "epoch:2 batch_done:189 Gen Loss: 7.75944 Disc Loss: 0.00179204 Q Losses: [0.010904184, 0.062020987]\n",
      "epoch:2 batch_done:190 Gen Loss: 15.6357 Disc Loss: 0.00115806 Q Losses: [0.0064878813, 0.048671901]\n",
      "epoch:2 batch_done:191 Gen Loss: 6.35279 Disc Loss: 0.00508143 Q Losses: [0.00831703, 0.046938069]\n",
      "epoch:2 batch_done:192 Gen Loss: 9.57858 Disc Loss: 0.000841406 Q Losses: [0.01499192, 0.05163791]\n",
      "epoch:2 batch_done:193 Gen Loss: 12.4867 Disc Loss: 0.000568128 Q Losses: [0.014853759, 0.046478543]\n",
      "epoch:2 batch_done:194 Gen Loss: 5.40641 Disc Loss: 0.0153649 Q Losses: [0.0068976553, 0.050306894]\n",
      "epoch:2 batch_done:195 Gen Loss: 12.4474 Disc Loss: 0.000630932 Q Losses: [0.0056695882, 0.058811277]\n",
      "epoch:2 batch_done:196 Gen Loss: 11.4315 Disc Loss: 0.000846382 Q Losses: [0.013097298, 0.04886096]\n",
      "epoch:2 batch_done:197 Gen Loss: 5.37695 Disc Loss: 0.0141666 Q Losses: [0.011772871, 0.046306312]\n",
      "epoch:2 batch_done:198 Gen Loss: 5.98438 Disc Loss: 0.00822002 Q Losses: [0.0073561575, 0.051449731]\n",
      "epoch:2 batch_done:199 Gen Loss: 9.95785 Disc Loss: 0.00745814 Q Losses: [0.0060195504, 0.05506222]\n",
      "epoch:2 batch_done:200 Gen Loss: 13.4606 Disc Loss: 0.00381154 Q Losses: [0.0081402408, 0.043905973]\n",
      "epoch:2 batch_done:201 Gen Loss: 11.5079 Disc Loss: 0.000334313 Q Losses: [0.0062316018, 0.049675621]\n",
      "epoch:2 batch_done:202 Gen Loss: 6.31655 Disc Loss: 0.00344982 Q Losses: [0.014087715, 0.053847089]\n",
      "epoch:2 batch_done:203 Gen Loss: 6.26019 Disc Loss: 0.00369859 Q Losses: [0.011326159, 0.045961834]\n",
      "epoch:2 batch_done:204 Gen Loss: 9.18232 Disc Loss: 0.00168326 Q Losses: [0.018843228, 0.049529232]\n",
      "epoch:2 batch_done:205 Gen Loss: 17.012 Disc Loss: 0.0010621 Q Losses: [0.010460215, 0.045148652]\n",
      "epoch:2 batch_done:206 Gen Loss: 6.27244 Disc Loss: 0.00450077 Q Losses: [0.014634048, 0.047741383]\n",
      "epoch:2 batch_done:207 Gen Loss: 9.77168 Disc Loss: 0.000326897 Q Losses: [0.0075115305, 0.054723684]\n",
      "epoch:3 batch_done:1 Gen Loss: 10.9463 Disc Loss: 0.00165794 Q Losses: [0.0099914446, 0.051384896]\n",
      "epoch:3 batch_done:2 Gen Loss: 5.53933 Disc Loss: 0.0175586 Q Losses: [0.0088994326, 0.050234288]\n",
      "epoch:3 batch_done:3 Gen Loss: 6.29436 Disc Loss: 0.00650256 Q Losses: [0.012749432, 0.047639962]\n",
      "epoch:3 batch_done:4 Gen Loss: 10.3694 Disc Loss: 0.00150274 Q Losses: [0.0077971672, 0.045990553]\n",
      "epoch:3 batch_done:5 Gen Loss: 12.8765 Disc Loss: 0.0004801 Q Losses: [0.0098896725, 0.045488738]\n",
      "epoch:3 batch_done:6 Gen Loss: 8.17204 Disc Loss: 0.000527782 Q Losses: [0.0081341956, 0.050861396]\n",
      "epoch:3 batch_done:7 Gen Loss: 6.42159 Disc Loss: 0.0027695 Q Losses: [0.0075482139, 0.0485733]\n",
      "epoch:3 batch_done:8 Gen Loss: 5.68804 Disc Loss: 0.00736175 Q Losses: [0.0089765852, 0.044554077]\n",
      "epoch:3 batch_done:9 Gen Loss: 8.80257 Disc Loss: 0.000632821 Q Losses: [0.0074581425, 0.044431083]\n",
      "epoch:3 batch_done:10 Gen Loss: 12.4037 Disc Loss: 0.000786776 Q Losses: [0.0078939181, 0.042207357]\n",
      "epoch:3 batch_done:11 Gen Loss: 10.5234 Disc Loss: 0.000638538 Q Losses: [0.010735678, 0.049424566]\n",
      "epoch:3 batch_done:12 Gen Loss: 7.02648 Disc Loss: 0.00278581 Q Losses: [0.0077426541, 0.048232991]\n",
      "epoch:3 batch_done:13 Gen Loss: 6.68667 Disc Loss: 0.00175605 Q Losses: [0.006509413, 0.046931908]\n",
      "epoch:3 batch_done:14 Gen Loss: 9.0327 Disc Loss: 0.00186709 Q Losses: [0.010334676, 0.050914541]\n",
      "epoch:3 batch_done:15 Gen Loss: 5.57549 Disc Loss: 0.00803962 Q Losses: [0.0085256984, 0.04146561]\n",
      "epoch:3 batch_done:16 Gen Loss: 5.94449 Disc Loss: 0.00724362 Q Losses: [0.016344957, 0.040383257]\n",
      "epoch:3 batch_done:17 Gen Loss: 6.97169 Disc Loss: 0.00254887 Q Losses: [0.012554443, 0.045615502]\n",
      "epoch:3 batch_done:18 Gen Loss: 10.3599 Disc Loss: 0.000881957 Q Losses: [0.015318714, 0.045188807]\n",
      "epoch:3 batch_done:19 Gen Loss: 5.52759 Disc Loss: 0.0177533 Q Losses: [0.0084563112, 0.044971351]\n",
      "epoch:3 batch_done:20 Gen Loss: 6.25718 Disc Loss: 0.00582317 Q Losses: [0.017546928, 0.050643843]\n",
      "epoch:3 batch_done:21 Gen Loss: 10.8018 Disc Loss: 0.00105321 Q Losses: [0.008070942, 0.045469012]\n",
      "epoch:3 batch_done:22 Gen Loss: 12.6953 Disc Loss: 0.00143185 Q Losses: [0.014912035, 0.047063805]\n",
      "epoch:3 batch_done:23 Gen Loss: 5.54212 Disc Loss: 0.0121038 Q Losses: [0.0080266586, 0.044493474]\n",
      "epoch:3 batch_done:24 Gen Loss: 6.05991 Disc Loss: 0.00820227 Q Losses: [0.022612059, 0.043468963]\n",
      "epoch:3 batch_done:25 Gen Loss: 8.68209 Disc Loss: 0.00397858 Q Losses: [0.013582162, 0.043998845]\n",
      "epoch:3 batch_done:26 Gen Loss: 12.6739 Disc Loss: 0.00918458 Q Losses: [0.0097712297, 0.048246603]\n",
      "epoch:3 batch_done:27 Gen Loss: 8.1485 Disc Loss: 0.00102151 Q Losses: [0.0074660247, 0.044840589]\n",
      "epoch:3 batch_done:28 Gen Loss: 5.54415 Disc Loss: 0.00834557 Q Losses: [0.011391273, 0.044493511]\n",
      "epoch:3 batch_done:29 Gen Loss: 7.01128 Disc Loss: 0.00216835 Q Losses: [0.014467806, 0.057812244]\n",
      "epoch:3 batch_done:30 Gen Loss: 7.47521 Disc Loss: 0.0064913 Q Losses: [0.011734063, 0.040446363]\n",
      "epoch:3 batch_done:31 Gen Loss: 7.65172 Disc Loss: 0.00151751 Q Losses: [0.011859707, 0.049886074]\n",
      "epoch:3 batch_done:32 Gen Loss: 8.09111 Disc Loss: 0.000669791 Q Losses: [0.0069275489, 0.041898292]\n",
      "epoch:3 batch_done:33 Gen Loss: 5.54787 Disc Loss: 0.00857596 Q Losses: [0.010829102, 0.041154359]\n",
      "epoch:3 batch_done:34 Gen Loss: 7.27876 Disc Loss: 0.00139992 Q Losses: [0.010372667, 0.038936198]\n",
      "epoch:3 batch_done:35 Gen Loss: 9.68636 Disc Loss: 0.000271442 Q Losses: [0.0081443703, 0.040335555]\n",
      "epoch:3 batch_done:36 Gen Loss: 8.99856 Disc Loss: 0.000492764 Q Losses: [0.0084846392, 0.041602358]\n",
      "epoch:3 batch_done:37 Gen Loss: 5.56466 Disc Loss: 0.00873652 Q Losses: [0.014402329, 0.042364091]\n",
      "epoch:3 batch_done:38 Gen Loss: 5.95248 Disc Loss: 0.00702747 Q Losses: [0.01009223, 0.048755474]\n",
      "epoch:3 batch_done:39 Gen Loss: 11.7542 Disc Loss: 0.000967384 Q Losses: [0.0098007359, 0.039278533]\n",
      "epoch:3 batch_done:40 Gen Loss: 9.82268 Disc Loss: 0.000544255 Q Losses: [0.0081649628, 0.049830452]\n",
      "epoch:3 batch_done:41 Gen Loss: 7.84 Disc Loss: 0.013958 Q Losses: [0.0063521303, 0.058156271]\n",
      "epoch:3 batch_done:42 Gen Loss: 7.31112 Disc Loss: 0.0010309 Q Losses: [0.01439166, 0.0460646]\n",
      "epoch:3 batch_done:43 Gen Loss: 6.22642 Disc Loss: 0.00258517 Q Losses: [0.020228636, 0.042622879]\n",
      "epoch:3 batch_done:44 Gen Loss: 7.34147 Disc Loss: 0.00141806 Q Losses: [0.013282681, 0.041879591]\n",
      "epoch:3 batch_done:45 Gen Loss: 9.39131 Disc Loss: 0.000396097 Q Losses: [0.015892455, 0.04169675]\n",
      "epoch:3 batch_done:46 Gen Loss: 5.51189 Disc Loss: 0.0111839 Q Losses: [0.0071650068, 0.050174691]\n",
      "epoch:3 batch_done:47 Gen Loss: 6.03919 Disc Loss: 0.00701179 Q Losses: [0.0080491286, 0.039623119]\n",
      "epoch:3 batch_done:48 Gen Loss: 5.9945 Disc Loss: 0.0154323 Q Losses: [0.0092010889, 0.041968003]\n",
      "epoch:3 batch_done:49 Gen Loss: 7.93326 Disc Loss: 0.00150588 Q Losses: [0.0098556401, 0.036990419]\n",
      "epoch:3 batch_done:50 Gen Loss: 9.46316 Disc Loss: 0.00232085 Q Losses: [0.0099605136, 0.044506948]\n",
      "epoch:3 batch_done:51 Gen Loss: 12.0267 Disc Loss: 0.00275312 Q Losses: [0.012463508, 0.039116904]\n",
      "epoch:3 batch_done:52 Gen Loss: 5.96229 Disc Loss: 0.00395937 Q Losses: [0.011825914, 0.046068627]\n",
      "epoch:3 batch_done:53 Gen Loss: 9.11104 Disc Loss: 0.0032888 Q Losses: [0.0097935162, 0.049004667]\n",
      "epoch:3 batch_done:54 Gen Loss: 9.4001 Disc Loss: 0.00170395 Q Losses: [0.01418341, 0.047312643]\n",
      "epoch:3 batch_done:55 Gen Loss: 7.47632 Disc Loss: 0.00159489 Q Losses: [0.0083039738, 0.043805681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 batch_done:56 Gen Loss: 5.89779 Disc Loss: 0.0042687 Q Losses: [0.0095276786, 0.046828084]\n",
      "epoch:3 batch_done:57 Gen Loss: 6.74868 Disc Loss: 0.00212457 Q Losses: [0.0079741385, 0.0384795]\n",
      "epoch:3 batch_done:58 Gen Loss: 7.39022 Disc Loss: 0.00120434 Q Losses: [0.012891172, 0.043470878]\n",
      "epoch:3 batch_done:59 Gen Loss: 10.2862 Disc Loss: 0.003689 Q Losses: [0.02251495, 0.038392186]\n",
      "epoch:3 batch_done:60 Gen Loss: 7.64801 Disc Loss: 0.000965292 Q Losses: [0.010890955, 0.047256857]\n",
      "epoch:3 batch_done:61 Gen Loss: 7.83863 Disc Loss: 0.000848182 Q Losses: [0.0099692959, 0.042024758]\n",
      "epoch:3 batch_done:62 Gen Loss: 5.48559 Disc Loss: 0.0102395 Q Losses: [0.00948954, 0.039328411]\n",
      "epoch:3 batch_done:63 Gen Loss: 7.36191 Disc Loss: 0.00146352 Q Losses: [0.009129867, 0.039083973]\n",
      "epoch:3 batch_done:64 Gen Loss: 9.03703 Disc Loss: 0.00052599 Q Losses: [0.00881893, 0.039017215]\n",
      "epoch:3 batch_done:65 Gen Loss: 7.37864 Disc Loss: 0.00100797 Q Losses: [0.010787891, 0.04151566]\n",
      "epoch:3 batch_done:66 Gen Loss: 6.35388 Disc Loss: 0.00729394 Q Losses: [0.0088479957, 0.042655058]\n",
      "epoch:3 batch_done:67 Gen Loss: 7.86108 Disc Loss: 0.00482015 Q Losses: [0.0063663172, 0.038417429]\n",
      "epoch:3 batch_done:68 Gen Loss: 7.85953 Disc Loss: 0.00122642 Q Losses: [0.0084932167, 0.040347047]\n",
      "epoch:3 batch_done:69 Gen Loss: 7.16497 Disc Loss: 0.0029618 Q Losses: [0.0088906251, 0.041479811]\n",
      "epoch:3 batch_done:70 Gen Loss: 6.68202 Disc Loss: 0.0366368 Q Losses: [0.0070157945, 0.036337856]\n",
      "epoch:3 batch_done:71 Gen Loss: 7.30401 Disc Loss: 0.00713317 Q Losses: [0.0092913788, 0.040462598]\n",
      "epoch:3 batch_done:72 Gen Loss: 7.85386 Disc Loss: 0.00388517 Q Losses: [0.011443374, 0.038125232]\n",
      "epoch:3 batch_done:73 Gen Loss: 8.051 Disc Loss: 0.0133845 Q Losses: [0.0099607613, 0.038743831]\n",
      "epoch:3 batch_done:74 Gen Loss: 12.0749 Disc Loss: 0.00168792 Q Losses: [0.013440333, 0.03997327]\n",
      "epoch:3 batch_done:75 Gen Loss: 11.5183 Disc Loss: 0.00126558 Q Losses: [0.012492139, 0.038023338]\n",
      "epoch:3 batch_done:76 Gen Loss: 7.51344 Disc Loss: 0.00621544 Q Losses: [0.012552339, 0.035834298]\n",
      "epoch:3 batch_done:77 Gen Loss: 6.2004 Disc Loss: 0.00735868 Q Losses: [0.0087281838, 0.044370219]\n",
      "epoch:3 batch_done:78 Gen Loss: 8.91563 Disc Loss: 0.00334537 Q Losses: [0.0056620394, 0.04758054]\n",
      "epoch:3 batch_done:79 Gen Loss: 9.44019 Disc Loss: 0.000804687 Q Losses: [0.015646834, 0.045305539]\n",
      "epoch:3 batch_done:80 Gen Loss: 8.48819 Disc Loss: 0.00200855 Q Losses: [0.010597663, 0.040182739]\n",
      "epoch:3 batch_done:81 Gen Loss: 6.35998 Disc Loss: 0.00346178 Q Losses: [0.011170123, 0.037285455]\n",
      "epoch:3 batch_done:82 Gen Loss: 6.04753 Disc Loss: 0.00555671 Q Losses: [0.0064009293, 0.04348509]\n",
      "epoch:3 batch_done:83 Gen Loss: 6.07395 Disc Loss: 0.0164137 Q Losses: [0.016433949, 0.035878297]\n",
      "epoch:3 batch_done:84 Gen Loss: 8.60736 Disc Loss: 0.00106329 Q Losses: [0.0079925656, 0.039124429]\n",
      "epoch:3 batch_done:85 Gen Loss: 9.5803 Disc Loss: 0.00386801 Q Losses: [0.0099552665, 0.036721766]\n",
      "epoch:3 batch_done:86 Gen Loss: 7.61887 Disc Loss: 0.00194294 Q Losses: [0.0093002683, 0.045413978]\n",
      "epoch:3 batch_done:87 Gen Loss: 6.6878 Disc Loss: 0.00223289 Q Losses: [0.014355243, 0.035574034]\n",
      "epoch:3 batch_done:88 Gen Loss: 5.94207 Disc Loss: 0.00653264 Q Losses: [0.011900653, 0.039455239]\n",
      "epoch:3 batch_done:89 Gen Loss: 8.52477 Disc Loss: 0.00260647 Q Losses: [0.016214166, 0.050642554]\n",
      "epoch:3 batch_done:90 Gen Loss: 6.03767 Disc Loss: 0.0138784 Q Losses: [0.010218192, 0.046282127]\n",
      "epoch:3 batch_done:91 Gen Loss: 6.4962 Disc Loss: 0.0410222 Q Losses: [0.016464673, 0.045369748]\n",
      "epoch:3 batch_done:92 Gen Loss: 6.65069 Disc Loss: 0.0025465 Q Losses: [0.010840338, 0.042494629]\n",
      "epoch:3 batch_done:93 Gen Loss: 5.74637 Disc Loss: 0.00682789 Q Losses: [0.017632382, 0.046207752]\n",
      "epoch:3 batch_done:94 Gen Loss: 7.51401 Disc Loss: 0.00125289 Q Losses: [0.010345182, 0.045932211]\n",
      "epoch:3 batch_done:95 Gen Loss: 8.22254 Disc Loss: 0.033373 Q Losses: [0.0074239615, 0.038739458]\n",
      "epoch:3 batch_done:96 Gen Loss: 10.0295 Disc Loss: 0.00354388 Q Losses: [0.012157734, 0.043663453]\n",
      "epoch:3 batch_done:97 Gen Loss: 12.2274 Disc Loss: 0.0319291 Q Losses: [0.016503109, 0.039032795]\n",
      "epoch:3 batch_done:98 Gen Loss: 12.0223 Disc Loss: 0.0234693 Q Losses: [0.010897863, 0.038834948]\n",
      "epoch:3 batch_done:99 Gen Loss: 7.19956 Disc Loss: 0.000724612 Q Losses: [0.0071742674, 0.045677904]\n",
      "epoch:3 batch_done:100 Gen Loss: 5.65552 Disc Loss: 0.00397513 Q Losses: [0.0083912918, 0.040569212]\n",
      "epoch:3 batch_done:101 Gen Loss: 5.97675 Disc Loss: 0.00469149 Q Losses: [0.0065330686, 0.043382853]\n",
      "epoch:3 batch_done:102 Gen Loss: 7.18772 Disc Loss: 0.00168988 Q Losses: [0.0087876013, 0.041535679]\n",
      "epoch:3 batch_done:103 Gen Loss: 12.939 Disc Loss: 0.0013646 Q Losses: [0.012956692, 0.038552795]\n",
      "epoch:3 batch_done:104 Gen Loss: 7.01578 Disc Loss: 0.013245 Q Losses: [0.0090691866, 0.035544161]\n",
      "epoch:3 batch_done:105 Gen Loss: 6.1198 Disc Loss: 0.00292482 Q Losses: [0.0072483001, 0.041612487]\n",
      "epoch:3 batch_done:106 Gen Loss: 5.9164 Disc Loss: 0.00397881 Q Losses: [0.0072251344, 0.034476064]\n",
      "epoch:3 batch_done:107 Gen Loss: 6.09431 Disc Loss: 0.00921333 Q Losses: [0.007399586, 0.060635544]\n",
      "epoch:3 batch_done:108 Gen Loss: 11.5269 Disc Loss: 0.00116138 Q Losses: [0.0075645014, 0.04384435]\n",
      "epoch:3 batch_done:109 Gen Loss: 8.16013 Disc Loss: 0.00139668 Q Losses: [0.0075137992, 0.038312249]\n",
      "epoch:3 batch_done:110 Gen Loss: 8.77831 Disc Loss: 0.00959801 Q Losses: [0.0065818671, 0.03748522]\n",
      "epoch:3 batch_done:111 Gen Loss: 7.06347 Disc Loss: 0.00102585 Q Losses: [0.0097171012, 0.041421626]\n",
      "epoch:3 batch_done:112 Gen Loss: 5.99968 Disc Loss: 0.00362151 Q Losses: [0.0061977655, 0.049361475]\n",
      "epoch:3 batch_done:113 Gen Loss: 6.07865 Disc Loss: 0.00628412 Q Losses: [0.0061930362, 0.035863727]\n",
      "epoch:3 batch_done:114 Gen Loss: 6.37109 Disc Loss: 0.00815551 Q Losses: [0.012817355, 0.047117375]\n",
      "epoch:3 batch_done:115 Gen Loss: 7.78464 Disc Loss: 0.00164998 Q Losses: [0.031311836, 0.036522232]\n",
      "epoch:3 batch_done:116 Gen Loss: 8.92743 Disc Loss: 0.00368161 Q Losses: [0.012451616, 0.034666047]\n",
      "epoch:3 batch_done:117 Gen Loss: 8.8468 Disc Loss: 0.00101913 Q Losses: [0.0068466058, 0.046363704]\n",
      "epoch:3 batch_done:118 Gen Loss: 6.81873 Disc Loss: 0.00276944 Q Losses: [0.010645615, 0.038789585]\n",
      "epoch:3 batch_done:119 Gen Loss: 9.17704 Disc Loss: 0.00114974 Q Losses: [0.0096204868, 0.046535514]\n",
      "epoch:3 batch_done:120 Gen Loss: 7.31799 Disc Loss: 0.00627732 Q Losses: [0.0095817037, 0.036358483]\n",
      "epoch:3 batch_done:121 Gen Loss: 5.6474 Disc Loss: 0.00605743 Q Losses: [0.012651819, 0.034572501]\n",
      "epoch:3 batch_done:122 Gen Loss: 6.01835 Disc Loss: 0.00509722 Q Losses: [0.0071251816, 0.05110113]\n",
      "epoch:3 batch_done:123 Gen Loss: 10.7327 Disc Loss: 0.00111344 Q Losses: [0.0063476106, 0.038842149]\n",
      "epoch:3 batch_done:124 Gen Loss: 7.397 Disc Loss: 0.0116583 Q Losses: [0.012940402, 0.039910611]\n",
      "epoch:3 batch_done:125 Gen Loss: 6.90558 Disc Loss: 0.000860081 Q Losses: [0.010248214, 0.037029572]\n",
      "epoch:3 batch_done:126 Gen Loss: 6.5656 Disc Loss: 0.00177061 Q Losses: [0.006549052, 0.045551986]\n",
      "epoch:3 batch_done:127 Gen Loss: 6.78322 Disc Loss: 0.00185315 Q Losses: [0.011387365, 0.045727618]\n",
      "epoch:3 batch_done:128 Gen Loss: 6.4354 Disc Loss: 0.00257076 Q Losses: [0.0071429219, 0.036502611]\n",
      "epoch:3 batch_done:129 Gen Loss: 6.04451 Disc Loss: 0.00648284 Q Losses: [0.0069314018, 0.034109231]\n",
      "epoch:3 batch_done:130 Gen Loss: 5.81279 Disc Loss: 0.0140605 Q Losses: [0.0071091633, 0.036901407]\n",
      "epoch:3 batch_done:131 Gen Loss: 7.05804 Disc Loss: 0.00818309 Q Losses: [0.028592847, 0.036821969]\n",
      "epoch:3 batch_done:132 Gen Loss: 10.4992 Disc Loss: 0.00125397 Q Losses: [0.013626071, 0.042114656]\n",
      "epoch:3 batch_done:133 Gen Loss: 5.75018 Disc Loss: 0.0105215 Q Losses: [0.0079997089, 0.049576283]\n",
      "epoch:3 batch_done:134 Gen Loss: 7.57155 Disc Loss: 0.00122523 Q Losses: [0.010155013, 0.045994606]\n",
      "epoch:3 batch_done:135 Gen Loss: 8.63405 Disc Loss: 0.000306911 Q Losses: [0.012467626, 0.033508353]\n",
      "epoch:3 batch_done:136 Gen Loss: 6.08935 Disc Loss: 0.00823441 Q Losses: [0.0095413988, 0.038085986]\n",
      "epoch:3 batch_done:137 Gen Loss: 6.83753 Disc Loss: 0.0030593 Q Losses: [0.0098709995, 0.03608977]\n",
      "epoch:3 batch_done:138 Gen Loss: 6.40549 Disc Loss: 0.00750789 Q Losses: [0.0088269282, 0.036792561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 batch_done:139 Gen Loss: 7.44163 Disc Loss: 0.00209496 Q Losses: [0.019853013, 0.036650352]\n",
      "epoch:3 batch_done:140 Gen Loss: 7.64639 Disc Loss: 0.00283001 Q Losses: [0.0075168689, 0.047300279]\n",
      "epoch:3 batch_done:141 Gen Loss: 9.30136 Disc Loss: 0.00136579 Q Losses: [0.0077272495, 0.038409211]\n",
      "epoch:3 batch_done:142 Gen Loss: 8.88245 Disc Loss: 0.00111444 Q Losses: [0.010058448, 0.03665024]\n",
      "epoch:3 batch_done:143 Gen Loss: 5.70127 Disc Loss: 0.0107178 Q Losses: [0.008648077, 0.035307921]\n",
      "epoch:3 batch_done:144 Gen Loss: 7.62646 Disc Loss: 0.00122578 Q Losses: [0.023646072, 0.03426078]\n",
      "epoch:3 batch_done:145 Gen Loss: 10.9639 Disc Loss: 0.00081165 Q Losses: [0.0077810404, 0.036459424]\n",
      "epoch:3 batch_done:146 Gen Loss: 5.88206 Disc Loss: 0.00862591 Q Losses: [0.012379984, 0.044954836]\n",
      "epoch:3 batch_done:147 Gen Loss: 9.00905 Disc Loss: 0.00249187 Q Losses: [0.0062345713, 0.0419503]\n",
      "epoch:3 batch_done:148 Gen Loss: 9.61751 Disc Loss: 0.0544433 Q Losses: [0.0085744048, 0.039208267]\n",
      "epoch:3 batch_done:149 Gen Loss: 8.14722 Disc Loss: 0.0463878 Q Losses: [0.0097144255, 0.036632076]\n",
      "epoch:3 batch_done:150 Gen Loss: 10.9815 Disc Loss: 0.00337366 Q Losses: [0.018843014, 0.035245012]\n",
      "epoch:3 batch_done:151 Gen Loss: 10.7766 Disc Loss: 0.000516461 Q Losses: [0.0087563396, 0.033767149]\n",
      "epoch:3 batch_done:152 Gen Loss: 10.6354 Disc Loss: 0.00478145 Q Losses: [0.0050827488, 0.036661759]\n",
      "epoch:3 batch_done:153 Gen Loss: 10.1763 Disc Loss: 0.0190991 Q Losses: [0.008875438, 0.041305251]\n",
      "epoch:3 batch_done:154 Gen Loss: 6.67596 Disc Loss: 0.000925607 Q Losses: [0.0087040532, 0.032625873]\n",
      "epoch:3 batch_done:155 Gen Loss: 7.0201 Disc Loss: 0.000810629 Q Losses: [0.0096607888, 0.035627775]\n",
      "epoch:3 batch_done:156 Gen Loss: 12.4624 Disc Loss: 6.46572e-05 Q Losses: [0.0081795109, 0.035667546]\n",
      "epoch:3 batch_done:157 Gen Loss: 6.83574 Disc Loss: 0.0208241 Q Losses: [0.0051362501, 0.033451855]\n",
      "epoch:3 batch_done:158 Gen Loss: 7.56417 Disc Loss: 0.00369158 Q Losses: [0.0093834233, 0.030774359]\n",
      "epoch:3 batch_done:159 Gen Loss: 10.2002 Disc Loss: 0.000319196 Q Losses: [0.0064993151, 0.041767128]\n",
      "epoch:3 batch_done:160 Gen Loss: 7.16791 Disc Loss: 0.00171232 Q Losses: [0.013323219, 0.03337514]\n",
      "epoch:3 batch_done:161 Gen Loss: 10.2461 Disc Loss: 0.000844322 Q Losses: [0.0057481565, 0.029053338]\n",
      "epoch:3 batch_done:162 Gen Loss: 9.11723 Disc Loss: 0.00126101 Q Losses: [0.0062784879, 0.029664692]\n",
      "epoch:3 batch_done:163 Gen Loss: 9.21203 Disc Loss: 0.0117473 Q Losses: [0.0073983339, 0.032866314]\n",
      "epoch:3 batch_done:164 Gen Loss: 5.51309 Disc Loss: 0.00775671 Q Losses: [0.011721168, 0.03276068]\n",
      "epoch:3 batch_done:165 Gen Loss: 10.1718 Disc Loss: 0.000320369 Q Losses: [0.0091342675, 0.032705627]\n",
      "epoch:3 batch_done:166 Gen Loss: 7.04322 Disc Loss: 0.00128937 Q Losses: [0.010969745, 0.033195242]\n",
      "epoch:3 batch_done:167 Gen Loss: 9.08745 Disc Loss: 0.000408072 Q Losses: [0.0087762177, 0.031565156]\n",
      "epoch:3 batch_done:168 Gen Loss: 8.85333 Disc Loss: 0.000411033 Q Losses: [0.017178547, 0.033954401]\n",
      "epoch:3 batch_done:169 Gen Loss: 5.97073 Disc Loss: 0.0113939 Q Losses: [0.0075272899, 0.044206716]\n",
      "epoch:3 batch_done:170 Gen Loss: 6.49256 Disc Loss: 0.00649405 Q Losses: [0.011168106, 0.030826649]\n",
      "epoch:3 batch_done:171 Gen Loss: 8.46351 Disc Loss: 0.00553265 Q Losses: [0.011104485, 0.034415212]\n",
      "epoch:3 batch_done:172 Gen Loss: 9.66489 Disc Loss: 0.00301947 Q Losses: [0.0081971418, 0.029084533]\n",
      "epoch:3 batch_done:173 Gen Loss: 7.02594 Disc Loss: 0.00106909 Q Losses: [0.0086588236, 0.030131329]\n",
      "epoch:3 batch_done:174 Gen Loss: 6.51528 Disc Loss: 0.014765 Q Losses: [0.013812961, 0.031473219]\n",
      "epoch:3 batch_done:175 Gen Loss: 7.1991 Disc Loss: 0.00481588 Q Losses: [0.010349972, 0.033615015]\n",
      "epoch:3 batch_done:176 Gen Loss: 6.04807 Disc Loss: 0.0110378 Q Losses: [0.008167956, 0.038259916]\n",
      "epoch:3 batch_done:177 Gen Loss: 8.60847 Disc Loss: 0.00206193 Q Losses: [0.0087672062, 0.029035367]\n",
      "epoch:3 batch_done:178 Gen Loss: 9.88266 Disc Loss: 0.000327048 Q Losses: [0.0071119852, 0.032359175]\n",
      "epoch:3 batch_done:179 Gen Loss: 5.69624 Disc Loss: 0.00863841 Q Losses: [0.0071101575, 0.03093829]\n",
      "epoch:3 batch_done:180 Gen Loss: 6.39432 Disc Loss: 0.00224892 Q Losses: [0.012258038, 0.035992395]\n",
      "epoch:3 batch_done:181 Gen Loss: 8.77801 Disc Loss: 0.000590785 Q Losses: [0.014578603, 0.033919573]\n",
      "epoch:3 batch_done:182 Gen Loss: 6.18336 Disc Loss: 0.00923163 Q Losses: [0.010504559, 0.032973275]\n",
      "epoch:3 batch_done:183 Gen Loss: 6.60333 Disc Loss: 0.00577291 Q Losses: [0.0097161597, 0.037308425]\n",
      "epoch:3 batch_done:184 Gen Loss: 9.13663 Disc Loss: 0.000254881 Q Losses: [0.011244794, 0.028930224]\n",
      "epoch:3 batch_done:185 Gen Loss: 7.38628 Disc Loss: 0.000969467 Q Losses: [0.011473171, 0.034330346]\n",
      "epoch:3 batch_done:186 Gen Loss: 6.4863 Disc Loss: 0.0030927 Q Losses: [0.012312038, 0.062667161]\n",
      "epoch:3 batch_done:187 Gen Loss: 6.56382 Disc Loss: 0.00507824 Q Losses: [0.0071371729, 0.03049124]\n",
      "epoch:3 batch_done:188 Gen Loss: 6.97796 Disc Loss: 0.00172958 Q Losses: [0.011513794, 0.042264424]\n",
      "epoch:3 batch_done:189 Gen Loss: 8.10761 Disc Loss: 0.0262533 Q Losses: [0.0087865591, 0.034835484]\n",
      "epoch:3 batch_done:190 Gen Loss: 9.28792 Disc Loss: 0.042863 Q Losses: [0.011309832, 0.034377389]\n",
      "epoch:3 batch_done:191 Gen Loss: 11.3435 Disc Loss: 0.000822897 Q Losses: [0.011725911, 0.036861099]\n",
      "epoch:3 batch_done:192 Gen Loss: 11.5216 Disc Loss: 0.00612557 Q Losses: [0.0082993647, 0.031830877]\n",
      "epoch:3 batch_done:193 Gen Loss: 8.24763 Disc Loss: 0.00876143 Q Losses: [0.010852879, 0.039938368]\n",
      "epoch:3 batch_done:194 Gen Loss: 8.57575 Disc Loss: 0.000375002 Q Losses: [0.012972582, 0.033043988]\n",
      "epoch:3 batch_done:195 Gen Loss: 9.1445 Disc Loss: 0.000144811 Q Losses: [0.0065712417, 0.047044009]\n",
      "epoch:3 batch_done:196 Gen Loss: 9.6932 Disc Loss: 0.000161053 Q Losses: [0.0087980581, 0.050240036]\n",
      "epoch:3 batch_done:197 Gen Loss: 6.9284 Disc Loss: 0.00286134 Q Losses: [0.015499458, 0.030655073]\n",
      "epoch:3 batch_done:198 Gen Loss: 5.6598 Disc Loss: 0.00714061 Q Losses: [0.0090873763, 0.04319001]\n",
      "epoch:3 batch_done:199 Gen Loss: 7.66571 Disc Loss: 0.000550981 Q Losses: [0.010142529, 0.035985943]\n",
      "epoch:3 batch_done:200 Gen Loss: 7.34988 Disc Loss: 0.00104491 Q Losses: [0.013795224, 0.041215338]\n",
      "epoch:3 batch_done:201 Gen Loss: 6.48787 Disc Loss: 0.00315402 Q Losses: [0.0083395308, 0.030315537]\n",
      "epoch:3 batch_done:202 Gen Loss: 15.5474 Disc Loss: 0.059528 Q Losses: [0.0079678362, 0.036966521]\n",
      "epoch:3 batch_done:203 Gen Loss: 1.19656 Disc Loss: 0.188139 Q Losses: [0.0088680461, 0.03426414]\n",
      "epoch:3 batch_done:204 Gen Loss: 48.2777 Disc Loss: 0.781513 Q Losses: [0.015434474, 0.032863803]\n",
      "epoch:3 batch_done:205 Gen Loss: 30.2243 Disc Loss: 18.1743 Q Losses: [0.0069437623, 0.049461611]\n",
      "epoch:3 batch_done:206 Gen Loss: 0.427614 Disc Loss: 2.42144e-08 Q Losses: [0.011998644, 0.047197592]\n",
      "epoch:3 batch_done:207 Gen Loss: 32.5531 Disc Loss: 9.32525 Q Losses: [0.0095164496, 0.043311313]\n",
      "epoch:4 batch_done:1 Gen Loss: 18.7661 Disc Loss: 4.65362 Q Losses: [0.010210257, 0.053592302]\n",
      "epoch:4 batch_done:2 Gen Loss: 6.69283 Disc Loss: 0.497232 Q Losses: [0.017939949, 0.048747621]\n",
      "epoch:4 batch_done:3 Gen Loss: 2.23149 Disc Loss: 0.0152591 Q Losses: [0.013397203, 0.063999221]\n",
      "epoch:4 batch_done:4 Gen Loss: 4.28872 Disc Loss: 0.126453 Q Losses: [0.011883616, 0.059167095]\n",
      "epoch:4 batch_done:5 Gen Loss: 5.07987 Disc Loss: 0.0647646 Q Losses: [0.017139353, 0.056913354]\n",
      "epoch:4 batch_done:6 Gen Loss: 5.62345 Disc Loss: 0.126144 Q Losses: [0.011898596, 0.042343087]\n",
      "epoch:4 batch_done:7 Gen Loss: 8.72112 Disc Loss: 0.387474 Q Losses: [0.015581289, 0.054875385]\n",
      "epoch:4 batch_done:8 Gen Loss: 5.13151 Disc Loss: 0.257566 Q Losses: [0.023444604, 0.061935607]\n",
      "epoch:4 batch_done:9 Gen Loss: 16.3583 Disc Loss: 0.738138 Q Losses: [0.011207679, 0.05271478]\n",
      "epoch:4 batch_done:10 Gen Loss: 5.29462 Disc Loss: 3.93042 Q Losses: [0.013956138, 0.047547184]\n",
      "epoch:4 batch_done:11 Gen Loss: 1.96563 Disc Loss: 0.0681959 Q Losses: [0.024251945, 0.048107088]\n",
      "epoch:4 batch_done:12 Gen Loss: 16.1997 Disc Loss: 0.518894 Q Losses: [0.018945552, 0.045833677]\n",
      "epoch:4 batch_done:13 Gen Loss: 19.8605 Disc Loss: 0.0577612 Q Losses: [0.01222364, 0.050139159]\n",
      "epoch:4 batch_done:14 Gen Loss: 16.9135 Disc Loss: 0.207681 Q Losses: [0.012751311, 0.050830033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:15 Gen Loss: 12.5041 Disc Loss: 0.184071 Q Losses: [0.015543118, 0.050542101]\n",
      "epoch:4 batch_done:16 Gen Loss: 8.25763 Disc Loss: 0.0892453 Q Losses: [0.0184846, 0.041202672]\n",
      "epoch:4 batch_done:17 Gen Loss: 4.52524 Disc Loss: 0.037375 Q Losses: [0.017297652, 0.044301294]\n",
      "epoch:4 batch_done:18 Gen Loss: 21.5664 Disc Loss: 0.578601 Q Losses: [0.0092202276, 0.038322635]\n",
      "epoch:4 batch_done:19 Gen Loss: 21.4625 Disc Loss: 0.92178 Q Losses: [0.007050646, 0.058297988]\n",
      "epoch:4 batch_done:20 Gen Loss: 19.3238 Disc Loss: 0.0145888 Q Losses: [0.0081138723, 0.047276638]\n",
      "epoch:4 batch_done:21 Gen Loss: 16.0252 Disc Loss: 0.028651 Q Losses: [0.0092344396, 0.043859776]\n",
      "epoch:4 batch_done:22 Gen Loss: 12.0484 Disc Loss: 0.000415891 Q Losses: [0.009757489, 0.061361201]\n",
      "epoch:4 batch_done:23 Gen Loss: 8.61906 Disc Loss: 0.000566564 Q Losses: [0.0095419362, 0.042764939]\n",
      "epoch:4 batch_done:24 Gen Loss: 5.26763 Disc Loss: 0.0155137 Q Losses: [0.0096774716, 0.040703177]\n",
      "epoch:4 batch_done:25 Gen Loss: 20.6126 Disc Loss: 0.41565 Q Losses: [0.0083458927, 0.043525577]\n",
      "epoch:4 batch_done:26 Gen Loss: 23.4683 Disc Loss: 0.249795 Q Losses: [0.0075214175, 0.03416805]\n",
      "epoch:4 batch_done:27 Gen Loss: 20.0956 Disc Loss: 0.417566 Q Losses: [0.0094874315, 0.042640187]\n",
      "epoch:4 batch_done:28 Gen Loss: 17.5921 Disc Loss: 0.00427467 Q Losses: [0.010648565, 0.04861417]\n",
      "epoch:4 batch_done:29 Gen Loss: 13.3739 Disc Loss: 0.0430991 Q Losses: [0.0085906321, 0.045565531]\n",
      "epoch:4 batch_done:30 Gen Loss: 8.32046 Disc Loss: 0.00059516 Q Losses: [0.010833581, 0.034980614]\n",
      "epoch:4 batch_done:31 Gen Loss: 8.55895 Disc Loss: 0.0808025 Q Losses: [0.0088164667, 0.037099641]\n",
      "epoch:4 batch_done:32 Gen Loss: 13.5624 Disc Loss: 0.13546 Q Losses: [0.010064026, 0.048323158]\n",
      "epoch:4 batch_done:33 Gen Loss: 9.47594 Disc Loss: 0.12585 Q Losses: [0.013872131, 0.046062708]\n",
      "epoch:4 batch_done:34 Gen Loss: 24.4377 Disc Loss: 0.366199 Q Losses: [0.010178423, 0.031565689]\n",
      "epoch:4 batch_done:35 Gen Loss: 20.7457 Disc Loss: 0.83271 Q Losses: [0.011573306, 0.04157288]\n",
      "epoch:4 batch_done:36 Gen Loss: 14.235 Disc Loss: 0.0092647 Q Losses: [0.010301415, 0.036741771]\n",
      "epoch:4 batch_done:37 Gen Loss: 5.73323 Disc Loss: 0.00488067 Q Losses: [0.012442911, 0.046323929]\n",
      "epoch:4 batch_done:38 Gen Loss: 39.9828 Disc Loss: 2.06729 Q Losses: [0.011029562, 0.04377]\n",
      "epoch:4 batch_done:39 Gen Loss: 27.3807 Disc Loss: 11.0802 Q Losses: [0.013666229, 0.040965397]\n",
      "epoch:4 batch_done:40 Gen Loss: 15.3789 Disc Loss: 1.3382 Q Losses: [0.018443514, 0.041815668]\n",
      "epoch:4 batch_done:41 Gen Loss: 6.90169 Disc Loss: 0.00849512 Q Losses: [0.020102372, 0.045312263]\n",
      "epoch:4 batch_done:42 Gen Loss: 7.69915 Disc Loss: 0.258241 Q Losses: [0.013908764, 0.034985889]\n",
      "epoch:4 batch_done:43 Gen Loss: 7.3933 Disc Loss: 0.0485185 Q Losses: [0.01194701, 0.038974475]\n",
      "epoch:4 batch_done:44 Gen Loss: 6.24897 Disc Loss: 0.0406431 Q Losses: [0.008890776, 0.03918843]\n",
      "epoch:4 batch_done:45 Gen Loss: 6.29471 Disc Loss: 0.0521683 Q Losses: [0.01163481, 0.046825241]\n",
      "epoch:4 batch_done:46 Gen Loss: 8.20652 Disc Loss: 0.141044 Q Losses: [0.011922928, 0.034783948]\n",
      "epoch:4 batch_done:47 Gen Loss: 6.97223 Disc Loss: 0.0821048 Q Losses: [0.012115786, 0.038706999]\n",
      "epoch:4 batch_done:48 Gen Loss: 9.31053 Disc Loss: 0.161126 Q Losses: [0.020852106, 0.047286015]\n",
      "epoch:4 batch_done:49 Gen Loss: 5.57206 Disc Loss: 0.429221 Q Losses: [0.011921195, 0.040214535]\n",
      "epoch:4 batch_done:50 Gen Loss: 16.9444 Disc Loss: 0.658424 Q Losses: [0.0082313176, 0.041563742]\n",
      "epoch:4 batch_done:51 Gen Loss: 18.7313 Disc Loss: 0.538653 Q Losses: [0.015977114, 0.043415304]\n",
      "epoch:4 batch_done:52 Gen Loss: 14.9351 Disc Loss: 0.308363 Q Losses: [0.016734149, 0.038428985]\n",
      "epoch:4 batch_done:53 Gen Loss: 9.25014 Disc Loss: 0.0617212 Q Losses: [0.014040684, 0.041086771]\n",
      "epoch:4 batch_done:54 Gen Loss: 4.06903 Disc Loss: 0.119712 Q Losses: [0.010100675, 0.039737903]\n",
      "epoch:4 batch_done:55 Gen Loss: 30.082 Disc Loss: 1.4089 Q Losses: [0.0078844838, 0.045397386]\n",
      "epoch:4 batch_done:56 Gen Loss: 32.9012 Disc Loss: 1.08703 Q Losses: [0.0078382734, 0.031503908]\n",
      "epoch:4 batch_done:57 Gen Loss: 25.9583 Disc Loss: 0.852631 Q Losses: [0.011038152, 0.058238294]\n",
      "epoch:4 batch_done:58 Gen Loss: 17.308 Disc Loss: 0.0326679 Q Losses: [0.012242166, 0.045994699]\n",
      "epoch:4 batch_done:59 Gen Loss: 8.89307 Disc Loss: 0.0030114 Q Losses: [0.0098108817, 0.06028726]\n",
      "epoch:4 batch_done:60 Gen Loss: 5.53092 Disc Loss: 0.0572731 Q Losses: [0.014486084, 0.047680855]\n",
      "epoch:4 batch_done:61 Gen Loss: 22.7637 Disc Loss: 0.610082 Q Losses: [0.012435412, 0.041767877]\n",
      "epoch:4 batch_done:62 Gen Loss: 27.0767 Disc Loss: 0.216224 Q Losses: [0.0084717972, 0.050265942]\n",
      "epoch:4 batch_done:63 Gen Loss: 23.99 Disc Loss: 0.635673 Q Losses: [0.012191094, 0.047688767]\n",
      "epoch:4 batch_done:64 Gen Loss: 16.5631 Disc Loss: 0.314688 Q Losses: [0.013019004, 0.031738222]\n",
      "epoch:4 batch_done:65 Gen Loss: 9.4756 Disc Loss: 0.000945714 Q Losses: [0.014124732, 0.049446076]\n",
      "epoch:4 batch_done:66 Gen Loss: 4.76824 Disc Loss: 0.0077968 Q Losses: [0.028325874, 0.036550462]\n",
      "epoch:4 batch_done:67 Gen Loss: 9.06903 Disc Loss: 0.192084 Q Losses: [0.022815235, 0.040161684]\n",
      "epoch:4 batch_done:68 Gen Loss: 10.0816 Disc Loss: 0.0023242 Q Losses: [0.0095687006, 0.052525792]\n",
      "epoch:4 batch_done:69 Gen Loss: 9.10696 Disc Loss: 0.00450496 Q Losses: [0.016186226, 0.040817976]\n",
      "epoch:4 batch_done:70 Gen Loss: 7.34423 Disc Loss: 0.0177964 Q Losses: [0.0096814344, 0.040230475]\n",
      "epoch:4 batch_done:71 Gen Loss: 4.99737 Disc Loss: 0.0234133 Q Losses: [0.01524594, 0.035647873]\n",
      "epoch:4 batch_done:72 Gen Loss: 7.98086 Disc Loss: 0.144293 Q Losses: [0.010378934, 0.034499168]\n",
      "epoch:4 batch_done:73 Gen Loss: 7.80111 Disc Loss: 0.059979 Q Losses: [0.014283933, 0.060544498]\n",
      "epoch:4 batch_done:74 Gen Loss: 6.22607 Disc Loss: 0.0732065 Q Losses: [0.0215203, 0.035562992]\n",
      "epoch:4 batch_done:75 Gen Loss: 3.79118 Disc Loss: 0.133239 Q Losses: [0.0080969362, 0.031780392]\n",
      "epoch:4 batch_done:76 Gen Loss: 8.20483 Disc Loss: 0.168185 Q Losses: [0.0085768998, 0.044107601]\n",
      "epoch:4 batch_done:77 Gen Loss: 8.46223 Disc Loss: 0.0576967 Q Losses: [0.011247383, 0.035031848]\n",
      "epoch:4 batch_done:78 Gen Loss: 7.27591 Disc Loss: 0.0378781 Q Losses: [0.0078576934, 0.030143183]\n",
      "epoch:4 batch_done:79 Gen Loss: 5.21752 Disc Loss: 0.0373423 Q Losses: [0.009659281, 0.031441003]\n",
      "epoch:4 batch_done:80 Gen Loss: 6.09749 Disc Loss: 0.0654086 Q Losses: [0.016137164, 0.026013363]\n",
      "epoch:4 batch_done:81 Gen Loss: 6.67591 Disc Loss: 0.0271901 Q Losses: [0.0051483423, 0.026839163]\n",
      "epoch:4 batch_done:82 Gen Loss: 6.16453 Disc Loss: 0.0375499 Q Losses: [0.0091897063, 0.026271224]\n",
      "epoch:4 batch_done:83 Gen Loss: 5.53869 Disc Loss: 0.0662208 Q Losses: [0.01377657, 0.040124737]\n",
      "epoch:4 batch_done:84 Gen Loss: 7.79857 Disc Loss: 0.0929092 Q Losses: [0.010843884, 0.030414909]\n",
      "epoch:4 batch_done:85 Gen Loss: 7.52271 Disc Loss: 0.0226234 Q Losses: [0.0084179044, 0.034424782]\n",
      "epoch:4 batch_done:86 Gen Loss: 5.92451 Disc Loss: 0.0430933 Q Losses: [0.0066343057, 0.02566983]\n",
      "epoch:4 batch_done:87 Gen Loss: 6.76389 Disc Loss: 0.0657083 Q Losses: [0.0093413033, 0.031151105]\n",
      "epoch:4 batch_done:88 Gen Loss: 6.60238 Disc Loss: 0.0441677 Q Losses: [0.0081058033, 0.032318514]\n",
      "epoch:4 batch_done:89 Gen Loss: 5.95198 Disc Loss: 0.0586237 Q Losses: [0.0087427413, 0.027045259]\n",
      "epoch:4 batch_done:90 Gen Loss: 5.69677 Disc Loss: 0.0831398 Q Losses: [0.011895096, 0.041181214]\n",
      "epoch:4 batch_done:91 Gen Loss: 5.38229 Disc Loss: 0.0739236 Q Losses: [0.0097221099, 0.037203327]\n",
      "epoch:4 batch_done:92 Gen Loss: 6.70241 Disc Loss: 0.0747839 Q Losses: [0.01183085, 0.028654609]\n",
      "epoch:4 batch_done:93 Gen Loss: 6.2963 Disc Loss: 0.0432068 Q Losses: [0.0087346891, 0.039244756]\n",
      "epoch:4 batch_done:94 Gen Loss: 6.48189 Disc Loss: 0.0469342 Q Losses: [0.007790911, 0.023586761]\n",
      "epoch:4 batch_done:95 Gen Loss: 5.8912 Disc Loss: 0.046176 Q Losses: [0.010343038, 0.03853666]\n",
      "epoch:4 batch_done:96 Gen Loss: 3.13444 Disc Loss: 0.117708 Q Losses: [0.0067381044, 0.029009134]\n",
      "epoch:4 batch_done:97 Gen Loss: 27.8533 Disc Loss: 0.456098 Q Losses: [0.010181356, 0.024101119]\n",
      "epoch:4 batch_done:98 Gen Loss: 7.59122 Disc Loss: 6.97042 Q Losses: [0.011673204, 0.025160709]\n",
      "epoch:4 batch_done:99 Gen Loss: 0.0645861 Disc Loss: 0.0226362 Q Losses: [0.0096705221, 0.02263841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:100 Gen Loss: 36.0393 Disc Loss: 3.31733 Q Losses: [0.0077451756, 0.027311791]\n",
      "epoch:4 batch_done:101 Gen Loss: 37.8499 Disc Loss: 1.91764 Q Losses: [0.0060323277, 0.022581909]\n",
      "epoch:4 batch_done:102 Gen Loss: 29.6748 Disc Loss: 0.768542 Q Losses: [0.0074647921, 0.026076064]\n",
      "epoch:4 batch_done:103 Gen Loss: 20.1062 Disc Loss: 0.0324835 Q Losses: [0.010000843, 0.037141852]\n",
      "epoch:4 batch_done:104 Gen Loss: 12.7764 Disc Loss: 0.00722351 Q Losses: [0.0088935457, 0.027681198]\n",
      "epoch:4 batch_done:105 Gen Loss: 7.39889 Disc Loss: 0.00699775 Q Losses: [0.0051869177, 0.027982224]\n",
      "epoch:4 batch_done:106 Gen Loss: 4.87361 Disc Loss: 0.0516401 Q Losses: [0.0053480566, 0.021516653]\n",
      "epoch:4 batch_done:107 Gen Loss: 12.1166 Disc Loss: 0.259169 Q Losses: [0.0055033658, 0.023051951]\n",
      "epoch:4 batch_done:108 Gen Loss: 14.077 Disc Loss: 0.0210432 Q Losses: [0.010491181, 0.028213803]\n",
      "epoch:4 batch_done:109 Gen Loss: 13.0187 Disc Loss: 0.0543279 Q Losses: [0.0057694926, 0.024519708]\n",
      "epoch:4 batch_done:110 Gen Loss: 10.7035 Disc Loss: 0.0290174 Q Losses: [0.0055045821, 0.031656973]\n",
      "epoch:4 batch_done:111 Gen Loss: 7.83853 Disc Loss: 0.0193429 Q Losses: [0.01093263, 0.019741768]\n",
      "epoch:4 batch_done:112 Gen Loss: 4.84877 Disc Loss: 0.0339462 Q Losses: [0.015847109, 0.019382713]\n",
      "epoch:4 batch_done:113 Gen Loss: 9.61279 Disc Loss: 0.173102 Q Losses: [0.0063634366, 0.02115982]\n",
      "epoch:4 batch_done:114 Gen Loss: 9.28288 Disc Loss: 0.0903318 Q Losses: [0.013869203, 0.022659414]\n",
      "epoch:4 batch_done:115 Gen Loss: 6.56532 Disc Loss: 0.108583 Q Losses: [0.0060548871, 0.018521387]\n",
      "epoch:4 batch_done:116 Gen Loss: 4.80706 Disc Loss: 0.0784048 Q Losses: [0.0079696551, 0.018226299]\n",
      "epoch:4 batch_done:117 Gen Loss: 9.52458 Disc Loss: 0.177587 Q Losses: [0.0065552462, 0.023146264]\n",
      "epoch:4 batch_done:118 Gen Loss: 9.80921 Disc Loss: 0.0549424 Q Losses: [0.0068635759, 0.021721866]\n",
      "epoch:4 batch_done:119 Gen Loss: 6.90078 Disc Loss: 0.17374 Q Losses: [0.0085059451, 0.019154526]\n",
      "epoch:4 batch_done:120 Gen Loss: 4.69847 Disc Loss: 0.0278876 Q Losses: [0.011234906, 0.02375868]\n",
      "epoch:4 batch_done:121 Gen Loss: 5.4253 Disc Loss: 0.0724972 Q Losses: [0.01732547, 0.019030485]\n",
      "epoch:4 batch_done:122 Gen Loss: 6.4925 Disc Loss: 0.00926464 Q Losses: [0.0093880538, 0.018120263]\n",
      "epoch:4 batch_done:123 Gen Loss: 5.59559 Disc Loss: 0.0254445 Q Losses: [0.0058885356, 0.02318703]\n",
      "epoch:4 batch_done:124 Gen Loss: 5.38015 Disc Loss: 0.0411515 Q Losses: [0.01103974, 0.019212596]\n",
      "epoch:4 batch_done:125 Gen Loss: 6.33211 Disc Loss: 0.0669583 Q Losses: [0.008392782, 0.016206158]\n",
      "epoch:4 batch_done:126 Gen Loss: 6.84622 Disc Loss: 0.0240451 Q Losses: [0.013738997, 0.021672875]\n",
      "epoch:4 batch_done:127 Gen Loss: 7.75401 Disc Loss: 0.0152482 Q Losses: [0.0066182781, 0.018222902]\n",
      "epoch:4 batch_done:128 Gen Loss: 6.97471 Disc Loss: 0.0133968 Q Losses: [0.0085934186, 0.022054356]\n",
      "epoch:4 batch_done:129 Gen Loss: 5.06406 Disc Loss: 0.0233535 Q Losses: [0.0090720449, 0.016996203]\n",
      "epoch:4 batch_done:130 Gen Loss: 5.29581 Disc Loss: 0.0474274 Q Losses: [0.0083070397, 0.021711987]\n",
      "epoch:4 batch_done:131 Gen Loss: 5.35685 Disc Loss: 0.0214537 Q Losses: [0.0086210957, 0.016850229]\n",
      "epoch:4 batch_done:132 Gen Loss: 6.2155 Disc Loss: 0.0153176 Q Losses: [0.0056254403, 0.021289214]\n",
      "epoch:4 batch_done:133 Gen Loss: 7.57933 Disc Loss: 0.105534 Q Losses: [0.011725059, 0.016695488]\n",
      "epoch:4 batch_done:134 Gen Loss: 9.52158 Disc Loss: 0.0108157 Q Losses: [0.010999232, 0.021846682]\n",
      "epoch:4 batch_done:135 Gen Loss: 10.3927 Disc Loss: 0.0575611 Q Losses: [0.0091653019, 0.018247262]\n",
      "epoch:4 batch_done:136 Gen Loss: 7.85637 Disc Loss: 0.0187322 Q Losses: [0.0052047353, 0.020095378]\n",
      "epoch:4 batch_done:137 Gen Loss: 6.82431 Disc Loss: 0.0127694 Q Losses: [0.0045931586, 0.027207572]\n",
      "epoch:4 batch_done:138 Gen Loss: 5.71363 Disc Loss: 0.0101018 Q Losses: [0.0081564775, 0.016691653]\n",
      "epoch:4 batch_done:139 Gen Loss: 5.38672 Disc Loss: 0.00738533 Q Losses: [0.011764676, 0.026423026]\n",
      "epoch:4 batch_done:140 Gen Loss: 6.25607 Disc Loss: 0.00543094 Q Losses: [0.0077271368, 0.021255825]\n",
      "epoch:4 batch_done:141 Gen Loss: 9.01178 Disc Loss: 0.126154 Q Losses: [0.005311396, 0.02036608]\n",
      "epoch:4 batch_done:142 Gen Loss: 11.8858 Disc Loss: 0.00520391 Q Losses: [0.0072734947, 0.01929998]\n",
      "epoch:4 batch_done:143 Gen Loss: 12.6622 Disc Loss: 0.0541705 Q Losses: [0.012812933, 0.016144603]\n",
      "epoch:4 batch_done:144 Gen Loss: 9.29242 Disc Loss: 0.0245443 Q Losses: [0.0082463929, 0.019517215]\n",
      "epoch:4 batch_done:145 Gen Loss: 7.30149 Disc Loss: 0.0407274 Q Losses: [0.010306999, 0.025459439]\n",
      "epoch:4 batch_done:146 Gen Loss: 6.22621 Disc Loss: 0.0228141 Q Losses: [0.0086490875, 0.019921031]\n",
      "epoch:4 batch_done:147 Gen Loss: 9.23578 Disc Loss: 0.112156 Q Losses: [0.0079942029, 0.02321458]\n",
      "epoch:4 batch_done:148 Gen Loss: 9.09452 Disc Loss: 0.0336824 Q Losses: [0.0070757447, 0.024458023]\n",
      "epoch:4 batch_done:149 Gen Loss: 9.85621 Disc Loss: 0.00367513 Q Losses: [0.0076706167, 0.020097956]\n",
      "epoch:4 batch_done:150 Gen Loss: 9.01573 Disc Loss: 0.104494 Q Losses: [0.011342772, 0.016910208]\n",
      "epoch:4 batch_done:151 Gen Loss: 5.10451 Disc Loss: 0.00922796 Q Losses: [0.0089826398, 0.020332014]\n",
      "epoch:4 batch_done:152 Gen Loss: 5.72547 Disc Loss: 0.0655792 Q Losses: [0.014067102, 0.020336125]\n",
      "epoch:4 batch_done:153 Gen Loss: 7.08805 Disc Loss: 0.0505315 Q Losses: [0.0090520019, 0.019233834]\n",
      "epoch:4 batch_done:154 Gen Loss: 10.1245 Disc Loss: 0.0167518 Q Losses: [0.0084335329, 0.025436055]\n",
      "epoch:4 batch_done:155 Gen Loss: 8.52212 Disc Loss: 0.00861717 Q Losses: [0.0085503589, 0.022782713]\n",
      "epoch:4 batch_done:156 Gen Loss: 7.22995 Disc Loss: 0.0482061 Q Losses: [0.0071486924, 0.020944944]\n",
      "epoch:4 batch_done:157 Gen Loss: 5.12447 Disc Loss: 0.00980233 Q Losses: [0.024230633, 0.021052506]\n",
      "epoch:4 batch_done:158 Gen Loss: 5.72681 Disc Loss: 0.00968583 Q Losses: [0.02127442, 0.021352943]\n",
      "epoch:4 batch_done:159 Gen Loss: 6.36892 Disc Loss: 0.00613348 Q Losses: [0.0098096337, 0.020878846]\n",
      "epoch:4 batch_done:160 Gen Loss: 5.49746 Disc Loss: 0.0104754 Q Losses: [0.011190467, 0.019956436]\n",
      "epoch:4 batch_done:161 Gen Loss: 27.7089 Disc Loss: 0.630036 Q Losses: [0.0084523875, 0.020405803]\n",
      "epoch:4 batch_done:162 Gen Loss: 22.6601 Disc Loss: 2.82234 Q Losses: [0.0064171953, 0.019433837]\n",
      "epoch:4 batch_done:163 Gen Loss: 18.4728 Disc Loss: 0.0114323 Q Losses: [0.021813143, 0.031326495]\n",
      "epoch:4 batch_done:164 Gen Loss: 16.0699 Disc Loss: 0.003728 Q Losses: [0.015156864, 0.024413474]\n",
      "epoch:4 batch_done:165 Gen Loss: 13.4501 Disc Loss: 0.00290445 Q Losses: [0.012689477, 0.026982039]\n",
      "epoch:4 batch_done:166 Gen Loss: 9.93697 Disc Loss: 0.000308376 Q Losses: [0.012663731, 0.022034602]\n",
      "epoch:4 batch_done:167 Gen Loss: 5.75838 Disc Loss: 0.00349581 Q Losses: [0.010413354, 0.032651529]\n",
      "epoch:4 batch_done:168 Gen Loss: 5.51896 Disc Loss: 0.0355125 Q Losses: [0.0098674856, 0.027358182]\n",
      "epoch:4 batch_done:169 Gen Loss: 6.75267 Disc Loss: 0.00474276 Q Losses: [0.0069609657, 0.024052862]\n",
      "epoch:4 batch_done:170 Gen Loss: 5.91465 Disc Loss: 0.0101621 Q Losses: [0.0099254241, 0.023565246]\n",
      "epoch:4 batch_done:171 Gen Loss: 6.33325 Disc Loss: 0.00538152 Q Losses: [0.0085836099, 0.035359997]\n",
      "epoch:4 batch_done:172 Gen Loss: 5.57965 Disc Loss: 0.0139311 Q Losses: [0.0055188974, 0.027673323]\n",
      "epoch:4 batch_done:173 Gen Loss: 5.51658 Disc Loss: 0.0160243 Q Losses: [0.01535205, 0.021825319]\n",
      "epoch:4 batch_done:174 Gen Loss: 8.96783 Disc Loss: 0.101946 Q Losses: [0.011071708, 0.029710494]\n",
      "epoch:4 batch_done:175 Gen Loss: 10.2157 Disc Loss: 0.03243 Q Losses: [0.011397375, 0.036546499]\n",
      "epoch:4 batch_done:176 Gen Loss: 10.2308 Disc Loss: 0.0289938 Q Losses: [0.0098690819, 0.029969219]\n",
      "epoch:4 batch_done:177 Gen Loss: 8.8226 Disc Loss: 0.0397537 Q Losses: [0.021651518, 0.020637475]\n",
      "epoch:4 batch_done:178 Gen Loss: 9.49141 Disc Loss: 0.00581603 Q Losses: [0.0095058698, 0.021525618]\n",
      "epoch:4 batch_done:179 Gen Loss: 7.36371 Disc Loss: 0.00729205 Q Losses: [0.0096250409, 0.022218656]\n",
      "epoch:4 batch_done:180 Gen Loss: 5.96515 Disc Loss: 0.00353938 Q Losses: [0.0094373692, 0.020462545]\n",
      "epoch:4 batch_done:181 Gen Loss: 6.72527 Disc Loss: 0.00453019 Q Losses: [0.0091324672, 0.021442994]\n",
      "epoch:4 batch_done:182 Gen Loss: 5.45643 Disc Loss: 0.0283723 Q Losses: [0.0095411744, 0.029690534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:183 Gen Loss: 7.78839 Disc Loss: 0.00162957 Q Losses: [0.0070955013, 0.019519232]\n",
      "epoch:4 batch_done:184 Gen Loss: 5.73954 Disc Loss: 0.0111645 Q Losses: [0.0057784878, 0.024372235]\n",
      "epoch:4 batch_done:185 Gen Loss: 8.77344 Disc Loss: 0.00129605 Q Losses: [0.00990941, 0.019235702]\n",
      "epoch:4 batch_done:186 Gen Loss: 8.20339 Disc Loss: 0.00460885 Q Losses: [0.0099173803, 0.023422331]\n",
      "epoch:4 batch_done:187 Gen Loss: 8.75071 Disc Loss: 0.00124777 Q Losses: [0.0057239649, 0.02240607]\n",
      "epoch:4 batch_done:188 Gen Loss: 5.63912 Disc Loss: 0.0189898 Q Losses: [0.010041757, 0.020994339]\n",
      "epoch:4 batch_done:189 Gen Loss: 6.83961 Disc Loss: 0.00206195 Q Losses: [0.0086413594, 0.02030104]\n",
      "epoch:4 batch_done:190 Gen Loss: 7.15105 Disc Loss: 0.00160234 Q Losses: [0.006606292, 0.025249505]\n",
      "epoch:4 batch_done:191 Gen Loss: 6.7686 Disc Loss: 0.00208479 Q Losses: [0.011849069, 0.018434335]\n",
      "epoch:4 batch_done:192 Gen Loss: 7.76572 Disc Loss: 0.0791808 Q Losses: [0.0074263616, 0.020645805]\n",
      "epoch:4 batch_done:193 Gen Loss: 9.60564 Disc Loss: 0.00812702 Q Losses: [0.0073324679, 0.022620657]\n",
      "epoch:4 batch_done:194 Gen Loss: 10.8027 Disc Loss: 0.05813 Q Losses: [0.0080702482, 0.019755613]\n",
      "epoch:4 batch_done:195 Gen Loss: 11.992 Disc Loss: 0.0137669 Q Losses: [0.011550089, 0.016955076]\n",
      "epoch:4 batch_done:196 Gen Loss: 5.52768 Disc Loss: 0.00660879 Q Losses: [0.0060118008, 0.016955566]\n",
      "epoch:4 batch_done:197 Gen Loss: 5.26503 Disc Loss: 0.0119727 Q Losses: [0.0060437843, 0.016509563]\n",
      "epoch:4 batch_done:198 Gen Loss: 9.6641 Disc Loss: 0.00055201 Q Losses: [0.0073418454, 0.018499911]\n",
      "epoch:4 batch_done:199 Gen Loss: 14.6542 Disc Loss: 0.0108823 Q Losses: [0.0094673689, 0.018983027]\n",
      "epoch:4 batch_done:200 Gen Loss: 6.4313 Disc Loss: 0.00381041 Q Losses: [0.0084522441, 0.017808197]\n",
      "epoch:4 batch_done:201 Gen Loss: 11.0839 Disc Loss: 0.00482373 Q Losses: [0.0086341444, 0.024360111]\n",
      "epoch:4 batch_done:202 Gen Loss: 6.78348 Disc Loss: 0.00222122 Q Losses: [0.0079496689, 0.022079522]\n",
      "epoch:4 batch_done:203 Gen Loss: 5.52933 Disc Loss: 0.00729238 Q Losses: [0.0068586166, 0.020010885]\n",
      "epoch:4 batch_done:204 Gen Loss: 8.81582 Disc Loss: 0.000663751 Q Losses: [0.0075668287, 0.017829377]\n",
      "epoch:4 batch_done:205 Gen Loss: 5.47441 Disc Loss: 0.0141739 Q Losses: [0.00707391, 0.020870967]\n",
      "epoch:4 batch_done:206 Gen Loss: 10.0136 Disc Loss: 0.000812836 Q Losses: [0.0069859987, 0.017762704]\n",
      "epoch:4 batch_done:207 Gen Loss: 17.6864 Disc Loss: 0.247264 Q Losses: [0.014518404, 0.016548896]\n",
      "epoch:5 batch_done:1 Gen Loss: 17.0848 Disc Loss: 0.395346 Q Losses: [0.0093719382, 0.016389498]\n",
      "epoch:5 batch_done:2 Gen Loss: 17.7983 Disc Loss: 0.0110494 Q Losses: [0.013169955, 0.017288482]\n",
      "epoch:5 batch_done:3 Gen Loss: 16.5765 Disc Loss: 0.0183291 Q Losses: [0.0062793596, 0.017982438]\n",
      "epoch:5 batch_done:4 Gen Loss: 17.8273 Disc Loss: 0.000575873 Q Losses: [0.007440852, 0.01721004]\n",
      "epoch:5 batch_done:5 Gen Loss: 11.4996 Disc Loss: 0.000629221 Q Losses: [0.0095015569, 0.017311439]\n",
      "epoch:5 batch_done:6 Gen Loss: 10.1553 Disc Loss: 9.82995e-05 Q Losses: [0.0052654748, 0.01646862]\n",
      "epoch:5 batch_done:7 Gen Loss: 10.255 Disc Loss: 0.000134718 Q Losses: [0.0074197967, 0.022086795]\n",
      "epoch:5 batch_done:8 Gen Loss: 14.9574 Disc Loss: 0.000201527 Q Losses: [0.007407032, 0.015944444]\n",
      "epoch:5 batch_done:9 Gen Loss: 8.26636 Disc Loss: 0.000339782 Q Losses: [0.010004861, 0.016800918]\n",
      "epoch:5 batch_done:10 Gen Loss: 14.2293 Disc Loss: 0.000166501 Q Losses: [0.0072982456, 0.016752271]\n",
      "epoch:5 batch_done:11 Gen Loss: 7.18964 Disc Loss: 0.000866932 Q Losses: [0.0098825563, 0.025684422]\n",
      "epoch:5 batch_done:12 Gen Loss: 6.62999 Disc Loss: 0.00166274 Q Losses: [0.0066556055, 0.022221435]\n",
      "epoch:5 batch_done:13 Gen Loss: 8.99907 Disc Loss: 0.000352807 Q Losses: [0.0093783392, 0.01879124]\n",
      "epoch:5 batch_done:14 Gen Loss: 11.2724 Disc Loss: 0.000231337 Q Losses: [0.0075876308, 0.019380327]\n",
      "epoch:5 batch_done:15 Gen Loss: 7.32842 Disc Loss: 0.0585529 Q Losses: [0.025784615, 0.020319451]\n",
      "epoch:5 batch_done:16 Gen Loss: 8.95053 Disc Loss: 0.00836525 Q Losses: [0.010755789, 0.022403337]\n",
      "epoch:5 batch_done:17 Gen Loss: 16.832 Disc Loss: 0.000606263 Q Losses: [0.0069101155, 0.019635294]\n",
      "epoch:5 batch_done:18 Gen Loss: 7.24483 Disc Loss: 0.00514279 Q Losses: [0.013580183, 0.024913061]\n",
      "epoch:5 batch_done:19 Gen Loss: 9.54857 Disc Loss: 0.00296221 Q Losses: [0.016044265, 0.022727732]\n",
      "epoch:5 batch_done:20 Gen Loss: 12.0391 Disc Loss: 0.00250077 Q Losses: [0.0090683931, 0.0209108]\n",
      "epoch:5 batch_done:21 Gen Loss: 12.4972 Disc Loss: 0.148685 Q Losses: [0.0060427859, 0.021285076]\n",
      "epoch:5 batch_done:22 Gen Loss: 18.6727 Disc Loss: 0.0540877 Q Losses: [0.0073882071, 0.022849169]\n",
      "epoch:5 batch_done:23 Gen Loss: 24.4652 Disc Loss: 0.0711806 Q Losses: [0.0059880679, 0.015550758]\n",
      "epoch:5 batch_done:24 Gen Loss: 9.37688 Disc Loss: 0.138083 Q Losses: [0.0074318219, 0.017880017]\n",
      "epoch:5 batch_done:25 Gen Loss: 11.0746 Disc Loss: 0.00764773 Q Losses: [0.007391436, 0.020914404]\n",
      "epoch:5 batch_done:26 Gen Loss: 15.2068 Disc Loss: 7.36477e-05 Q Losses: [0.0096912254, 0.020531621]\n",
      "epoch:5 batch_done:27 Gen Loss: 10.3561 Disc Loss: 6.77039e-05 Q Losses: [0.0069569959, 0.024541711]\n",
      "epoch:5 batch_done:28 Gen Loss: 15.0713 Disc Loss: 0.000160667 Q Losses: [0.0052427119, 0.023704402]\n",
      "epoch:5 batch_done:29 Gen Loss: 5.89 Disc Loss: 0.0261464 Q Losses: [0.00824848, 0.02667387]\n",
      "epoch:5 batch_done:30 Gen Loss: 6.89603 Disc Loss: 0.00411528 Q Losses: [0.007825112, 0.020656139]\n",
      "epoch:5 batch_done:31 Gen Loss: 15.536 Disc Loss: 0.00015169 Q Losses: [0.0084407888, 0.02703445]\n",
      "epoch:5 batch_done:32 Gen Loss: 10.1075 Disc Loss: 0.000198782 Q Losses: [0.0080720764, 0.021288872]\n",
      "epoch:5 batch_done:33 Gen Loss: 13.6645 Disc Loss: 0.000132508 Q Losses: [0.0072038183, 0.017718744]\n",
      "epoch:5 batch_done:34 Gen Loss: 8.30587 Disc Loss: 0.000345725 Q Losses: [0.00968251, 0.01999101]\n",
      "epoch:5 batch_done:35 Gen Loss: 8.76337 Disc Loss: 0.000382173 Q Losses: [0.016129937, 0.025187319]\n",
      "epoch:5 batch_done:36 Gen Loss: 12.8577 Disc Loss: 0.000333063 Q Losses: [0.0086809751, 0.021265335]\n",
      "epoch:5 batch_done:37 Gen Loss: 10.3385 Disc Loss: 0.0982399 Q Losses: [0.0056053866, 0.016744506]\n",
      "epoch:5 batch_done:38 Gen Loss: 16.1574 Disc Loss: 0.00621917 Q Losses: [0.0074386159, 0.022608366]\n",
      "epoch:5 batch_done:39 Gen Loss: 18.4302 Disc Loss: 0.0081805 Q Losses: [0.0071056769, 0.017698253]\n",
      "epoch:5 batch_done:40 Gen Loss: 19.0986 Disc Loss: 0.0263006 Q Losses: [0.011886986, 0.020761995]\n",
      "epoch:5 batch_done:41 Gen Loss: 11.9833 Disc Loss: 0.00720972 Q Losses: [0.0066339364, 0.016758619]\n",
      "epoch:5 batch_done:42 Gen Loss: 10.7357 Disc Loss: 0.00255946 Q Losses: [0.008686021, 0.017961569]\n",
      "epoch:5 batch_done:43 Gen Loss: 12.2355 Disc Loss: 0.00268387 Q Losses: [0.0085285436, 0.017232846]\n",
      "epoch:5 batch_done:44 Gen Loss: 17.3891 Disc Loss: 0.0129112 Q Losses: [0.0097298473, 0.015951131]\n",
      "epoch:5 batch_done:45 Gen Loss: 18.2941 Disc Loss: 0.00356448 Q Losses: [0.0058288146, 0.016693162]\n",
      "epoch:5 batch_done:46 Gen Loss: 8.3571 Disc Loss: 0.00152288 Q Losses: [0.017035332, 0.021087456]\n",
      "epoch:5 batch_done:47 Gen Loss: 13.0074 Disc Loss: 0.000671818 Q Losses: [0.0063295197, 0.016591346]\n",
      "epoch:5 batch_done:48 Gen Loss: 12.6385 Disc Loss: 0.00379338 Q Losses: [0.0088179773, 0.019026555]\n",
      "epoch:5 batch_done:49 Gen Loss: 14.0421 Disc Loss: 0.00111496 Q Losses: [0.013775996, 0.023783552]\n",
      "epoch:5 batch_done:50 Gen Loss: 5.93005 Disc Loss: 0.00359339 Q Losses: [0.010596378, 0.017903438]\n",
      "epoch:5 batch_done:51 Gen Loss: 6.80424 Disc Loss: 0.00148811 Q Losses: [0.017660473, 0.021020168]\n",
      "epoch:5 batch_done:52 Gen Loss: 10.9131 Disc Loss: 0.00146366 Q Losses: [0.007234843, 0.018367689]\n",
      "epoch:5 batch_done:53 Gen Loss: 13.5635 Disc Loss: 0.000593109 Q Losses: [0.0075118714, 0.017452512]\n",
      "epoch:5 batch_done:54 Gen Loss: 7.46986 Disc Loss: 0.0648081 Q Losses: [0.01097461, 0.018845212]\n",
      "epoch:5 batch_done:55 Gen Loss: 10.1464 Disc Loss: 0.00347309 Q Losses: [0.0068836096, 0.018888695]\n",
      "epoch:5 batch_done:56 Gen Loss: 15.4653 Disc Loss: 0.00694957 Q Losses: [0.0069127763, 0.015345513]\n",
      "epoch:5 batch_done:57 Gen Loss: 8.76396 Disc Loss: 0.00951694 Q Losses: [0.012062182, 0.017326195]\n",
      "epoch:5 batch_done:58 Gen Loss: 9.70474 Disc Loss: 0.0306359 Q Losses: [0.013847536, 0.022667501]\n",
      "epoch:5 batch_done:59 Gen Loss: 8.79224 Disc Loss: 0.00204834 Q Losses: [0.010905267, 0.01768191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch_done:60 Gen Loss: 13.6084 Disc Loss: 0.00123196 Q Losses: [0.0067380704, 0.020698035]\n",
      "epoch:5 batch_done:61 Gen Loss: 5.69807 Disc Loss: 0.00457519 Q Losses: [0.0086970786, 0.017357297]\n",
      "epoch:5 batch_done:62 Gen Loss: 5.82027 Disc Loss: 0.00493651 Q Losses: [0.0070784241, 0.019928535]\n",
      "epoch:5 batch_done:63 Gen Loss: 13.381 Disc Loss: 0.00298774 Q Losses: [0.011967855, 0.017207177]\n",
      "epoch:5 batch_done:64 Gen Loss: 7.07787 Disc Loss: 0.0035744 Q Losses: [0.0063647423, 0.016099107]\n",
      "epoch:5 batch_done:65 Gen Loss: 6.0036 Disc Loss: 0.00373959 Q Losses: [0.0079981796, 0.016341668]\n",
      "epoch:5 batch_done:66 Gen Loss: 9.44152 Disc Loss: 0.00175394 Q Losses: [0.0074447361, 0.019250952]\n",
      "epoch:5 batch_done:67 Gen Loss: 5.35459 Disc Loss: 0.0145886 Q Losses: [0.012340895, 0.017316738]\n",
      "epoch:5 batch_done:68 Gen Loss: 7.23229 Disc Loss: 0.00184909 Q Losses: [0.005168003, 0.01812074]\n",
      "epoch:5 batch_done:69 Gen Loss: 8.35352 Disc Loss: 0.0293104 Q Losses: [0.010901919, 0.018447269]\n",
      "epoch:5 batch_done:70 Gen Loss: 9.52413 Disc Loss: 0.000810483 Q Losses: [0.0053529688, 0.019183643]\n",
      "epoch:5 batch_done:71 Gen Loss: 6.51939 Disc Loss: 0.0478531 Q Losses: [0.01320536, 0.018989952]\n",
      "epoch:5 batch_done:72 Gen Loss: 8.98806 Disc Loss: 0.00095008 Q Losses: [0.010414576, 0.020251261]\n",
      "epoch:5 batch_done:73 Gen Loss: 8.81628 Disc Loss: 0.00391216 Q Losses: [0.008686183, 0.017744027]\n",
      "epoch:5 batch_done:74 Gen Loss: 9.87169 Disc Loss: 0.00137813 Q Losses: [0.0080911675, 0.015613812]\n",
      "epoch:5 batch_done:75 Gen Loss: 9.44774 Disc Loss: 0.00327925 Q Losses: [0.0082472488, 0.017432939]\n",
      "epoch:5 batch_done:76 Gen Loss: 7.23498 Disc Loss: 0.00554838 Q Losses: [0.0068227313, 0.016005881]\n",
      "epoch:5 batch_done:77 Gen Loss: 7.74792 Disc Loss: 0.00101303 Q Losses: [0.0089756846, 0.013552006]\n",
      "epoch:5 batch_done:78 Gen Loss: 8.12811 Disc Loss: 0.000622892 Q Losses: [0.0074908263, 0.015541279]\n",
      "epoch:5 batch_done:79 Gen Loss: 5.45787 Disc Loss: 0.0143001 Q Losses: [0.0091876164, 0.018683933]\n",
      "epoch:5 batch_done:80 Gen Loss: 7.51803 Disc Loss: 0.00158988 Q Losses: [0.010594649, 0.017798368]\n",
      "epoch:5 batch_done:81 Gen Loss: 7.33244 Disc Loss: 0.00188919 Q Losses: [0.010910746, 0.021123931]\n",
      "epoch:5 batch_done:82 Gen Loss: 10.8637 Disc Loss: 0.00154954 Q Losses: [0.0096278321, 0.017482098]\n",
      "epoch:5 batch_done:83 Gen Loss: 6.82936 Disc Loss: 0.00454259 Q Losses: [0.0082065668, 0.021083796]\n",
      "epoch:5 batch_done:84 Gen Loss: 9.33388 Disc Loss: 0.00255024 Q Losses: [0.016401414, 0.014568476]\n",
      "epoch:5 batch_done:85 Gen Loss: 7.65145 Disc Loss: 0.000891742 Q Losses: [0.01091085, 0.013178116]\n",
      "epoch:5 batch_done:86 Gen Loss: 7.79674 Disc Loss: 0.00267801 Q Losses: [0.0087393932, 0.015942167]\n",
      "epoch:5 batch_done:87 Gen Loss: 5.6052 Disc Loss: 0.0541369 Q Losses: [0.022254352, 0.016199905]\n",
      "epoch:5 batch_done:88 Gen Loss: 7.08676 Disc Loss: 0.00441433 Q Losses: [0.0071058106, 0.018634778]\n",
      "epoch:5 batch_done:89 Gen Loss: 9.0401 Disc Loss: 0.0254696 Q Losses: [0.010699405, 0.017563637]\n",
      "epoch:5 batch_done:90 Gen Loss: 14.9737 Disc Loss: 0.000128367 Q Losses: [0.01344634, 0.015955716]\n",
      "epoch:5 batch_done:91 Gen Loss: 5.53388 Disc Loss: 0.0188163 Q Losses: [0.0061916937, 0.020711768]\n",
      "epoch:5 batch_done:92 Gen Loss: 6.55031 Disc Loss: 0.00567866 Q Losses: [0.0092892172, 0.015901517]\n",
      "epoch:5 batch_done:93 Gen Loss: 13.1435 Disc Loss: 0.00105126 Q Losses: [0.010775521, 0.019737544]\n",
      "epoch:5 batch_done:94 Gen Loss: 6.42988 Disc Loss: 0.00345237 Q Losses: [0.012536801, 0.015358645]\n",
      "epoch:5 batch_done:95 Gen Loss: 9.05473 Disc Loss: 0.000669568 Q Losses: [0.010260776, 0.016575012]\n",
      "epoch:5 batch_done:96 Gen Loss: 16.2015 Disc Loss: 0.000498915 Q Losses: [0.0098703969, 0.01885242]\n",
      "epoch:5 batch_done:97 Gen Loss: 5.59891 Disc Loss: 0.00727345 Q Losses: [0.0098268278, 0.017601933]\n",
      "epoch:5 batch_done:98 Gen Loss: 7.46283 Disc Loss: 0.00173673 Q Losses: [0.0071039726, 0.018627156]\n",
      "epoch:5 batch_done:99 Gen Loss: 16.5439 Disc Loss: 0.000581331 Q Losses: [0.011673404, 0.023475558]\n",
      "epoch:5 batch_done:100 Gen Loss: 6.29246 Disc Loss: 0.00452616 Q Losses: [0.0085827718, 0.016741741]\n",
      "epoch:5 batch_done:101 Gen Loss: 5.43649 Disc Loss: 0.0113177 Q Losses: [0.0073839175, 0.019082736]\n",
      "epoch:5 batch_done:102 Gen Loss: 5.95313 Disc Loss: 0.00623979 Q Losses: [0.0072670132, 0.020780101]\n",
      "epoch:5 batch_done:103 Gen Loss: 11.7061 Disc Loss: 0.00347331 Q Losses: [0.013197714, 0.016739603]\n",
      "epoch:5 batch_done:104 Gen Loss: 17.2039 Disc Loss: 0.00107498 Q Losses: [0.0087616462, 0.030463941]\n",
      "epoch:5 batch_done:105 Gen Loss: 7.72257 Disc Loss: 0.00741858 Q Losses: [0.01008213, 0.017183578]\n",
      "epoch:5 batch_done:106 Gen Loss: 9.44943 Disc Loss: 0.00269925 Q Losses: [0.0057919184, 0.01922692]\n",
      "epoch:5 batch_done:107 Gen Loss: 11.967 Disc Loss: 0.00310645 Q Losses: [0.0093256906, 0.02394644]\n",
      "epoch:5 batch_done:108 Gen Loss: 6.15376 Disc Loss: 0.00258502 Q Losses: [0.0073539736, 0.019573437]\n",
      "epoch:5 batch_done:109 Gen Loss: 5.86388 Disc Loss: 0.00450531 Q Losses: [0.010251901, 0.018957505]\n",
      "epoch:5 batch_done:110 Gen Loss: 10.2302 Disc Loss: 0.00038542 Q Losses: [0.027076285, 0.017584743]\n",
      "epoch:5 batch_done:111 Gen Loss: 5.81296 Disc Loss: 0.017812 Q Losses: [0.0049342951, 0.020914227]\n",
      "epoch:5 batch_done:112 Gen Loss: 6.65991 Disc Loss: 0.00563053 Q Losses: [0.0078203855, 0.018887278]\n",
      "epoch:5 batch_done:113 Gen Loss: 6.40672 Disc Loss: 0.00466896 Q Losses: [0.0072915046, 0.02334868]\n",
      "epoch:5 batch_done:114 Gen Loss: 10.8897 Disc Loss: 0.00266618 Q Losses: [0.010912884, 0.021355141]\n",
      "epoch:5 batch_done:115 Gen Loss: 8.86286 Disc Loss: 0.00513154 Q Losses: [0.0097828582, 0.021115864]\n",
      "epoch:5 batch_done:116 Gen Loss: 10.6114 Disc Loss: 0.00333777 Q Losses: [0.0079746544, 0.020273324]\n",
      "epoch:5 batch_done:117 Gen Loss: 9.83094 Disc Loss: 0.00170889 Q Losses: [0.010346184, 0.022788003]\n",
      "epoch:5 batch_done:118 Gen Loss: 10.0655 Disc Loss: 0.000757444 Q Losses: [0.018311422, 0.022711694]\n",
      "epoch:5 batch_done:119 Gen Loss: 7.67677 Disc Loss: 0.0010591 Q Losses: [0.0095439581, 0.022692978]\n",
      "epoch:5 batch_done:120 Gen Loss: 8.9681 Disc Loss: 0.000754471 Q Losses: [0.011432603, 0.016713025]\n",
      "epoch:5 batch_done:121 Gen Loss: 8.77125 Disc Loss: 0.000504754 Q Losses: [0.0098633775, 0.023006674]\n",
      "epoch:5 batch_done:122 Gen Loss: 5.41015 Disc Loss: 0.0118181 Q Losses: [0.0072632208, 0.0150567]\n",
      "epoch:5 batch_done:123 Gen Loss: 5.87701 Disc Loss: 0.0141775 Q Losses: [0.011025673, 0.018329699]\n",
      "epoch:5 batch_done:124 Gen Loss: 6.97059 Disc Loss: 0.00331465 Q Losses: [0.011595499, 0.016703166]\n",
      "epoch:5 batch_done:125 Gen Loss: 8.98077 Disc Loss: 0.00407152 Q Losses: [0.017786065, 0.019533148]\n",
      "epoch:5 batch_done:126 Gen Loss: 5.83869 Disc Loss: 0.00923735 Q Losses: [0.0090123527, 0.015454124]\n",
      "epoch:5 batch_done:127 Gen Loss: 7.39119 Disc Loss: 0.00538059 Q Losses: [0.013224921, 0.02571442]\n",
      "epoch:5 batch_done:128 Gen Loss: 5.45711 Disc Loss: 0.00793019 Q Losses: [0.010165813, 0.015106055]\n",
      "epoch:5 batch_done:129 Gen Loss: 8.20339 Disc Loss: 0.00110284 Q Losses: [0.0056451424, 0.020273849]\n",
      "epoch:5 batch_done:130 Gen Loss: 6.64275 Disc Loss: 0.00234935 Q Losses: [0.0077522737, 0.016931653]\n",
      "epoch:5 batch_done:131 Gen Loss: 7.55966 Disc Loss: 0.00400785 Q Losses: [0.0093994373, 0.01720598]\n",
      "epoch:5 batch_done:132 Gen Loss: 9.55824 Disc Loss: 0.00254247 Q Losses: [0.0087169204, 0.022494717]\n",
      "epoch:5 batch_done:133 Gen Loss: 9.52857 Disc Loss: 0.000259194 Q Losses: [0.0084312046, 0.0173901]\n",
      "epoch:5 batch_done:134 Gen Loss: 6.24526 Disc Loss: 0.00252281 Q Losses: [0.00936714, 0.017467462]\n",
      "epoch:5 batch_done:135 Gen Loss: 5.83957 Disc Loss: 0.0191756 Q Losses: [0.0083274674, 0.018100332]\n",
      "epoch:5 batch_done:136 Gen Loss: 6.54305 Disc Loss: 0.00721624 Q Losses: [0.012150292, 0.027593693]\n",
      "epoch:5 batch_done:137 Gen Loss: 8.1374 Disc Loss: 0.00337823 Q Losses: [0.0075729638, 0.024292046]\n",
      "epoch:5 batch_done:138 Gen Loss: 13.6745 Disc Loss: 0.00347234 Q Losses: [0.0083280168, 0.018909547]\n",
      "epoch:5 batch_done:139 Gen Loss: 6.38032 Disc Loss: 0.00760178 Q Losses: [0.010941228, 0.017764885]\n",
      "epoch:5 batch_done:140 Gen Loss: 9.68713 Disc Loss: 0.00152844 Q Losses: [0.010299584, 0.014958795]\n",
      "epoch:5 batch_done:141 Gen Loss: 10.6726 Disc Loss: 0.000709353 Q Losses: [0.012305448, 0.01522465]\n",
      "epoch:5 batch_done:142 Gen Loss: 6.69104 Disc Loss: 0.00449433 Q Losses: [0.014849965, 0.018441617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch_done:143 Gen Loss: 7.13853 Disc Loss: 0.0088087 Q Losses: [0.0095955208, 0.019568106]\n",
      "epoch:5 batch_done:144 Gen Loss: 13.312 Disc Loss: 0.00853293 Q Losses: [0.011244943, 0.014587091]\n",
      "epoch:5 batch_done:145 Gen Loss: 8.37306 Disc Loss: 0.000754534 Q Losses: [0.017038053, 0.025875635]\n",
      "epoch:5 batch_done:146 Gen Loss: 5.46074 Disc Loss: 0.0111757 Q Losses: [0.0095782876, 0.013818809]\n",
      "epoch:5 batch_done:147 Gen Loss: 5.65843 Disc Loss: 0.0167896 Q Losses: [0.016278084, 0.024134301]\n",
      "epoch:5 batch_done:148 Gen Loss: 8.72933 Disc Loss: 0.0039502 Q Losses: [0.0065157493, 0.01664876]\n",
      "epoch:5 batch_done:149 Gen Loss: 9.50599 Disc Loss: 0.000845021 Q Losses: [0.0081022866, 0.016146813]\n",
      "epoch:5 batch_done:150 Gen Loss: 11.9626 Disc Loss: 0.000150384 Q Losses: [0.01017311, 0.017745493]\n",
      "epoch:5 batch_done:151 Gen Loss: 5.70494 Disc Loss: 0.0118525 Q Losses: [0.0089430306, 0.020290937]\n",
      "epoch:5 batch_done:152 Gen Loss: 8.005 Disc Loss: 0.00126297 Q Losses: [0.0203867, 0.015004116]\n",
      "epoch:5 batch_done:153 Gen Loss: 9.64776 Disc Loss: 0.00118012 Q Losses: [0.019282643, 0.018029392]\n",
      "epoch:5 batch_done:154 Gen Loss: 8.14387 Disc Loss: 0.00116921 Q Losses: [0.0094986307, 0.026942756]\n",
      "epoch:5 batch_done:155 Gen Loss: 9.91842 Disc Loss: 0.000724103 Q Losses: [0.0077973455, 0.016466513]\n",
      "epoch:5 batch_done:156 Gen Loss: 5.8066 Disc Loss: 0.0115637 Q Losses: [0.010262913, 0.023175837]\n",
      "epoch:5 batch_done:157 Gen Loss: 6.82268 Disc Loss: 0.00341755 Q Losses: [0.0060852612, 0.015922999]\n",
      "epoch:5 batch_done:158 Gen Loss: 11.1941 Disc Loss: 0.00201556 Q Losses: [0.0074569387, 0.018013339]\n",
      "epoch:5 batch_done:159 Gen Loss: 8.90328 Disc Loss: 0.0012666 Q Losses: [0.0088881124, 0.016220473]\n",
      "epoch:5 batch_done:160 Gen Loss: 8.59999 Disc Loss: 0.00256076 Q Losses: [0.0071720118, 0.017918941]\n",
      "epoch:5 batch_done:161 Gen Loss: 5.74034 Disc Loss: 0.0145621 Q Losses: [0.0080092838, 0.017411921]\n",
      "epoch:5 batch_done:162 Gen Loss: 7.49837 Disc Loss: 0.00181886 Q Losses: [0.0095451046, 0.015292458]\n",
      "epoch:5 batch_done:163 Gen Loss: 6.16597 Disc Loss: 0.00256747 Q Losses: [0.0075068162, 0.017277803]\n",
      "epoch:5 batch_done:164 Gen Loss: 10.4266 Disc Loss: 0.000445889 Q Losses: [0.0071871481, 0.021256043]\n",
      "epoch:5 batch_done:165 Gen Loss: 5.96307 Disc Loss: 0.00440535 Q Losses: [0.0058443979, 0.02203761]\n",
      "epoch:5 batch_done:166 Gen Loss: 6.36918 Disc Loss: 0.00611287 Q Losses: [0.0051634093, 0.014324018]\n",
      "epoch:5 batch_done:167 Gen Loss: 7.77585 Disc Loss: 0.000622831 Q Losses: [0.012888338, 0.015633877]\n",
      "epoch:5 batch_done:168 Gen Loss: 9.79269 Disc Loss: 0.000350869 Q Losses: [0.016345279, 0.014673267]\n",
      "epoch:5 batch_done:169 Gen Loss: 6.7635 Disc Loss: 0.00167756 Q Losses: [0.0050898609, 0.01806804]\n",
      "epoch:5 batch_done:170 Gen Loss: 7.26223 Disc Loss: 0.00108081 Q Losses: [0.0061427848, 0.016443085]\n",
      "epoch:5 batch_done:171 Gen Loss: 5.83131 Disc Loss: 0.0104908 Q Losses: [0.0054308604, 0.01980374]\n",
      "epoch:5 batch_done:172 Gen Loss: 6.79738 Disc Loss: 0.00348512 Q Losses: [0.007152752, 0.01488612]\n",
      "epoch:5 batch_done:173 Gen Loss: 6.67685 Disc Loss: 0.00397185 Q Losses: [0.0083515234, 0.015684266]\n",
      "epoch:5 batch_done:174 Gen Loss: 8.00823 Disc Loss: 0.000888588 Q Losses: [0.0093590412, 0.015400121]\n",
      "epoch:5 batch_done:175 Gen Loss: 5.2635 Disc Loss: 0.015521 Q Losses: [0.0067121638, 0.013519929]\n",
      "epoch:5 batch_done:176 Gen Loss: 5.93194 Disc Loss: 0.00778184 Q Losses: [0.0078103985, 0.012724852]\n",
      "epoch:5 batch_done:177 Gen Loss: 6.46999 Disc Loss: 0.00473126 Q Losses: [0.010908709, 0.024912575]\n",
      "epoch:5 batch_done:178 Gen Loss: 7.98733 Disc Loss: 0.0023158 Q Losses: [0.0082648033, 0.031570319]\n",
      "epoch:5 batch_done:179 Gen Loss: 7.89355 Disc Loss: 0.0738746 Q Losses: [0.0088604167, 0.020301696]\n",
      "epoch:5 batch_done:180 Gen Loss: 4.55035 Disc Loss: 0.00150584 Q Losses: [0.011473665, 0.017990345]\n",
      "epoch:5 batch_done:181 Gen Loss: 8.43652 Disc Loss: 0.0447646 Q Losses: [0.0069782548, 0.026009675]\n",
      "epoch:5 batch_done:182 Gen Loss: 11.4409 Disc Loss: 0.00017956 Q Losses: [0.0084484713, 0.035451405]\n",
      "epoch:5 batch_done:183 Gen Loss: 12.8289 Disc Loss: 0.0303839 Q Losses: [0.011397971, 0.091678895]\n",
      "epoch:5 batch_done:184 Gen Loss: 14.2737 Disc Loss: 0.0260119 Q Losses: [0.0074559003, 0.059212856]\n",
      "epoch:5 batch_done:185 Gen Loss: 11.2616 Disc Loss: 0.00157067 Q Losses: [0.014713259, 0.059212562]\n",
      "epoch:5 batch_done:186 Gen Loss: 13.8301 Disc Loss: 0.0010869 Q Losses: [0.022497768, 0.14555947]\n",
      "epoch:5 batch_done:187 Gen Loss: 12.5807 Disc Loss: 0.0155596 Q Losses: [0.015950657, 0.24459083]\n",
      "epoch:5 batch_done:188 Gen Loss: 14.8395 Disc Loss: 0.00473377 Q Losses: [0.024029218, 0.45941621]\n",
      "epoch:5 batch_done:189 Gen Loss: 1.32699 Disc Loss: 0.356278 Q Losses: [0.030950049, 0.7505455]\n",
      "epoch:5 batch_done:190 Gen Loss: 23.5484 Disc Loss: 8.93921e-05 Q Losses: [0.029815476, 0.73877478]\n",
      "epoch:5 batch_done:191 Gen Loss: 20.1999 Disc Loss: 2.14951e-06 Q Losses: [0.049059875, 0.64862669]\n",
      "epoch:5 batch_done:192 Gen Loss: 21.506 Disc Loss: 8.17703e-07 Q Losses: [0.047647946, 0.66943085]\n",
      "epoch:5 batch_done:193 Gen Loss: 26.8318 Disc Loss: 8.99923e-05 Q Losses: [0.046346128, 1.1529002]\n",
      "epoch:5 batch_done:194 Gen Loss: 30.9853 Disc Loss: 0.000734525 Q Losses: [0.056161519, 0.51307464]\n",
      "epoch:5 batch_done:195 Gen Loss: 27.4408 Disc Loss: 0.009205 Q Losses: [0.033069775, 0.47377068]\n",
      "epoch:5 batch_done:196 Gen Loss: 18.3354 Disc Loss: 0.057432 Q Losses: [0.041621141, 0.54098201]\n",
      "epoch:5 batch_done:197 Gen Loss: 6.24026 Disc Loss: 0.00477039 Q Losses: [0.028000098, 0.26915288]\n",
      "epoch:5 batch_done:198 Gen Loss: 50.2824 Disc Loss: 9.08167 Q Losses: [0.025730569, 0.25170738]\n",
      "epoch:5 batch_done:199 Gen Loss: 59.1709 Disc Loss: 0.0184566 Q Losses: [0.036538266, 0.27763164]\n",
      "epoch:5 batch_done:200 Gen Loss: 57.1547 Disc Loss: 0.161571 Q Losses: [0.026441393, 0.14027615]\n",
      "epoch:5 batch_done:201 Gen Loss: 55.3033 Disc Loss: 0.0123742 Q Losses: [0.030001931, 0.10008661]\n",
      "epoch:5 batch_done:202 Gen Loss: 53.5324 Disc Loss: 0.00876149 Q Losses: [0.030650184, 0.095382661]\n",
      "epoch:5 batch_done:203 Gen Loss: 49.948 Disc Loss: 0.00160719 Q Losses: [0.066047922, 0.06435436]\n",
      "epoch:5 batch_done:204 Gen Loss: 48.2596 Disc Loss: 0.000548953 Q Losses: [0.040228542, 0.062284816]\n",
      "epoch:5 batch_done:205 Gen Loss: 46.4624 Disc Loss: 0.00394822 Q Losses: [0.031914227, 0.074083611]\n",
      "epoch:5 batch_done:206 Gen Loss: 44.5616 Disc Loss: 0.000349338 Q Losses: [0.01174066, 0.068411134]\n",
      "epoch:5 batch_done:207 Gen Loss: 42.9681 Disc Loss: 0.000348495 Q Losses: [0.016974762, 0.052986242]\n",
      "epoch:6 batch_done:1 Gen Loss: 41.446 Disc Loss: 2.21194e-05 Q Losses: [0.0095375134, 0.11261819]\n",
      "epoch:6 batch_done:2 Gen Loss: 42.5551 Disc Loss: 0.00158235 Q Losses: [0.025431059, 0.17137724]\n",
      "epoch:6 batch_done:3 Gen Loss: 42.7112 Disc Loss: 0.00199795 Q Losses: [0.022612957, 0.046575647]\n",
      "epoch:6 batch_done:4 Gen Loss: 42.8829 Disc Loss: 0.000839937 Q Losses: [0.013425639, 0.031678759]\n",
      "epoch:6 batch_done:5 Gen Loss: 42.3421 Disc Loss: 6.98869e-05 Q Losses: [0.01562552, 0.037257388]\n",
      "epoch:6 batch_done:6 Gen Loss: 41.4065 Disc Loss: 8.61146e-05 Q Losses: [0.011979314, 0.032222353]\n",
      "epoch:6 batch_done:7 Gen Loss: 40.1157 Disc Loss: 0.000110419 Q Losses: [0.009932545, 0.031531982]\n",
      "epoch:6 batch_done:8 Gen Loss: 38.8646 Disc Loss: 0.0043246 Q Losses: [0.011347394, 0.041167893]\n",
      "epoch:6 batch_done:9 Gen Loss: 37.881 Disc Loss: 0.000200812 Q Losses: [0.0095535126, 0.030329511]\n",
      "epoch:6 batch_done:10 Gen Loss: 36.7879 Disc Loss: 6.67512e-05 Q Losses: [0.013804005, 0.031044509]\n",
      "epoch:6 batch_done:11 Gen Loss: 36.0298 Disc Loss: 0.000141414 Q Losses: [0.0069285552, 0.02746927]\n",
      "epoch:6 batch_done:12 Gen Loss: 35.2813 Disc Loss: 0.000520642 Q Losses: [0.015632913, 0.022579802]\n",
      "epoch:6 batch_done:13 Gen Loss: 34.8273 Disc Loss: 3.06341e-05 Q Losses: [0.0062034731, 0.016027147]\n",
      "epoch:6 batch_done:14 Gen Loss: 34.0671 Disc Loss: 0.000249895 Q Losses: [0.011320651, 0.017037721]\n",
      "epoch:6 batch_done:15 Gen Loss: 33.2736 Disc Loss: 5.16424e-05 Q Losses: [0.011011476, 0.02084725]\n",
      "epoch:6 batch_done:16 Gen Loss: 32.6947 Disc Loss: 4.75182e-05 Q Losses: [0.0068491381, 0.017913738]\n",
      "epoch:6 batch_done:17 Gen Loss: 32.0034 Disc Loss: 0.000225947 Q Losses: [0.0066530211, 0.026087988]\n",
      "epoch:6 batch_done:18 Gen Loss: 31.1002 Disc Loss: 0.000256275 Q Losses: [0.0081425998, 0.016760234]\n",
      "epoch:6 batch_done:19 Gen Loss: 30.0756 Disc Loss: 0.000241672 Q Losses: [0.0080380263, 0.025896829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:20 Gen Loss: 29.1 Disc Loss: 8.93226e-05 Q Losses: [0.012232895, 0.018113147]\n",
      "epoch:6 batch_done:21 Gen Loss: 28.7174 Disc Loss: 0.00137056 Q Losses: [0.016407676, 0.015048252]\n",
      "epoch:6 batch_done:22 Gen Loss: 28.0452 Disc Loss: 1.71807e-05 Q Losses: [0.0088457363, 0.013746204]\n",
      "epoch:6 batch_done:23 Gen Loss: 27.6241 Disc Loss: 0.000304606 Q Losses: [0.010477668, 0.015135823]\n",
      "epoch:6 batch_done:24 Gen Loss: 27.1857 Disc Loss: 0.000173063 Q Losses: [0.010930494, 0.019840453]\n",
      "epoch:6 batch_done:25 Gen Loss: 26.6208 Disc Loss: 0.000303834 Q Losses: [0.0085173426, 0.017941888]\n",
      "epoch:6 batch_done:26 Gen Loss: 26.2798 Disc Loss: 7.62577e-05 Q Losses: [0.0065372055, 0.01241046]\n",
      "epoch:6 batch_done:27 Gen Loss: 25.6355 Disc Loss: 0.000236632 Q Losses: [0.0086453557, 0.012576444]\n",
      "epoch:6 batch_done:28 Gen Loss: 24.9811 Disc Loss: 0.000202498 Q Losses: [0.0070797377, 0.015018752]\n",
      "epoch:6 batch_done:29 Gen Loss: 24.6244 Disc Loss: 5.17852e-05 Q Losses: [0.008406478, 0.016087826]\n",
      "epoch:6 batch_done:30 Gen Loss: 24.0058 Disc Loss: 0.000715074 Q Losses: [0.01756583, 0.012510055]\n",
      "epoch:6 batch_done:31 Gen Loss: 22.7619 Disc Loss: 1.02488e-05 Q Losses: [0.014338144, 0.022985261]\n",
      "epoch:6 batch_done:32 Gen Loss: 21.9876 Disc Loss: 0.00228019 Q Losses: [0.011074018, 0.015830781]\n",
      "epoch:6 batch_done:33 Gen Loss: 21.6126 Disc Loss: 1.4111e-05 Q Losses: [0.013825431, 0.012852191]\n",
      "epoch:6 batch_done:34 Gen Loss: 21.1383 Disc Loss: 0.000276466 Q Losses: [0.0073944144, 0.014342122]\n",
      "epoch:6 batch_done:35 Gen Loss: 20.7417 Disc Loss: 0.000299089 Q Losses: [0.011132671, 0.01658465]\n",
      "epoch:6 batch_done:36 Gen Loss: 19.9761 Disc Loss: 0.000913473 Q Losses: [0.0060657775, 0.013077907]\n",
      "epoch:6 batch_done:37 Gen Loss: 19.4117 Disc Loss: 0.000372829 Q Losses: [0.0046862364, 0.015429186]\n",
      "epoch:6 batch_done:38 Gen Loss: 18.6407 Disc Loss: 1.49717e-05 Q Losses: [0.013242902, 0.018772572]\n",
      "epoch:6 batch_done:39 Gen Loss: 17.5446 Disc Loss: 0.000199731 Q Losses: [0.0072226175, 0.012429805]\n",
      "epoch:6 batch_done:40 Gen Loss: 16.3972 Disc Loss: 0.000801189 Q Losses: [0.018089164, 0.012237658]\n",
      "epoch:6 batch_done:41 Gen Loss: 15.0077 Disc Loss: 0.00634124 Q Losses: [0.014416826, 0.014699424]\n",
      "epoch:6 batch_done:42 Gen Loss: 13.8338 Disc Loss: 3.75448e-05 Q Losses: [0.0094641969, 0.012866552]\n",
      "epoch:6 batch_done:43 Gen Loss: 12.1536 Disc Loss: 8.82798e-05 Q Losses: [0.007918423, 0.011868292]\n",
      "epoch:6 batch_done:44 Gen Loss: 10.3174 Disc Loss: 0.000411914 Q Losses: [0.0079288017, 0.0121883]\n",
      "epoch:6 batch_done:45 Gen Loss: 8.86026 Disc Loss: 0.000533534 Q Losses: [0.029880617, 0.011855388]\n",
      "epoch:6 batch_done:46 Gen Loss: 6.46711 Disc Loss: 0.00966043 Q Losses: [0.026370775, 0.01812974]\n",
      "epoch:6 batch_done:47 Gen Loss: 6.25457 Disc Loss: 0.0146459 Q Losses: [0.019812619, 0.014570249]\n",
      "epoch:6 batch_done:48 Gen Loss: 6.89169 Disc Loss: 0.00655968 Q Losses: [0.012384107, 0.011852554]\n",
      "epoch:6 batch_done:49 Gen Loss: 7.18431 Disc Loss: 0.00307325 Q Losses: [0.014978483, 0.012854873]\n",
      "epoch:6 batch_done:50 Gen Loss: 7.32747 Disc Loss: 0.0028786 Q Losses: [0.033713441, 0.0101768]\n",
      "epoch:6 batch_done:51 Gen Loss: 7.12077 Disc Loss: 0.00166031 Q Losses: [0.027533289, 0.010845399]\n",
      "epoch:6 batch_done:52 Gen Loss: 6.72938 Disc Loss: 0.00227703 Q Losses: [0.014348264, 0.012271737]\n",
      "epoch:6 batch_done:53 Gen Loss: 6.44587 Disc Loss: 0.00357722 Q Losses: [0.016382491, 0.015701335]\n",
      "epoch:6 batch_done:54 Gen Loss: 6.67249 Disc Loss: 0.00277708 Q Losses: [0.0094769653, 0.011399869]\n",
      "epoch:6 batch_done:55 Gen Loss: 6.62419 Disc Loss: 0.0112762 Q Losses: [0.008199038, 0.012079102]\n",
      "epoch:6 batch_done:56 Gen Loss: 6.45106 Disc Loss: 0.00379016 Q Losses: [0.01091486, 0.012920504]\n",
      "epoch:6 batch_done:57 Gen Loss: 6.70424 Disc Loss: 0.00232328 Q Losses: [0.0078616478, 0.011167286]\n",
      "epoch:6 batch_done:58 Gen Loss: 6.75377 Disc Loss: 0.00218854 Q Losses: [0.011204059, 0.011841636]\n",
      "epoch:6 batch_done:59 Gen Loss: 6.63579 Disc Loss: 0.00254335 Q Losses: [0.0058710305, 0.010223589]\n",
      "epoch:6 batch_done:60 Gen Loss: 6.33711 Disc Loss: 0.0048132 Q Losses: [0.011976143, 0.012267844]\n",
      "epoch:6 batch_done:61 Gen Loss: 6.49333 Disc Loss: 0.00923762 Q Losses: [0.0053395978, 0.010189512]\n",
      "epoch:6 batch_done:62 Gen Loss: 6.8952 Disc Loss: 0.00608959 Q Losses: [0.0043433709, 0.010683456]\n",
      "epoch:6 batch_done:63 Gen Loss: 6.62358 Disc Loss: 0.0284823 Q Losses: [0.0086983591, 0.0093132295]\n",
      "epoch:6 batch_done:64 Gen Loss: 6.6554 Disc Loss: 0.00483865 Q Losses: [0.012747134, 0.011474522]\n",
      "epoch:6 batch_done:65 Gen Loss: 6.7286 Disc Loss: 0.00443953 Q Losses: [0.006308605, 0.0094496403]\n",
      "epoch:6 batch_done:66 Gen Loss: 6.64652 Disc Loss: 0.005982 Q Losses: [0.0062031941, 0.015126352]\n",
      "epoch:6 batch_done:67 Gen Loss: 6.73252 Disc Loss: 0.00524449 Q Losses: [0.0054199975, 0.012703143]\n",
      "epoch:6 batch_done:68 Gen Loss: 6.74789 Disc Loss: 0.00591417 Q Losses: [0.006487851, 0.01175428]\n",
      "epoch:6 batch_done:69 Gen Loss: 6.5503 Disc Loss: 0.0106536 Q Losses: [0.0052128225, 0.010292347]\n",
      "epoch:6 batch_done:70 Gen Loss: 6.66599 Disc Loss: 0.00423235 Q Losses: [0.0056918403, 0.010934295]\n",
      "epoch:6 batch_done:71 Gen Loss: 6.65452 Disc Loss: 0.00523316 Q Losses: [0.0060804216, 0.010501904]\n",
      "epoch:6 batch_done:72 Gen Loss: 6.65963 Disc Loss: 0.00504029 Q Losses: [0.011428381, 0.013624899]\n",
      "epoch:6 batch_done:73 Gen Loss: 5.96212 Disc Loss: 0.0174242 Q Losses: [0.015232734, 0.013743223]\n",
      "epoch:6 batch_done:74 Gen Loss: 7.36248 Disc Loss: 0.0234229 Q Losses: [0.0066008223, 0.0094286464]\n",
      "epoch:6 batch_done:75 Gen Loss: 7.37383 Disc Loss: 0.0148788 Q Losses: [0.0060194251, 0.012152722]\n",
      "epoch:6 batch_done:76 Gen Loss: 7.07384 Disc Loss: 0.00774203 Q Losses: [0.0069392333, 0.0093485592]\n",
      "epoch:6 batch_done:77 Gen Loss: 7.12279 Disc Loss: 0.009539 Q Losses: [0.0045555988, 0.011597013]\n",
      "epoch:6 batch_done:78 Gen Loss: 6.65933 Disc Loss: 0.0126791 Q Losses: [0.012958352, 0.0098632313]\n",
      "epoch:6 batch_done:79 Gen Loss: 5.50419 Disc Loss: 0.0337522 Q Losses: [0.0055322312, 0.01078134]\n",
      "epoch:6 batch_done:80 Gen Loss: 6.82238 Disc Loss: 0.0138794 Q Losses: [0.015825706, 0.010483559]\n",
      "epoch:6 batch_done:81 Gen Loss: 7.06053 Disc Loss: 0.00845369 Q Losses: [0.0062986771, 0.011160625]\n",
      "epoch:6 batch_done:82 Gen Loss: 6.70893 Disc Loss: 0.00573802 Q Losses: [0.006117932, 0.0097594317]\n",
      "epoch:6 batch_done:83 Gen Loss: 5.99155 Disc Loss: 0.00992491 Q Losses: [0.0065455027, 0.011548955]\n",
      "epoch:6 batch_done:84 Gen Loss: 5.10351 Disc Loss: 0.028677 Q Losses: [0.012020424, 0.01433729]\n",
      "epoch:6 batch_done:85 Gen Loss: 10.4163 Disc Loss: 0.0426224 Q Losses: [0.008425599, 0.010885684]\n",
      "epoch:6 batch_done:86 Gen Loss: 10.804 Disc Loss: 0.0118313 Q Losses: [0.010115569, 0.012640389]\n",
      "epoch:6 batch_done:87 Gen Loss: 7.91959 Disc Loss: 0.0030832 Q Losses: [0.006611356, 0.0098618055]\n",
      "epoch:6 batch_done:88 Gen Loss: 4.42343 Disc Loss: 0.0191595 Q Losses: [0.0043614339, 0.011124376]\n",
      "epoch:6 batch_done:89 Gen Loss: 3.96725 Disc Loss: 0.0314216 Q Losses: [0.016202191, 0.021651052]\n",
      "epoch:6 batch_done:90 Gen Loss: 19.2357 Disc Loss: 0.063025 Q Losses: [0.018298287, 0.01370353]\n",
      "epoch:6 batch_done:91 Gen Loss: -0.0 Disc Loss: 0.765953 Q Losses: [0.0086633675, 0.017121712]\n",
      "epoch:6 batch_done:92 Gen Loss: nan Disc Loss: inf Q Losses: [0.1664342, 2.303205]\n",
      "epoch:6 batch_done:93 Gen Loss: nan Disc Loss: nan Q Losses: [0.16576104, 2.3031807]\n",
      "epoch:6 batch_done:94 Gen Loss: nan Disc Loss: nan Q Losses: [0.16424316, 2.3025093]\n",
      "epoch:6 batch_done:95 Gen Loss: nan Disc Loss: nan Q Losses: [0.13481003, 2.3043833]\n",
      "epoch:6 batch_done:96 Gen Loss: nan Disc Loss: nan Q Losses: [0.180153, 2.3025069]\n",
      "epoch:6 batch_done:97 Gen Loss: nan Disc Loss: nan Q Losses: [0.18485817, 2.3033886]\n",
      "epoch:6 batch_done:98 Gen Loss: nan Disc Loss: nan Q Losses: [0.15743116, 2.3033009]\n",
      "epoch:6 batch_done:99 Gen Loss: nan Disc Loss: nan Q Losses: [0.15028968, 2.3033142]\n",
      "epoch:6 batch_done:100 Gen Loss: nan Disc Loss: nan Q Losses: [0.1732443, 2.3038144]\n",
      "epoch:6 batch_done:101 Gen Loss: nan Disc Loss: nan Q Losses: [0.16638499, 2.304554]\n",
      "epoch:6 batch_done:102 Gen Loss: nan Disc Loss: nan Q Losses: [0.17978701, 2.3031943]\n",
      "epoch:6 batch_done:103 Gen Loss: nan Disc Loss: nan Q Losses: [0.14130662, 2.3026626]\n",
      "epoch:6 batch_done:104 Gen Loss: nan Disc Loss: nan Q Losses: [0.15767217, 2.3043988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:105 Gen Loss: nan Disc Loss: nan Q Losses: [0.17817004, 2.3031847]\n",
      "epoch:6 batch_done:106 Gen Loss: nan Disc Loss: nan Q Losses: [0.16320875, 2.3034496]\n",
      "epoch:6 batch_done:107 Gen Loss: nan Disc Loss: nan Q Losses: [0.17545015, 2.3025935]\n",
      "epoch:6 batch_done:108 Gen Loss: nan Disc Loss: nan Q Losses: [0.18826561, 2.3038874]\n",
      "epoch:6 batch_done:109 Gen Loss: nan Disc Loss: nan Q Losses: [0.1322889, 2.3043103]\n",
      "epoch:6 batch_done:110 Gen Loss: nan Disc Loss: nan Q Losses: [0.16826636, 2.3038313]\n",
      "epoch:6 batch_done:111 Gen Loss: nan Disc Loss: nan Q Losses: [0.15854865, 2.3044639]\n",
      "epoch:6 batch_done:112 Gen Loss: nan Disc Loss: nan Q Losses: [0.18391442, 2.301003]\n",
      "epoch:6 batch_done:113 Gen Loss: nan Disc Loss: nan Q Losses: [0.18240926, 2.3024499]\n",
      "epoch:6 batch_done:114 Gen Loss: nan Disc Loss: nan Q Losses: [0.17733583, 2.3025856]\n",
      "epoch:6 batch_done:115 Gen Loss: nan Disc Loss: nan Q Losses: [0.12708825, 2.3018217]\n",
      "epoch:6 batch_done:116 Gen Loss: nan Disc Loss: nan Q Losses: [0.19119109, 2.3026156]\n",
      "epoch:6 batch_done:117 Gen Loss: nan Disc Loss: nan Q Losses: [0.15514219, 2.3032327]\n",
      "epoch:6 batch_done:118 Gen Loss: nan Disc Loss: nan Q Losses: [0.15484861, 2.3021367]\n",
      "epoch:6 batch_done:119 Gen Loss: nan Disc Loss: nan Q Losses: [0.15466365, 2.3032608]\n",
      "epoch:6 batch_done:120 Gen Loss: nan Disc Loss: nan Q Losses: [0.17637539, 2.3021686]\n",
      "epoch:6 batch_done:121 Gen Loss: nan Disc Loss: nan Q Losses: [0.15189658, 2.301806]\n",
      "epoch:6 batch_done:122 Gen Loss: nan Disc Loss: nan Q Losses: [0.16843173, 2.3033834]\n",
      "epoch:6 batch_done:123 Gen Loss: nan Disc Loss: nan Q Losses: [0.1402477, 2.3021669]\n",
      "epoch:6 batch_done:124 Gen Loss: nan Disc Loss: nan Q Losses: [0.15274256, 2.3040543]\n",
      "epoch:6 batch_done:125 Gen Loss: nan Disc Loss: nan Q Losses: [0.16233562, 2.3055725]\n",
      "epoch:6 batch_done:126 Gen Loss: nan Disc Loss: nan Q Losses: [0.14952116, 2.3031344]\n",
      "epoch:6 batch_done:127 Gen Loss: nan Disc Loss: nan Q Losses: [0.19508883, 2.3025033]\n",
      "epoch:6 batch_done:128 Gen Loss: nan Disc Loss: nan Q Losses: [0.18187615, 2.3029432]\n",
      "epoch:6 batch_done:129 Gen Loss: nan Disc Loss: nan Q Losses: [0.17582865, 2.3024931]\n",
      "epoch:6 batch_done:130 Gen Loss: nan Disc Loss: nan Q Losses: [0.13079458, 2.3034534]\n",
      "epoch:6 batch_done:131 Gen Loss: nan Disc Loss: nan Q Losses: [0.17614859, 2.3014357]\n",
      "epoch:6 batch_done:132 Gen Loss: nan Disc Loss: nan Q Losses: [0.15340109, 2.3025491]\n",
      "epoch:6 batch_done:133 Gen Loss: nan Disc Loss: nan Q Losses: [0.19561973, 2.3033144]\n",
      "epoch:6 batch_done:134 Gen Loss: nan Disc Loss: nan Q Losses: [0.15668312, 2.3030028]\n",
      "epoch:6 batch_done:135 Gen Loss: nan Disc Loss: nan Q Losses: [0.16084069, 2.3024859]\n",
      "epoch:6 batch_done:136 Gen Loss: nan Disc Loss: nan Q Losses: [0.16095501, 2.3027625]\n",
      "epoch:6 batch_done:137 Gen Loss: nan Disc Loss: nan Q Losses: [0.15977305, 2.3039122]\n",
      "epoch:6 batch_done:138 Gen Loss: nan Disc Loss: nan Q Losses: [0.16759312, 2.3032069]\n",
      "epoch:6 batch_done:139 Gen Loss: nan Disc Loss: nan Q Losses: [0.17534834, 2.3027995]\n",
      "epoch:6 batch_done:140 Gen Loss: nan Disc Loss: nan Q Losses: [0.14801557, 2.3026843]\n",
      "epoch:6 batch_done:141 Gen Loss: nan Disc Loss: nan Q Losses: [0.15188906, 2.3030705]\n",
      "epoch:6 batch_done:142 Gen Loss: nan Disc Loss: nan Q Losses: [0.12889437, 2.3027506]\n",
      "epoch:6 batch_done:143 Gen Loss: nan Disc Loss: nan Q Losses: [0.16143942, 2.3025715]\n",
      "epoch:6 batch_done:144 Gen Loss: nan Disc Loss: nan Q Losses: [0.19368958, 2.3024344]\n",
      "epoch:6 batch_done:145 Gen Loss: nan Disc Loss: nan Q Losses: [0.19151482, 2.3016548]\n",
      "epoch:6 batch_done:146 Gen Loss: nan Disc Loss: nan Q Losses: [0.16020903, 2.3014078]\n",
      "epoch:6 batch_done:147 Gen Loss: nan Disc Loss: nan Q Losses: [0.18209323, 2.3027236]\n",
      "epoch:6 batch_done:148 Gen Loss: nan Disc Loss: nan Q Losses: [0.18923894, 2.3029656]\n",
      "epoch:6 batch_done:149 Gen Loss: nan Disc Loss: nan Q Losses: [0.1421749, 2.3038812]\n",
      "epoch:6 batch_done:150 Gen Loss: nan Disc Loss: nan Q Losses: [0.17204335, 2.3014112]\n",
      "epoch:6 batch_done:151 Gen Loss: nan Disc Loss: nan Q Losses: [0.16086674, 2.3019605]\n",
      "epoch:6 batch_done:152 Gen Loss: nan Disc Loss: nan Q Losses: [0.15668452, 2.3021641]\n",
      "epoch:6 batch_done:153 Gen Loss: nan Disc Loss: nan Q Losses: [0.15580681, 2.302268]\n",
      "epoch:6 batch_done:154 Gen Loss: nan Disc Loss: nan Q Losses: [0.16768523, 2.3034348]\n",
      "epoch:6 batch_done:155 Gen Loss: nan Disc Loss: nan Q Losses: [0.17654736, 2.3030386]\n",
      "epoch:6 batch_done:156 Gen Loss: nan Disc Loss: nan Q Losses: [0.15940805, 2.3017912]\n",
      "epoch:6 batch_done:157 Gen Loss: nan Disc Loss: nan Q Losses: [0.16549294, 2.3022175]\n",
      "epoch:6 batch_done:158 Gen Loss: nan Disc Loss: nan Q Losses: [0.14802256, 2.3021901]\n",
      "epoch:6 batch_done:159 Gen Loss: nan Disc Loss: nan Q Losses: [0.17148304, 2.3020382]\n",
      "epoch:6 batch_done:160 Gen Loss: nan Disc Loss: nan Q Losses: [0.19076583, 2.3022461]\n",
      "epoch:6 batch_done:161 Gen Loss: nan Disc Loss: nan Q Losses: [0.16475594, 2.3027258]\n",
      "epoch:6 batch_done:162 Gen Loss: nan Disc Loss: nan Q Losses: [0.15722048, 2.3018239]\n",
      "epoch:6 batch_done:163 Gen Loss: nan Disc Loss: nan Q Losses: [0.15007403, 2.3013859]\n",
      "epoch:6 batch_done:164 Gen Loss: nan Disc Loss: nan Q Losses: [0.12865061, 2.3020909]\n",
      "epoch:6 batch_done:165 Gen Loss: nan Disc Loss: nan Q Losses: [0.15914723, 2.3041439]\n",
      "epoch:6 batch_done:166 Gen Loss: nan Disc Loss: nan Q Losses: [0.17850478, 2.3028102]\n",
      "epoch:6 batch_done:167 Gen Loss: nan Disc Loss: nan Q Losses: [0.11466207, 2.3035655]\n",
      "epoch:6 batch_done:168 Gen Loss: nan Disc Loss: nan Q Losses: [0.16198252, 2.3016868]\n",
      "epoch:6 batch_done:169 Gen Loss: nan Disc Loss: nan Q Losses: [0.18574606, 2.3030381]\n",
      "epoch:6 batch_done:170 Gen Loss: nan Disc Loss: nan Q Losses: [0.17100644, 2.3008144]\n",
      "epoch:6 batch_done:171 Gen Loss: nan Disc Loss: nan Q Losses: [0.12848419, 2.3017316]\n",
      "epoch:6 batch_done:172 Gen Loss: nan Disc Loss: nan Q Losses: [0.14809529, 2.302573]\n",
      "epoch:6 batch_done:173 Gen Loss: nan Disc Loss: nan Q Losses: [0.17875546, 2.3028445]\n",
      "epoch:6 batch_done:174 Gen Loss: nan Disc Loss: nan Q Losses: [0.17391539, 2.3030012]\n",
      "epoch:6 batch_done:175 Gen Loss: nan Disc Loss: nan Q Losses: [0.17367318, 2.3029826]\n",
      "epoch:6 batch_done:176 Gen Loss: nan Disc Loss: nan Q Losses: [0.14418827, 2.3022761]\n",
      "epoch:6 batch_done:177 Gen Loss: nan Disc Loss: nan Q Losses: [0.13410988, 2.3020649]\n",
      "epoch:6 batch_done:178 Gen Loss: nan Disc Loss: nan Q Losses: [0.16225365, 2.3021739]\n",
      "epoch:6 batch_done:179 Gen Loss: nan Disc Loss: nan Q Losses: [0.15047894, 2.3016632]\n",
      "epoch:6 batch_done:180 Gen Loss: nan Disc Loss: nan Q Losses: [0.16362952, 2.302212]\n",
      "epoch:6 batch_done:181 Gen Loss: nan Disc Loss: nan Q Losses: [0.20790851, 2.3029268]\n",
      "epoch:6 batch_done:182 Gen Loss: nan Disc Loss: nan Q Losses: [0.17401263, 2.3032436]\n",
      "epoch:6 batch_done:183 Gen Loss: nan Disc Loss: nan Q Losses: [0.18021193, 2.3033218]\n",
      "epoch:6 batch_done:184 Gen Loss: nan Disc Loss: nan Q Losses: [0.1793762, 2.3027706]\n",
      "epoch:6 batch_done:185 Gen Loss: nan Disc Loss: nan Q Losses: [0.16718881, 2.3031554]\n",
      "epoch:6 batch_done:186 Gen Loss: nan Disc Loss: nan Q Losses: [0.18008834, 2.3012567]\n",
      "epoch:6 batch_done:187 Gen Loss: nan Disc Loss: nan Q Losses: [0.16048463, 2.3016424]\n",
      "epoch:6 batch_done:188 Gen Loss: nan Disc Loss: nan Q Losses: [0.16032445, 2.3027596]\n",
      "epoch:6 batch_done:189 Gen Loss: nan Disc Loss: nan Q Losses: [0.1493226, 2.3042223]\n",
      "epoch:6 batch_done:190 Gen Loss: nan Disc Loss: nan Q Losses: [0.20137078, 2.303299]\n",
      "epoch:6 batch_done:191 Gen Loss: nan Disc Loss: nan Q Losses: [0.16588968, 2.3021674]\n",
      "epoch:6 batch_done:192 Gen Loss: nan Disc Loss: nan Q Losses: [0.14924172, 2.3029304]\n",
      "epoch:6 batch_done:193 Gen Loss: nan Disc Loss: nan Q Losses: [0.19631541, 2.3030205]\n",
      "epoch:6 batch_done:194 Gen Loss: nan Disc Loss: nan Q Losses: [0.22298943, 2.3012805]\n",
      "epoch:6 batch_done:195 Gen Loss: nan Disc Loss: nan Q Losses: [0.17341317, 2.300796]\n",
      "epoch:6 batch_done:196 Gen Loss: nan Disc Loss: nan Q Losses: [0.17850462, 2.3003159]\n",
      "epoch:6 batch_done:197 Gen Loss: nan Disc Loss: nan Q Losses: [0.18058968, 2.3031812]\n",
      "epoch:6 batch_done:198 Gen Loss: nan Disc Loss: nan Q Losses: [0.19211155, 2.3026793]\n",
      "epoch:6 batch_done:199 Gen Loss: nan Disc Loss: nan Q Losses: [0.17597543, 2.3032639]\n",
      "epoch:6 batch_done:200 Gen Loss: nan Disc Loss: nan Q Losses: [0.17055598, 2.3009992]\n",
      "epoch:6 batch_done:201 Gen Loss: nan Disc Loss: nan Q Losses: [0.13951193, 2.300633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:202 Gen Loss: nan Disc Loss: nan Q Losses: [0.15941733, 2.3029633]\n",
      "epoch:6 batch_done:203 Gen Loss: nan Disc Loss: nan Q Losses: [0.17648262, 2.3023126]\n",
      "epoch:6 batch_done:204 Gen Loss: nan Disc Loss: nan Q Losses: [0.14232895, 2.3024673]\n",
      "epoch:6 batch_done:205 Gen Loss: nan Disc Loss: nan Q Losses: [0.16162065, 2.3030846]\n",
      "epoch:6 batch_done:206 Gen Loss: nan Disc Loss: nan Q Losses: [0.15838151, 2.3040719]\n",
      "epoch:6 batch_done:207 Gen Loss: nan Disc Loss: nan Q Losses: [0.19977927, 2.304049]\n",
      "epoch:7 batch_done:1 Gen Loss: nan Disc Loss: nan Q Losses: [0.17140049, 2.3021131]\n",
      "epoch:7 batch_done:2 Gen Loss: nan Disc Loss: nan Q Losses: [0.17038792, 2.3017504]\n",
      "epoch:7 batch_done:3 Gen Loss: nan Disc Loss: nan Q Losses: [0.17267913, 2.300796]\n",
      "epoch:7 batch_done:4 Gen Loss: nan Disc Loss: nan Q Losses: [0.14234339, 2.3043914]\n",
      "epoch:7 batch_done:5 Gen Loss: nan Disc Loss: nan Q Losses: [0.12366902, 2.3050413]\n",
      "epoch:7 batch_done:6 Gen Loss: nan Disc Loss: nan Q Losses: [0.12652028, 2.3021603]\n",
      "epoch:7 batch_done:7 Gen Loss: nan Disc Loss: nan Q Losses: [0.15200892, 2.3033957]\n",
      "epoch:7 batch_done:8 Gen Loss: nan Disc Loss: nan Q Losses: [0.19058189, 2.3023205]\n",
      "epoch:7 batch_done:9 Gen Loss: nan Disc Loss: nan Q Losses: [0.14609078, 2.3031287]\n",
      "epoch:7 batch_done:10 Gen Loss: nan Disc Loss: nan Q Losses: [0.1720219, 2.3023748]\n",
      "epoch:7 batch_done:11 Gen Loss: nan Disc Loss: nan Q Losses: [0.15219241, 2.3021584]\n",
      "epoch:7 batch_done:12 Gen Loss: nan Disc Loss: nan Q Losses: [0.16976145, 2.3042889]\n",
      "epoch:7 batch_done:13 Gen Loss: nan Disc Loss: nan Q Losses: [0.18911166, 2.3023582]\n",
      "epoch:7 batch_done:14 Gen Loss: nan Disc Loss: nan Q Losses: [0.15793824, 2.2998075]\n",
      "epoch:7 batch_done:15 Gen Loss: nan Disc Loss: nan Q Losses: [0.18661407, 2.3033981]\n",
      "epoch:7 batch_done:16 Gen Loss: nan Disc Loss: nan Q Losses: [0.15732293, 2.3031487]\n",
      "epoch:7 batch_done:17 Gen Loss: nan Disc Loss: nan Q Losses: [0.17469503, 2.3031785]\n",
      "epoch:7 batch_done:18 Gen Loss: nan Disc Loss: nan Q Losses: [0.13902128, 2.302532]\n",
      "epoch:7 batch_done:19 Gen Loss: nan Disc Loss: nan Q Losses: [0.15491627, 2.3049612]\n",
      "epoch:7 batch_done:20 Gen Loss: nan Disc Loss: nan Q Losses: [0.1459938, 2.301523]\n",
      "epoch:7 batch_done:21 Gen Loss: nan Disc Loss: nan Q Losses: [0.19674431, 2.3021762]\n",
      "epoch:7 batch_done:22 Gen Loss: nan Disc Loss: nan Q Losses: [0.12871975, 2.3021584]\n",
      "epoch:7 batch_done:23 Gen Loss: nan Disc Loss: nan Q Losses: [0.2044301, 2.3016272]\n",
      "epoch:7 batch_done:24 Gen Loss: nan Disc Loss: nan Q Losses: [0.15067932, 2.3003263]\n",
      "epoch:7 batch_done:25 Gen Loss: nan Disc Loss: nan Q Losses: [0.12124355, 2.3024483]\n",
      "epoch:7 batch_done:26 Gen Loss: nan Disc Loss: nan Q Losses: [0.1466023, 2.303371]\n",
      "epoch:7 batch_done:27 Gen Loss: nan Disc Loss: nan Q Losses: [0.16028899, 2.3048408]\n",
      "epoch:7 batch_done:28 Gen Loss: nan Disc Loss: nan Q Losses: [0.15324771, 2.3024549]\n",
      "epoch:7 batch_done:29 Gen Loss: nan Disc Loss: nan Q Losses: [0.14866784, 2.3024673]\n",
      "epoch:7 batch_done:30 Gen Loss: nan Disc Loss: nan Q Losses: [0.16221254, 2.3019054]\n",
      "epoch:7 batch_done:31 Gen Loss: nan Disc Loss: nan Q Losses: [0.18225127, 2.3009517]\n",
      "epoch:7 batch_done:32 Gen Loss: nan Disc Loss: nan Q Losses: [0.17949662, 2.3021252]\n",
      "epoch:7 batch_done:33 Gen Loss: nan Disc Loss: nan Q Losses: [0.16133159, 2.3034382]\n",
      "epoch:7 batch_done:34 Gen Loss: nan Disc Loss: nan Q Losses: [0.16221786, 2.3034916]\n",
      "epoch:7 batch_done:35 Gen Loss: nan Disc Loss: nan Q Losses: [0.15088174, 2.3020244]\n",
      "epoch:7 batch_done:36 Gen Loss: nan Disc Loss: nan Q Losses: [0.1607285, 2.2999325]\n",
      "epoch:7 batch_done:37 Gen Loss: nan Disc Loss: nan Q Losses: [0.16703443, 2.304744]\n",
      "epoch:7 batch_done:38 Gen Loss: nan Disc Loss: nan Q Losses: [0.16353738, 2.3037584]\n",
      "epoch:7 batch_done:39 Gen Loss: nan Disc Loss: nan Q Losses: [0.18112692, 2.3019767]\n",
      "epoch:7 batch_done:40 Gen Loss: nan Disc Loss: nan Q Losses: [0.14247546, 2.3023901]\n",
      "epoch:7 batch_done:41 Gen Loss: nan Disc Loss: nan Q Losses: [0.16329956, 2.3019657]\n",
      "epoch:7 batch_done:42 Gen Loss: nan Disc Loss: nan Q Losses: [0.1598925, 2.3022995]\n",
      "epoch:7 batch_done:43 Gen Loss: nan Disc Loss: nan Q Losses: [0.14731666, 2.302532]\n",
      "epoch:7 batch_done:44 Gen Loss: nan Disc Loss: nan Q Losses: [0.17117625, 2.3030326]\n",
      "epoch:7 batch_done:45 Gen Loss: nan Disc Loss: nan Q Losses: [0.15542036, 2.3025689]\n",
      "epoch:7 batch_done:46 Gen Loss: nan Disc Loss: nan Q Losses: [0.18283789, 2.3024151]\n",
      "epoch:7 batch_done:47 Gen Loss: nan Disc Loss: nan Q Losses: [0.19695602, 2.3036432]\n",
      "epoch:7 batch_done:48 Gen Loss: nan Disc Loss: nan Q Losses: [0.16193691, 2.3025544]\n",
      "epoch:7 batch_done:49 Gen Loss: nan Disc Loss: nan Q Losses: [0.15852815, 2.3022785]\n",
      "epoch:7 batch_done:50 Gen Loss: nan Disc Loss: nan Q Losses: [0.17799646, 2.3014331]\n",
      "epoch:7 batch_done:51 Gen Loss: nan Disc Loss: nan Q Losses: [0.14574374, 2.3016744]\n",
      "epoch:7 batch_done:52 Gen Loss: nan Disc Loss: nan Q Losses: [0.16790558, 2.3036077]\n",
      "epoch:7 batch_done:53 Gen Loss: nan Disc Loss: nan Q Losses: [0.15040128, 2.3028016]\n",
      "epoch:7 batch_done:54 Gen Loss: nan Disc Loss: nan Q Losses: [0.14703616, 2.3022394]\n",
      "epoch:7 batch_done:55 Gen Loss: nan Disc Loss: nan Q Losses: [0.15257193, 2.3021722]\n",
      "epoch:7 batch_done:56 Gen Loss: nan Disc Loss: nan Q Losses: [0.19939619, 2.302402]\n",
      "epoch:7 batch_done:57 Gen Loss: nan Disc Loss: nan Q Losses: [0.17338505, 2.3013427]\n",
      "epoch:7 batch_done:58 Gen Loss: nan Disc Loss: nan Q Losses: [0.15292428, 2.3017392]\n",
      "epoch:7 batch_done:59 Gen Loss: nan Disc Loss: nan Q Losses: [0.15494165, 2.3022406]\n",
      "epoch:7 batch_done:60 Gen Loss: nan Disc Loss: nan Q Losses: [0.17173503, 2.3034925]\n",
      "epoch:7 batch_done:61 Gen Loss: nan Disc Loss: nan Q Losses: [0.17944935, 2.3002348]\n",
      "epoch:7 batch_done:62 Gen Loss: nan Disc Loss: nan Q Losses: [0.18229346, 2.3051171]\n",
      "epoch:7 batch_done:63 Gen Loss: nan Disc Loss: nan Q Losses: [0.14025341, 2.3034925]\n",
      "epoch:7 batch_done:64 Gen Loss: nan Disc Loss: nan Q Losses: [0.17014538, 2.3026347]\n",
      "epoch:7 batch_done:65 Gen Loss: nan Disc Loss: nan Q Losses: [0.16709784, 2.3007469]\n",
      "epoch:7 batch_done:66 Gen Loss: nan Disc Loss: nan Q Losses: [0.16266608, 2.3021595]\n",
      "epoch:7 batch_done:67 Gen Loss: nan Disc Loss: nan Q Losses: [0.15985686, 2.3038969]\n",
      "epoch:7 batch_done:68 Gen Loss: nan Disc Loss: nan Q Losses: [0.17984682, 2.3026063]\n",
      "epoch:7 batch_done:69 Gen Loss: nan Disc Loss: nan Q Losses: [0.14439392, 2.3003199]\n",
      "epoch:7 batch_done:70 Gen Loss: nan Disc Loss: nan Q Losses: [0.11335687, 2.299916]\n",
      "epoch:7 batch_done:71 Gen Loss: nan Disc Loss: nan Q Losses: [0.16814676, 2.3025599]\n",
      "epoch:7 batch_done:72 Gen Loss: nan Disc Loss: nan Q Losses: [0.15512587, 2.3009219]\n",
      "epoch:7 batch_done:73 Gen Loss: nan Disc Loss: nan Q Losses: [0.1831882, 2.2999735]\n",
      "epoch:7 batch_done:74 Gen Loss: nan Disc Loss: nan Q Losses: [0.17820626, 2.3031001]\n",
      "epoch:7 batch_done:75 Gen Loss: nan Disc Loss: nan Q Losses: [0.16315651, 2.3043563]\n",
      "epoch:7 batch_done:76 Gen Loss: nan Disc Loss: nan Q Losses: [0.191624, 2.3024089]\n",
      "epoch:7 batch_done:77 Gen Loss: nan Disc Loss: nan Q Losses: [0.19746795, 2.3039808]\n",
      "epoch:7 batch_done:78 Gen Loss: nan Disc Loss: nan Q Losses: [0.17275739, 2.3013165]\n",
      "epoch:7 batch_done:79 Gen Loss: nan Disc Loss: nan Q Losses: [0.18376353, 2.3010039]\n",
      "epoch:7 batch_done:80 Gen Loss: nan Disc Loss: nan Q Losses: [0.13925532, 2.303247]\n",
      "epoch:7 batch_done:81 Gen Loss: nan Disc Loss: nan Q Losses: [0.17118517, 2.3032417]\n",
      "epoch:7 batch_done:82 Gen Loss: nan Disc Loss: nan Q Losses: [0.18115376, 2.3005545]\n",
      "epoch:7 batch_done:83 Gen Loss: nan Disc Loss: nan Q Losses: [0.15496324, 2.3013937]\n",
      "epoch:7 batch_done:84 Gen Loss: nan Disc Loss: nan Q Losses: [0.17085603, 2.3038454]\n",
      "epoch:7 batch_done:85 Gen Loss: nan Disc Loss: nan Q Losses: [0.17921862, 2.3031821]\n",
      "epoch:7 batch_done:86 Gen Loss: nan Disc Loss: nan Q Losses: [0.16858184, 2.3010335]\n",
      "epoch:7 batch_done:87 Gen Loss: nan Disc Loss: nan Q Losses: [0.18566133, 2.3010774]\n",
      "epoch:7 batch_done:88 Gen Loss: nan Disc Loss: nan Q Losses: [0.17324543, 2.3038559]\n",
      "epoch:7 batch_done:89 Gen Loss: nan Disc Loss: nan Q Losses: [0.15198526, 2.3024762]\n",
      "epoch:7 batch_done:90 Gen Loss: nan Disc Loss: nan Q Losses: [0.13863233, 2.3044291]\n",
      "epoch:7 batch_done:91 Gen Loss: nan Disc Loss: nan Q Losses: [0.1961408, 2.3055053]\n",
      "epoch:7 batch_done:92 Gen Loss: nan Disc Loss: nan Q Losses: [0.17414179, 2.3034577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 batch_done:93 Gen Loss: nan Disc Loss: nan Q Losses: [0.17020719, 2.3030858]\n",
      "epoch:7 batch_done:94 Gen Loss: nan Disc Loss: nan Q Losses: [0.14816427, 2.3036935]\n",
      "epoch:7 batch_done:95 Gen Loss: nan Disc Loss: nan Q Losses: [0.17379227, 2.3031847]\n",
      "epoch:7 batch_done:96 Gen Loss: nan Disc Loss: nan Q Losses: [0.13238801, 2.3037069]\n",
      "epoch:7 batch_done:97 Gen Loss: nan Disc Loss: nan Q Losses: [0.14782253, 2.3033116]\n",
      "epoch:7 batch_done:98 Gen Loss: nan Disc Loss: nan Q Losses: [0.17266083, 2.3024502]\n",
      "epoch:7 batch_done:99 Gen Loss: nan Disc Loss: nan Q Losses: [0.17503092, 2.3037062]\n",
      "epoch:7 batch_done:100 Gen Loss: nan Disc Loss: nan Q Losses: [0.15737194, 2.3049791]\n",
      "epoch:7 batch_done:101 Gen Loss: nan Disc Loss: nan Q Losses: [0.15375999, 2.3033338]\n",
      "epoch:7 batch_done:102 Gen Loss: nan Disc Loss: nan Q Losses: [0.15993991, 2.3019671]\n",
      "epoch:7 batch_done:103 Gen Loss: nan Disc Loss: nan Q Losses: [0.16888167, 2.3030708]\n",
      "epoch:7 batch_done:104 Gen Loss: nan Disc Loss: nan Q Losses: [0.14446215, 2.3049512]\n",
      "epoch:7 batch_done:105 Gen Loss: nan Disc Loss: nan Q Losses: [0.20813891, 2.3035154]\n",
      "epoch:7 batch_done:106 Gen Loss: nan Disc Loss: nan Q Losses: [0.16102439, 2.3044243]\n",
      "epoch:7 batch_done:107 Gen Loss: nan Disc Loss: nan Q Losses: [0.17941239, 2.3026388]\n",
      "epoch:7 batch_done:108 Gen Loss: nan Disc Loss: nan Q Losses: [0.14268778, 2.3033597]\n",
      "epoch:7 batch_done:109 Gen Loss: nan Disc Loss: nan Q Losses: [0.14415146, 2.3025546]\n",
      "epoch:7 batch_done:110 Gen Loss: nan Disc Loss: nan Q Losses: [0.15938927, 2.3021517]\n",
      "epoch:7 batch_done:111 Gen Loss: nan Disc Loss: nan Q Losses: [0.14672472, 2.3031454]\n",
      "epoch:7 batch_done:112 Gen Loss: nan Disc Loss: nan Q Losses: [0.17587051, 2.3045278]\n",
      "epoch:7 batch_done:113 Gen Loss: nan Disc Loss: nan Q Losses: [0.18954447, 2.3021755]\n",
      "epoch:7 batch_done:114 Gen Loss: nan Disc Loss: nan Q Losses: [0.15015516, 2.3003697]\n",
      "epoch:7 batch_done:115 Gen Loss: nan Disc Loss: nan Q Losses: [0.18592443, 2.3036141]\n",
      "epoch:7 batch_done:116 Gen Loss: nan Disc Loss: nan Q Losses: [0.19538906, 2.3029835]\n",
      "epoch:7 batch_done:117 Gen Loss: nan Disc Loss: nan Q Losses: [0.16469517, 2.3001168]\n",
      "epoch:7 batch_done:118 Gen Loss: nan Disc Loss: nan Q Losses: [0.20218639, 2.3021975]\n",
      "epoch:7 batch_done:119 Gen Loss: nan Disc Loss: nan Q Losses: [0.15974918, 2.3022797]\n",
      "epoch:7 batch_done:120 Gen Loss: nan Disc Loss: nan Q Losses: [0.15296654, 2.3033972]\n",
      "epoch:7 batch_done:121 Gen Loss: nan Disc Loss: nan Q Losses: [0.14505449, 2.3018322]\n",
      "epoch:7 batch_done:122 Gen Loss: nan Disc Loss: nan Q Losses: [0.17170046, 2.3011274]\n",
      "epoch:7 batch_done:123 Gen Loss: nan Disc Loss: nan Q Losses: [0.18583019, 2.3022254]\n",
      "epoch:7 batch_done:124 Gen Loss: nan Disc Loss: nan Q Losses: [0.15555984, 2.3023815]\n",
      "epoch:7 batch_done:125 Gen Loss: nan Disc Loss: nan Q Losses: [0.17808595, 2.3020959]\n",
      "epoch:7 batch_done:126 Gen Loss: nan Disc Loss: nan Q Losses: [0.14958562, 2.3019743]\n",
      "epoch:7 batch_done:127 Gen Loss: nan Disc Loss: nan Q Losses: [0.16187534, 2.3021913]\n",
      "epoch:7 batch_done:128 Gen Loss: nan Disc Loss: nan Q Losses: [0.19327939, 2.3000746]\n",
      "epoch:7 batch_done:129 Gen Loss: nan Disc Loss: nan Q Losses: [0.16866109, 2.3018737]\n",
      "epoch:7 batch_done:130 Gen Loss: nan Disc Loss: nan Q Losses: [0.13774654, 2.3027291]\n",
      "epoch:7 batch_done:131 Gen Loss: nan Disc Loss: nan Q Losses: [0.16518229, 2.3018994]\n",
      "epoch:7 batch_done:132 Gen Loss: nan Disc Loss: nan Q Losses: [0.14589256, 2.3020577]\n",
      "epoch:7 batch_done:133 Gen Loss: nan Disc Loss: nan Q Losses: [0.17949779, 2.3026633]\n",
      "epoch:7 batch_done:134 Gen Loss: nan Disc Loss: nan Q Losses: [0.15858564, 2.301775]\n",
      "epoch:7 batch_done:135 Gen Loss: nan Disc Loss: nan Q Losses: [0.19579545, 2.3025479]\n",
      "epoch:7 batch_done:136 Gen Loss: nan Disc Loss: nan Q Losses: [0.16009641, 2.3040636]\n",
      "epoch:7 batch_done:137 Gen Loss: nan Disc Loss: nan Q Losses: [0.15801722, 2.303956]\n",
      "epoch:7 batch_done:138 Gen Loss: nan Disc Loss: nan Q Losses: [0.17460108, 2.3016968]\n",
      "epoch:7 batch_done:139 Gen Loss: nan Disc Loss: nan Q Losses: [0.18375185, 2.3013077]\n",
      "epoch:7 batch_done:140 Gen Loss: nan Disc Loss: nan Q Losses: [0.1887611, 2.3030438]\n",
      "epoch:7 batch_done:141 Gen Loss: nan Disc Loss: nan Q Losses: [0.16963868, 2.3037767]\n",
      "epoch:7 batch_done:142 Gen Loss: nan Disc Loss: nan Q Losses: [0.16737433, 2.3010325]\n",
      "epoch:7 batch_done:143 Gen Loss: nan Disc Loss: nan Q Losses: [0.18088752, 2.3026783]\n",
      "epoch:7 batch_done:144 Gen Loss: nan Disc Loss: nan Q Losses: [0.15010944, 2.3002758]\n",
      "epoch:7 batch_done:145 Gen Loss: nan Disc Loss: nan Q Losses: [0.14562021, 2.3018131]\n",
      "epoch:7 batch_done:146 Gen Loss: nan Disc Loss: nan Q Losses: [0.18242037, 2.3021126]\n",
      "epoch:7 batch_done:147 Gen Loss: nan Disc Loss: nan Q Losses: [0.14836788, 2.2991664]\n",
      "epoch:7 batch_done:148 Gen Loss: nan Disc Loss: nan Q Losses: [0.16460401, 2.302388]\n",
      "epoch:7 batch_done:149 Gen Loss: nan Disc Loss: nan Q Losses: [0.14915025, 2.3026118]\n",
      "epoch:7 batch_done:150 Gen Loss: nan Disc Loss: nan Q Losses: [0.15147227, 2.3008533]\n",
      "epoch:7 batch_done:151 Gen Loss: nan Disc Loss: nan Q Losses: [0.16927534, 2.3033919]\n",
      "epoch:7 batch_done:152 Gen Loss: nan Disc Loss: nan Q Losses: [0.2081417, 2.3016424]\n",
      "epoch:7 batch_done:153 Gen Loss: nan Disc Loss: nan Q Losses: [0.151061, 2.3025436]\n",
      "epoch:7 batch_done:154 Gen Loss: nan Disc Loss: nan Q Losses: [0.15740353, 2.3004513]\n",
      "epoch:7 batch_done:155 Gen Loss: nan Disc Loss: nan Q Losses: [0.16851161, 2.3020802]\n",
      "epoch:7 batch_done:156 Gen Loss: nan Disc Loss: nan Q Losses: [0.17545189, 2.30336]\n",
      "epoch:7 batch_done:157 Gen Loss: nan Disc Loss: nan Q Losses: [0.16214129, 2.3052366]\n",
      "epoch:7 batch_done:158 Gen Loss: nan Disc Loss: nan Q Losses: [0.15569079, 2.300529]\n",
      "epoch:7 batch_done:159 Gen Loss: nan Disc Loss: nan Q Losses: [0.15662459, 2.3024812]\n",
      "epoch:7 batch_done:160 Gen Loss: nan Disc Loss: nan Q Losses: [0.15368441, 2.3019185]\n",
      "epoch:7 batch_done:161 Gen Loss: nan Disc Loss: nan Q Losses: [0.16856763, 2.3041449]\n",
      "epoch:7 batch_done:162 Gen Loss: nan Disc Loss: nan Q Losses: [0.15296924, 2.3042316]\n",
      "epoch:7 batch_done:163 Gen Loss: nan Disc Loss: nan Q Losses: [0.17028677, 2.3028569]\n",
      "epoch:7 batch_done:164 Gen Loss: nan Disc Loss: nan Q Losses: [0.18947856, 2.3036418]\n",
      "epoch:7 batch_done:165 Gen Loss: nan Disc Loss: nan Q Losses: [0.16407308, 2.3041735]\n",
      "epoch:7 batch_done:166 Gen Loss: nan Disc Loss: nan Q Losses: [0.17839918, 2.3028886]\n",
      "epoch:7 batch_done:167 Gen Loss: nan Disc Loss: nan Q Losses: [0.15579911, 2.3002737]\n",
      "epoch:7 batch_done:168 Gen Loss: nan Disc Loss: nan Q Losses: [0.16618241, 2.3039005]\n",
      "epoch:7 batch_done:169 Gen Loss: nan Disc Loss: nan Q Losses: [0.17872262, 2.3016119]\n",
      "epoch:7 batch_done:170 Gen Loss: nan Disc Loss: nan Q Losses: [0.1681197, 2.3025587]\n",
      "epoch:7 batch_done:171 Gen Loss: nan Disc Loss: nan Q Losses: [0.16947323, 2.3031745]\n",
      "epoch:7 batch_done:172 Gen Loss: nan Disc Loss: nan Q Losses: [0.13977362, 2.3031318]\n",
      "epoch:7 batch_done:173 Gen Loss: nan Disc Loss: nan Q Losses: [0.1639597, 2.2997861]\n",
      "epoch:7 batch_done:174 Gen Loss: nan Disc Loss: nan Q Losses: [0.1552549, 2.299933]\n",
      "epoch:7 batch_done:175 Gen Loss: nan Disc Loss: nan Q Losses: [0.18031248, 2.3049994]\n",
      "epoch:7 batch_done:176 Gen Loss: nan Disc Loss: nan Q Losses: [0.17524397, 2.3017776]\n",
      "epoch:7 batch_done:177 Gen Loss: nan Disc Loss: nan Q Losses: [0.15895806, 2.297626]\n",
      "epoch:7 batch_done:178 Gen Loss: nan Disc Loss: nan Q Losses: [0.1200323, 2.3034925]\n",
      "epoch:7 batch_done:179 Gen Loss: nan Disc Loss: nan Q Losses: [0.19105545, 2.3036902]\n",
      "epoch:7 batch_done:180 Gen Loss: nan Disc Loss: nan Q Losses: [0.18841931, 2.3049343]\n",
      "epoch:7 batch_done:181 Gen Loss: nan Disc Loss: nan Q Losses: [0.18542063, 2.3019753]\n",
      "epoch:7 batch_done:182 Gen Loss: nan Disc Loss: nan Q Losses: [0.1670194, 2.3038986]\n",
      "epoch:7 batch_done:183 Gen Loss: nan Disc Loss: nan Q Losses: [0.17278637, 2.3033032]\n",
      "epoch:7 batch_done:184 Gen Loss: nan Disc Loss: nan Q Losses: [0.1720342, 2.3046298]\n",
      "epoch:7 batch_done:185 Gen Loss: nan Disc Loss: nan Q Losses: [0.16049242, 2.3015633]\n",
      "epoch:7 batch_done:186 Gen Loss: nan Disc Loss: nan Q Losses: [0.19895184, 2.3019631]\n",
      "epoch:7 batch_done:187 Gen Loss: nan Disc Loss: nan Q Losses: [0.19321844, 2.3029113]\n",
      "epoch:7 batch_done:188 Gen Loss: nan Disc Loss: nan Q Losses: [0.15469754, 2.302274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 batch_done:189 Gen Loss: nan Disc Loss: nan Q Losses: [0.16581225, 2.3028791]\n",
      "epoch:7 batch_done:190 Gen Loss: nan Disc Loss: nan Q Losses: [0.16511691, 2.3019748]\n",
      "epoch:7 batch_done:191 Gen Loss: nan Disc Loss: nan Q Losses: [0.20554143, 2.303062]\n",
      "epoch:7 batch_done:192 Gen Loss: nan Disc Loss: nan Q Losses: [0.13427515, 2.302114]\n",
      "epoch:7 batch_done:193 Gen Loss: nan Disc Loss: nan Q Losses: [0.16612831, 2.3030357]\n",
      "epoch:7 batch_done:194 Gen Loss: nan Disc Loss: nan Q Losses: [0.17019059, 2.3016086]\n",
      "epoch:7 batch_done:195 Gen Loss: nan Disc Loss: nan Q Losses: [0.20250425, 2.3024364]\n",
      "epoch:7 batch_done:196 Gen Loss: nan Disc Loss: nan Q Losses: [0.15203544, 2.3011341]\n",
      "epoch:7 batch_done:197 Gen Loss: nan Disc Loss: nan Q Losses: [0.15731472, 2.3039494]\n",
      "epoch:7 batch_done:198 Gen Loss: nan Disc Loss: nan Q Losses: [0.15876821, 2.3006034]\n",
      "epoch:7 batch_done:199 Gen Loss: nan Disc Loss: nan Q Losses: [0.15067944, 2.299633]\n",
      "epoch:7 batch_done:200 Gen Loss: nan Disc Loss: nan Q Losses: [0.18617159, 2.303175]\n",
      "epoch:7 batch_done:201 Gen Loss: nan Disc Loss: nan Q Losses: [0.1507048, 2.3021493]\n",
      "epoch:7 batch_done:202 Gen Loss: nan Disc Loss: nan Q Losses: [0.15460894, 2.3018198]\n",
      "epoch:7 batch_done:203 Gen Loss: nan Disc Loss: nan Q Losses: [0.18647027, 2.3042932]\n",
      "epoch:7 batch_done:204 Gen Loss: nan Disc Loss: nan Q Losses: [0.15963015, 2.3028102]\n",
      "epoch:7 batch_done:205 Gen Loss: nan Disc Loss: nan Q Losses: [0.14136483, 2.3014903]\n",
      "epoch:7 batch_done:206 Gen Loss: nan Disc Loss: nan Q Losses: [0.18821239, 2.3012929]\n",
      "epoch:7 batch_done:207 Gen Loss: nan Disc Loss: nan Q Losses: [0.13534138, 2.3012223]\n",
      "epoch:8 batch_done:1 Gen Loss: nan Disc Loss: nan Q Losses: [0.17179531, 2.3012669]\n",
      "epoch:8 batch_done:2 Gen Loss: nan Disc Loss: nan Q Losses: [0.17038852, 2.3041658]\n",
      "epoch:8 batch_done:3 Gen Loss: nan Disc Loss: nan Q Losses: [0.20226473, 2.3000975]\n",
      "epoch:8 batch_done:4 Gen Loss: nan Disc Loss: nan Q Losses: [0.13899994, 2.3052394]\n",
      "epoch:8 batch_done:5 Gen Loss: nan Disc Loss: nan Q Losses: [0.16132429, 2.3032613]\n",
      "epoch:8 batch_done:6 Gen Loss: nan Disc Loss: nan Q Losses: [0.14086089, 2.3021443]\n",
      "epoch:8 batch_done:7 Gen Loss: nan Disc Loss: nan Q Losses: [0.18883565, 2.3025012]\n",
      "epoch:8 batch_done:8 Gen Loss: nan Disc Loss: nan Q Losses: [0.15897557, 2.3031657]\n",
      "epoch:8 batch_done:9 Gen Loss: nan Disc Loss: nan Q Losses: [0.17519431, 2.3011632]\n",
      "epoch:8 batch_done:10 Gen Loss: nan Disc Loss: nan Q Losses: [0.15156016, 2.3019595]\n",
      "epoch:8 batch_done:11 Gen Loss: nan Disc Loss: nan Q Losses: [0.18157676, 2.3044224]\n",
      "epoch:8 batch_done:12 Gen Loss: nan Disc Loss: nan Q Losses: [0.18652736, 2.3029957]\n",
      "epoch:8 batch_done:13 Gen Loss: nan Disc Loss: nan Q Losses: [0.20448592, 2.3029683]\n",
      "epoch:8 batch_done:14 Gen Loss: nan Disc Loss: nan Q Losses: [0.15437035, 2.3028336]\n",
      "epoch:8 batch_done:15 Gen Loss: nan Disc Loss: nan Q Losses: [0.1823509, 2.30018]\n",
      "epoch:8 batch_done:16 Gen Loss: nan Disc Loss: nan Q Losses: [0.16024356, 2.2993691]\n",
      "epoch:8 batch_done:17 Gen Loss: nan Disc Loss: nan Q Losses: [0.1952938, 2.3026028]\n",
      "epoch:8 batch_done:18 Gen Loss: nan Disc Loss: nan Q Losses: [0.15864089, 2.3033006]\n",
      "epoch:8 batch_done:19 Gen Loss: nan Disc Loss: nan Q Losses: [0.16377819, 2.3010287]\n",
      "epoch:8 batch_done:20 Gen Loss: nan Disc Loss: nan Q Losses: [0.1704732, 2.3042486]\n",
      "epoch:8 batch_done:21 Gen Loss: nan Disc Loss: nan Q Losses: [0.16203739, 2.3018081]\n",
      "epoch:8 batch_done:22 Gen Loss: nan Disc Loss: nan Q Losses: [0.17328772, 2.2993116]\n",
      "epoch:8 batch_done:23 Gen Loss: nan Disc Loss: nan Q Losses: [0.15852636, 2.3034837]\n",
      "epoch:8 batch_done:24 Gen Loss: nan Disc Loss: nan Q Losses: [0.14722086, 2.3014257]\n",
      "epoch:8 batch_done:25 Gen Loss: nan Disc Loss: nan Q Losses: [0.18220928, 2.3033648]\n",
      "epoch:8 batch_done:26 Gen Loss: nan Disc Loss: nan Q Losses: [0.14837891, 2.3034627]\n",
      "epoch:8 batch_done:27 Gen Loss: nan Disc Loss: nan Q Losses: [0.15213385, 2.3006425]\n",
      "epoch:8 batch_done:28 Gen Loss: nan Disc Loss: nan Q Losses: [0.15144455, 2.3022447]\n",
      "epoch:8 batch_done:29 Gen Loss: nan Disc Loss: nan Q Losses: [0.16060658, 2.3014588]\n",
      "epoch:8 batch_done:30 Gen Loss: nan Disc Loss: nan Q Losses: [0.20607123, 2.3038752]\n",
      "epoch:8 batch_done:31 Gen Loss: nan Disc Loss: nan Q Losses: [0.19068614, 2.3015766]\n",
      "epoch:8 batch_done:32 Gen Loss: nan Disc Loss: nan Q Losses: [0.13771451, 2.3043759]\n",
      "epoch:8 batch_done:33 Gen Loss: nan Disc Loss: nan Q Losses: [0.18400964, 2.3046565]\n",
      "epoch:8 batch_done:34 Gen Loss: nan Disc Loss: nan Q Losses: [0.1686964, 2.303978]\n",
      "epoch:8 batch_done:35 Gen Loss: nan Disc Loss: nan Q Losses: [0.13516672, 2.3018835]\n",
      "epoch:8 batch_done:36 Gen Loss: nan Disc Loss: nan Q Losses: [0.15944758, 2.3025334]\n",
      "epoch:8 batch_done:37 Gen Loss: nan Disc Loss: nan Q Losses: [0.17306317, 2.3032155]\n",
      "epoch:8 batch_done:38 Gen Loss: nan Disc Loss: nan Q Losses: [0.1706745, 2.302979]\n",
      "epoch:8 batch_done:39 Gen Loss: nan Disc Loss: nan Q Losses: [0.15317389, 2.3041365]\n",
      "epoch:8 batch_done:40 Gen Loss: nan Disc Loss: nan Q Losses: [0.16023517, 2.3016248]\n",
      "epoch:8 batch_done:41 Gen Loss: nan Disc Loss: nan Q Losses: [0.14812285, 2.3011637]\n",
      "epoch:8 batch_done:42 Gen Loss: nan Disc Loss: nan Q Losses: [0.14818242, 2.3030653]\n",
      "epoch:8 batch_done:43 Gen Loss: nan Disc Loss: nan Q Losses: [0.15701677, 2.3015327]\n",
      "epoch:8 batch_done:44 Gen Loss: nan Disc Loss: nan Q Losses: [0.15221035, 2.3054521]\n",
      "epoch:8 batch_done:45 Gen Loss: nan Disc Loss: nan Q Losses: [0.17055491, 2.3016381]\n",
      "epoch:8 batch_done:46 Gen Loss: nan Disc Loss: nan Q Losses: [0.18357733, 2.2992373]\n",
      "epoch:8 batch_done:47 Gen Loss: nan Disc Loss: nan Q Losses: [0.17041764, 2.304523]\n",
      "epoch:8 batch_done:48 Gen Loss: nan Disc Loss: nan Q Losses: [0.15444833, 2.3020906]\n",
      "epoch:8 batch_done:49 Gen Loss: nan Disc Loss: nan Q Losses: [0.16875279, 2.3037882]\n",
      "epoch:8 batch_done:50 Gen Loss: nan Disc Loss: nan Q Losses: [0.17220172, 2.3037105]\n",
      "epoch:8 batch_done:51 Gen Loss: nan Disc Loss: nan Q Losses: [0.17596328, 2.3006442]\n",
      "epoch:8 batch_done:52 Gen Loss: nan Disc Loss: nan Q Losses: [0.1962612, 2.3035345]\n",
      "epoch:8 batch_done:53 Gen Loss: nan Disc Loss: nan Q Losses: [0.1937151, 2.300272]\n",
      "epoch:8 batch_done:54 Gen Loss: nan Disc Loss: nan Q Losses: [0.15237957, 2.302433]\n",
      "epoch:8 batch_done:55 Gen Loss: nan Disc Loss: nan Q Losses: [0.17237061, 2.3010874]\n",
      "epoch:8 batch_done:56 Gen Loss: nan Disc Loss: nan Q Losses: [0.19511138, 2.3007493]\n",
      "epoch:8 batch_done:57 Gen Loss: nan Disc Loss: nan Q Losses: [0.19421525, 2.3003669]\n",
      "epoch:8 batch_done:58 Gen Loss: nan Disc Loss: nan Q Losses: [0.15805657, 2.3039916]\n",
      "epoch:8 batch_done:59 Gen Loss: nan Disc Loss: nan Q Losses: [0.17325351, 2.3052542]\n",
      "epoch:8 batch_done:60 Gen Loss: nan Disc Loss: nan Q Losses: [0.16692603, 2.3028204]\n",
      "epoch:8 batch_done:61 Gen Loss: nan Disc Loss: nan Q Losses: [0.16112411, 2.3029602]\n",
      "epoch:8 batch_done:62 Gen Loss: nan Disc Loss: nan Q Losses: [0.15745565, 2.3011255]\n",
      "epoch:8 batch_done:63 Gen Loss: nan Disc Loss: nan Q Losses: [0.17426187, 2.3035324]\n",
      "epoch:8 batch_done:64 Gen Loss: nan Disc Loss: nan Q Losses: [0.13543317, 2.3014541]\n",
      "epoch:8 batch_done:65 Gen Loss: nan Disc Loss: nan Q Losses: [0.15365732, 2.3040531]\n",
      "epoch:8 batch_done:66 Gen Loss: nan Disc Loss: nan Q Losses: [0.13728654, 2.3030303]\n",
      "epoch:8 batch_done:67 Gen Loss: nan Disc Loss: nan Q Losses: [0.16255698, 2.303391]\n",
      "epoch:8 batch_done:68 Gen Loss: nan Disc Loss: nan Q Losses: [0.15957293, 2.3028104]\n",
      "epoch:8 batch_done:69 Gen Loss: nan Disc Loss: nan Q Losses: [0.18757308, 2.303453]\n",
      "epoch:8 batch_done:70 Gen Loss: nan Disc Loss: nan Q Losses: [0.17023343, 2.3030524]\n",
      "epoch:8 batch_done:71 Gen Loss: nan Disc Loss: nan Q Losses: [0.14550933, 2.3052626]\n",
      "epoch:8 batch_done:72 Gen Loss: nan Disc Loss: nan Q Losses: [0.13853736, 2.3009014]\n",
      "epoch:8 batch_done:73 Gen Loss: nan Disc Loss: nan Q Losses: [0.19112799, 2.304136]\n",
      "epoch:8 batch_done:74 Gen Loss: nan Disc Loss: nan Q Losses: [0.18986407, 2.3035135]\n",
      "epoch:8 batch_done:75 Gen Loss: nan Disc Loss: nan Q Losses: [0.15183067, 2.3013749]\n",
      "epoch:8 batch_done:76 Gen Loss: nan Disc Loss: nan Q Losses: [0.18407483, 2.3036869]\n",
      "epoch:8 batch_done:77 Gen Loss: nan Disc Loss: nan Q Losses: [0.17094567, 2.3022275]\n",
      "epoch:8 batch_done:78 Gen Loss: nan Disc Loss: nan Q Losses: [0.18286675, 2.3007345]\n",
      "epoch:8 batch_done:79 Gen Loss: nan Disc Loss: nan Q Losses: [0.15935384, 2.3027349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:80 Gen Loss: nan Disc Loss: nan Q Losses: [0.14430264, 2.3048515]\n",
      "epoch:8 batch_done:81 Gen Loss: nan Disc Loss: nan Q Losses: [0.16349912, 2.3027148]\n",
      "epoch:8 batch_done:82 Gen Loss: nan Disc Loss: nan Q Losses: [0.16123256, 2.3044662]\n",
      "epoch:8 batch_done:83 Gen Loss: nan Disc Loss: nan Q Losses: [0.14476608, 2.3013241]\n",
      "epoch:8 batch_done:84 Gen Loss: nan Disc Loss: nan Q Losses: [0.17208292, 2.3031361]\n",
      "epoch:8 batch_done:85 Gen Loss: nan Disc Loss: nan Q Losses: [0.17900917, 2.3016376]\n",
      "epoch:8 batch_done:86 Gen Loss: nan Disc Loss: nan Q Losses: [0.14583881, 2.3022904]\n",
      "epoch:8 batch_done:87 Gen Loss: nan Disc Loss: nan Q Losses: [0.1647839, 2.3042474]\n",
      "epoch:8 batch_done:88 Gen Loss: nan Disc Loss: nan Q Losses: [0.17170683, 2.301759]\n",
      "epoch:8 batch_done:89 Gen Loss: nan Disc Loss: nan Q Losses: [0.14889742, 2.3023825]\n",
      "epoch:8 batch_done:90 Gen Loss: nan Disc Loss: nan Q Losses: [0.1523141, 2.301954]\n",
      "epoch:8 batch_done:91 Gen Loss: nan Disc Loss: nan Q Losses: [0.14140143, 2.3022087]\n",
      "epoch:8 batch_done:92 Gen Loss: nan Disc Loss: nan Q Losses: [0.13030016, 2.304163]\n",
      "epoch:8 batch_done:93 Gen Loss: nan Disc Loss: nan Q Losses: [0.14544176, 2.3036702]\n",
      "epoch:8 batch_done:94 Gen Loss: nan Disc Loss: nan Q Losses: [0.18067089, 2.3024626]\n",
      "epoch:8 batch_done:95 Gen Loss: nan Disc Loss: nan Q Losses: [0.17896925, 2.3014011]\n",
      "epoch:8 batch_done:96 Gen Loss: nan Disc Loss: nan Q Losses: [0.14014244, 2.300704]\n",
      "epoch:8 batch_done:97 Gen Loss: nan Disc Loss: nan Q Losses: [0.13323069, 2.3032231]\n",
      "epoch:8 batch_done:98 Gen Loss: nan Disc Loss: nan Q Losses: [0.17755878, 2.3006954]\n",
      "epoch:8 batch_done:99 Gen Loss: nan Disc Loss: nan Q Losses: [0.17673935, 2.3025966]\n",
      "epoch:8 batch_done:100 Gen Loss: nan Disc Loss: nan Q Losses: [0.15154138, 2.3015714]\n",
      "epoch:8 batch_done:101 Gen Loss: nan Disc Loss: nan Q Losses: [0.17571452, 2.3011439]\n",
      "epoch:8 batch_done:102 Gen Loss: nan Disc Loss: nan Q Losses: [0.17648228, 2.3054097]\n",
      "epoch:8 batch_done:103 Gen Loss: nan Disc Loss: nan Q Losses: [0.13060567, 2.2994227]\n",
      "epoch:8 batch_done:104 Gen Loss: nan Disc Loss: nan Q Losses: [0.12707397, 2.3022335]\n",
      "epoch:8 batch_done:105 Gen Loss: nan Disc Loss: nan Q Losses: [0.13582417, 2.3035073]\n",
      "epoch:8 batch_done:106 Gen Loss: nan Disc Loss: nan Q Losses: [0.15582578, 2.3039939]\n",
      "epoch:8 batch_done:107 Gen Loss: nan Disc Loss: nan Q Losses: [0.13595895, 2.3038936]\n",
      "epoch:8 batch_done:108 Gen Loss: nan Disc Loss: nan Q Losses: [0.14801912, 2.3027654]\n",
      "epoch:8 batch_done:109 Gen Loss: nan Disc Loss: nan Q Losses: [0.16576698, 2.3022904]\n",
      "epoch:8 batch_done:110 Gen Loss: nan Disc Loss: nan Q Losses: [0.19348496, 2.3043423]\n",
      "epoch:8 batch_done:111 Gen Loss: nan Disc Loss: nan Q Losses: [0.15944648, 2.3024578]\n",
      "epoch:8 batch_done:112 Gen Loss: nan Disc Loss: nan Q Losses: [0.16515137, 2.3039341]\n",
      "epoch:8 batch_done:113 Gen Loss: nan Disc Loss: nan Q Losses: [0.15673828, 2.3016453]\n",
      "epoch:8 batch_done:114 Gen Loss: nan Disc Loss: nan Q Losses: [0.18385807, 2.3019571]\n",
      "epoch:8 batch_done:115 Gen Loss: nan Disc Loss: nan Q Losses: [0.17554453, 2.3031137]\n",
      "epoch:8 batch_done:116 Gen Loss: nan Disc Loss: nan Q Losses: [0.16807351, 2.3022556]\n",
      "epoch:8 batch_done:117 Gen Loss: nan Disc Loss: nan Q Losses: [0.18010801, 2.3013835]\n",
      "epoch:8 batch_done:118 Gen Loss: nan Disc Loss: nan Q Losses: [0.19046924, 2.3032205]\n",
      "epoch:8 batch_done:119 Gen Loss: nan Disc Loss: nan Q Losses: [0.16386169, 2.3030922]\n",
      "epoch:8 batch_done:120 Gen Loss: nan Disc Loss: nan Q Losses: [0.18743508, 2.3027124]\n",
      "epoch:8 batch_done:121 Gen Loss: nan Disc Loss: nan Q Losses: [0.17345569, 2.3014994]\n",
      "epoch:8 batch_done:122 Gen Loss: nan Disc Loss: nan Q Losses: [0.18184873, 2.3045287]\n",
      "epoch:8 batch_done:123 Gen Loss: nan Disc Loss: nan Q Losses: [0.15886575, 2.3037703]\n",
      "epoch:8 batch_done:124 Gen Loss: nan Disc Loss: nan Q Losses: [0.15530983, 2.3023643]\n",
      "epoch:8 batch_done:125 Gen Loss: nan Disc Loss: nan Q Losses: [0.14318, 2.3030851]\n",
      "epoch:8 batch_done:126 Gen Loss: nan Disc Loss: nan Q Losses: [0.19571486, 2.3057761]\n",
      "epoch:8 batch_done:127 Gen Loss: nan Disc Loss: nan Q Losses: [0.18872491, 2.3021672]\n",
      "epoch:8 batch_done:128 Gen Loss: nan Disc Loss: nan Q Losses: [0.15853533, 2.3012526]\n",
      "epoch:8 batch_done:129 Gen Loss: nan Disc Loss: nan Q Losses: [0.18078473, 2.3039463]\n",
      "epoch:8 batch_done:130 Gen Loss: nan Disc Loss: nan Q Losses: [0.12584904, 2.3036275]\n",
      "epoch:8 batch_done:131 Gen Loss: nan Disc Loss: nan Q Losses: [0.15770471, 2.3034937]\n",
      "epoch:8 batch_done:132 Gen Loss: nan Disc Loss: nan Q Losses: [0.17819631, 2.3008614]\n",
      "epoch:8 batch_done:133 Gen Loss: nan Disc Loss: nan Q Losses: [0.13258997, 2.3012605]\n",
      "epoch:8 batch_done:134 Gen Loss: nan Disc Loss: nan Q Losses: [0.1616374, 2.3004794]\n",
      "epoch:8 batch_done:135 Gen Loss: nan Disc Loss: nan Q Losses: [0.15438674, 2.301115]\n",
      "epoch:8 batch_done:136 Gen Loss: nan Disc Loss: nan Q Losses: [0.15199378, 2.3040061]\n",
      "epoch:8 batch_done:137 Gen Loss: nan Disc Loss: nan Q Losses: [0.16043395, 2.3029499]\n",
      "epoch:8 batch_done:138 Gen Loss: nan Disc Loss: nan Q Losses: [0.15138748, 2.302494]\n",
      "epoch:8 batch_done:139 Gen Loss: nan Disc Loss: nan Q Losses: [0.19059177, 2.3039773]\n",
      "epoch:8 batch_done:140 Gen Loss: nan Disc Loss: nan Q Losses: [0.17420457, 2.3018866]\n",
      "epoch:8 batch_done:141 Gen Loss: nan Disc Loss: nan Q Losses: [0.15024032, 2.3028483]\n",
      "epoch:8 batch_done:142 Gen Loss: nan Disc Loss: nan Q Losses: [0.15316106, 2.3023553]\n",
      "epoch:8 batch_done:143 Gen Loss: nan Disc Loss: nan Q Losses: [0.17017955, 2.3043339]\n",
      "epoch:8 batch_done:144 Gen Loss: nan Disc Loss: nan Q Losses: [0.16379677, 2.3040037]\n",
      "epoch:8 batch_done:145 Gen Loss: nan Disc Loss: nan Q Losses: [0.16014519, 2.3051057]\n",
      "epoch:8 batch_done:146 Gen Loss: nan Disc Loss: nan Q Losses: [0.17934939, 2.3033526]\n",
      "epoch:8 batch_done:147 Gen Loss: nan Disc Loss: nan Q Losses: [0.15568364, 2.3011701]\n",
      "epoch:8 batch_done:148 Gen Loss: nan Disc Loss: nan Q Losses: [0.11349775, 2.3008447]\n",
      "epoch:8 batch_done:149 Gen Loss: nan Disc Loss: nan Q Losses: [0.16814344, 2.3022809]\n",
      "epoch:8 batch_done:150 Gen Loss: nan Disc Loss: nan Q Losses: [0.17401397, 2.3022094]\n",
      "epoch:8 batch_done:151 Gen Loss: nan Disc Loss: nan Q Losses: [0.21252999, 2.3024609]\n",
      "epoch:8 batch_done:152 Gen Loss: nan Disc Loss: nan Q Losses: [0.20467803, 2.3021927]\n",
      "epoch:8 batch_done:153 Gen Loss: nan Disc Loss: nan Q Losses: [0.13785979, 2.3047163]\n",
      "epoch:8 batch_done:154 Gen Loss: nan Disc Loss: nan Q Losses: [0.15611768, 2.3032861]\n",
      "epoch:8 batch_done:155 Gen Loss: nan Disc Loss: nan Q Losses: [0.17662902, 2.3050735]\n",
      "epoch:8 batch_done:156 Gen Loss: nan Disc Loss: nan Q Losses: [0.12862101, 2.3008299]\n",
      "epoch:8 batch_done:157 Gen Loss: nan Disc Loss: nan Q Losses: [0.14852074, 2.3028953]\n",
      "epoch:8 batch_done:158 Gen Loss: nan Disc Loss: nan Q Losses: [0.20372853, 2.3025475]\n",
      "epoch:8 batch_done:159 Gen Loss: nan Disc Loss: nan Q Losses: [0.15910459, 2.3027194]\n",
      "epoch:8 batch_done:160 Gen Loss: nan Disc Loss: nan Q Losses: [0.18197501, 2.3024936]\n",
      "epoch:8 batch_done:161 Gen Loss: nan Disc Loss: nan Q Losses: [0.17867312, 2.301265]\n",
      "epoch:8 batch_done:162 Gen Loss: nan Disc Loss: nan Q Losses: [0.16167673, 2.3042769]\n",
      "epoch:8 batch_done:163 Gen Loss: nan Disc Loss: nan Q Losses: [0.19693393, 2.3032355]\n",
      "epoch:8 batch_done:164 Gen Loss: nan Disc Loss: nan Q Losses: [0.166345, 2.3017097]\n",
      "epoch:8 batch_done:165 Gen Loss: nan Disc Loss: nan Q Losses: [0.13536274, 2.3023829]\n",
      "epoch:8 batch_done:166 Gen Loss: nan Disc Loss: nan Q Losses: [0.14342527, 2.3034599]\n",
      "epoch:8 batch_done:167 Gen Loss: nan Disc Loss: nan Q Losses: [0.15635969, 2.3027601]\n",
      "epoch:8 batch_done:168 Gen Loss: nan Disc Loss: nan Q Losses: [0.174992, 2.3045771]\n",
      "epoch:8 batch_done:169 Gen Loss: nan Disc Loss: nan Q Losses: [0.15641469, 2.3026025]\n",
      "epoch:8 batch_done:170 Gen Loss: nan Disc Loss: nan Q Losses: [0.17518961, 2.3012781]\n",
      "epoch:8 batch_done:171 Gen Loss: nan Disc Loss: nan Q Losses: [0.13395408, 2.3019295]\n",
      "epoch:8 batch_done:172 Gen Loss: nan Disc Loss: nan Q Losses: [0.15076157, 2.3033824]\n",
      "epoch:8 batch_done:173 Gen Loss: nan Disc Loss: nan Q Losses: [0.1268436, 2.3023624]\n",
      "epoch:8 batch_done:174 Gen Loss: nan Disc Loss: nan Q Losses: [0.19935142, 2.3056474]\n",
      "epoch:8 batch_done:175 Gen Loss: nan Disc Loss: nan Q Losses: [0.16926971, 2.3028226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:176 Gen Loss: nan Disc Loss: nan Q Losses: [0.19277388, 2.3037114]\n",
      "epoch:8 batch_done:177 Gen Loss: nan Disc Loss: nan Q Losses: [0.15096232, 2.3011808]\n",
      "epoch:8 batch_done:178 Gen Loss: nan Disc Loss: nan Q Losses: [0.18142059, 2.3030515]\n",
      "epoch:8 batch_done:179 Gen Loss: nan Disc Loss: nan Q Losses: [0.15012866, 2.3010254]\n",
      "epoch:8 batch_done:180 Gen Loss: nan Disc Loss: nan Q Losses: [0.16117816, 2.3017282]\n",
      "epoch:8 batch_done:181 Gen Loss: nan Disc Loss: nan Q Losses: [0.19138768, 2.3030639]\n",
      "epoch:8 batch_done:182 Gen Loss: nan Disc Loss: nan Q Losses: [0.17076172, 2.3019943]\n",
      "epoch:8 batch_done:183 Gen Loss: nan Disc Loss: nan Q Losses: [0.16565034, 2.3039629]\n",
      "epoch:8 batch_done:184 Gen Loss: nan Disc Loss: nan Q Losses: [0.16251536, 2.3038306]\n",
      "epoch:8 batch_done:185 Gen Loss: nan Disc Loss: nan Q Losses: [0.15109605, 2.3044868]\n",
      "epoch:8 batch_done:186 Gen Loss: nan Disc Loss: nan Q Losses: [0.19205862, 2.3027663]\n",
      "epoch:8 batch_done:187 Gen Loss: nan Disc Loss: nan Q Losses: [0.18207258, 2.2996948]\n",
      "epoch:8 batch_done:188 Gen Loss: nan Disc Loss: nan Q Losses: [0.15253595, 2.3028398]\n",
      "epoch:8 batch_done:189 Gen Loss: nan Disc Loss: nan Q Losses: [0.16355971, 2.3028255]\n",
      "epoch:8 batch_done:190 Gen Loss: nan Disc Loss: nan Q Losses: [0.17335862, 2.3027189]\n",
      "epoch:8 batch_done:191 Gen Loss: nan Disc Loss: nan Q Losses: [0.18272014, 2.3034945]\n",
      "epoch:8 batch_done:192 Gen Loss: nan Disc Loss: nan Q Losses: [0.18128583, 2.3030748]\n",
      "epoch:8 batch_done:193 Gen Loss: nan Disc Loss: nan Q Losses: [0.16936573, 2.3040352]\n",
      "epoch:8 batch_done:194 Gen Loss: nan Disc Loss: nan Q Losses: [0.17793092, 2.3006258]\n",
      "epoch:8 batch_done:195 Gen Loss: nan Disc Loss: nan Q Losses: [0.1651457, 2.3017464]\n",
      "epoch:8 batch_done:196 Gen Loss: nan Disc Loss: nan Q Losses: [0.15568092, 2.2999363]\n",
      "epoch:8 batch_done:197 Gen Loss: nan Disc Loss: nan Q Losses: [0.15092936, 2.3040287]\n",
      "epoch:8 batch_done:198 Gen Loss: nan Disc Loss: nan Q Losses: [0.16957539, 2.3020091]\n",
      "epoch:8 batch_done:199 Gen Loss: nan Disc Loss: nan Q Losses: [0.19838235, 2.3014302]\n",
      "epoch:8 batch_done:200 Gen Loss: nan Disc Loss: nan Q Losses: [0.18207768, 2.302408]\n",
      "epoch:8 batch_done:201 Gen Loss: nan Disc Loss: nan Q Losses: [0.16110238, 2.3035493]\n",
      "epoch:8 batch_done:202 Gen Loss: nan Disc Loss: nan Q Losses: [0.12509623, 2.3043385]\n",
      "epoch:8 batch_done:203 Gen Loss: nan Disc Loss: nan Q Losses: [0.1615881, 2.3034663]\n",
      "epoch:8 batch_done:204 Gen Loss: nan Disc Loss: nan Q Losses: [0.17246602, 2.3052592]\n",
      "epoch:8 batch_done:205 Gen Loss: nan Disc Loss: nan Q Losses: [0.18482375, 2.2991605]\n",
      "epoch:8 batch_done:206 Gen Loss: nan Disc Loss: nan Q Losses: [0.15598764, 2.3022377]\n",
      "epoch:8 batch_done:207 Gen Loss: nan Disc Loss: nan Q Losses: [0.14862892, 2.3016779]\n",
      "epoch:9 batch_done:1 Gen Loss: nan Disc Loss: nan Q Losses: [0.16275521, 2.3047929]\n",
      "epoch:9 batch_done:2 Gen Loss: nan Disc Loss: nan Q Losses: [0.12689991, 2.3032508]\n",
      "epoch:9 batch_done:3 Gen Loss: nan Disc Loss: nan Q Losses: [0.16025344, 2.3010359]\n",
      "epoch:9 batch_done:4 Gen Loss: nan Disc Loss: nan Q Losses: [0.18070695, 2.3028498]\n",
      "epoch:9 batch_done:5 Gen Loss: nan Disc Loss: nan Q Losses: [0.17196752, 2.3026552]\n",
      "epoch:9 batch_done:6 Gen Loss: nan Disc Loss: nan Q Losses: [0.16082242, 2.30372]\n",
      "epoch:9 batch_done:7 Gen Loss: nan Disc Loss: nan Q Losses: [0.17572494, 2.3031051]\n",
      "epoch:9 batch_done:8 Gen Loss: nan Disc Loss: nan Q Losses: [0.176411, 2.3018913]\n",
      "epoch:9 batch_done:9 Gen Loss: nan Disc Loss: nan Q Losses: [0.17336974, 2.3030527]\n",
      "epoch:9 batch_done:10 Gen Loss: nan Disc Loss: nan Q Losses: [0.1575398, 2.3010094]\n",
      "epoch:9 batch_done:11 Gen Loss: nan Disc Loss: nan Q Losses: [0.14764899, 2.3039308]\n",
      "epoch:9 batch_done:12 Gen Loss: nan Disc Loss: nan Q Losses: [0.15564322, 2.3037207]\n",
      "epoch:9 batch_done:13 Gen Loss: nan Disc Loss: nan Q Losses: [0.14898962, 2.3033068]\n",
      "epoch:9 batch_done:14 Gen Loss: nan Disc Loss: nan Q Losses: [0.16799921, 2.2999282]\n",
      "epoch:9 batch_done:15 Gen Loss: nan Disc Loss: nan Q Losses: [0.16516769, 2.3008442]\n",
      "epoch:9 batch_done:16 Gen Loss: nan Disc Loss: nan Q Losses: [0.15240619, 2.3020122]\n",
      "epoch:9 batch_done:17 Gen Loss: nan Disc Loss: nan Q Losses: [0.14669511, 2.3013725]\n",
      "epoch:9 batch_done:18 Gen Loss: nan Disc Loss: nan Q Losses: [0.14124741, 2.3019509]\n",
      "epoch:9 batch_done:19 Gen Loss: nan Disc Loss: nan Q Losses: [0.17918149, 2.3034697]\n",
      "epoch:9 batch_done:20 Gen Loss: nan Disc Loss: nan Q Losses: [0.17424172, 2.3017237]\n",
      "epoch:9 batch_done:21 Gen Loss: nan Disc Loss: nan Q Losses: [0.14454135, 2.3024988]\n",
      "epoch:9 batch_done:22 Gen Loss: nan Disc Loss: nan Q Losses: [0.14404589, 2.3025742]\n",
      "epoch:9 batch_done:23 Gen Loss: nan Disc Loss: nan Q Losses: [0.13132197, 2.3053098]\n",
      "epoch:9 batch_done:24 Gen Loss: nan Disc Loss: nan Q Losses: [0.17503907, 2.3050928]\n",
      "epoch:9 batch_done:25 Gen Loss: nan Disc Loss: nan Q Losses: [0.15385698, 2.3039391]\n",
      "epoch:9 batch_done:26 Gen Loss: nan Disc Loss: nan Q Losses: [0.17429209, 2.3018479]\n",
      "epoch:9 batch_done:27 Gen Loss: nan Disc Loss: nan Q Losses: [0.18415931, 2.3015912]\n",
      "epoch:9 batch_done:28 Gen Loss: nan Disc Loss: nan Q Losses: [0.1546734, 2.3024971]\n",
      "epoch:9 batch_done:29 Gen Loss: nan Disc Loss: nan Q Losses: [0.15599626, 2.3001463]\n",
      "epoch:9 batch_done:30 Gen Loss: nan Disc Loss: nan Q Losses: [0.15989758, 2.3027568]\n",
      "epoch:9 batch_done:31 Gen Loss: nan Disc Loss: nan Q Losses: [0.20647338, 2.3013694]\n",
      "epoch:9 batch_done:32 Gen Loss: nan Disc Loss: nan Q Losses: [0.17533652, 2.3009267]\n",
      "epoch:9 batch_done:33 Gen Loss: nan Disc Loss: nan Q Losses: [0.17956299, 2.3027773]\n",
      "epoch:9 batch_done:34 Gen Loss: nan Disc Loss: nan Q Losses: [0.15537319, 2.3021827]\n",
      "epoch:9 batch_done:35 Gen Loss: nan Disc Loss: nan Q Losses: [0.16908845, 2.3022728]\n",
      "epoch:9 batch_done:36 Gen Loss: nan Disc Loss: nan Q Losses: [0.15814796, 2.3004131]\n",
      "epoch:9 batch_done:37 Gen Loss: nan Disc Loss: nan Q Losses: [0.16343614, 2.3038898]\n",
      "epoch:9 batch_done:38 Gen Loss: nan Disc Loss: nan Q Losses: [0.15713319, 2.3026166]\n",
      "epoch:9 batch_done:39 Gen Loss: nan Disc Loss: nan Q Losses: [0.17563024, 2.3041339]\n",
      "epoch:9 batch_done:40 Gen Loss: nan Disc Loss: nan Q Losses: [0.12735429, 2.3029411]\n",
      "epoch:9 batch_done:41 Gen Loss: nan Disc Loss: nan Q Losses: [0.19412526, 2.3033957]\n",
      "epoch:9 batch_done:42 Gen Loss: nan Disc Loss: nan Q Losses: [0.18728203, 2.3028982]\n",
      "epoch:9 batch_done:43 Gen Loss: nan Disc Loss: nan Q Losses: [0.14942703, 2.3033838]\n",
      "epoch:9 batch_done:44 Gen Loss: nan Disc Loss: nan Q Losses: [0.19785278, 2.3032465]\n",
      "epoch:9 batch_done:45 Gen Loss: nan Disc Loss: nan Q Losses: [0.21149346, 2.3022759]\n",
      "epoch:9 batch_done:46 Gen Loss: nan Disc Loss: nan Q Losses: [0.17511484, 2.3040342]\n",
      "epoch:9 batch_done:47 Gen Loss: nan Disc Loss: nan Q Losses: [0.15985449, 2.3031607]\n",
      "epoch:9 batch_done:48 Gen Loss: nan Disc Loss: nan Q Losses: [0.15580875, 2.3020947]\n",
      "epoch:9 batch_done:49 Gen Loss: nan Disc Loss: nan Q Losses: [0.13676694, 2.3011975]\n",
      "epoch:9 batch_done:50 Gen Loss: nan Disc Loss: nan Q Losses: [0.14775094, 2.3012948]\n",
      "epoch:9 batch_done:51 Gen Loss: nan Disc Loss: nan Q Losses: [0.1853559, 2.3027749]\n",
      "epoch:9 batch_done:52 Gen Loss: nan Disc Loss: nan Q Losses: [0.15360209, 2.3038301]\n",
      "epoch:9 batch_done:53 Gen Loss: nan Disc Loss: nan Q Losses: [0.15743539, 2.3033566]\n",
      "epoch:9 batch_done:54 Gen Loss: nan Disc Loss: nan Q Losses: [0.17861709, 2.3048816]\n",
      "epoch:9 batch_done:55 Gen Loss: nan Disc Loss: nan Q Losses: [0.16500707, 2.3039179]\n",
      "epoch:9 batch_done:56 Gen Loss: nan Disc Loss: nan Q Losses: [0.19091782, 2.305002]\n",
      "epoch:9 batch_done:57 Gen Loss: nan Disc Loss: nan Q Losses: [0.19046462, 2.3024964]\n",
      "epoch:9 batch_done:58 Gen Loss: nan Disc Loss: nan Q Losses: [0.16604763, 2.3027422]\n",
      "epoch:9 batch_done:59 Gen Loss: nan Disc Loss: nan Q Losses: [0.17702706, 2.3041813]\n",
      "epoch:9 batch_done:60 Gen Loss: nan Disc Loss: nan Q Losses: [0.13538998, 2.3012762]\n",
      "epoch:9 batch_done:61 Gen Loss: nan Disc Loss: nan Q Losses: [0.16366237, 2.2997942]\n",
      "epoch:9 batch_done:62 Gen Loss: nan Disc Loss: nan Q Losses: [0.18399888, 2.3010111]\n",
      "epoch:9 batch_done:63 Gen Loss: nan Disc Loss: nan Q Losses: [0.19128403, 2.302762]\n",
      "epoch:9 batch_done:64 Gen Loss: nan Disc Loss: nan Q Losses: [0.18292634, 2.3016295]\n",
      "epoch:9 batch_done:65 Gen Loss: nan Disc Loss: nan Q Losses: [0.18915476, 2.3037679]\n",
      "epoch:9 batch_done:66 Gen Loss: nan Disc Loss: nan Q Losses: [0.14965424, 2.3016782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 batch_done:67 Gen Loss: nan Disc Loss: nan Q Losses: [0.15527678, 2.304646]\n",
      "epoch:9 batch_done:68 Gen Loss: nan Disc Loss: nan Q Losses: [0.16669948, 2.3031709]\n",
      "epoch:9 batch_done:69 Gen Loss: nan Disc Loss: nan Q Losses: [0.15157077, 2.3012805]\n",
      "epoch:9 batch_done:70 Gen Loss: nan Disc Loss: nan Q Losses: [0.16373155, 2.3030005]\n",
      "epoch:9 batch_done:71 Gen Loss: nan Disc Loss: nan Q Losses: [0.14972392, 2.3026977]\n",
      "epoch:9 batch_done:72 Gen Loss: nan Disc Loss: nan Q Losses: [0.1694003, 2.3023336]\n",
      "epoch:9 batch_done:73 Gen Loss: nan Disc Loss: nan Q Losses: [0.16803825, 2.30339]\n",
      "epoch:9 batch_done:74 Gen Loss: nan Disc Loss: nan Q Losses: [0.16761717, 2.3016934]\n",
      "epoch:9 batch_done:75 Gen Loss: nan Disc Loss: nan Q Losses: [0.17494738, 2.3010106]\n",
      "epoch:9 batch_done:76 Gen Loss: nan Disc Loss: nan Q Losses: [0.16457349, 2.3040223]\n",
      "epoch:9 batch_done:77 Gen Loss: nan Disc Loss: nan Q Losses: [0.1843549, 2.3039808]\n",
      "epoch:9 batch_done:78 Gen Loss: nan Disc Loss: nan Q Losses: [0.15333036, 2.3009243]\n",
      "epoch:9 batch_done:79 Gen Loss: nan Disc Loss: nan Q Losses: [0.16882312, 2.3039]\n",
      "epoch:9 batch_done:80 Gen Loss: nan Disc Loss: nan Q Losses: [0.19018678, 2.3047802]\n",
      "epoch:9 batch_done:81 Gen Loss: nan Disc Loss: nan Q Losses: [0.18704601, 2.3032541]\n",
      "epoch:9 batch_done:82 Gen Loss: nan Disc Loss: nan Q Losses: [0.17671221, 2.3040335]\n",
      "epoch:9 batch_done:83 Gen Loss: nan Disc Loss: nan Q Losses: [0.16083063, 2.3014874]\n",
      "epoch:9 batch_done:84 Gen Loss: nan Disc Loss: nan Q Losses: [0.16045506, 2.3034854]\n",
      "epoch:9 batch_done:85 Gen Loss: nan Disc Loss: nan Q Losses: [0.15645722, 2.3035231]\n",
      "epoch:9 batch_done:86 Gen Loss: nan Disc Loss: nan Q Losses: [0.15808626, 2.3021154]\n",
      "epoch:9 batch_done:87 Gen Loss: nan Disc Loss: nan Q Losses: [0.16137558, 2.3026083]\n",
      "epoch:9 batch_done:88 Gen Loss: nan Disc Loss: nan Q Losses: [0.16246659, 2.3001924]\n",
      "epoch:9 batch_done:89 Gen Loss: nan Disc Loss: nan Q Losses: [0.17452988, 2.3022246]\n",
      "epoch:9 batch_done:90 Gen Loss: nan Disc Loss: nan Q Losses: [0.1441917, 2.3011427]\n",
      "epoch:9 batch_done:91 Gen Loss: nan Disc Loss: nan Q Losses: [0.18344039, 2.3005047]\n",
      "epoch:9 batch_done:92 Gen Loss: nan Disc Loss: nan Q Losses: [0.16985749, 2.3030639]\n",
      "epoch:9 batch_done:93 Gen Loss: nan Disc Loss: nan Q Losses: [0.1771495, 2.3009062]\n",
      "epoch:9 batch_done:94 Gen Loss: nan Disc Loss: nan Q Losses: [0.15419832, 2.3040216]\n",
      "epoch:9 batch_done:95 Gen Loss: nan Disc Loss: nan Q Losses: [0.18511447, 2.3011899]\n",
      "epoch:9 batch_done:96 Gen Loss: nan Disc Loss: nan Q Losses: [0.14534877, 2.3042755]\n",
      "epoch:9 batch_done:97 Gen Loss: nan Disc Loss: nan Q Losses: [0.18384446, 2.302433]\n",
      "epoch:9 batch_done:98 Gen Loss: nan Disc Loss: nan Q Losses: [0.18513042, 2.302155]\n",
      "epoch:9 batch_done:99 Gen Loss: nan Disc Loss: nan Q Losses: [0.15268438, 2.3027575]\n",
      "epoch:9 batch_done:100 Gen Loss: nan Disc Loss: nan Q Losses: [0.15922895, 2.3000846]\n",
      "epoch:9 batch_done:101 Gen Loss: nan Disc Loss: nan Q Losses: [0.14215484, 2.3027329]\n",
      "epoch:9 batch_done:102 Gen Loss: nan Disc Loss: nan Q Losses: [0.1795954, 2.3025632]\n",
      "epoch:9 batch_done:103 Gen Loss: nan Disc Loss: nan Q Losses: [0.1446166, 2.302968]\n",
      "epoch:9 batch_done:104 Gen Loss: nan Disc Loss: nan Q Losses: [0.17549261, 2.3014426]\n",
      "epoch:9 batch_done:105 Gen Loss: nan Disc Loss: nan Q Losses: [0.14626478, 2.3053322]\n",
      "epoch:9 batch_done:106 Gen Loss: nan Disc Loss: nan Q Losses: [0.16440962, 2.3022633]\n",
      "epoch:9 batch_done:107 Gen Loss: nan Disc Loss: nan Q Losses: [0.18073452, 2.3035488]\n",
      "epoch:9 batch_done:108 Gen Loss: nan Disc Loss: nan Q Losses: [0.17805658, 2.3019702]\n",
      "epoch:9 batch_done:109 Gen Loss: nan Disc Loss: nan Q Losses: [0.15237355, 2.3042271]\n",
      "epoch:9 batch_done:110 Gen Loss: nan Disc Loss: nan Q Losses: [0.18430358, 2.3001158]\n",
      "epoch:9 batch_done:111 Gen Loss: nan Disc Loss: nan Q Losses: [0.15915531, 2.3039591]\n",
      "epoch:9 batch_done:112 Gen Loss: nan Disc Loss: nan Q Losses: [0.15793648, 2.303761]\n",
      "epoch:9 batch_done:113 Gen Loss: nan Disc Loss: nan Q Losses: [0.1373243, 2.3009548]\n",
      "epoch:9 batch_done:114 Gen Loss: nan Disc Loss: nan Q Losses: [0.16826308, 2.3008828]\n",
      "epoch:9 batch_done:115 Gen Loss: nan Disc Loss: nan Q Losses: [0.15696737, 2.3038764]\n",
      "epoch:9 batch_done:116 Gen Loss: nan Disc Loss: nan Q Losses: [0.16106337, 2.304209]\n",
      "epoch:9 batch_done:117 Gen Loss: nan Disc Loss: nan Q Losses: [0.14498003, 2.3032398]\n",
      "epoch:9 batch_done:118 Gen Loss: nan Disc Loss: nan Q Losses: [0.17027104, 2.3023517]\n",
      "epoch:9 batch_done:119 Gen Loss: nan Disc Loss: nan Q Losses: [0.19921508, 2.300704]\n",
      "epoch:9 batch_done:120 Gen Loss: nan Disc Loss: nan Q Losses: [0.15787378, 2.302743]\n",
      "epoch:9 batch_done:121 Gen Loss: nan Disc Loss: nan Q Losses: [0.16214392, 2.302557]\n",
      "epoch:9 batch_done:122 Gen Loss: nan Disc Loss: nan Q Losses: [0.19881743, 2.3040533]\n",
      "epoch:9 batch_done:123 Gen Loss: nan Disc Loss: nan Q Losses: [0.1702725, 2.3028708]\n",
      "epoch:9 batch_done:124 Gen Loss: nan Disc Loss: nan Q Losses: [0.17251295, 2.3028231]\n",
      "epoch:9 batch_done:125 Gen Loss: nan Disc Loss: nan Q Losses: [0.14127716, 2.3019686]\n",
      "epoch:9 batch_done:126 Gen Loss: nan Disc Loss: nan Q Losses: [0.17827144, 2.3018148]\n",
      "epoch:9 batch_done:127 Gen Loss: nan Disc Loss: nan Q Losses: [0.14241198, 2.3014619]\n",
      "epoch:9 batch_done:128 Gen Loss: nan Disc Loss: nan Q Losses: [0.19073857, 2.3024421]\n",
      "epoch:9 batch_done:129 Gen Loss: nan Disc Loss: nan Q Losses: [0.16416226, 2.3025627]\n",
      "epoch:9 batch_done:130 Gen Loss: nan Disc Loss: nan Q Losses: [0.18724853, 2.3025761]\n",
      "epoch:9 batch_done:131 Gen Loss: nan Disc Loss: nan Q Losses: [0.18328342, 2.3017464]\n",
      "epoch:9 batch_done:132 Gen Loss: nan Disc Loss: nan Q Losses: [0.1788564, 2.3044629]\n",
      "epoch:9 batch_done:133 Gen Loss: nan Disc Loss: nan Q Losses: [0.16866697, 2.3030469]\n",
      "epoch:9 batch_done:134 Gen Loss: nan Disc Loss: nan Q Losses: [0.15609613, 2.3025341]\n",
      "epoch:9 batch_done:135 Gen Loss: nan Disc Loss: nan Q Losses: [0.20095214, 2.3023953]\n",
      "epoch:9 batch_done:136 Gen Loss: nan Disc Loss: nan Q Losses: [0.17976132, 2.3036652]\n",
      "epoch:9 batch_done:137 Gen Loss: nan Disc Loss: nan Q Losses: [0.19482709, 2.3020945]\n",
      "epoch:9 batch_done:138 Gen Loss: nan Disc Loss: nan Q Losses: [0.15697262, 2.3027637]\n",
      "epoch:9 batch_done:139 Gen Loss: nan Disc Loss: nan Q Losses: [0.15546194, 2.3027062]\n",
      "epoch:9 batch_done:140 Gen Loss: nan Disc Loss: nan Q Losses: [0.15545864, 2.3021221]\n",
      "epoch:9 batch_done:141 Gen Loss: nan Disc Loss: nan Q Losses: [0.15781714, 2.301682]\n",
      "epoch:9 batch_done:142 Gen Loss: nan Disc Loss: nan Q Losses: [0.13633664, 2.3028417]\n",
      "epoch:9 batch_done:143 Gen Loss: nan Disc Loss: nan Q Losses: [0.15660864, 2.3015614]\n",
      "epoch:9 batch_done:144 Gen Loss: nan Disc Loss: nan Q Losses: [0.16192448, 2.3025527]\n",
      "epoch:9 batch_done:145 Gen Loss: nan Disc Loss: nan Q Losses: [0.15605375, 2.3022239]\n",
      "epoch:9 batch_done:146 Gen Loss: nan Disc Loss: nan Q Losses: [0.16929743, 2.3028238]\n",
      "epoch:9 batch_done:147 Gen Loss: nan Disc Loss: nan Q Losses: [0.18133385, 2.3021483]\n",
      "epoch:9 batch_done:148 Gen Loss: nan Disc Loss: nan Q Losses: [0.16733679, 2.303761]\n",
      "epoch:9 batch_done:149 Gen Loss: nan Disc Loss: nan Q Losses: [0.14644738, 2.3037496]\n",
      "epoch:9 batch_done:150 Gen Loss: nan Disc Loss: nan Q Losses: [0.16017334, 2.3008928]\n",
      "epoch:9 batch_done:151 Gen Loss: nan Disc Loss: nan Q Losses: [0.18274088, 2.3022842]\n",
      "epoch:9 batch_done:152 Gen Loss: nan Disc Loss: nan Q Losses: [0.17576212, 2.3023162]\n",
      "epoch:9 batch_done:153 Gen Loss: nan Disc Loss: nan Q Losses: [0.16986504, 2.3029461]\n",
      "epoch:9 batch_done:154 Gen Loss: nan Disc Loss: nan Q Losses: [0.15051685, 2.3026519]\n",
      "epoch:9 batch_done:155 Gen Loss: nan Disc Loss: nan Q Losses: [0.16635601, 2.3027625]\n",
      "epoch:9 batch_done:156 Gen Loss: nan Disc Loss: nan Q Losses: [0.14145713, 2.3020263]\n",
      "epoch:9 batch_done:157 Gen Loss: nan Disc Loss: nan Q Losses: [0.14962243, 2.3008661]\n",
      "epoch:9 batch_done:158 Gen Loss: nan Disc Loss: nan Q Losses: [0.17447531, 2.302773]\n",
      "epoch:9 batch_done:159 Gen Loss: nan Disc Loss: nan Q Losses: [0.16945568, 2.3014269]\n",
      "epoch:9 batch_done:160 Gen Loss: nan Disc Loss: nan Q Losses: [0.18460056, 2.301722]\n",
      "epoch:9 batch_done:161 Gen Loss: nan Disc Loss: nan Q Losses: [0.16168271, 2.3023324]\n",
      "epoch:9 batch_done:162 Gen Loss: nan Disc Loss: nan Q Losses: [0.13208945, 2.3026903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 batch_done:163 Gen Loss: nan Disc Loss: nan Q Losses: [0.14994434, 2.3013353]\n",
      "epoch:9 batch_done:164 Gen Loss: nan Disc Loss: nan Q Losses: [0.19107784, 2.3027587]\n",
      "epoch:9 batch_done:165 Gen Loss: nan Disc Loss: nan Q Losses: [0.18583195, 2.3033643]\n",
      "epoch:9 batch_done:166 Gen Loss: nan Disc Loss: nan Q Losses: [0.18792368, 2.3044629]\n",
      "epoch:9 batch_done:167 Gen Loss: nan Disc Loss: nan Q Losses: [0.19095665, 2.2999966]\n",
      "epoch:9 batch_done:168 Gen Loss: nan Disc Loss: nan Q Losses: [0.17991355, 2.3037391]\n",
      "epoch:9 batch_done:169 Gen Loss: nan Disc Loss: nan Q Losses: [0.21810028, 2.3003197]\n",
      "epoch:9 batch_done:170 Gen Loss: nan Disc Loss: nan Q Losses: [0.18768877, 2.3004956]\n",
      "epoch:9 batch_done:171 Gen Loss: nan Disc Loss: nan Q Losses: [0.15069, 2.3018098]\n",
      "epoch:9 batch_done:172 Gen Loss: nan Disc Loss: nan Q Losses: [0.16860631, 2.3032165]\n",
      "epoch:9 batch_done:173 Gen Loss: nan Disc Loss: nan Q Losses: [0.16573709, 2.3009377]\n",
      "epoch:9 batch_done:174 Gen Loss: nan Disc Loss: nan Q Losses: [0.1922619, 2.3009064]\n",
      "epoch:9 batch_done:175 Gen Loss: nan Disc Loss: nan Q Losses: [0.15769666, 2.3040593]\n",
      "epoch:9 batch_done:176 Gen Loss: nan Disc Loss: nan Q Losses: [0.15939823, 2.3039289]\n",
      "epoch:9 batch_done:177 Gen Loss: nan Disc Loss: nan Q Losses: [0.16235778, 2.3014965]\n",
      "epoch:9 batch_done:178 Gen Loss: nan Disc Loss: nan Q Losses: [0.16488078, 2.3028698]\n",
      "epoch:9 batch_done:179 Gen Loss: nan Disc Loss: nan Q Losses: [0.14494643, 2.3053267]\n",
      "epoch:9 batch_done:180 Gen Loss: nan Disc Loss: nan Q Losses: [0.17798808, 2.3040302]\n",
      "epoch:9 batch_done:181 Gen Loss: nan Disc Loss: nan Q Losses: [0.15787572, 2.3008165]\n",
      "epoch:9 batch_done:182 Gen Loss: nan Disc Loss: nan Q Losses: [0.135106, 2.3014555]\n",
      "epoch:9 batch_done:183 Gen Loss: nan Disc Loss: nan Q Losses: [0.14096059, 2.3020797]\n",
      "epoch:9 batch_done:184 Gen Loss: nan Disc Loss: nan Q Losses: [0.1681847, 2.3019037]\n",
      "epoch:9 batch_done:185 Gen Loss: nan Disc Loss: nan Q Losses: [0.15933976, 2.3018205]\n",
      "epoch:9 batch_done:186 Gen Loss: nan Disc Loss: nan Q Losses: [0.16379735, 2.3016076]\n",
      "epoch:9 batch_done:187 Gen Loss: nan Disc Loss: nan Q Losses: [0.14546087, 2.3026865]\n",
      "epoch:9 batch_done:188 Gen Loss: nan Disc Loss: nan Q Losses: [0.17302474, 2.2999735]\n",
      "epoch:9 batch_done:189 Gen Loss: nan Disc Loss: nan Q Losses: [0.13174543, 2.3005266]\n",
      "epoch:9 batch_done:190 Gen Loss: nan Disc Loss: nan Q Losses: [0.1806834, 2.3005238]\n",
      "epoch:9 batch_done:191 Gen Loss: nan Disc Loss: nan Q Losses: [0.16461223, 2.3028703]\n",
      "epoch:9 batch_done:192 Gen Loss: nan Disc Loss: nan Q Losses: [0.19499071, 2.3036213]\n",
      "epoch:9 batch_done:193 Gen Loss: nan Disc Loss: nan Q Losses: [0.16557281, 2.3028605]\n",
      "epoch:9 batch_done:194 Gen Loss: nan Disc Loss: nan Q Losses: [0.15178877, 2.3053703]\n",
      "epoch:9 batch_done:195 Gen Loss: nan Disc Loss: nan Q Losses: [0.162045, 2.3035092]\n",
      "epoch:9 batch_done:196 Gen Loss: nan Disc Loss: nan Q Losses: [0.13737264, 2.3040738]\n",
      "epoch:9 batch_done:197 Gen Loss: nan Disc Loss: nan Q Losses: [0.17212656, 2.3035245]\n",
      "epoch:9 batch_done:198 Gen Loss: nan Disc Loss: nan Q Losses: [0.17738736, 2.3008592]\n",
      "epoch:9 batch_done:199 Gen Loss: nan Disc Loss: nan Q Losses: [0.16149479, 2.3028789]\n",
      "epoch:9 batch_done:200 Gen Loss: nan Disc Loss: nan Q Losses: [0.1538582, 2.3044169]\n",
      "epoch:9 batch_done:201 Gen Loss: nan Disc Loss: nan Q Losses: [0.17232147, 2.3035841]\n",
      "epoch:9 batch_done:202 Gen Loss: nan Disc Loss: nan Q Losses: [0.1702708, 2.3033042]\n",
      "epoch:9 batch_done:203 Gen Loss: nan Disc Loss: nan Q Losses: [0.16939095, 2.3032012]\n",
      "epoch:9 batch_done:204 Gen Loss: nan Disc Loss: nan Q Losses: [0.18272106, 2.302546]\n",
      "epoch:9 batch_done:205 Gen Loss: nan Disc Loss: nan Q Losses: [0.17528376, 2.304261]\n",
      "epoch:9 batch_done:206 Gen Loss: nan Disc Loss: nan Q Losses: [0.16701026, 2.3039255]\n",
      "epoch:9 batch_done:207 Gen Loss: nan Disc Loss: nan Q Losses: [0.15303877, 2.3021116]\n",
      "epoch:10 batch_done:1 Gen Loss: nan Disc Loss: nan Q Losses: [0.1838131, 2.3032737]\n",
      "epoch:10 batch_done:2 Gen Loss: nan Disc Loss: nan Q Losses: [0.13283736, 2.3038952]\n",
      "epoch:10 batch_done:3 Gen Loss: nan Disc Loss: nan Q Losses: [0.15501742, 2.3015549]\n",
      "epoch:10 batch_done:4 Gen Loss: nan Disc Loss: nan Q Losses: [0.15125, 2.3033419]\n",
      "epoch:10 batch_done:5 Gen Loss: nan Disc Loss: nan Q Losses: [0.22167677, 2.3027067]\n",
      "epoch:10 batch_done:6 Gen Loss: nan Disc Loss: nan Q Losses: [0.1943617, 2.3046179]\n",
      "epoch:10 batch_done:7 Gen Loss: nan Disc Loss: nan Q Losses: [0.18325129, 2.3047917]\n",
      "epoch:10 batch_done:8 Gen Loss: nan Disc Loss: nan Q Losses: [0.15726602, 2.3016639]\n",
      "epoch:10 batch_done:9 Gen Loss: nan Disc Loss: nan Q Losses: [0.15791318, 2.3036356]\n",
      "epoch:10 batch_done:10 Gen Loss: nan Disc Loss: nan Q Losses: [0.18665162, 2.3002193]\n",
      "epoch:10 batch_done:11 Gen Loss: nan Disc Loss: nan Q Losses: [0.18901065, 2.3016586]\n",
      "epoch:10 batch_done:12 Gen Loss: nan Disc Loss: nan Q Losses: [0.16584572, 2.3008537]\n",
      "epoch:10 batch_done:13 Gen Loss: nan Disc Loss: nan Q Losses: [0.16355419, 2.3029366]\n",
      "epoch:10 batch_done:14 Gen Loss: nan Disc Loss: nan Q Losses: [0.17683007, 2.3029013]\n",
      "epoch:10 batch_done:15 Gen Loss: nan Disc Loss: nan Q Losses: [0.15382856, 2.3024845]\n",
      "epoch:10 batch_done:16 Gen Loss: nan Disc Loss: nan Q Losses: [0.15601641, 2.3012891]\n",
      "epoch:10 batch_done:17 Gen Loss: nan Disc Loss: nan Q Losses: [0.16306055, 2.3025985]\n",
      "epoch:10 batch_done:18 Gen Loss: nan Disc Loss: nan Q Losses: [0.17037779, 2.3024719]\n",
      "epoch:10 batch_done:19 Gen Loss: nan Disc Loss: nan Q Losses: [0.14769316, 2.3058848]\n",
      "epoch:10 batch_done:20 Gen Loss: nan Disc Loss: nan Q Losses: [0.16999671, 2.3042705]\n",
      "epoch:10 batch_done:21 Gen Loss: nan Disc Loss: nan Q Losses: [0.17345354, 2.302274]\n",
      "epoch:10 batch_done:22 Gen Loss: nan Disc Loss: nan Q Losses: [0.15767452, 2.302351]\n",
      "epoch:10 batch_done:23 Gen Loss: nan Disc Loss: nan Q Losses: [0.11836945, 2.3018138]\n",
      "epoch:10 batch_done:24 Gen Loss: nan Disc Loss: nan Q Losses: [0.21030301, 2.3007066]\n",
      "epoch:10 batch_done:25 Gen Loss: nan Disc Loss: nan Q Losses: [0.21533921, 2.3035455]\n",
      "epoch:10 batch_done:26 Gen Loss: nan Disc Loss: nan Q Losses: [0.14877535, 2.3034708]\n",
      "epoch:10 batch_done:27 Gen Loss: nan Disc Loss: nan Q Losses: [0.16955248, 2.3029389]\n",
      "epoch:10 batch_done:28 Gen Loss: nan Disc Loss: nan Q Losses: [0.1516099, 2.3033042]\n",
      "epoch:10 batch_done:29 Gen Loss: nan Disc Loss: nan Q Losses: [0.16283748, 2.3028121]\n",
      "epoch:10 batch_done:30 Gen Loss: nan Disc Loss: nan Q Losses: [0.16249162, 2.304359]\n",
      "epoch:10 batch_done:31 Gen Loss: nan Disc Loss: nan Q Losses: [0.18729278, 2.3015838]\n",
      "epoch:10 batch_done:32 Gen Loss: nan Disc Loss: nan Q Losses: [0.15298982, 2.3030558]\n",
      "epoch:10 batch_done:33 Gen Loss: nan Disc Loss: nan Q Losses: [0.15104653, 2.3040905]\n",
      "epoch:10 batch_done:34 Gen Loss: nan Disc Loss: nan Q Losses: [0.14167994, 2.3017588]\n",
      "epoch:10 batch_done:35 Gen Loss: nan Disc Loss: nan Q Losses: [0.15843193, 2.3022218]\n",
      "epoch:10 batch_done:36 Gen Loss: nan Disc Loss: nan Q Losses: [0.16380899, 2.3018296]\n",
      "epoch:10 batch_done:37 Gen Loss: nan Disc Loss: nan Q Losses: [0.19733149, 2.3016577]\n",
      "epoch:10 batch_done:38 Gen Loss: nan Disc Loss: nan Q Losses: [0.16243538, 2.3012848]\n",
      "epoch:10 batch_done:39 Gen Loss: nan Disc Loss: nan Q Losses: [0.17283003, 2.3029842]\n",
      "epoch:10 batch_done:40 Gen Loss: nan Disc Loss: nan Q Losses: [0.1584416, 2.3024986]\n",
      "epoch:10 batch_done:41 Gen Loss: nan Disc Loss: nan Q Losses: [0.17277622, 2.3043985]\n",
      "epoch:10 batch_done:42 Gen Loss: nan Disc Loss: nan Q Losses: [0.1634115, 2.3023093]\n",
      "epoch:10 batch_done:43 Gen Loss: nan Disc Loss: nan Q Losses: [0.16228116, 2.3015637]\n",
      "epoch:10 batch_done:44 Gen Loss: nan Disc Loss: nan Q Losses: [0.1450198, 2.3026733]\n",
      "epoch:10 batch_done:45 Gen Loss: nan Disc Loss: nan Q Losses: [0.15079573, 2.3053551]\n",
      "epoch:10 batch_done:46 Gen Loss: nan Disc Loss: nan Q Losses: [0.14827076, 2.2985456]\n",
      "epoch:10 batch_done:47 Gen Loss: nan Disc Loss: nan Q Losses: [0.18607205, 2.3023458]\n",
      "epoch:10 batch_done:48 Gen Loss: nan Disc Loss: nan Q Losses: [0.14500669, 2.304018]\n",
      "epoch:10 batch_done:49 Gen Loss: nan Disc Loss: nan Q Losses: [0.17431495, 2.3007286]\n",
      "epoch:10 batch_done:50 Gen Loss: nan Disc Loss: nan Q Losses: [0.16269706, 2.3030014]\n",
      "epoch:10 batch_done:51 Gen Loss: nan Disc Loss: nan Q Losses: [0.17703064, 2.3012135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch_done:52 Gen Loss: nan Disc Loss: nan Q Losses: [0.16673706, 2.3040471]\n",
      "epoch:10 batch_done:53 Gen Loss: nan Disc Loss: nan Q Losses: [0.15030131, 2.3019638]\n",
      "epoch:10 batch_done:54 Gen Loss: nan Disc Loss: nan Q Losses: [0.17023194, 2.3033328]\n",
      "epoch:10 batch_done:55 Gen Loss: nan Disc Loss: nan Q Losses: [0.15790305, 2.3043227]\n",
      "epoch:10 batch_done:56 Gen Loss: nan Disc Loss: nan Q Losses: [0.15839465, 2.3015978]\n",
      "epoch:10 batch_done:57 Gen Loss: nan Disc Loss: nan Q Losses: [0.18853232, 2.3024805]\n",
      "epoch:10 batch_done:58 Gen Loss: nan Disc Loss: nan Q Losses: [0.14543091, 2.3040061]\n",
      "epoch:10 batch_done:59 Gen Loss: nan Disc Loss: nan Q Losses: [0.14061508, 2.3030965]\n",
      "epoch:10 batch_done:60 Gen Loss: nan Disc Loss: nan Q Losses: [0.16125154, 2.3023157]\n",
      "epoch:10 batch_done:61 Gen Loss: nan Disc Loss: nan Q Losses: [0.16707444, 2.3003964]\n",
      "epoch:10 batch_done:62 Gen Loss: nan Disc Loss: nan Q Losses: [0.16282839, 2.303483]\n",
      "epoch:10 batch_done:63 Gen Loss: nan Disc Loss: nan Q Losses: [0.18460369, 2.3016536]\n",
      "epoch:10 batch_done:64 Gen Loss: nan Disc Loss: nan Q Losses: [0.15345046, 2.3010945]\n",
      "epoch:10 batch_done:65 Gen Loss: nan Disc Loss: nan Q Losses: [0.15164509, 2.3016951]\n",
      "epoch:10 batch_done:66 Gen Loss: nan Disc Loss: nan Q Losses: [0.17862777, 2.3044984]\n",
      "epoch:10 batch_done:67 Gen Loss: nan Disc Loss: nan Q Losses: [0.17405605, 2.3020992]\n",
      "epoch:10 batch_done:68 Gen Loss: nan Disc Loss: nan Q Losses: [0.16348347, 2.302182]\n",
      "epoch:10 batch_done:69 Gen Loss: nan Disc Loss: nan Q Losses: [0.18778357, 2.30372]\n",
      "epoch:10 batch_done:70 Gen Loss: nan Disc Loss: nan Q Losses: [0.16511741, 2.3020382]\n",
      "epoch:10 batch_done:71 Gen Loss: nan Disc Loss: nan Q Losses: [0.19325712, 2.300401]\n",
      "epoch:10 batch_done:72 Gen Loss: nan Disc Loss: nan Q Losses: [0.17032459, 2.3022761]\n",
      "epoch:10 batch_done:73 Gen Loss: nan Disc Loss: nan Q Losses: [0.15268138, 2.3020184]\n",
      "epoch:10 batch_done:74 Gen Loss: nan Disc Loss: nan Q Losses: [0.19115308, 2.3003657]\n",
      "epoch:10 batch_done:75 Gen Loss: nan Disc Loss: nan Q Losses: [0.14117543, 2.3040819]\n",
      "epoch:10 batch_done:76 Gen Loss: nan Disc Loss: nan Q Losses: [0.17453909, 2.302114]\n",
      "epoch:10 batch_done:77 Gen Loss: nan Disc Loss: nan Q Losses: [0.16338523, 2.301569]\n",
      "epoch:10 batch_done:78 Gen Loss: nan Disc Loss: nan Q Losses: [0.17161766, 2.3025408]\n",
      "epoch:10 batch_done:79 Gen Loss: nan Disc Loss: nan Q Losses: [0.18201524, 2.302814]\n",
      "epoch:10 batch_done:80 Gen Loss: nan Disc Loss: nan Q Losses: [0.17269027, 2.3031995]\n",
      "epoch:10 batch_done:81 Gen Loss: nan Disc Loss: nan Q Losses: [0.135245, 2.3032231]\n",
      "epoch:10 batch_done:82 Gen Loss: nan Disc Loss: nan Q Losses: [0.15059893, 2.2992859]\n",
      "epoch:10 batch_done:83 Gen Loss: nan Disc Loss: nan Q Losses: [0.19497165, 2.3016849]\n",
      "epoch:10 batch_done:84 Gen Loss: nan Disc Loss: nan Q Losses: [0.1625495, 2.3022482]\n",
      "epoch:10 batch_done:85 Gen Loss: nan Disc Loss: nan Q Losses: [0.13640681, 2.3007829]\n",
      "epoch:10 batch_done:86 Gen Loss: nan Disc Loss: nan Q Losses: [0.19074762, 2.3027449]\n",
      "epoch:10 batch_done:87 Gen Loss: nan Disc Loss: nan Q Losses: [0.15148507, 2.3029518]\n",
      "epoch:10 batch_done:88 Gen Loss: nan Disc Loss: nan Q Losses: [0.1769831, 2.3008657]\n",
      "epoch:10 batch_done:89 Gen Loss: nan Disc Loss: nan Q Losses: [0.18643118, 2.3031757]\n",
      "epoch:10 batch_done:90 Gen Loss: nan Disc Loss: nan Q Losses: [0.13235039, 2.3017418]\n",
      "epoch:10 batch_done:91 Gen Loss: nan Disc Loss: nan Q Losses: [0.17643231, 2.302444]\n",
      "epoch:10 batch_done:92 Gen Loss: nan Disc Loss: nan Q Losses: [0.13705905, 2.3018322]\n",
      "epoch:10 batch_done:93 Gen Loss: nan Disc Loss: nan Q Losses: [0.1237233, 2.3007765]\n",
      "epoch:10 batch_done:94 Gen Loss: nan Disc Loss: nan Q Losses: [0.16875412, 2.3043337]\n",
      "epoch:10 batch_done:95 Gen Loss: nan Disc Loss: nan Q Losses: [0.18973726, 2.3025069]\n",
      "epoch:10 batch_done:96 Gen Loss: nan Disc Loss: nan Q Losses: [0.14589004, 2.3046312]\n",
      "epoch:10 batch_done:97 Gen Loss: nan Disc Loss: nan Q Losses: [0.17722726, 2.3030396]\n",
      "epoch:10 batch_done:98 Gen Loss: nan Disc Loss: nan Q Losses: [0.18633771, 2.3018363]\n",
      "epoch:10 batch_done:99 Gen Loss: nan Disc Loss: nan Q Losses: [0.14237431, 2.3031871]\n",
      "epoch:10 batch_done:100 Gen Loss: nan Disc Loss: nan Q Losses: [0.14960015, 2.3020458]\n",
      "epoch:10 batch_done:101 Gen Loss: nan Disc Loss: nan Q Losses: [0.16967264, 2.3030519]\n",
      "epoch:10 batch_done:102 Gen Loss: nan Disc Loss: nan Q Losses: [0.15947589, 2.3036022]\n",
      "epoch:10 batch_done:103 Gen Loss: nan Disc Loss: nan Q Losses: [0.15744212, 2.3056684]\n",
      "epoch:10 batch_done:104 Gen Loss: nan Disc Loss: nan Q Losses: [0.16116503, 2.3022501]\n",
      "epoch:10 batch_done:105 Gen Loss: nan Disc Loss: nan Q Losses: [0.1626737, 2.3030615]\n",
      "epoch:10 batch_done:106 Gen Loss: nan Disc Loss: nan Q Losses: [0.15423924, 2.3058081]\n",
      "epoch:10 batch_done:107 Gen Loss: nan Disc Loss: nan Q Losses: [0.14351848, 2.3029091]\n",
      "epoch:10 batch_done:108 Gen Loss: nan Disc Loss: nan Q Losses: [0.15495159, 2.3034282]\n",
      "epoch:10 batch_done:109 Gen Loss: nan Disc Loss: nan Q Losses: [0.17634095, 2.305027]\n",
      "epoch:10 batch_done:110 Gen Loss: nan Disc Loss: nan Q Losses: [0.16496766, 2.3028803]\n",
      "epoch:10 batch_done:111 Gen Loss: nan Disc Loss: nan Q Losses: [0.16381413, 2.3039792]\n",
      "epoch:10 batch_done:112 Gen Loss: nan Disc Loss: nan Q Losses: [0.20030424, 2.301661]\n",
      "epoch:10 batch_done:113 Gen Loss: nan Disc Loss: nan Q Losses: [0.18756732, 2.3020658]\n",
      "epoch:10 batch_done:114 Gen Loss: nan Disc Loss: nan Q Losses: [0.17425242, 2.3025396]\n",
      "epoch:10 batch_done:115 Gen Loss: nan Disc Loss: nan Q Losses: [0.13789211, 2.3010352]\n",
      "epoch:10 batch_done:116 Gen Loss: nan Disc Loss: nan Q Losses: [0.18457714, 2.3030934]\n",
      "epoch:10 batch_done:117 Gen Loss: nan Disc Loss: nan Q Losses: [0.17215699, 2.3021879]\n",
      "epoch:10 batch_done:118 Gen Loss: nan Disc Loss: nan Q Losses: [0.17074311, 2.3010535]\n",
      "epoch:10 batch_done:119 Gen Loss: nan Disc Loss: nan Q Losses: [0.17021167, 2.3023119]\n",
      "epoch:10 batch_done:120 Gen Loss: nan Disc Loss: nan Q Losses: [0.18377466, 2.3025224]\n",
      "epoch:10 batch_done:121 Gen Loss: nan Disc Loss: nan Q Losses: [0.15624054, 2.3034663]\n",
      "epoch:10 batch_done:122 Gen Loss: nan Disc Loss: nan Q Losses: [0.16500518, 2.3052769]\n",
      "epoch:10 batch_done:123 Gen Loss: nan Disc Loss: nan Q Losses: [0.16230449, 2.3007863]\n",
      "epoch:10 batch_done:124 Gen Loss: nan Disc Loss: nan Q Losses: [0.15244576, 2.3032081]\n",
      "epoch:10 batch_done:125 Gen Loss: nan Disc Loss: nan Q Losses: [0.1610962, 2.30371]\n",
      "epoch:10 batch_done:126 Gen Loss: nan Disc Loss: nan Q Losses: [0.14671201, 2.3023057]\n",
      "epoch:10 batch_done:127 Gen Loss: nan Disc Loss: nan Q Losses: [0.13238315, 2.3017709]\n",
      "epoch:10 batch_done:128 Gen Loss: nan Disc Loss: nan Q Losses: [0.16979924, 2.304172]\n",
      "epoch:10 batch_done:129 Gen Loss: nan Disc Loss: nan Q Losses: [0.18148541, 2.301743]\n",
      "epoch:10 batch_done:130 Gen Loss: nan Disc Loss: nan Q Losses: [0.14497451, 2.3015423]\n",
      "epoch:10 batch_done:131 Gen Loss: nan Disc Loss: nan Q Losses: [0.15042844, 2.3015006]\n",
      "epoch:10 batch_done:132 Gen Loss: nan Disc Loss: nan Q Losses: [0.19105369, 2.302968]\n",
      "epoch:10 batch_done:133 Gen Loss: nan Disc Loss: nan Q Losses: [0.15506139, 2.3046031]\n",
      "epoch:10 batch_done:134 Gen Loss: nan Disc Loss: nan Q Losses: [0.16763814, 2.303596]\n",
      "epoch:10 batch_done:135 Gen Loss: nan Disc Loss: nan Q Losses: [0.1794925, 2.3028235]\n",
      "epoch:10 batch_done:136 Gen Loss: nan Disc Loss: nan Q Losses: [0.14815092, 2.3006501]\n",
      "epoch:10 batch_done:137 Gen Loss: nan Disc Loss: nan Q Losses: [0.15165518, 2.3017519]\n",
      "epoch:10 batch_done:138 Gen Loss: nan Disc Loss: nan Q Losses: [0.15859848, 2.3023758]\n",
      "epoch:10 batch_done:139 Gen Loss: nan Disc Loss: nan Q Losses: [0.16385823, 2.3024626]\n",
      "epoch:10 batch_done:140 Gen Loss: nan Disc Loss: nan Q Losses: [0.16749126, 2.3012762]\n",
      "epoch:10 batch_done:141 Gen Loss: nan Disc Loss: nan Q Losses: [0.16995016, 2.3020387]\n",
      "epoch:10 batch_done:142 Gen Loss: nan Disc Loss: nan Q Losses: [0.13832609, 2.3031502]\n",
      "epoch:10 batch_done:143 Gen Loss: nan Disc Loss: nan Q Losses: [0.17798993, 2.302669]\n",
      "epoch:10 batch_done:144 Gen Loss: nan Disc Loss: nan Q Losses: [0.19058295, 2.3017817]\n",
      "epoch:10 batch_done:145 Gen Loss: nan Disc Loss: nan Q Losses: [0.15356144, 2.3036256]\n",
      "epoch:10 batch_done:146 Gen Loss: nan Disc Loss: nan Q Losses: [0.16720463, 2.3021746]\n",
      "epoch:10 batch_done:147 Gen Loss: nan Disc Loss: nan Q Losses: [0.14549042, 2.3008072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch_done:148 Gen Loss: nan Disc Loss: nan Q Losses: [0.17950176, 2.3032398]\n",
      "epoch:10 batch_done:149 Gen Loss: nan Disc Loss: nan Q Losses: [0.1613051, 2.302428]\n",
      "epoch:10 batch_done:150 Gen Loss: nan Disc Loss: nan Q Losses: [0.17886168, 2.3028679]\n",
      "epoch:10 batch_done:151 Gen Loss: nan Disc Loss: nan Q Losses: [0.15416804, 2.3015242]\n",
      "epoch:10 batch_done:152 Gen Loss: nan Disc Loss: nan Q Losses: [0.18672252, 2.3028781]\n",
      "epoch:10 batch_done:153 Gen Loss: nan Disc Loss: nan Q Losses: [0.1611837, 2.3022702]\n",
      "epoch:10 batch_done:154 Gen Loss: nan Disc Loss: nan Q Losses: [0.16070892, 2.3019598]\n",
      "epoch:10 batch_done:155 Gen Loss: nan Disc Loss: nan Q Losses: [0.15984958, 2.3019295]\n",
      "epoch:10 batch_done:156 Gen Loss: nan Disc Loss: nan Q Losses: [0.17115496, 2.3009977]\n",
      "epoch:10 batch_done:157 Gen Loss: nan Disc Loss: nan Q Losses: [0.17987153, 2.3031089]\n",
      "epoch:10 batch_done:158 Gen Loss: nan Disc Loss: nan Q Losses: [0.20884533, 2.3033586]\n",
      "epoch:10 batch_done:159 Gen Loss: nan Disc Loss: nan Q Losses: [0.17231622, 2.3019896]\n",
      "epoch:10 batch_done:160 Gen Loss: nan Disc Loss: nan Q Losses: [0.129526, 2.3032231]\n",
      "epoch:10 batch_done:161 Gen Loss: nan Disc Loss: nan Q Losses: [0.18896115, 2.3048756]\n",
      "epoch:10 batch_done:162 Gen Loss: nan Disc Loss: nan Q Losses: [0.18708079, 2.302115]\n",
      "epoch:10 batch_done:163 Gen Loss: nan Disc Loss: nan Q Losses: [0.16364615, 2.3030519]\n",
      "epoch:10 batch_done:164 Gen Loss: nan Disc Loss: nan Q Losses: [0.17177653, 2.3017778]\n",
      "epoch:10 batch_done:165 Gen Loss: nan Disc Loss: nan Q Losses: [0.15600739, 2.301651]\n",
      "epoch:10 batch_done:166 Gen Loss: nan Disc Loss: nan Q Losses: [0.16793938, 2.3032856]\n",
      "epoch:10 batch_done:167 Gen Loss: nan Disc Loss: nan Q Losses: [0.18971452, 2.3037946]\n",
      "epoch:10 batch_done:168 Gen Loss: nan Disc Loss: nan Q Losses: [0.19105649, 2.3027611]\n",
      "epoch:10 batch_done:169 Gen Loss: nan Disc Loss: nan Q Losses: [0.17792532, 2.3037913]\n",
      "epoch:10 batch_done:170 Gen Loss: nan Disc Loss: nan Q Losses: [0.14221793, 2.3034112]\n",
      "epoch:10 batch_done:171 Gen Loss: nan Disc Loss: nan Q Losses: [0.16971219, 2.3039186]\n",
      "epoch:10 batch_done:172 Gen Loss: nan Disc Loss: nan Q Losses: [0.15594485, 2.3030586]\n",
      "epoch:10 batch_done:173 Gen Loss: nan Disc Loss: nan Q Losses: [0.15678117, 2.3041978]\n",
      "epoch:10 batch_done:174 Gen Loss: nan Disc Loss: nan Q Losses: [0.16429228, 2.3022499]\n",
      "epoch:10 batch_done:175 Gen Loss: nan Disc Loss: nan Q Losses: [0.17226146, 2.3021693]\n",
      "epoch:10 batch_done:176 Gen Loss: nan Disc Loss: nan Q Losses: [0.14717266, 2.3007798]\n",
      "epoch:10 batch_done:177 Gen Loss: nan Disc Loss: nan Q Losses: [0.14072646, 2.3029544]\n",
      "epoch:10 batch_done:178 Gen Loss: nan Disc Loss: nan Q Losses: [0.16766334, 2.3037183]\n",
      "epoch:10 batch_done:179 Gen Loss: nan Disc Loss: nan Q Losses: [0.1385141, 2.3037064]\n",
      "epoch:10 batch_done:180 Gen Loss: nan Disc Loss: nan Q Losses: [0.15893222, 2.3031917]\n",
      "epoch:10 batch_done:181 Gen Loss: nan Disc Loss: nan Q Losses: [0.15019092, 2.3026834]\n",
      "epoch:10 batch_done:182 Gen Loss: nan Disc Loss: nan Q Losses: [0.17620444, 2.3027577]\n",
      "epoch:10 batch_done:183 Gen Loss: nan Disc Loss: nan Q Losses: [0.15235259, 2.3001177]\n",
      "epoch:10 batch_done:184 Gen Loss: nan Disc Loss: nan Q Losses: [0.16818386, 2.2993956]\n",
      "epoch:10 batch_done:185 Gen Loss: nan Disc Loss: nan Q Losses: [0.18909231, 2.3029094]\n",
      "epoch:10 batch_done:186 Gen Loss: nan Disc Loss: nan Q Losses: [0.15243354, 2.3036008]\n",
      "epoch:10 batch_done:187 Gen Loss: nan Disc Loss: nan Q Losses: [0.18299139, 2.3004262]\n",
      "epoch:10 batch_done:188 Gen Loss: nan Disc Loss: nan Q Losses: [0.18592764, 2.3039598]\n",
      "epoch:10 batch_done:189 Gen Loss: nan Disc Loss: nan Q Losses: [0.19268133, 2.302052]\n",
      "epoch:10 batch_done:190 Gen Loss: nan Disc Loss: nan Q Losses: [0.18346903, 2.302465]\n",
      "epoch:10 batch_done:191 Gen Loss: nan Disc Loss: nan Q Losses: [0.18313308, 2.3014622]\n",
      "epoch:10 batch_done:192 Gen Loss: nan Disc Loss: nan Q Losses: [0.14766449, 2.3031945]\n",
      "epoch:10 batch_done:193 Gen Loss: nan Disc Loss: nan Q Losses: [0.20007955, 2.3019955]\n",
      "epoch:10 batch_done:194 Gen Loss: nan Disc Loss: nan Q Losses: [0.16252133, 2.3033686]\n",
      "epoch:10 batch_done:195 Gen Loss: nan Disc Loss: nan Q Losses: [0.15489203, 2.3022785]\n",
      "epoch:10 batch_done:196 Gen Loss: nan Disc Loss: nan Q Losses: [0.18015458, 2.3036084]\n",
      "epoch:10 batch_done:197 Gen Loss: nan Disc Loss: nan Q Losses: [0.1597648, 2.3021092]\n",
      "epoch:10 batch_done:198 Gen Loss: nan Disc Loss: nan Q Losses: [0.15526319, 2.3051591]\n",
      "epoch:10 batch_done:199 Gen Loss: nan Disc Loss: nan Q Losses: [0.15469274, 2.3025162]\n",
      "epoch:10 batch_done:200 Gen Loss: nan Disc Loss: nan Q Losses: [0.13882193, 2.3012104]\n",
      "epoch:10 batch_done:201 Gen Loss: nan Disc Loss: nan Q Losses: [0.13744685, 2.3042107]\n",
      "epoch:10 batch_done:202 Gen Loss: nan Disc Loss: nan Q Losses: [0.13615568, 2.3037825]\n",
      "epoch:10 batch_done:203 Gen Loss: nan Disc Loss: nan Q Losses: [0.16325432, 2.302068]\n",
      "epoch:10 batch_done:204 Gen Loss: nan Disc Loss: nan Q Losses: [0.17689282, 2.304234]\n",
      "epoch:10 batch_done:205 Gen Loss: nan Disc Loss: nan Q Losses: [0.17579404, 2.3018088]\n",
      "epoch:10 batch_done:206 Gen Loss: nan Disc Loss: nan Q Losses: [0.16133972, 2.3033667]\n",
      "epoch:10 batch_done:207 Gen Loss: nan Disc Loss: nan Q Losses: [0.15686777, 2.3042555]\n",
      "Saved Model on  10\n",
      "epoch:11 batch_done:1 Gen Loss: nan Disc Loss: nan Q Losses: [0.18182783, 2.3023648]\n",
      "epoch:11 batch_done:2 Gen Loss: nan Disc Loss: nan Q Losses: [0.17459431, 2.3026989]\n",
      "epoch:11 batch_done:3 Gen Loss: nan Disc Loss: nan Q Losses: [0.19891754, 2.3030624]\n",
      "epoch:11 batch_done:4 Gen Loss: nan Disc Loss: nan Q Losses: [0.13490924, 2.301585]\n",
      "epoch:11 batch_done:5 Gen Loss: nan Disc Loss: nan Q Losses: [0.19096652, 2.3025684]\n",
      "epoch:11 batch_done:6 Gen Loss: nan Disc Loss: nan Q Losses: [0.15938817, 2.3015285]\n",
      "epoch:11 batch_done:7 Gen Loss: nan Disc Loss: nan Q Losses: [0.16304025, 2.3031549]\n",
      "epoch:11 batch_done:8 Gen Loss: nan Disc Loss: nan Q Losses: [0.19189498, 2.3040853]\n",
      "epoch:11 batch_done:9 Gen Loss: nan Disc Loss: nan Q Losses: [0.13932136, 2.3037765]\n",
      "epoch:11 batch_done:10 Gen Loss: nan Disc Loss: nan Q Losses: [0.14894658, 2.3025141]\n",
      "epoch:11 batch_done:11 Gen Loss: nan Disc Loss: nan Q Losses: [0.18155564, 2.3031011]\n",
      "epoch:11 batch_done:12 Gen Loss: nan Disc Loss: nan Q Losses: [0.15798911, 2.3021479]\n",
      "epoch:11 batch_done:13 Gen Loss: nan Disc Loss: nan Q Losses: [0.14648683, 2.3032682]\n",
      "epoch:11 batch_done:14 Gen Loss: nan Disc Loss: nan Q Losses: [0.14142615, 2.302351]\n",
      "epoch:11 batch_done:15 Gen Loss: nan Disc Loss: nan Q Losses: [0.16393109, 2.3022299]\n",
      "epoch:11 batch_done:16 Gen Loss: nan Disc Loss: nan Q Losses: [0.17483199, 2.3026791]\n",
      "epoch:11 batch_done:17 Gen Loss: nan Disc Loss: nan Q Losses: [0.1401659, 2.3023291]\n",
      "epoch:11 batch_done:18 Gen Loss: nan Disc Loss: nan Q Losses: [0.15284128, 2.3031287]\n",
      "epoch:11 batch_done:19 Gen Loss: nan Disc Loss: nan Q Losses: [0.21335098, 2.3038979]\n",
      "epoch:11 batch_done:20 Gen Loss: nan Disc Loss: nan Q Losses: [0.13223702, 2.3032339]\n",
      "epoch:11 batch_done:21 Gen Loss: nan Disc Loss: nan Q Losses: [0.15146105, 2.3031678]\n",
      "epoch:11 batch_done:22 Gen Loss: nan Disc Loss: nan Q Losses: [0.15503216, 2.3043692]\n",
      "epoch:11 batch_done:23 Gen Loss: nan Disc Loss: nan Q Losses: [0.20479661, 2.303798]\n",
      "epoch:11 batch_done:24 Gen Loss: nan Disc Loss: nan Q Losses: [0.14818045, 2.3038814]\n",
      "epoch:11 batch_done:25 Gen Loss: nan Disc Loss: nan Q Losses: [0.14813718, 2.3053794]\n",
      "epoch:11 batch_done:26 Gen Loss: nan Disc Loss: nan Q Losses: [0.16783312, 2.3019631]\n",
      "epoch:11 batch_done:27 Gen Loss: nan Disc Loss: nan Q Losses: [0.17777951, 2.3024487]\n",
      "epoch:11 batch_done:28 Gen Loss: nan Disc Loss: nan Q Losses: [0.17474151, 2.3038454]\n",
      "epoch:11 batch_done:29 Gen Loss: nan Disc Loss: nan Q Losses: [0.15515664, 2.3019462]\n",
      "epoch:11 batch_done:30 Gen Loss: nan Disc Loss: nan Q Losses: [0.16535601, 2.3013563]\n",
      "epoch:11 batch_done:31 Gen Loss: nan Disc Loss: nan Q Losses: [0.16305356, 2.3055372]\n",
      "epoch:11 batch_done:32 Gen Loss: nan Disc Loss: nan Q Losses: [0.16928013, 2.301971]\n",
      "epoch:11 batch_done:33 Gen Loss: nan Disc Loss: nan Q Losses: [0.16064113, 2.3037262]\n",
      "epoch:11 batch_done:34 Gen Loss: nan Disc Loss: nan Q Losses: [0.16980058, 2.3041272]\n",
      "epoch:11 batch_done:35 Gen Loss: nan Disc Loss: nan Q Losses: [0.1901948, 2.302114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 batch_done:36 Gen Loss: nan Disc Loss: nan Q Losses: [0.14699268, 2.3009071]\n",
      "epoch:11 batch_done:37 Gen Loss: nan Disc Loss: nan Q Losses: [0.17827363, 2.3018484]\n",
      "epoch:11 batch_done:38 Gen Loss: nan Disc Loss: nan Q Losses: [0.16100727, 2.3031259]\n",
      "epoch:11 batch_done:39 Gen Loss: nan Disc Loss: nan Q Losses: [0.16628501, 2.3040385]\n",
      "epoch:11 batch_done:40 Gen Loss: nan Disc Loss: nan Q Losses: [0.16883489, 2.3029149]\n",
      "epoch:11 batch_done:41 Gen Loss: nan Disc Loss: nan Q Losses: [0.19426537, 2.3011332]\n",
      "epoch:11 batch_done:42 Gen Loss: nan Disc Loss: nan Q Losses: [0.14268567, 2.301332]\n",
      "epoch:11 batch_done:43 Gen Loss: nan Disc Loss: nan Q Losses: [0.16535132, 2.3028131]\n",
      "epoch:11 batch_done:44 Gen Loss: nan Disc Loss: nan Q Losses: [0.17234345, 2.3027041]\n",
      "epoch:11 batch_done:45 Gen Loss: nan Disc Loss: nan Q Losses: [0.14944983, 2.3032188]\n",
      "epoch:11 batch_done:46 Gen Loss: nan Disc Loss: nan Q Losses: [0.14086767, 2.305618]\n",
      "epoch:11 batch_done:47 Gen Loss: nan Disc Loss: nan Q Losses: [0.14391039, 2.304388]\n",
      "epoch:11 batch_done:48 Gen Loss: nan Disc Loss: nan Q Losses: [0.15971769, 2.3018327]\n",
      "epoch:11 batch_done:49 Gen Loss: nan Disc Loss: nan Q Losses: [0.16877171, 2.3038275]\n",
      "epoch:11 batch_done:50 Gen Loss: nan Disc Loss: nan Q Losses: [0.19399057, 2.3014126]\n",
      "epoch:11 batch_done:51 Gen Loss: nan Disc Loss: nan Q Losses: [0.14024556, 2.3023133]\n",
      "epoch:11 batch_done:52 Gen Loss: nan Disc Loss: nan Q Losses: [0.14983103, 2.3044827]\n",
      "epoch:11 batch_done:53 Gen Loss: nan Disc Loss: nan Q Losses: [0.17009535, 2.3015432]\n",
      "epoch:11 batch_done:54 Gen Loss: nan Disc Loss: nan Q Losses: [0.17555729, 2.3014848]\n",
      "epoch:11 batch_done:55 Gen Loss: nan Disc Loss: nan Q Losses: [0.19186978, 2.3009832]\n",
      "epoch:11 batch_done:56 Gen Loss: nan Disc Loss: nan Q Losses: [0.19928172, 2.3032584]\n",
      "epoch:11 batch_done:57 Gen Loss: nan Disc Loss: nan Q Losses: [0.20241278, 2.3021879]\n",
      "epoch:11 batch_done:58 Gen Loss: nan Disc Loss: nan Q Losses: [0.13939369, 2.3025424]\n",
      "epoch:11 batch_done:59 Gen Loss: nan Disc Loss: nan Q Losses: [0.15926877, 2.3042178]\n",
      "epoch:11 batch_done:60 Gen Loss: nan Disc Loss: nan Q Losses: [0.1554932, 2.3018129]\n",
      "epoch:11 batch_done:61 Gen Loss: nan Disc Loss: nan Q Losses: [0.19816655, 2.3012099]\n",
      "epoch:11 batch_done:62 Gen Loss: nan Disc Loss: nan Q Losses: [0.16715658, 2.302434]\n",
      "epoch:11 batch_done:63 Gen Loss: nan Disc Loss: nan Q Losses: [0.15249626, 2.3036592]\n",
      "epoch:11 batch_done:64 Gen Loss: nan Disc Loss: nan Q Losses: [0.17684111, 2.3031061]\n",
      "epoch:11 batch_done:65 Gen Loss: nan Disc Loss: nan Q Losses: [0.17410851, 2.304013]\n",
      "epoch:11 batch_done:66 Gen Loss: nan Disc Loss: nan Q Losses: [0.14839518, 2.3028724]\n",
      "epoch:11 batch_done:67 Gen Loss: nan Disc Loss: nan Q Losses: [0.15545063, 2.3022263]\n",
      "epoch:11 batch_done:68 Gen Loss: nan Disc Loss: nan Q Losses: [0.16368705, 2.3033855]\n",
      "epoch:11 batch_done:69 Gen Loss: nan Disc Loss: nan Q Losses: [0.16804039, 2.3038599]\n",
      "epoch:11 batch_done:70 Gen Loss: nan Disc Loss: nan Q Losses: [0.13605535, 2.3006401]\n",
      "epoch:11 batch_done:71 Gen Loss: nan Disc Loss: nan Q Losses: [0.20287198, 2.3039446]\n",
      "epoch:11 batch_done:72 Gen Loss: nan Disc Loss: nan Q Losses: [0.17282176, 2.3037863]\n",
      "epoch:11 batch_done:73 Gen Loss: nan Disc Loss: nan Q Losses: [0.15159035, 2.3027244]\n",
      "epoch:11 batch_done:74 Gen Loss: nan Disc Loss: nan Q Losses: [0.14959458, 2.3025794]\n",
      "epoch:11 batch_done:75 Gen Loss: nan Disc Loss: nan Q Losses: [0.1673477, 2.3033316]\n",
      "epoch:11 batch_done:76 Gen Loss: nan Disc Loss: nan Q Losses: [0.1797637, 2.3024666]\n",
      "epoch:11 batch_done:77 Gen Loss: nan Disc Loss: nan Q Losses: [0.14698853, 2.3018594]\n",
      "epoch:11 batch_done:78 Gen Loss: nan Disc Loss: nan Q Losses: [0.17274559, 2.3031888]\n",
      "epoch:11 batch_done:79 Gen Loss: nan Disc Loss: nan Q Losses: [0.17183173, 2.3023665]\n",
      "epoch:11 batch_done:80 Gen Loss: nan Disc Loss: nan Q Losses: [0.16538957, 2.3002698]\n",
      "epoch:11 batch_done:81 Gen Loss: nan Disc Loss: nan Q Losses: [0.19268902, 2.3027625]\n",
      "epoch:11 batch_done:82 Gen Loss: nan Disc Loss: nan Q Losses: [0.18663816, 2.3031664]\n",
      "epoch:11 batch_done:83 Gen Loss: nan Disc Loss: nan Q Losses: [0.20637034, 2.3007319]\n",
      "epoch:11 batch_done:84 Gen Loss: nan Disc Loss: nan Q Losses: [0.12646437, 2.3029509]\n",
      "epoch:11 batch_done:85 Gen Loss: nan Disc Loss: nan Q Losses: [0.15979275, 2.3029289]\n",
      "epoch:11 batch_done:86 Gen Loss: nan Disc Loss: nan Q Losses: [0.18754207, 2.3020535]\n",
      "epoch:11 batch_done:87 Gen Loss: nan Disc Loss: nan Q Losses: [0.16781625, 2.3028939]\n",
      "epoch:11 batch_done:88 Gen Loss: nan Disc Loss: nan Q Losses: [0.16948977, 2.301569]\n",
      "epoch:11 batch_done:89 Gen Loss: nan Disc Loss: nan Q Losses: [0.16869645, 2.3028002]\n",
      "epoch:11 batch_done:90 Gen Loss: nan Disc Loss: nan Q Losses: [0.19167569, 2.3013909]\n",
      "epoch:11 batch_done:91 Gen Loss: nan Disc Loss: nan Q Losses: [0.17597239, 2.3016729]\n",
      "epoch:11 batch_done:92 Gen Loss: nan Disc Loss: nan Q Losses: [0.13787302, 2.3023572]\n",
      "epoch:11 batch_done:93 Gen Loss: nan Disc Loss: nan Q Losses: [0.17994931, 2.3023462]\n",
      "epoch:11 batch_done:94 Gen Loss: nan Disc Loss: nan Q Losses: [0.15846428, 2.3037615]\n",
      "epoch:11 batch_done:95 Gen Loss: nan Disc Loss: nan Q Losses: [0.17820752, 2.3026774]\n",
      "epoch:11 batch_done:96 Gen Loss: nan Disc Loss: nan Q Losses: [0.13938473, 2.3012316]\n",
      "epoch:11 batch_done:97 Gen Loss: nan Disc Loss: nan Q Losses: [0.18115549, 2.3025331]\n",
      "epoch:11 batch_done:98 Gen Loss: nan Disc Loss: nan Q Losses: [0.15022697, 2.3024521]\n",
      "epoch:11 batch_done:99 Gen Loss: nan Disc Loss: nan Q Losses: [0.16447607, 2.3023343]\n",
      "epoch:11 batch_done:100 Gen Loss: nan Disc Loss: nan Q Losses: [0.1383484, 2.3039625]\n",
      "epoch:11 batch_done:101 Gen Loss: nan Disc Loss: nan Q Losses: [0.14109698, 2.3041925]\n",
      "epoch:11 batch_done:102 Gen Loss: nan Disc Loss: nan Q Losses: [0.14838649, 2.3024781]\n",
      "epoch:11 batch_done:103 Gen Loss: nan Disc Loss: nan Q Losses: [0.15197165, 2.3041885]\n",
      "epoch:11 batch_done:104 Gen Loss: nan Disc Loss: nan Q Losses: [0.14946723, 2.3035307]\n",
      "epoch:11 batch_done:105 Gen Loss: nan Disc Loss: nan Q Losses: [0.1955971, 2.3024755]\n",
      "epoch:11 batch_done:106 Gen Loss: nan Disc Loss: nan Q Losses: [0.17421204, 2.3019695]\n",
      "epoch:11 batch_done:107 Gen Loss: nan Disc Loss: nan Q Losses: [0.13040827, 2.3038974]\n",
      "epoch:11 batch_done:108 Gen Loss: nan Disc Loss: nan Q Losses: [0.142267, 2.3011103]\n",
      "epoch:11 batch_done:109 Gen Loss: nan Disc Loss: nan Q Losses: [0.14064631, 2.3010206]\n",
      "epoch:11 batch_done:110 Gen Loss: nan Disc Loss: nan Q Losses: [0.1769768, 2.301918]\n",
      "epoch:11 batch_done:111 Gen Loss: nan Disc Loss: nan Q Losses: [0.19190705, 2.3036306]\n",
      "epoch:11 batch_done:112 Gen Loss: nan Disc Loss: nan Q Losses: [0.18238325, 2.3040447]\n",
      "epoch:11 batch_done:113 Gen Loss: nan Disc Loss: nan Q Losses: [0.167868, 2.3016605]\n",
      "epoch:11 batch_done:114 Gen Loss: nan Disc Loss: nan Q Losses: [0.17643581, 2.3006692]\n",
      "epoch:11 batch_done:115 Gen Loss: nan Disc Loss: nan Q Losses: [0.17360479, 2.3011069]\n",
      "epoch:11 batch_done:116 Gen Loss: nan Disc Loss: nan Q Losses: [0.16532964, 2.3042254]\n",
      "epoch:11 batch_done:117 Gen Loss: nan Disc Loss: nan Q Losses: [0.20760253, 2.3037896]\n",
      "epoch:11 batch_done:118 Gen Loss: nan Disc Loss: nan Q Losses: [0.18521467, 2.3030634]\n",
      "epoch:11 batch_done:119 Gen Loss: nan Disc Loss: nan Q Losses: [0.13746633, 2.3007658]\n",
      "epoch:11 batch_done:120 Gen Loss: nan Disc Loss: nan Q Losses: [0.1606234, 2.3029695]\n",
      "epoch:11 batch_done:121 Gen Loss: nan Disc Loss: nan Q Losses: [0.17420757, 2.3027771]\n",
      "epoch:11 batch_done:122 Gen Loss: nan Disc Loss: nan Q Losses: [0.16912818, 2.3033361]\n",
      "epoch:11 batch_done:123 Gen Loss: nan Disc Loss: nan Q Losses: [0.13074404, 2.3043215]\n",
      "epoch:11 batch_done:124 Gen Loss: nan Disc Loss: nan Q Losses: [0.15488741, 2.3041747]\n",
      "epoch:11 batch_done:125 Gen Loss: nan Disc Loss: nan Q Losses: [0.16184768, 2.3046603]\n",
      "epoch:11 batch_done:126 Gen Loss: nan Disc Loss: nan Q Losses: [0.15928775, 2.3007469]\n",
      "epoch:11 batch_done:127 Gen Loss: nan Disc Loss: nan Q Losses: [0.13033777, 2.3036873]\n",
      "epoch:11 batch_done:128 Gen Loss: nan Disc Loss: nan Q Losses: [0.1757523, 2.304203]\n",
      "epoch:11 batch_done:129 Gen Loss: nan Disc Loss: nan Q Losses: [0.14400627, 2.3011043]\n",
      "epoch:11 batch_done:130 Gen Loss: nan Disc Loss: nan Q Losses: [0.1417025, 2.3032331]\n",
      "epoch:11 batch_done:131 Gen Loss: nan Disc Loss: nan Q Losses: [0.15217602, 2.3028932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 batch_done:132 Gen Loss: nan Disc Loss: nan Q Losses: [0.18676773, 2.3019378]\n",
      "epoch:11 batch_done:133 Gen Loss: nan Disc Loss: nan Q Losses: [0.19059274, 2.302151]\n",
      "epoch:11 batch_done:134 Gen Loss: nan Disc Loss: nan Q Losses: [0.15178475, 2.3011532]\n",
      "epoch:11 batch_done:135 Gen Loss: nan Disc Loss: nan Q Losses: [0.17691568, 2.3008964]\n",
      "epoch:11 batch_done:136 Gen Loss: nan Disc Loss: nan Q Losses: [0.14319187, 2.3029506]\n",
      "epoch:11 batch_done:137 Gen Loss: nan Disc Loss: nan Q Losses: [0.14940996, 2.303354]\n",
      "epoch:11 batch_done:138 Gen Loss: nan Disc Loss: nan Q Losses: [0.13158064, 2.3031077]\n",
      "epoch:11 batch_done:139 Gen Loss: nan Disc Loss: nan Q Losses: [0.16987012, 2.3021739]\n"
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "# blow up after 81800\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/Session#run\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "c_val = 10\n",
    "\n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 1000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        train_data_filenames=permutation(train_data_filenames) # mini-batch\n",
    "        data_left = len(train_data_filenames)\n",
    "        batch_counter = 0\n",
    "        while data_left>0:\n",
    "            batch_size_to_train = min(batch_size, data_left)          \n",
    "            \n",
    "            zs = np.random.uniform(-1.0,1.0,size=[batch_size_to_train,z_size]).astype(np.float32) #Generate a random z batch\n",
    "            #print(\"zs shape:\",zs.shape)\n",
    "            \n",
    "            #lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "            lcat = np.random.randint(0,c_val,[batch_size_to_train,len(categorical_list)]) #Generate random c batch\n",
    "            \n",
    "            lcont = np.random.uniform(-1,1,[batch_size_to_train,number_continuous]) #\n",
    "\n",
    "            #xs = read_train_data_random_batch(train_data_filenames, batchsize=batch_size_to_train)\n",
    "            xs = read_train_data_mini_batch(train_data_filenames, batch_counter*batch_size, batch_size_to_train)\n",
    "            \n",
    "            _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "            _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "            _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "\n",
    "            data_left = data_left - batch_size_to_train\n",
    "            batch_counter +=1\n",
    "            if batch_counter%1 == 0 or data_left == 0:\n",
    "                z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32)\n",
    "                lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "                a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "                b = np.reshape(a,[c_val*c_val,1])\n",
    "                lcont_sample = b\n",
    "                samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample})\n",
    "                if not os.path.exists(sample_directory):\n",
    "                    os.makedirs(sample_directory)\n",
    "                save_images(np.reshape(samples[0:100],[100,32,32,3]),[10,10],sample_directory+'/fig'\\\n",
    "                            +str(i)+'_'+str(batch_counter)+'.png')\n",
    "                \n",
    "                print (\"epoch:\"+str(i)+\" batch_done:\"+str(batch_counter) \\\n",
    "                       +\" Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            \n",
    "             \n",
    "        \"\"\"\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "            #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "            latent_fixed = np.ones((c_val*c_val,1))\n",
    "            lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "            \n",
    "            #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "            #b = np.reshape(a,[100,1])\n",
    "            b = np.reshape(a,[c_val*c_val,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            #\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "            save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig'+str(i)+'.png')\n",
    "        \"\"\"\n",
    "        \n",
    "        if i % 10 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model on \", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\"\"\"\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "c_val = 10\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "    #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #lcat_sample = np.reshape(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]),[100,1])\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "    #print(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]))\n",
    "    #latent_fixed = np.ones((c_val*c_val,1))*50\n",
    "    latent_fixed = np.zeros((c_val*c_val,1))\n",
    "    #lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "    # good shape\n",
    "    lcat_sample = np.hstack([lcat_sample,latent_fixed])\n",
    "            \n",
    "    #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #a = a = np.ones((c_val*c_val,1))*-0.5\n",
    "    #a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #b = np.reshape(a,[100,1])\n",
    "    b = np.reshape(a,[c_val*c_val,1])\n",
    "    #c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)\n",
    "    c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)+8\n",
    "    #angle\n",
    "    lcont_sample = np.hstack([b,c])\n",
    "    # width\n",
    "    #lcont_sample = np.hstack([c,b])\n",
    "    \n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test_4'+'.png')\n",
    "    save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig_test_13'+'.png')\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35tf]",
   "language": "python",
   "name": "conda-env-dlpy35tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
