{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorials walks through an implementation of InfoGAN as described in [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n",
    "\n",
    "To learn more about InfoGAN, see this [Medium post](https://medium.com/p/dd710852db46) on them. To lean more about GANs generally, see [this one](https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.692jyamki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=128,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=64,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=32,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,32,[3,3],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,64,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,128,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 64 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 2 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "z_lats.append(z_in)\n",
    "z_lats.append(latent_cont_in)\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log((Dg/(1-Dg)))) #KL Divergence optimizer\n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.002,beta1=0.5)\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.79031 Disc Loss: 1.5016 Q Losses: [0.32335082, 2.3207371]\n",
      "Gen Loss: 1.82406 Disc Loss: 0.609251 Q Losses: [0.31904018, 2.23101]\n",
      "Gen Loss: 0.876855 Disc Loss: 1.16787 Q Losses: [0.26609984, 2.1999965]\n",
      "Gen Loss: -0.304921 Disc Loss: 0.91534 Q Losses: [0.295378, 2.1652937]\n",
      "Gen Loss: 2.09705 Disc Loss: 0.614919 Q Losses: [0.26285461, 2.1180503]\n",
      "Gen Loss: -0.0676654 Disc Loss: 0.806385 Q Losses: [0.23404573, 1.9408627]\n",
      "Gen Loss: 3.13505 Disc Loss: 1.23561 Q Losses: [0.21497063, 1.6629473]\n",
      "Gen Loss: 2.4614 Disc Loss: 0.784163 Q Losses: [0.22309634, 1.1739209]\n",
      "Gen Loss: 2.34707 Disc Loss: 1.06216 Q Losses: [0.23212638, 0.79810965]\n",
      "Gen Loss: 1.73454 Disc Loss: 1.15643 Q Losses: [0.22892669, 0.55085021]\n",
      "Gen Loss: 0.994044 Disc Loss: 0.701606 Q Losses: [0.24068964, 0.34241346]\n",
      "Saved Model\n",
      "Gen Loss: 2.74178 Disc Loss: 0.801123 Q Losses: [0.22962236, 0.33507472]\n",
      "Gen Loss: -0.0067191 Disc Loss: 1.05469 Q Losses: [0.16314949, 0.13594747]\n",
      "Gen Loss: -0.214347 Disc Loss: 1.26753 Q Losses: [0.1811783, 0.19352207]\n",
      "Gen Loss: 0.756961 Disc Loss: 1.02046 Q Losses: [0.14161684, 0.20004643]\n",
      "Gen Loss: 0.693703 Disc Loss: 0.929356 Q Losses: [0.17053454, 0.1535918]\n",
      "Gen Loss: 0.39533 Disc Loss: 1.08955 Q Losses: [0.14445981, 0.22008735]\n",
      "Gen Loss: -0.533394 Disc Loss: 1.0978 Q Losses: [0.11658743, 0.11027337]\n",
      "Gen Loss: -0.482195 Disc Loss: 1.35607 Q Losses: [0.14054239, 0.12878354]\n",
      "Gen Loss: 1.04223 Disc Loss: 0.832413 Q Losses: [0.12417454, 0.095452026]\n",
      "Gen Loss: 0.803241 Disc Loss: 0.987482 Q Losses: [0.12637427, 0.087129183]\n",
      "Saved Model\n",
      "Gen Loss: -0.0388902 Disc Loss: 1.01496 Q Losses: [0.13237113, 0.16305146]\n",
      "Gen Loss: 0.585151 Disc Loss: 0.895889 Q Losses: [0.088503256, 0.13581939]\n",
      "Gen Loss: 0.651089 Disc Loss: 1.03234 Q Losses: [0.10722564, 0.069598541]\n",
      "Gen Loss: -0.0435091 Disc Loss: 1.17908 Q Losses: [0.10213051, 0.071147241]\n",
      "Gen Loss: -0.271386 Disc Loss: 1.03695 Q Losses: [0.1325261, 0.06876865]\n",
      "Gen Loss: 1.77769 Disc Loss: 1.01178 Q Losses: [0.10399564, 0.03512577]\n",
      "Gen Loss: 2.22594 Disc Loss: 1.60842 Q Losses: [0.10847273, 0.070275337]\n",
      "Gen Loss: 0.415466 Disc Loss: 0.987591 Q Losses: [0.097955503, 0.1169348]\n",
      "Gen Loss: 2.09219 Disc Loss: 0.716316 Q Losses: [0.089402467, 0.10229723]\n",
      "Gen Loss: 0.18152 Disc Loss: 1.05486 Q Losses: [0.094744585, 0.10159811]\n",
      "Saved Model\n",
      "Gen Loss: -1.93412 Disc Loss: 1.84602 Q Losses: [0.10799573, 0.13352823]\n",
      "Gen Loss: 0.976419 Disc Loss: 0.891935 Q Losses: [0.10958222, 0.065296911]\n",
      "Gen Loss: 0.602344 Disc Loss: 1.27706 Q Losses: [0.063121833, 0.081752576]\n",
      "Gen Loss: 1.06808 Disc Loss: 0.992115 Q Losses: [0.10423876, 0.12760097]\n",
      "Gen Loss: 1.07563 Disc Loss: 1.11696 Q Losses: [0.10392302, 0.12226744]\n",
      "Gen Loss: 0.897523 Disc Loss: 1.08686 Q Losses: [0.10406408, 0.066601187]\n",
      "Gen Loss: 0.603116 Disc Loss: 1.07134 Q Losses: [0.088430457, 0.13914402]\n",
      "Gen Loss: 0.525444 Disc Loss: 1.07972 Q Losses: [0.07858108, 0.030633828]\n",
      "Gen Loss: 0.0841319 Disc Loss: 1.18585 Q Losses: [0.081860155, 0.03791862]\n",
      "Gen Loss: 0.0773182 Disc Loss: 1.18954 Q Losses: [0.093680963, 0.027729429]\n",
      "Saved Model\n",
      "Gen Loss: 0.984699 Disc Loss: 1.06065 Q Losses: [0.088444419, 0.033202287]\n",
      "Gen Loss: -0.130443 Disc Loss: 1.1862 Q Losses: [0.087929495, 0.15931216]\n",
      "Gen Loss: 0.593418 Disc Loss: 1.01159 Q Losses: [0.08793027, 0.039834887]\n",
      "Gen Loss: 0.410129 Disc Loss: 1.09367 Q Losses: [0.094371349, 0.10840876]\n",
      "Gen Loss: 0.648203 Disc Loss: 1.09516 Q Losses: [0.077409804, 0.012759382]\n",
      "Gen Loss: 0.193583 Disc Loss: 0.953999 Q Losses: [0.05610358, 0.017941624]\n",
      "Gen Loss: 0.623817 Disc Loss: 0.933614 Q Losses: [0.069843419, 0.046942264]\n",
      "Gen Loss: 0.27684 Disc Loss: 1.22407 Q Losses: [0.078649297, 0.10340128]\n",
      "Gen Loss: -0.588478 Disc Loss: 1.17809 Q Losses: [0.065072343, 0.017781744]\n",
      "Gen Loss: 1.85822 Disc Loss: 0.944656 Q Losses: [0.071264043, 0.030828206]\n",
      "Saved Model\n",
      "Gen Loss: 1.93756 Disc Loss: 1.08493 Q Losses: [0.095631227, 0.0056782351]\n",
      "Gen Loss: 1.13955 Disc Loss: 0.901641 Q Losses: [0.076534942, 0.048466049]\n",
      "Gen Loss: -1.23279 Disc Loss: 1.50272 Q Losses: [0.068142951, 0.049148705]\n",
      "Gen Loss: 0.405211 Disc Loss: 1.01978 Q Losses: [0.075867549, 0.034403674]\n",
      "Gen Loss: -0.118204 Disc Loss: 1.17689 Q Losses: [0.056980576, 0.029434431]\n",
      "Gen Loss: 1.02117 Disc Loss: 1.00549 Q Losses: [0.072716951, 0.0045894021]\n",
      "Gen Loss: 0.257349 Disc Loss: 1.09905 Q Losses: [0.082152337, 0.0049629528]\n",
      "Gen Loss: 0.40138 Disc Loss: 1.00924 Q Losses: [0.060605548, 0.01246294]\n",
      "Gen Loss: -0.499327 Disc Loss: 1.40592 Q Losses: [0.064264268, 0.0072538811]\n",
      "Gen Loss: -1.73263 Disc Loss: 1.37775 Q Losses: [0.075986549, 0.0065191388]\n",
      "Saved Model\n",
      "Gen Loss: 0.130309 Disc Loss: 1.03135 Q Losses: [0.071056493, 0.0095068207]\n",
      "Gen Loss: 1.8327 Disc Loss: 0.93077 Q Losses: [0.048549511, 0.050218552]\n",
      "Gen Loss: 1.19725 Disc Loss: 0.973835 Q Losses: [0.072556458, 0.040719777]\n",
      "Gen Loss: 1.10563 Disc Loss: 0.896787 Q Losses: [0.089665279, 0.049492702]\n",
      "Gen Loss: 1.56965 Disc Loss: 0.720691 Q Losses: [0.072220184, 0.033549603]\n",
      "Gen Loss: 1.33562 Disc Loss: 1.0038 Q Losses: [0.064264096, 0.0041722292]\n",
      "Gen Loss: -0.371956 Disc Loss: 1.1479 Q Losses: [0.051096179, 0.0090014432]\n",
      "Gen Loss: 0.552913 Disc Loss: 1.0626 Q Losses: [0.064032882, 0.013350799]\n",
      "Gen Loss: 1.80096 Disc Loss: 1.27432 Q Losses: [0.057156648, 0.033247165]\n",
      "Gen Loss: 0.00130227 Disc Loss: 1.10302 Q Losses: [0.056309901, 0.019237194]\n",
      "Saved Model\n",
      "Gen Loss: 1.20344 Disc Loss: 0.939176 Q Losses: [0.060953252, 0.030219477]\n",
      "Gen Loss: 2.30152 Disc Loss: 1.06294 Q Losses: [0.046863236, 0.0054043625]\n",
      "Gen Loss: 1.858 Disc Loss: 1.36349 Q Losses: [0.066633902, 0.02093742]\n",
      "Gen Loss: 1.01044 Disc Loss: 0.931192 Q Losses: [0.055108987, 0.0016419392]\n",
      "Gen Loss: -0.318631 Disc Loss: 0.905748 Q Losses: [0.065533891, 0.0095650526]\n",
      "Gen Loss: 0.104578 Disc Loss: 0.96977 Q Losses: [0.064271852, 0.023281693]\n",
      "Gen Loss: 1.55604 Disc Loss: 0.954705 Q Losses: [0.041063152, 0.0034592031]\n",
      "Gen Loss: 1.08508 Disc Loss: 0.979677 Q Losses: [0.061089925, 0.0025830274]\n",
      "Gen Loss: -0.972451 Disc Loss: 1.30148 Q Losses: [0.051228561, 0.011544985]\n",
      "Gen Loss: 1.40767 Disc Loss: 1.08126 Q Losses: [0.046402454, 0.0030502591]\n",
      "Saved Model\n",
      "Gen Loss: -0.207816 Disc Loss: 1.16593 Q Losses: [0.047796838, 0.019876199]\n",
      "Gen Loss: 0.328723 Disc Loss: 0.975861 Q Losses: [0.047265798, 0.0081264004]\n",
      "Gen Loss: 0.655102 Disc Loss: 1.0151 Q Losses: [0.045683198, 0.0011457605]\n",
      "Gen Loss: 2.08186 Disc Loss: 0.826819 Q Losses: [0.042472068, 0.0016890081]\n",
      "Gen Loss: 0.28642 Disc Loss: 0.898442 Q Losses: [0.048199963, 0.00081642088]\n",
      "Gen Loss: 0.711702 Disc Loss: 1.03974 Q Losses: [0.054359771, 0.0028264222]\n",
      "Gen Loss: -0.78997 Disc Loss: 1.56095 Q Losses: [0.052657753, 0.0068238294]\n",
      "Gen Loss: 1.66198 Disc Loss: 0.843183 Q Losses: [0.047193732, 0.00060088001]\n",
      "Gen Loss: 2.99082 Disc Loss: 1.3144 Q Losses: [0.050600599, 0.0039103264]\n",
      "Gen Loss: 2.14167 Disc Loss: 1.07645 Q Losses: [0.038747497, 0.027876969]\n",
      "Saved Model\n",
      "Gen Loss: 0.560401 Disc Loss: 0.977929 Q Losses: [0.05392243, 0.0031608022]\n",
      "Gen Loss: 2.54602 Disc Loss: 1.10955 Q Losses: [0.048331343, 0.0011022663]\n",
      "Gen Loss: -0.222667 Disc Loss: 0.883881 Q Losses: [0.046081528, 0.0067178607]\n",
      "Gen Loss: 3.18967 Disc Loss: 2.22518 Q Losses: [0.046734795, 0.0037767715]\n",
      "Gen Loss: -0.124341 Disc Loss: 1.29222 Q Losses: [0.04177963, 0.0045164572]\n",
      "Gen Loss: 2.69894 Disc Loss: 1.74399 Q Losses: [0.042158544, 0.003640071]\n",
      "Gen Loss: 1.75459 Disc Loss: 0.804049 Q Losses: [0.044250034, 0.00065969361]\n",
      "Gen Loss: 1.48663 Disc Loss: 0.946794 Q Losses: [0.037233479, 0.0079062143]\n",
      "Gen Loss: 0.858621 Disc Loss: 0.898598 Q Losses: [0.043677688, 0.0052899616]\n",
      "Gen Loss: 3.23834 Disc Loss: 1.49926 Q Losses: [0.047411989, 0.0077751325]\n",
      "Saved Model\n",
      "Gen Loss: 4.17822 Disc Loss: 1.32848 Q Losses: [0.077834666, 0.001669692]\n",
      "Gen Loss: 1.1683 Disc Loss: 0.86179 Q Losses: [0.043534148, 0.0018073923]\n",
      "Gen Loss: 0.598734 Disc Loss: 1.06471 Q Losses: [0.03469307, 0.0014225534]\n",
      "Gen Loss: 1.32269 Disc Loss: 0.895678 Q Losses: [0.047682956, 0.0017156983]\n",
      "Gen Loss: 0.38172 Disc Loss: 1.41578 Q Losses: [0.036718242, 0.019910557]\n",
      "Gen Loss: 2.96809 Disc Loss: 1.073 Q Losses: [0.057041612, 0.011138634]\n",
      "Gen Loss: 1.94891 Disc Loss: 0.723136 Q Losses: [0.029951863, 0.0081721107]\n",
      "Gen Loss: 0.923343 Disc Loss: 0.798024 Q Losses: [0.042013958, 0.00060065219]\n",
      "Gen Loss: 1.0348 Disc Loss: 0.836005 Q Losses: [0.042493045, 0.017278574]\n",
      "Gen Loss: 0.3225 Disc Loss: 1.24431 Q Losses: [0.054613531, 0.00072215684]\n",
      "Saved Model\n",
      "Gen Loss: -0.432907 Disc Loss: 1.28596 Q Losses: [0.036276381, 0.0021757269]\n",
      "Gen Loss: -0.997986 Disc Loss: 2.16142 Q Losses: [0.051652536, 0.0036896789]\n",
      "Gen Loss: 1.54337 Disc Loss: 0.849862 Q Losses: [0.032733832, 0.003133449]\n",
      "Gen Loss: 2.03318 Disc Loss: 0.95715 Q Losses: [0.034663472, 0.001094072]\n",
      "Gen Loss: 0.894358 Disc Loss: 0.840512 Q Losses: [0.04297027, 0.024110701]\n",
      "Gen Loss: 0.63956 Disc Loss: 0.852567 Q Losses: [0.039158497, 0.00070937525]\n",
      "Gen Loss: 2.52856 Disc Loss: 1.16742 Q Losses: [0.046597328, 0.0090597142]\n",
      "Gen Loss: 0.518594 Disc Loss: 0.735539 Q Losses: [0.065837532, 0.0010395205]\n",
      "Gen Loss: 2.11896 Disc Loss: 1.03808 Q Losses: [0.044302933, 0.02832927]\n",
      "Gen Loss: 0.336167 Disc Loss: 0.826349 Q Losses: [0.040932216, 0.0037637032]\n",
      "Saved Model\n",
      "Gen Loss: 0.739357 Disc Loss: 0.894205 Q Losses: [0.035046283, 0.00048170373]\n",
      "Gen Loss: 1.18662 Disc Loss: 0.822249 Q Losses: [0.034486059, 0.00046818348]\n",
      "Gen Loss: 1.13436 Disc Loss: 0.757348 Q Losses: [0.045138799, 0.0051733833]\n",
      "Gen Loss: 1.60961 Disc Loss: 0.856845 Q Losses: [0.043667279, 0.0060614422]\n",
      "Gen Loss: 0.396056 Disc Loss: 1.1583 Q Losses: [0.039266579, 0.00053289637]\n",
      "Gen Loss: 0.432835 Disc Loss: 0.802559 Q Losses: [0.042535916, 0.0066346866]\n",
      "Gen Loss: 0.593132 Disc Loss: 1.12292 Q Losses: [0.044089571, 0.0082802242]\n",
      "Gen Loss: 0.502224 Disc Loss: 0.923008 Q Losses: [0.042256214, 0.027814895]\n",
      "Gen Loss: 0.425717 Disc Loss: 0.893148 Q Losses: [0.028373854, 0.0010667723]\n",
      "Gen Loss: 0.152183 Disc Loss: 1.07646 Q Losses: [0.034174256, 0.00026152216]\n",
      "Saved Model\n",
      "Gen Loss: 2.13116 Disc Loss: 0.758079 Q Losses: [0.038274556, 0.00097318523]\n",
      "Gen Loss: 0.690597 Disc Loss: 0.881475 Q Losses: [0.034506433, 0.00045505431]\n",
      "Gen Loss: 2.63701 Disc Loss: 1.03836 Q Losses: [0.050680675, 0.0061330409]\n",
      "Gen Loss: 0.834977 Disc Loss: 0.896968 Q Losses: [0.032754332, 0.0014628468]\n",
      "Gen Loss: -0.347233 Disc Loss: 1.17682 Q Losses: [0.038518347, 0.0014072509]\n",
      "Gen Loss: -0.256928 Disc Loss: 1.0638 Q Losses: [0.035961375, 0.00080512848]\n",
      "Gen Loss: 0.401252 Disc Loss: 0.834055 Q Losses: [0.034426242, 0.009725986]\n",
      "Gen Loss: 1.85298 Disc Loss: 1.03138 Q Losses: [0.035363559, 0.00020840441]\n",
      "Gen Loss: 1.78191 Disc Loss: 0.813422 Q Losses: [0.048222151, 0.005518557]\n",
      "Gen Loss: 1.81632 Disc Loss: 0.658789 Q Losses: [0.041316304, 0.0019263724]\n",
      "Saved Model\n",
      "Gen Loss: 0.648222 Disc Loss: 0.886401 Q Losses: [0.029308667, 0.00035307524]\n",
      "Gen Loss: 1.80715 Disc Loss: 0.820347 Q Losses: [0.033994853, 0.0022684969]\n",
      "Gen Loss: 2.29479 Disc Loss: 1.12715 Q Losses: [0.043319784, 0.0046286508]\n",
      "Gen Loss: 0.970432 Disc Loss: 0.835769 Q Losses: [0.035095684, 0.00036268856]\n",
      "Gen Loss: 1.32488 Disc Loss: 0.748479 Q Losses: [0.034231316, 0.0015476785]\n",
      "Gen Loss: 0.771982 Disc Loss: 1.05338 Q Losses: [0.030799257, 0.0039617699]\n",
      "Gen Loss: 2.23325 Disc Loss: 0.860098 Q Losses: [0.039662596, 0.0013059939]\n",
      "Gen Loss: 1.13488 Disc Loss: 0.716021 Q Losses: [0.033014223, 0.023213059]\n",
      "Gen Loss: 1.35473 Disc Loss: 0.799763 Q Losses: [0.03102335, 0.0013163212]\n",
      "Gen Loss: 1.69658 Disc Loss: 0.736102 Q Losses: [0.033459306, 0.0057816901]\n",
      "Saved Model\n",
      "Gen Loss: 0.814218 Disc Loss: 0.846195 Q Losses: [0.032897752, 0.00058129226]\n",
      "Gen Loss: 2.56837 Disc Loss: 1.11131 Q Losses: [0.039146334, 0.0027812736]\n",
      "Gen Loss: 1.58596 Disc Loss: 0.874455 Q Losses: [0.049385231, 0.0026842516]\n",
      "Gen Loss: 0.508444 Disc Loss: 0.839069 Q Losses: [0.03854249, 0.018782267]\n",
      "Gen Loss: 0.000475522 Disc Loss: 0.978027 Q Losses: [0.032592181, 0.002908655]\n",
      "Gen Loss: -0.156734 Disc Loss: 1.01768 Q Losses: [0.031798344, 0.00088963035]\n",
      "Gen Loss: 0.486329 Disc Loss: 0.953923 Q Losses: [0.033501133, 0.00023407617]\n",
      "Gen Loss: 1.86877 Disc Loss: 0.877974 Q Losses: [0.03338445, 0.062529281]\n",
      "Gen Loss: 2.31082 Disc Loss: 1.05831 Q Losses: [0.041395299, 0.0010062282]\n",
      "Gen Loss: -0.428976 Disc Loss: 1.16061 Q Losses: [0.03670039, 0.045110613]\n",
      "Saved Model\n",
      "Gen Loss: 0.10615 Disc Loss: 1.07618 Q Losses: [0.034795228, 0.0016820624]\n",
      "Gen Loss: 2.30458 Disc Loss: 0.673954 Q Losses: [0.038000256, 0.00064913643]\n",
      "Gen Loss: 2.05082 Disc Loss: 0.719041 Q Losses: [0.02624682, 0.0029164271]\n",
      "Gen Loss: 0.184316 Disc Loss: 0.877817 Q Losses: [0.039566621, 0.0064726751]\n",
      "Gen Loss: 0.0644514 Disc Loss: 1.02345 Q Losses: [0.042062018, 0.0013463494]\n",
      "Gen Loss: 1.9358 Disc Loss: 0.967116 Q Losses: [0.037459183, 0.017497899]\n",
      "Gen Loss: 1.5367 Disc Loss: 0.842411 Q Losses: [0.042495616, 0.001676293]\n",
      "Gen Loss: 1.76319 Disc Loss: 0.888054 Q Losses: [0.041139908, 0.0054204417]\n",
      "Gen Loss: -0.16742 Disc Loss: 1.30843 Q Losses: [0.034683671, 0.00065454654]\n",
      "Gen Loss: 1.32399 Disc Loss: 0.772141 Q Losses: [0.042855345, 0.0020128023]\n",
      "Saved Model\n",
      "Gen Loss: 0.167473 Disc Loss: 1.01746 Q Losses: [0.039699022, 0.00029055122]\n",
      "Gen Loss: 1.86889 Disc Loss: 0.715537 Q Losses: [0.035272036, 0.0025225743]\n",
      "Gen Loss: 0.78074 Disc Loss: 0.958011 Q Losses: [0.06257081, 0.018061999]\n",
      "Gen Loss: 0.41895 Disc Loss: 1.1291 Q Losses: [0.038588665, 0.00022522471]\n",
      "Gen Loss: 0.546549 Disc Loss: 0.926876 Q Losses: [0.060874455, 0.0013691594]\n",
      "Gen Loss: 2.06611 Disc Loss: 0.909352 Q Losses: [0.032896839, 0.042757504]\n",
      "Gen Loss: -0.388865 Disc Loss: 1.40505 Q Losses: [0.04702048, 0.017075257]\n",
      "Gen Loss: 0.0583291 Disc Loss: 1.75584 Q Losses: [0.03356583, 0.00073252904]\n",
      "Gen Loss: 1.27221 Disc Loss: 0.832997 Q Losses: [0.030517576, 0.00094211492]\n",
      "Gen Loss: 1.72136 Disc Loss: 0.804616 Q Losses: [0.044502582, 0.0002144194]\n",
      "Saved Model\n",
      "Gen Loss: 1.73166 Disc Loss: 0.907034 Q Losses: [0.03230647, 0.0087708151]\n",
      "Gen Loss: 1.26696 Disc Loss: 1.31035 Q Losses: [0.033975914, 0.03882698]\n",
      "Gen Loss: 2.25099 Disc Loss: 0.851724 Q Losses: [0.054780431, 0.0012086804]\n",
      "Gen Loss: 1.28901 Disc Loss: 0.680252 Q Losses: [0.027414404, 0.00082069461]\n",
      "Gen Loss: 1.38243 Disc Loss: 0.844701 Q Losses: [0.046180651, 0.008737403]\n",
      "Gen Loss: 1.80634 Disc Loss: 0.935556 Q Losses: [0.040218294, 0.0010375591]\n",
      "Gen Loss: 1.19458 Disc Loss: 0.872601 Q Losses: [0.039221257, 0.0001599137]\n",
      "Gen Loss: 3.1122 Disc Loss: 1.02559 Q Losses: [0.036411993, 0.0022769822]\n",
      "Gen Loss: 1.00276 Disc Loss: 1.26928 Q Losses: [0.043603599, 0.010270983]\n",
      "Gen Loss: 1.33419 Disc Loss: 0.876051 Q Losses: [0.031009786, 0.0035538746]\n",
      "Saved Model\n",
      "Gen Loss: 2.30923 Disc Loss: 0.723794 Q Losses: [0.031597428, 0.0012427376]\n",
      "Gen Loss: 0.62086 Disc Loss: 0.951756 Q Losses: [0.040896736, 0.00015838412]\n",
      "Gen Loss: -0.0539687 Disc Loss: 0.986112 Q Losses: [0.05178085, 0.00028477245]\n",
      "Gen Loss: 1.32378 Disc Loss: 0.684668 Q Losses: [0.041205093, 0.0073777246]\n",
      "Gen Loss: 2.61762 Disc Loss: 0.783702 Q Losses: [0.040818788, 0.00025197538]\n",
      "Gen Loss: 2.16378 Disc Loss: 0.670226 Q Losses: [0.028374843, 0.00015247632]\n",
      "Gen Loss: 0.424147 Disc Loss: 0.910578 Q Losses: [0.036527835, 0.00097869942]\n",
      "Gen Loss: 1.8247 Disc Loss: 0.741324 Q Losses: [0.034215417, 0.00075421919]\n",
      "Gen Loss: 1.13554 Disc Loss: 0.740059 Q Losses: [0.034904949, 0.0046607321]\n",
      "Gen Loss: 1.38272 Disc Loss: 0.769215 Q Losses: [0.05130858, 0.0011096915]\n",
      "Saved Model\n",
      "Gen Loss: 3.44774 Disc Loss: 1.61991 Q Losses: [0.035568707, 0.0067181727]\n",
      "Gen Loss: 0.940508 Disc Loss: 1.11929 Q Losses: [0.023455894, 0.00055666949]\n",
      "Gen Loss: 1.3 Disc Loss: 0.698664 Q Losses: [0.04288438, 0.00056611322]\n",
      "Gen Loss: 2.24761 Disc Loss: 0.548345 Q Losses: [0.029942224, 0.00046321616]\n",
      "Gen Loss: 1.83011 Disc Loss: 0.656922 Q Losses: [0.040346548, 0.0010321368]\n",
      "Gen Loss: 1.01134 Disc Loss: 0.656294 Q Losses: [0.033312775, 0.00025764585]\n",
      "Gen Loss: 0.413783 Disc Loss: 1.4502 Q Losses: [0.039471976, 0.043640021]\n",
      "Gen Loss: 0.844637 Disc Loss: 0.78257 Q Losses: [0.033004686, 0.0017308933]\n",
      "Gen Loss: 1.71566 Disc Loss: 0.894673 Q Losses: [0.030338803, 0.0015126712]\n",
      "Gen Loss: 2.13289 Disc Loss: 0.853336 Q Losses: [0.027267564, 0.00055785675]\n",
      "Saved Model\n",
      "Gen Loss: 1.70426 Disc Loss: 0.632717 Q Losses: [0.026517764, 0.055886116]\n",
      "Gen Loss: 1.49108 Disc Loss: 0.791152 Q Losses: [0.027185187, 0.00047320087]\n",
      "Gen Loss: 2.70678 Disc Loss: 0.80931 Q Losses: [0.038915943, 0.024670955]\n",
      "Gen Loss: 1.25163 Disc Loss: 0.840463 Q Losses: [0.034362435, 0.014291775]\n",
      "Gen Loss: 1.13524 Disc Loss: 0.733255 Q Losses: [0.031989377, 0.0029614591]\n",
      "Gen Loss: 1.88373 Disc Loss: 0.763419 Q Losses: [0.037293509, 0.0013255808]\n",
      "Gen Loss: 1.71346 Disc Loss: 0.65378 Q Losses: [0.029596169, 0.00029002639]\n",
      "Gen Loss: 0.540435 Disc Loss: 0.901177 Q Losses: [0.039908424, 0.0027560415]\n",
      "Gen Loss: 2.21475 Disc Loss: 0.758063 Q Losses: [0.042334903, 0.0073611136]\n",
      "Gen Loss: 1.23202 Disc Loss: 0.858446 Q Losses: [0.029791793, 0.049143165]\n",
      "Saved Model\n",
      "Gen Loss: 0.519693 Disc Loss: 0.917842 Q Losses: [0.045732111, 0.00021833496]\n",
      "Gen Loss: 2.763 Disc Loss: 0.710755 Q Losses: [0.034598984, 0.0024440531]\n",
      "Gen Loss: 1.09596 Disc Loss: 0.892966 Q Losses: [0.045356952, 0.011823939]\n",
      "Gen Loss: -1.15234 Disc Loss: 1.6101 Q Losses: [0.05211769, 0.010011096]\n",
      "Gen Loss: 1.01835 Disc Loss: 1.00932 Q Losses: [0.030536413, 0.070820138]\n",
      "Gen Loss: 1.86562 Disc Loss: 1.40413 Q Losses: [0.029523186, 0.001361059]\n",
      "Gen Loss: 2.78487 Disc Loss: 1.24028 Q Losses: [0.033726096, 0.00041678711]\n",
      "Gen Loss: 2.96379 Disc Loss: 1.38239 Q Losses: [0.026010953, 0.0043125744]\n",
      "Gen Loss: 1.88605 Disc Loss: 0.871564 Q Losses: [0.035625398, 0.00075678027]\n",
      "Gen Loss: 1.69514 Disc Loss: 0.561136 Q Losses: [0.037965737, 0.00095490401]\n",
      "Saved Model\n",
      "Gen Loss: -1.51489 Disc Loss: 1.48766 Q Losses: [0.030547082, 0.019766415]\n",
      "Gen Loss: 0.69553 Disc Loss: 0.992477 Q Losses: [0.030323362, 0.0061204224]\n",
      "Gen Loss: 3.60459 Disc Loss: 1.09054 Q Losses: [0.037741736, 0.0023059729]\n",
      "Gen Loss: 2.27071 Disc Loss: 0.709327 Q Losses: [0.041622926, 0.00043903282]\n",
      "Gen Loss: 0.59303 Disc Loss: 1.22621 Q Losses: [0.033972628, 0.047043499]\n",
      "Gen Loss: 1.19511 Disc Loss: 0.590495 Q Losses: [0.028103404, 0.00011411599]\n",
      "Gen Loss: 3.10102 Disc Loss: 0.723085 Q Losses: [0.031085705, 0.0004607194]\n",
      "Gen Loss: 1.28139 Disc Loss: 0.845168 Q Losses: [0.032928996, 0.0019493527]\n",
      "Gen Loss: -1.15113 Disc Loss: 1.7422 Q Losses: [0.042311035, 0.00063340308]\n",
      "Gen Loss: 1.30983 Disc Loss: 0.583245 Q Losses: [0.030176392, 0.0010618696]\n",
      "Saved Model\n",
      "Gen Loss: 1.7929 Disc Loss: 0.722884 Q Losses: [0.037787452, 0.0043053199]\n",
      "Gen Loss: 1.06028 Disc Loss: 0.787923 Q Losses: [0.036517382, 0.00017833911]\n",
      "Gen Loss: 0.974798 Disc Loss: 0.687648 Q Losses: [0.027530536, 0.0012306855]\n",
      "Gen Loss: 0.870243 Disc Loss: 0.908339 Q Losses: [0.035661057, 0.0035767907]\n",
      "Gen Loss: 1.78828 Disc Loss: 0.756074 Q Losses: [0.031122712, 0.00061794015]\n",
      "Gen Loss: 4.25507 Disc Loss: 0.932028 Q Losses: [0.033921849, 0.02043058]\n",
      "Gen Loss: -1.06359 Disc Loss: 1.18921 Q Losses: [0.024339104, 0.00046919729]\n",
      "Gen Loss: -0.132179 Disc Loss: 1.39352 Q Losses: [0.026348967, 0.0012961568]\n",
      "Gen Loss: 0.308092 Disc Loss: 1.07286 Q Losses: [0.03859593, 0.054958746]\n",
      "Gen Loss: 2.51191 Disc Loss: 0.578104 Q Losses: [0.028270639, 0.0001665634]\n",
      "Saved Model\n",
      "Gen Loss: 2.37077 Disc Loss: 0.836608 Q Losses: [0.039889857, 0.00069623173]\n",
      "Gen Loss: 1.66323 Disc Loss: 0.53991 Q Losses: [0.034550585, 0.016777528]\n",
      "Gen Loss: 1.63485 Disc Loss: 0.601801 Q Losses: [0.024979915, 8.9925627e-05]\n",
      "Gen Loss: 2.44159 Disc Loss: 0.6734 Q Losses: [0.032143027, 0.0014500655]\n",
      "Gen Loss: 2.17539 Disc Loss: 0.919785 Q Losses: [0.035749696, 0.0010999168]\n",
      "Gen Loss: 2.50776 Disc Loss: 0.646803 Q Losses: [0.032892816, 0.0047903694]\n",
      "Gen Loss: 0.331118 Disc Loss: 1.0296 Q Losses: [0.027885962, 0.071127623]\n",
      "Gen Loss: 0.995305 Disc Loss: 0.764457 Q Losses: [0.038004376, 0.0020423236]\n",
      "Gen Loss: 1.46194 Disc Loss: 1.05153 Q Losses: [0.024895165, 0.00034858874]\n",
      "Gen Loss: 1.22855 Disc Loss: 0.759068 Q Losses: [0.026720781, 0.011816686]\n",
      "Saved Model\n",
      "Gen Loss: 3.19182 Disc Loss: 1.91757 Q Losses: [0.033114389, 0.00056943705]\n",
      "Gen Loss: 2.60839 Disc Loss: 0.657014 Q Losses: [0.02249055, 0.0018851723]\n",
      "Gen Loss: 3.56915 Disc Loss: 0.585119 Q Losses: [0.030665874, 0.014062047]\n",
      "Gen Loss: 0.0382575 Disc Loss: 1.07641 Q Losses: [0.031296462, 7.034911e-05]\n",
      "Gen Loss: 3.45402 Disc Loss: 0.882416 Q Losses: [0.029419433, 0.00027801021]\n",
      "Gen Loss: 1.70617 Disc Loss: 0.725637 Q Losses: [0.026397873, 0.00035276101]\n",
      "Gen Loss: 2.91467 Disc Loss: 0.926652 Q Losses: [0.028880203, 8.4195824e-05]\n",
      "Gen Loss: 0.417327 Disc Loss: 1.12947 Q Losses: [0.041510239, 0.0020511453]\n",
      "Gen Loss: -0.232412 Disc Loss: 0.87624 Q Losses: [0.040744055, 0.0011390386]\n",
      "Gen Loss: 4.048 Disc Loss: 0.9616 Q Losses: [0.023720507, 0.00044569297]\n",
      "Saved Model\n",
      "Gen Loss: 3.29598 Disc Loss: 1.22203 Q Losses: [0.027383134, 0.0079874173]\n",
      "Gen Loss: 1.03849 Disc Loss: 0.68284 Q Losses: [0.033357158, 0.0051879562]\n",
      "Gen Loss: 1.67276 Disc Loss: 0.785893 Q Losses: [0.024701338, 0.0058610104]\n",
      "Gen Loss: 1.10169 Disc Loss: 0.819184 Q Losses: [0.029098593, 0.0087880213]\n",
      "Gen Loss: 3.20858 Disc Loss: 0.499701 Q Losses: [0.035648845, 0.0001957535]\n",
      "Gen Loss: 0.642245 Disc Loss: 0.803139 Q Losses: [0.043517701, 0.0032791039]\n",
      "Gen Loss: 3.26583 Disc Loss: 0.835716 Q Losses: [0.029450856, 0.00069406605]\n",
      "Gen Loss: 0.328592 Disc Loss: 0.676676 Q Losses: [0.030090693, 0.055421758]\n",
      "Gen Loss: 0.846216 Disc Loss: 0.832648 Q Losses: [0.030424256, 0.0094414866]\n",
      "Gen Loss: 2.89638 Disc Loss: 0.809884 Q Losses: [0.020924132, 0.00042428181]\n",
      "Saved Model\n",
      "Gen Loss: 1.96816 Disc Loss: 0.772211 Q Losses: [0.02222475, 0.00010565865]\n",
      "Gen Loss: 1.68252 Disc Loss: 1.01612 Q Losses: [0.036289427, 0.0013883875]\n",
      "Gen Loss: 1.50053 Disc Loss: 0.735141 Q Losses: [0.029244214, 0.0002148949]\n",
      "Gen Loss: 1.4213 Disc Loss: 0.579044 Q Losses: [0.024001678, 0.010139873]\n",
      "Gen Loss: 0.993305 Disc Loss: 0.689962 Q Losses: [0.035517219, 0.033780862]\n",
      "Gen Loss: -0.599775 Disc Loss: 0.990685 Q Losses: [0.024294155, 0.006190192]\n",
      "Gen Loss: 2.45103 Disc Loss: 0.986831 Q Losses: [0.026234653, 0.00073084637]\n",
      "Gen Loss: 2.86141 Disc Loss: 0.654749 Q Losses: [0.030855745, 0.0002992879]\n",
      "Gen Loss: 1.49524 Disc Loss: 0.676391 Q Losses: [0.027367301, 0.11497332]\n",
      "Gen Loss: 1.24165 Disc Loss: 0.770917 Q Losses: [0.022164468, 0.00055091799]\n",
      "Saved Model\n",
      "Gen Loss: 2.45109 Disc Loss: 0.781235 Q Losses: [0.022335142, 0.00010404683]\n",
      "Gen Loss: -0.498896 Disc Loss: 0.992215 Q Losses: [0.026992703, 0.00047719461]\n",
      "Gen Loss: 1.98934 Disc Loss: 0.654946 Q Losses: [0.037218235, 0.00010361629]\n",
      "Gen Loss: 2.33783 Disc Loss: 0.621132 Q Losses: [0.022711398, 0.000211085]\n",
      "Gen Loss: 0.504856 Disc Loss: 1.01444 Q Losses: [0.043341216, 0.0027888159]\n",
      "Gen Loss: 3.10985 Disc Loss: 0.903011 Q Losses: [0.032768343, 0.0011359364]\n",
      "Gen Loss: 3.45139 Disc Loss: 0.951958 Q Losses: [0.028600086, 0.00053433148]\n",
      "Gen Loss: 2.50377 Disc Loss: 0.544799 Q Losses: [0.028179834, 0.0008149693]\n",
      "Gen Loss: 1.71309 Disc Loss: 0.556747 Q Losses: [0.028464429, 0.00011422507]\n",
      "Gen Loss: 1.69038 Disc Loss: 0.752142 Q Losses: [0.036779411, 0.00036856672]\n",
      "Saved Model\n",
      "Gen Loss: 3.83689 Disc Loss: 0.919109 Q Losses: [0.029534161, 0.011305714]\n",
      "Gen Loss: 1.82055 Disc Loss: 0.58916 Q Losses: [0.030539511, 9.0450369e-05]\n",
      "Gen Loss: 2.45996 Disc Loss: 0.716562 Q Losses: [0.028451074, 0.0015921162]\n",
      "Gen Loss: 2.68045 Disc Loss: 0.809803 Q Losses: [0.023014283, 0.00046078936]\n",
      "Gen Loss: 2.02403 Disc Loss: 0.711182 Q Losses: [0.027372278, 0.00015961836]\n",
      "Gen Loss: 1.93732 Disc Loss: 0.622191 Q Losses: [0.020945396, 0.0019985917]\n",
      "Gen Loss: 0.640976 Disc Loss: 1.04855 Q Losses: [0.030767273, 0.043933086]\n",
      "Gen Loss: 2.08074 Disc Loss: 0.526015 Q Losses: [0.025943112, 0.0007104844]\n",
      "Gen Loss: 1.49403 Disc Loss: 0.607503 Q Losses: [0.042841572, 0.0001615604]\n",
      "Gen Loss: 0.49128 Disc Loss: 1.0338 Q Losses: [0.030396042, 7.2132578e-05]\n",
      "Saved Model\n",
      "Gen Loss: 0.610695 Disc Loss: 0.899947 Q Losses: [0.029394615, 0.00010999363]\n",
      "Gen Loss: 2.56621 Disc Loss: 0.494955 Q Losses: [0.027771952, 0.00046503433]\n",
      "Gen Loss: 1.82386 Disc Loss: 0.421227 Q Losses: [0.02253861, 9.3693321e-05]\n",
      "Gen Loss: 2.54695 Disc Loss: 0.463059 Q Losses: [0.03218323, 0.0038685193]\n",
      "Gen Loss: 2.4707 Disc Loss: 0.805523 Q Losses: [0.027382668, 0.011868546]\n",
      "Gen Loss: 2.21003 Disc Loss: 0.550079 Q Losses: [0.036338776, 0.0014848474]\n",
      "Gen Loss: 1.95741 Disc Loss: 0.641431 Q Losses: [0.029548995, 0.00038594258]\n",
      "Gen Loss: 1.80676 Disc Loss: 0.595847 Q Losses: [0.028914709, 0.11698801]\n",
      "Gen Loss: 1.2755 Disc Loss: 0.784231 Q Losses: [0.026880406, 0.00043208146]\n",
      "Gen Loss: 3.38012 Disc Loss: 0.944947 Q Losses: [0.021678777, 0.00063560775]\n",
      "Saved Model\n",
      "Gen Loss: 0.907254 Disc Loss: 0.751379 Q Losses: [0.025721535, 0.0006019104]\n",
      "Gen Loss: 4.31826 Disc Loss: 1.94684 Q Losses: [0.023707315, 0.0001854654]\n",
      "Gen Loss: 1.61549 Disc Loss: 0.706075 Q Losses: [0.027550314, 0.00023012492]\n",
      "Gen Loss: 1.89216 Disc Loss: 0.553079 Q Losses: [0.027447842, 0.00013554559]\n",
      "Gen Loss: 2.41453 Disc Loss: 0.562156 Q Losses: [0.022103421, 0.00011465212]\n",
      "Gen Loss: 4.28876 Disc Loss: 0.81451 Q Losses: [0.021621766, 9.1973605e-05]\n",
      "Gen Loss: 2.79414 Disc Loss: 0.460479 Q Losses: [0.03266089, 0.0001255064]\n",
      "Gen Loss: 2.1059 Disc Loss: 0.666513 Q Losses: [0.0233675, 0.00044191341]\n",
      "Gen Loss: 3.23273 Disc Loss: 0.659652 Q Losses: [0.026888475, 0.0030218505]\n",
      "Gen Loss: 0.960921 Disc Loss: 0.701556 Q Losses: [0.029455088, 0.00047113345]\n",
      "Saved Model\n",
      "Gen Loss: 1.45283 Disc Loss: 0.780801 Q Losses: [0.029118581, 0.00022500513]\n",
      "Gen Loss: 1.69265 Disc Loss: 0.583889 Q Losses: [0.038437843, 0.051770899]\n",
      "Gen Loss: 1.99649 Disc Loss: 0.556699 Q Losses: [0.033005379, 0.00089854735]\n",
      "Gen Loss: 1.73067 Disc Loss: 0.627885 Q Losses: [0.030201752, 0.0025096419]\n",
      "Gen Loss: 2.20007 Disc Loss: 0.665573 Q Losses: [0.030457862, 0.00038572057]\n",
      "Gen Loss: 3.0767 Disc Loss: 0.479989 Q Losses: [0.027105261, 0.00057027576]\n",
      "Gen Loss: 4.84997 Disc Loss: 1.23818 Q Losses: [0.023891833, 0.0010061639]\n",
      "Gen Loss: 1.51347 Disc Loss: 0.858649 Q Losses: [0.023590736, 0.00016599325]\n",
      "Gen Loss: 1.40159 Disc Loss: 0.691986 Q Losses: [0.026143212, 0.0022224134]\n",
      "Gen Loss: 2.23501 Disc Loss: 0.890744 Q Losses: [0.023596555, 0.00037997251]\n",
      "Saved Model\n",
      "Gen Loss: 0.635484 Disc Loss: 0.689289 Q Losses: [0.025576834, 0.0001755355]\n",
      "Gen Loss: 2.69068 Disc Loss: 0.447555 Q Losses: [0.026325114, 0.0054797442]\n",
      "Gen Loss: 1.86245 Disc Loss: 0.435914 Q Losses: [0.049234442, 0.069693372]\n",
      "Gen Loss: 1.83932 Disc Loss: 0.596831 Q Losses: [0.028825056, 0.00019871895]\n",
      "Gen Loss: 1.75801 Disc Loss: 0.785916 Q Losses: [0.023145996, 0.0001804675]\n",
      "Gen Loss: 3.0496 Disc Loss: 0.491594 Q Losses: [0.024269193, 0.00099638978]\n",
      "Gen Loss: 2.12174 Disc Loss: 0.653828 Q Losses: [0.024896897, 7.1519928e-05]\n",
      "Gen Loss: 2.82637 Disc Loss: 0.457087 Q Losses: [0.028333787, 0.00054161472]\n",
      "Gen Loss: 1.76202 Disc Loss: 0.985432 Q Losses: [0.027051982, 0.0018652901]\n",
      "Gen Loss: 3.67789 Disc Loss: 0.586924 Q Losses: [0.04408003, 0.021059943]\n",
      "Saved Model\n",
      "Gen Loss: 0.00172444 Disc Loss: 0.948014 Q Losses: [0.024337661, 0.00015169104]\n",
      "Gen Loss: 2.93088 Disc Loss: 0.534845 Q Losses: [0.026700886, 0.00043540692]\n",
      "Gen Loss: 3.14092 Disc Loss: 0.653369 Q Losses: [0.033837833, 0.0018157141]\n",
      "Gen Loss: -0.136532 Disc Loss: 0.963103 Q Losses: [0.022524364, 0.0002062227]\n",
      "Gen Loss: 3.69537 Disc Loss: 0.763971 Q Losses: [0.034971081, 0.00032055791]\n",
      "Gen Loss: 1.6831 Disc Loss: 0.77047 Q Losses: [0.020956706, 0.012303022]\n",
      "Gen Loss: 1.91501 Disc Loss: 0.624246 Q Losses: [0.021482216, 0.0027776642]\n",
      "Gen Loss: 0.806188 Disc Loss: 0.97911 Q Losses: [0.033756167, 0.00090638734]\n",
      "Gen Loss: 0.129424 Disc Loss: 0.82292 Q Losses: [0.036490455, 0.0001616443]\n",
      "Gen Loss: 2.20187 Disc Loss: 0.465009 Q Losses: [0.029567264, 0.00060649938]\n",
      "Saved Model\n",
      "Gen Loss: 2.45209 Disc Loss: 0.459535 Q Losses: [0.023157639, 0.0081721665]\n",
      "Gen Loss: 3.09726 Disc Loss: 0.670221 Q Losses: [0.025208015, 7.8642122e-05]\n",
      "Gen Loss: 2.82849 Disc Loss: 0.599559 Q Losses: [0.021501541, 0.0031388346]\n",
      "Gen Loss: 3.12496 Disc Loss: 0.996744 Q Losses: [0.032569416, 0.00075420708]\n",
      "Gen Loss: 1.81452 Disc Loss: 0.731196 Q Losses: [0.037217796, 0.001259583]\n",
      "Gen Loss: 1.98991 Disc Loss: 0.820936 Q Losses: [0.022698374, 0.009656854]\n",
      "Gen Loss: 0.174843 Disc Loss: 0.833702 Q Losses: [0.02518234, 0.00024578508]\n",
      "Gen Loss: 3.78595 Disc Loss: 0.394407 Q Losses: [0.02919554, 0.00061192067]\n",
      "Gen Loss: 1.14505 Disc Loss: 0.771898 Q Losses: [0.027247701, 0.00022447346]\n",
      "Gen Loss: 0.971747 Disc Loss: 0.746944 Q Losses: [0.01835227, 0.00093300198]\n",
      "Saved Model\n",
      "Gen Loss: 1.75944 Disc Loss: 0.669122 Q Losses: [0.025752779, 0.00019806728]\n",
      "Gen Loss: 2.69878 Disc Loss: 0.607092 Q Losses: [0.029540699, 0.00025746867]\n",
      "Gen Loss: 1.76923 Disc Loss: 0.481922 Q Losses: [0.023452118, 0.00014329907]\n",
      "Gen Loss: 2.51588 Disc Loss: 0.747516 Q Losses: [0.025448252, 0.0011697806]\n",
      "Gen Loss: 1.60996 Disc Loss: 0.756199 Q Losses: [0.024116116, 0.0021315941]\n",
      "Gen Loss: 2.99495 Disc Loss: 0.665336 Q Losses: [0.023668569, 5.4352065e-05]\n",
      "Gen Loss: 3.30312 Disc Loss: 0.661008 Q Losses: [0.019936349, 0.0005440776]\n",
      "Gen Loss: 1.72217 Disc Loss: 0.789125 Q Losses: [0.020404914, 0.00056984497]\n",
      "Gen Loss: 0.67843 Disc Loss: 1.06544 Q Losses: [0.026611488, 8.3692204e-05]\n",
      "Gen Loss: 0.597063 Disc Loss: 0.797724 Q Losses: [0.025510915, 0.002499888]\n",
      "Saved Model\n",
      "Gen Loss: 0.936275 Disc Loss: 0.638026 Q Losses: [0.036320701, 0.00028651167]\n",
      "Gen Loss: 3.62595 Disc Loss: 0.537146 Q Losses: [0.033497639, 0.0016005678]\n",
      "Gen Loss: 2.32454 Disc Loss: 0.402322 Q Losses: [0.028462149, 0.00033782996]\n",
      "Gen Loss: 2.56632 Disc Loss: 0.495332 Q Losses: [0.022925016, 0.0058329455]\n",
      "Gen Loss: 2.14016 Disc Loss: 0.715511 Q Losses: [0.01965161, 6.6145803e-05]\n",
      "Gen Loss: 3.75186 Disc Loss: 0.919097 Q Losses: [0.019298244, 0.016236685]\n",
      "Gen Loss: 2.79179 Disc Loss: 0.57574 Q Losses: [0.03057866, 0.000419633]\n",
      "Gen Loss: 2.69117 Disc Loss: 0.767358 Q Losses: [0.020366855, 0.00038939581]\n",
      "Gen Loss: 3.10878 Disc Loss: 0.601291 Q Losses: [0.020203892, 0.0002600908]\n",
      "Gen Loss: 2.28712 Disc Loss: 0.5892 Q Losses: [0.023403959, 0.00025280102]\n",
      "Saved Model\n",
      "Gen Loss: 1.54273 Disc Loss: 0.553874 Q Losses: [0.021628518, 0.0018671132]\n",
      "Gen Loss: 1.92842 Disc Loss: 0.720245 Q Losses: [0.024418499, 0.0070447945]\n",
      "Gen Loss: 2.23219 Disc Loss: 0.647163 Q Losses: [0.019290229, 9.6000993e-05]\n",
      "Gen Loss: 2.88025 Disc Loss: 0.524752 Q Losses: [0.026466373, 0.00011792028]\n",
      "Gen Loss: 2.36919 Disc Loss: 0.891249 Q Losses: [0.023649249, 0.0013997541]\n",
      "Gen Loss: 2.5239 Disc Loss: 0.417734 Q Losses: [0.029056441, 0.0058487067]\n",
      "Gen Loss: 3.16209 Disc Loss: 0.689381 Q Losses: [0.042440306, 0.0060890815]\n",
      "Gen Loss: 2.285 Disc Loss: 0.568571 Q Losses: [0.021035105, 0.00019152387]\n",
      "Gen Loss: 1.85495 Disc Loss: 0.528818 Q Losses: [0.024013773, 0.00020085863]\n",
      "Gen Loss: 2.39653 Disc Loss: 0.631203 Q Losses: [0.026118493, 0.00029701582]\n",
      "Saved Model\n",
      "Gen Loss: 4.11819 Disc Loss: 0.780261 Q Losses: [0.026593585, 0.00045897596]\n",
      "Gen Loss: 2.64187 Disc Loss: 0.6379 Q Losses: [0.028922256, 0.014960263]\n",
      "Gen Loss: 2.18512 Disc Loss: 0.861521 Q Losses: [0.023453277, 0.0010218224]\n",
      "Gen Loss: 3.2706 Disc Loss: 0.754762 Q Losses: [0.024832664, 0.0002318355]\n",
      "Gen Loss: 2.42096 Disc Loss: 0.521067 Q Losses: [0.024283405, 9.8457451e-05]\n",
      "Gen Loss: 2.91835 Disc Loss: 0.356713 Q Losses: [0.021866366, 0.00010329506]\n",
      "Gen Loss: 2.54823 Disc Loss: 0.619176 Q Losses: [0.034846477, 0.00013059074]\n",
      "Gen Loss: 2.91387 Disc Loss: 0.595472 Q Losses: [0.028268991, 0.00020404345]\n",
      "Gen Loss: 2.81162 Disc Loss: 0.63777 Q Losses: [0.022492044, 0.00016145063]\n",
      "Gen Loss: 3.43575 Disc Loss: 0.587017 Q Losses: [0.026541818, 6.6658453e-05]\n",
      "Saved Model\n",
      "Gen Loss: -1.10495 Disc Loss: 2.21877 Q Losses: [0.019752121, 0.00049508625]\n",
      "Gen Loss: 3.47331 Disc Loss: 0.580902 Q Losses: [0.016211972, 0.0067083552]\n",
      "Gen Loss: 2.07422 Disc Loss: 0.664094 Q Losses: [0.018185601, 0.00039176218]\n",
      "Gen Loss: 2.30834 Disc Loss: 0.611654 Q Losses: [0.020746909, 4.8823946e-05]\n",
      "Gen Loss: 3.35657 Disc Loss: 0.505548 Q Losses: [0.020707287, 0.0073189475]\n",
      "Gen Loss: 1.89463 Disc Loss: 0.408636 Q Losses: [0.031601325, 0.0004547376]\n",
      "Gen Loss: 0.57614 Disc Loss: 0.964672 Q Losses: [0.029156931, 0.0077432701]\n",
      "Gen Loss: 2.44865 Disc Loss: 0.722764 Q Losses: [0.024573771, 0.061812814]\n",
      "Gen Loss: 2.18202 Disc Loss: 0.68439 Q Losses: [0.022102851, 0.0023571409]\n",
      "Gen Loss: 2.16814 Disc Loss: 0.911844 Q Losses: [0.026925087, 0.00020314465]\n",
      "Saved Model\n",
      "Gen Loss: 4.22716 Disc Loss: 0.626289 Q Losses: [0.033872157, 0.00092888932]\n",
      "Gen Loss: 3.51467 Disc Loss: 0.586077 Q Losses: [0.025306545, 0.00010327604]\n",
      "Gen Loss: 3.47522 Disc Loss: 0.949157 Q Losses: [0.028617959, 0.00010117669]\n",
      "Gen Loss: 1.41912 Disc Loss: 0.418773 Q Losses: [0.029072251, 0.008071973]\n",
      "Gen Loss: 4.13435 Disc Loss: 0.921412 Q Losses: [0.034655321, 0.0012643749]\n",
      "Gen Loss: 2.56323 Disc Loss: 0.553928 Q Losses: [0.026757801, 0.001853001]\n",
      "Gen Loss: 3.52709 Disc Loss: 0.76824 Q Losses: [0.020938845, 0.00062008551]\n",
      "Gen Loss: 2.32268 Disc Loss: 0.575076 Q Losses: [0.027489915, 0.012150479]\n",
      "Gen Loss: 3.79445 Disc Loss: 0.346157 Q Losses: [0.023069687, 0.00030048189]\n",
      "Gen Loss: 4.50424 Disc Loss: 1.34933 Q Losses: [0.022741843, 6.6946057e-05]\n",
      "Saved Model\n",
      "Gen Loss: 3.52602 Disc Loss: 0.559847 Q Losses: [0.027153749, 0.00033875028]\n",
      "Gen Loss: 4.47037 Disc Loss: 1.11793 Q Losses: [0.030294156, 0.00012341469]\n",
      "Gen Loss: 2.26797 Disc Loss: 0.470178 Q Losses: [0.023087624, 0.0028982325]\n",
      "Gen Loss: 3.82252 Disc Loss: 0.873132 Q Losses: [0.033093996, 0.0019639386]\n",
      "Gen Loss: 2.78259 Disc Loss: 0.683669 Q Losses: [0.017360613, 5.3626129e-05]\n",
      "Gen Loss: 4.46049 Disc Loss: 0.661248 Q Losses: [0.022835014, 0.00036975439]\n",
      "Gen Loss: 2.73082 Disc Loss: 0.446008 Q Losses: [0.01530047, 8.4827552e-05]\n",
      "Gen Loss: 1.76983 Disc Loss: 0.450195 Q Losses: [0.025009969, 0.0001038326]\n",
      "Gen Loss: 4.12426 Disc Loss: 0.756003 Q Losses: [0.019272864, 5.3501135e-05]\n",
      "Gen Loss: 3.4559 Disc Loss: 0.653041 Q Losses: [0.027491232, 0.00083313481]\n",
      "Saved Model\n",
      "Gen Loss: 4.33692 Disc Loss: 0.763139 Q Losses: [0.019763313, 0.00038716756]\n",
      "Gen Loss: 1.52416 Disc Loss: 0.68185 Q Losses: [0.023722272, 5.8448422e-05]\n",
      "Gen Loss: 3.91474 Disc Loss: 0.721203 Q Losses: [0.028931655, 7.9460326e-05]\n",
      "Gen Loss: 4.46703 Disc Loss: 0.88702 Q Losses: [0.026407618, 0.00079346873]\n",
      "Gen Loss: 1.5831 Disc Loss: 0.514765 Q Losses: [0.01989967, 0.00070653646]\n",
      "Gen Loss: 1.0375 Disc Loss: 0.701378 Q Losses: [0.019376645, 0.00026178302]\n",
      "Gen Loss: 7.17628 Disc Loss: 1.80842 Q Losses: [0.024156917, 0.00027723302]\n",
      "Gen Loss: 3.36267 Disc Loss: 0.55645 Q Losses: [0.032620966, 0.16773355]\n",
      "Gen Loss: 2.55438 Disc Loss: 0.733378 Q Losses: [0.021012649, 0.00015092341]\n",
      "Gen Loss: 2.18989 Disc Loss: 0.755534 Q Losses: [0.016728777, 0.052270673]\n",
      "Saved Model\n",
      "Gen Loss: 1.81161 Disc Loss: 0.769977 Q Losses: [0.027655343, 0.13060969]\n",
      "Gen Loss: 2.01132 Disc Loss: 0.600156 Q Losses: [0.023299068, 0.017389825]\n",
      "Gen Loss: 5.60565 Disc Loss: 1.76012 Q Losses: [0.034141287, 0.0013092344]\n",
      "Gen Loss: 2.373 Disc Loss: 0.550406 Q Losses: [0.031280048, 0.0012787248]\n",
      "Gen Loss: 2.90256 Disc Loss: 0.333677 Q Losses: [0.017472664, 0.00038015313]\n",
      "Gen Loss: 3.13094 Disc Loss: 0.63869 Q Losses: [0.022845905, 0.00052405987]\n",
      "Gen Loss: 3.28766 Disc Loss: 0.434543 Q Losses: [0.025464278, 0.00019285134]\n",
      "Gen Loss: 2.0909 Disc Loss: 0.537336 Q Losses: [0.027667075, 6.4147971e-05]\n",
      "Gen Loss: 1.87682 Disc Loss: 0.620235 Q Losses: [0.027907219, 0.0012672974]\n",
      "Gen Loss: 2.69241 Disc Loss: 0.510834 Q Losses: [0.021302165, 0.00011371231]\n",
      "Saved Model\n",
      "Gen Loss: 2.90555 Disc Loss: 0.5566 Q Losses: [0.022359788, 0.0040285485]\n",
      "Gen Loss: 4.2256 Disc Loss: 0.71153 Q Losses: [0.022726392, 0.00022894461]\n",
      "Gen Loss: 3.56217 Disc Loss: 1.0769 Q Losses: [0.027012713, 0.0010733476]\n",
      "Gen Loss: 2.15957 Disc Loss: 0.518993 Q Losses: [0.021299703, 0.00015581305]\n",
      "Gen Loss: 4.95322 Disc Loss: 1.55203 Q Losses: [0.022611145, 9.5987059e-05]\n",
      "Gen Loss: 2.82753 Disc Loss: 0.677202 Q Losses: [0.022642639, 0.00063148647]\n",
      "Gen Loss: 2.88344 Disc Loss: 0.722615 Q Losses: [0.024726415, 0.0025315487]\n",
      "Gen Loss: 3.56088 Disc Loss: 0.790364 Q Losses: [0.02284627, 0.0056172838]\n",
      "Gen Loss: 3.15142 Disc Loss: 0.364788 Q Losses: [0.022270825, 2.6812573e-05]\n",
      "Gen Loss: 2.805 Disc Loss: 0.438728 Q Losses: [0.024968121, 0.00066313526]\n",
      "Saved Model\n",
      "Gen Loss: 1.87652 Disc Loss: 0.611623 Q Losses: [0.018635098, 0.0075210775]\n",
      "Gen Loss: 3.15863 Disc Loss: 0.404495 Q Losses: [0.021717882, 0.00019085976]\n",
      "Gen Loss: 2.24024 Disc Loss: 0.867055 Q Losses: [0.023089835, 0.00010017527]\n",
      "Gen Loss: 2.54423 Disc Loss: 0.672147 Q Losses: [0.01688624, 0.1446978]\n",
      "Gen Loss: 1.98006 Disc Loss: 0.545535 Q Losses: [0.032492578, 0.00026875045]\n",
      "Gen Loss: 3.90985 Disc Loss: 0.629954 Q Losses: [0.026630413, 0.003586886]\n",
      "Gen Loss: 2.90035 Disc Loss: 0.652946 Q Losses: [0.027918413, 0.00012696953]\n",
      "Gen Loss: 3.67302 Disc Loss: 0.632244 Q Losses: [0.015215854, 4.8814498e-05]\n",
      "Gen Loss: 2.82639 Disc Loss: 0.555333 Q Losses: [0.018400034, 0.00050164136]\n",
      "Gen Loss: 3.7479 Disc Loss: 0.545207 Q Losses: [0.023144048, 7.1238828e-05]\n",
      "Saved Model\n",
      "Gen Loss: 3.04955 Disc Loss: 0.548651 Q Losses: [0.028542932, 0.0016072902]\n",
      "Gen Loss: 0.821991 Disc Loss: 1.20707 Q Losses: [0.024102023, 0.00034027331]\n",
      "Gen Loss: 3.37348 Disc Loss: 0.460842 Q Losses: [0.021465801, 0.00097959198]\n",
      "Gen Loss: 1.05339 Disc Loss: 0.674024 Q Losses: [0.025477689, 0.00018818928]\n",
      "Gen Loss: 1.84955 Disc Loss: 0.573431 Q Losses: [0.018076219, 9.7593336e-05]\n",
      "Gen Loss: 1.17803 Disc Loss: 0.608113 Q Losses: [0.017823458, 0.0023145685]\n",
      "Gen Loss: 4.64358 Disc Loss: 0.550656 Q Losses: [0.024341915, 0.00012097884]\n",
      "Gen Loss: 3.73307 Disc Loss: 0.243125 Q Losses: [0.020009417, 0.00011268264]\n",
      "Gen Loss: 2.55779 Disc Loss: 0.454086 Q Losses: [0.02286325, 0.00067215937]\n",
      "Gen Loss: 3.30046 Disc Loss: 0.403861 Q Losses: [0.022718497, 0.00035811667]\n",
      "Saved Model\n",
      "Gen Loss: 4.07695 Disc Loss: 0.779346 Q Losses: [0.018655475, 0.00015173871]\n",
      "Gen Loss: 1.87717 Disc Loss: 0.620979 Q Losses: [0.016139355, 8.2075305e-05]\n",
      "Gen Loss: 3.38327 Disc Loss: 0.627333 Q Losses: [0.022697752, 0.00010372058]\n",
      "Gen Loss: 2.9567 Disc Loss: 0.31665 Q Losses: [0.027853198, 0.078357995]\n",
      "Gen Loss: 3.9915 Disc Loss: 0.486508 Q Losses: [0.018550228, 0.00044486584]\n",
      "Gen Loss: 1.94258 Disc Loss: 0.652101 Q Losses: [0.023389168, 0.0024196978]\n",
      "Gen Loss: 3.87759 Disc Loss: 0.315291 Q Losses: [0.0165371, 0.00018051734]\n",
      "Gen Loss: 2.97082 Disc Loss: 0.779156 Q Losses: [0.02246004, 7.22029e-05]\n",
      "Gen Loss: 2.97574 Disc Loss: 0.270513 Q Losses: [0.022562869, 0.00059817254]\n",
      "Gen Loss: 4.04186 Disc Loss: 0.587526 Q Losses: [0.024176162, 0.021053692]\n",
      "Saved Model\n",
      "Gen Loss: 2.48499 Disc Loss: 0.39893 Q Losses: [0.02012893, 0.00027796801]\n",
      "Gen Loss: 1.3106 Disc Loss: 0.528087 Q Losses: [0.028105453, 9.224657e-05]\n",
      "Gen Loss: 1.04955 Disc Loss: 0.510983 Q Losses: [0.016464308, 0.00021956384]\n",
      "Gen Loss: 5.04046 Disc Loss: 0.819016 Q Losses: [0.033540681, 0.00017668889]\n",
      "Gen Loss: 3.39245 Disc Loss: 0.332825 Q Losses: [0.021559499, 0.0007276166]\n",
      "Gen Loss: 3.11239 Disc Loss: 0.46491 Q Losses: [0.024327617, 0.00016370893]\n",
      "Gen Loss: -1.44618 Disc Loss: 1.86851 Q Losses: [0.019891888, 0.00012642253]\n",
      "Gen Loss: 6.05702 Disc Loss: 1.12148 Q Losses: [0.019246807, 2.7381348e-05]\n",
      "Gen Loss: 0.740158 Disc Loss: 1.09365 Q Losses: [0.018319268, 0.00010635748]\n",
      "Gen Loss: 3.36083 Disc Loss: 0.610466 Q Losses: [0.028271992, 0.031286079]\n",
      "Saved Model\n",
      "Gen Loss: 0.0890329 Disc Loss: 0.707585 Q Losses: [0.025926147, 0.00015396863]\n",
      "Gen Loss: 2.65674 Disc Loss: 0.424291 Q Losses: [0.036553472, 8.2490849e-05]\n",
      "Gen Loss: 0.0793408 Disc Loss: 0.821266 Q Losses: [0.026493635, 0.00023231781]\n",
      "Gen Loss: 3.28708 Disc Loss: 0.615184 Q Losses: [0.040251728, 0.00028354395]\n",
      "Gen Loss: 5.36532 Disc Loss: 1.20991 Q Losses: [0.035023101, 9.598086e-05]\n",
      "Gen Loss: 3.74693 Disc Loss: 0.517835 Q Losses: [0.021333029, 0.00032090221]\n",
      "Gen Loss: 2.06433 Disc Loss: 0.615291 Q Losses: [0.018284241, 0.0010732346]\n",
      "Gen Loss: 3.41849 Disc Loss: 0.560575 Q Losses: [0.018940575, 0.00011902198]\n",
      "Gen Loss: 2.15971 Disc Loss: 0.436746 Q Losses: [0.024025803, 0.014654073]\n",
      "Gen Loss: 2.16471 Disc Loss: 0.572497 Q Losses: [0.022234904, 0.00053418742]\n",
      "Saved Model\n",
      "Gen Loss: 1.17059 Disc Loss: 0.574145 Q Losses: [0.021246843, 0.00028241839]\n",
      "Gen Loss: 1.97384 Disc Loss: 0.654229 Q Losses: [0.025087168, 0.003473181]\n",
      "Gen Loss: 1.78976 Disc Loss: 0.785501 Q Losses: [0.026039133, 0.0031406763]\n",
      "Gen Loss: 2.92741 Disc Loss: 0.351882 Q Losses: [0.020682134, 0.0015262348]\n",
      "Gen Loss: 3.30121 Disc Loss: 0.463618 Q Losses: [0.015839972, 0.00013671067]\n",
      "Gen Loss: 2.42255 Disc Loss: 0.610888 Q Losses: [0.021759648, 0.00033621234]\n",
      "Gen Loss: 3.99636 Disc Loss: 0.456441 Q Losses: [0.01999931, 0.047507659]\n",
      "Gen Loss: 3.61021 Disc Loss: 0.543547 Q Losses: [0.034007084, 0.0010309322]\n",
      "Gen Loss: 4.80725 Disc Loss: 0.845688 Q Losses: [0.026236851, 7.6746153e-05]\n",
      "Gen Loss: 2.29883 Disc Loss: 0.489099 Q Losses: [0.020286068, 7.8224308e-05]\n",
      "Saved Model\n",
      "Gen Loss: 0.706721 Disc Loss: 0.729176 Q Losses: [0.022917727, 0.00014060525]\n",
      "Gen Loss: 3.35685 Disc Loss: 0.41746 Q Losses: [0.021889811, 0.0027824254]\n",
      "Gen Loss: 0.769073 Disc Loss: 0.758371 Q Losses: [0.019853149, 0.00018325675]\n",
      "Gen Loss: 4.09179 Disc Loss: 0.668126 Q Losses: [0.021461945, 0.0039705862]\n",
      "Gen Loss: 3.93098 Disc Loss: 0.543875 Q Losses: [0.021851169, 0.00022812035]\n",
      "Gen Loss: 2.60704 Disc Loss: 0.433953 Q Losses: [0.021655761, 0.00015704666]\n",
      "Gen Loss: 6.43348 Disc Loss: 0.969781 Q Losses: [0.019196803, 6.5959321e-05]\n",
      "Gen Loss: 2.60736 Disc Loss: 0.653684 Q Losses: [0.019164145, 5.3408156e-05]\n",
      "Gen Loss: 3.30921 Disc Loss: 0.496369 Q Losses: [0.023717308, 0.0026432376]\n",
      "Gen Loss: 3.88232 Disc Loss: 0.407679 Q Losses: [0.022047698, 0.0001398677]\n",
      "Saved Model\n",
      "Gen Loss: 3.29949 Disc Loss: 0.535847 Q Losses: [0.018892851, 4.0082712e-05]\n",
      "Gen Loss: 1.33459 Disc Loss: 0.755457 Q Losses: [0.018062977, 0.00031957316]\n",
      "Gen Loss: 3.39507 Disc Loss: 0.352192 Q Losses: [0.022534519, 0.00025184976]\n",
      "Gen Loss: 2.78472 Disc Loss: 0.525997 Q Losses: [0.023556899, 2.1516258e-05]\n",
      "Gen Loss: 3.88964 Disc Loss: 0.544452 Q Losses: [0.026849208, 6.0478753e-05]\n",
      "Gen Loss: 4.06222 Disc Loss: 0.413965 Q Losses: [0.02611973, 0.00020419061]\n",
      "Gen Loss: 3.11795 Disc Loss: 0.473577 Q Losses: [0.024932168, 0.00075996842]\n",
      "Gen Loss: 1.51495 Disc Loss: 1.06245 Q Losses: [0.0202024, 0.00028863575]\n",
      "Gen Loss: 3.85383 Disc Loss: 0.572183 Q Losses: [0.036298916, 0.017087484]\n",
      "Gen Loss: 3.58929 Disc Loss: 0.498088 Q Losses: [0.022091754, 7.8716359e-05]\n",
      "Saved Model\n",
      "Gen Loss: 3.46965 Disc Loss: 0.345425 Q Losses: [0.020851456, 0.00026965715]\n",
      "Gen Loss: 3.37074 Disc Loss: 0.417703 Q Losses: [0.02084754, 0.043540377]\n",
      "Gen Loss: 4.0852 Disc Loss: 0.367944 Q Losses: [0.024256749, 9.331679e-05]\n",
      "Gen Loss: 3.83741 Disc Loss: 0.664956 Q Losses: [0.015701681, 4.6560592e-05]\n",
      "Gen Loss: 2.78617 Disc Loss: 0.35836 Q Losses: [0.032344259, 0.00021334262]\n",
      "Gen Loss: 3.69478 Disc Loss: 0.709585 Q Losses: [0.017306831, 0.00079022028]\n",
      "Gen Loss: 3.20929 Disc Loss: 0.29546 Q Losses: [0.015523739, 0.00093106384]\n",
      "Gen Loss: 2.49122 Disc Loss: 0.393857 Q Losses: [0.020078782, 0.00025934016]\n",
      "Gen Loss: 2.58084 Disc Loss: 0.36939 Q Losses: [0.018073849, 3.6116624e-05]\n",
      "Gen Loss: 2.95394 Disc Loss: 0.302177 Q Losses: [0.021062721, 3.9985211e-05]\n",
      "Saved Model\n",
      "Gen Loss: 4.6716 Disc Loss: 0.362325 Q Losses: [0.022191871, 5.214836e-05]\n",
      "Gen Loss: 4.06414 Disc Loss: 0.340944 Q Losses: [0.01805127, 0.00011982792]\n",
      "Gen Loss: 2.86099 Disc Loss: 0.495852 Q Losses: [0.025739282, 0.00019192733]\n",
      "Gen Loss: 3.32697 Disc Loss: 0.631763 Q Losses: [0.029601375, 0.00032820355]\n",
      "Gen Loss: 0.186263 Disc Loss: 1.13414 Q Losses: [0.02045653, 0.0024515856]\n",
      "Gen Loss: 3.98329 Disc Loss: 0.319736 Q Losses: [0.017772127, 0.0056409994]\n",
      "Gen Loss: 2.97927 Disc Loss: 0.562494 Q Losses: [0.028876984, 3.9659255e-05]\n",
      "Gen Loss: 3.41349 Disc Loss: 0.438447 Q Losses: [0.024592988, 0.00057425245]\n",
      "Gen Loss: 3.74363 Disc Loss: 0.625728 Q Losses: [0.023038551, 0.00011759885]\n",
      "Gen Loss: 0.359513 Disc Loss: 1.43688 Q Losses: [0.020490501, 0.00011240651]\n",
      "Saved Model\n",
      "Gen Loss: 4.73525 Disc Loss: 0.779429 Q Losses: [0.017807417, 0.00065885449]\n",
      "Gen Loss: 2.65643 Disc Loss: 0.377113 Q Losses: [0.023617564, 0.00015944225]\n",
      "Gen Loss: 3.35107 Disc Loss: 0.224229 Q Losses: [0.023987532, 0.00014157583]\n",
      "Gen Loss: 1.3153 Disc Loss: 0.850911 Q Losses: [0.02304733, 8.005587e-05]\n",
      "Gen Loss: 2.61702 Disc Loss: 0.680785 Q Losses: [0.030702733, 0.00045100792]\n",
      "Gen Loss: 3.18326 Disc Loss: 0.405532 Q Losses: [0.016158277, 0.00016790576]\n",
      "Gen Loss: 3.83352 Disc Loss: 0.319131 Q Losses: [0.044065811, 5.2669973e-05]\n",
      "Gen Loss: 3.94403 Disc Loss: 0.400786 Q Losses: [0.024515312, 0.0039445711]\n",
      "Gen Loss: 2.21148 Disc Loss: 0.340606 Q Losses: [0.021894429, 0.00016115184]\n",
      "Gen Loss: 3.72663 Disc Loss: 0.628349 Q Losses: [0.015134817, 0.0014619847]\n",
      "Saved Model\n",
      "Gen Loss: 4.85087 Disc Loss: 0.773981 Q Losses: [0.024128664, 0.041270658]\n",
      "Gen Loss: 3.57283 Disc Loss: 0.695775 Q Losses: [0.020572253, 0.0023015318]\n",
      "Gen Loss: 2.99326 Disc Loss: 0.71321 Q Losses: [0.025722314, 0.00012009969]\n",
      "Gen Loss: 2.67565 Disc Loss: 0.585888 Q Losses: [0.025177985, 8.9315261e-05]\n",
      "Gen Loss: 2.81309 Disc Loss: 0.467625 Q Losses: [0.020668373, 0.00038245786]\n",
      "Gen Loss: 1.95543 Disc Loss: 0.47449 Q Losses: [0.017480843, 6.7602261e-05]\n",
      "Gen Loss: 3.12478 Disc Loss: 0.597393 Q Losses: [0.026404068, 4.2502907e-05]\n",
      "Gen Loss: 2.42471 Disc Loss: 0.417198 Q Losses: [0.014239295, 0.00014045545]\n",
      "Gen Loss: 3.4461 Disc Loss: 0.348461 Q Losses: [0.017650392, 5.0531744e-05]\n",
      "Gen Loss: 1.5079 Disc Loss: 0.448752 Q Losses: [0.034750395, 0.00011790148]\n",
      "Saved Model\n",
      "Gen Loss: 4.26297 Disc Loss: 0.333746 Q Losses: [0.025616616, 0.0022340333]\n",
      "Gen Loss: 3.04579 Disc Loss: 0.416717 Q Losses: [0.021964811, 0.00046698484]\n",
      "Gen Loss: -0.275879 Disc Loss: 0.96671 Q Losses: [0.023956932, 0.00032881234]\n",
      "Gen Loss: 2.24971 Disc Loss: 0.455061 Q Losses: [0.025303999, 7.2590439e-05]\n",
      "Gen Loss: 5.4701 Disc Loss: 1.26502 Q Losses: [0.024183009, 0.00069851073]\n",
      "Gen Loss: 3.79665 Disc Loss: 0.390113 Q Losses: [0.019593213, 0.0002754294]\n",
      "Gen Loss: 5.05539 Disc Loss: 0.175108 Q Losses: [0.028200481, 0.00020444073]\n",
      "Gen Loss: 1.25805 Disc Loss: 0.426719 Q Losses: [0.02631622, 4.45605e-05]\n",
      "Gen Loss: 3.11899 Disc Loss: 0.356851 Q Losses: [0.021129424, 0.00014793022]\n",
      "Gen Loss: 1.51186 Disc Loss: 0.520091 Q Losses: [0.041164346, 6.3047279e-05]\n",
      "Saved Model\n",
      "Gen Loss: 2.97986 Disc Loss: 0.344569 Q Losses: [0.016464274, 0.012751386]\n",
      "Gen Loss: 4.64164 Disc Loss: 0.605309 Q Losses: [0.020840039, 0.00012545838]\n",
      "Gen Loss: 1.83284 Disc Loss: 0.621669 Q Losses: [0.02004214, 7.7383054e-05]\n",
      "Gen Loss: 4.02794 Disc Loss: 0.589443 Q Losses: [0.019594463, 7.4236312e-05]\n",
      "Gen Loss: 1.6601 Disc Loss: 0.996921 Q Losses: [0.018803008, 5.9717386e-05]\n",
      "Gen Loss: 3.63576 Disc Loss: 0.283091 Q Losses: [0.017077131, 2.5665229e-05]\n",
      "Gen Loss: 1.08556 Disc Loss: 0.857983 Q Losses: [0.017859228, 0.00059284945]\n",
      "Gen Loss: 3.22853 Disc Loss: 0.449359 Q Losses: [0.020290144, 0.00081942783]\n",
      "Gen Loss: 4.67392 Disc Loss: 0.366479 Q Losses: [0.017849602, 2.122892e-05]\n",
      "Gen Loss: 3.29768 Disc Loss: 0.295366 Q Losses: [0.030416744, 5.1741066e-05]\n",
      "Saved Model\n",
      "Gen Loss: 5.02057 Disc Loss: 1.10703 Q Losses: [0.018762765, 8.1885533e-05]\n",
      "Gen Loss: 4.55982 Disc Loss: 0.479348 Q Losses: [0.033560351, 0.00043073363]\n",
      "Gen Loss: 2.85584 Disc Loss: 0.461101 Q Losses: [0.026870053, 0.009293397]\n",
      "Gen Loss: 6.41505 Disc Loss: 1.11071 Q Losses: [0.020650564, 0.00036235992]\n",
      "Gen Loss: 4.36256 Disc Loss: 0.650697 Q Losses: [0.018964685, 0.00025054457]\n",
      "Gen Loss: 1.43921 Disc Loss: 0.889 Q Losses: [0.021997862, 3.8663882e-05]\n",
      "Gen Loss: 4.86037 Disc Loss: 0.272654 Q Losses: [0.024177518, 0.00055163878]\n",
      "Gen Loss: 4.52185 Disc Loss: 0.24178 Q Losses: [0.021126566, 0.014730063]\n",
      "Gen Loss: 4.17897 Disc Loss: 0.231349 Q Losses: [0.015984479, 4.847156e-05]\n",
      "Gen Loss: 4.5562 Disc Loss: 0.359479 Q Losses: [0.025312049, 0.047439139]\n",
      "Saved Model\n",
      "Gen Loss: 3.27754 Disc Loss: 0.426116 Q Losses: [0.016729981, 0.00016688995]\n",
      "Gen Loss: 2.43096 Disc Loss: 0.53544 Q Losses: [0.025537729, 5.1413186e-05]\n",
      "Gen Loss: 3.02077 Disc Loss: 0.291706 Q Losses: [0.021670729, 9.1862756e-05]\n",
      "Gen Loss: 2.37795 Disc Loss: 0.536729 Q Losses: [0.014969047, 4.4894878e-05]\n",
      "Gen Loss: 2.66937 Disc Loss: 0.463212 Q Losses: [0.024075292, 0.00072450918]\n",
      "Gen Loss: 1.94869 Disc Loss: 0.45968 Q Losses: [0.014779735, 0.00010523287]\n",
      "Gen Loss: 3.40822 Disc Loss: 0.466616 Q Losses: [0.022902632, 0.00011177048]\n",
      "Gen Loss: 4.55415 Disc Loss: 0.258964 Q Losses: [0.018710932, 0.0014671137]\n",
      "Gen Loss: 2.51083 Disc Loss: 0.350252 Q Losses: [0.017483346, 0.00012095424]\n",
      "Gen Loss: 3.87304 Disc Loss: 0.484928 Q Losses: [0.020525655, 4.6767163e-05]\n",
      "Saved Model\n",
      "Gen Loss: 5.07503 Disc Loss: 0.234524 Q Losses: [0.016567275, 0.0030161862]\n",
      "Gen Loss: 2.25474 Disc Loss: 0.473522 Q Losses: [0.018765349, 0.00019913873]\n",
      "Gen Loss: 3.59064 Disc Loss: 0.470274 Q Losses: [0.018996296, 0.00023463639]\n",
      "Gen Loss: 1.89306 Disc Loss: 0.537783 Q Losses: [0.020550098, 0.00022414244]\n",
      "Gen Loss: 2.53641 Disc Loss: 0.612659 Q Losses: [0.024966612, 0.00016347987]\n",
      "Gen Loss: 3.23359 Disc Loss: 0.331271 Q Losses: [0.015690476, 5.220162e-05]\n",
      "Gen Loss: 1.09564 Disc Loss: 2.35229 Q Losses: [0.031864017, 0.00029569736]\n",
      "Gen Loss: 5.31348 Disc Loss: 1.03807 Q Losses: [0.02149521, 0.00011833557]\n",
      "Gen Loss: 1.69275 Disc Loss: 0.596299 Q Losses: [0.022451669, 0.00044602336]\n",
      "Gen Loss: 3.4435 Disc Loss: 0.499015 Q Losses: [0.019163188, 0.00026203709]\n",
      "Saved Model\n",
      "Gen Loss: 3.80771 Disc Loss: 0.370004 Q Losses: [0.01800748, 0.0020436146]\n",
      "Gen Loss: 1.07763 Disc Loss: 0.61067 Q Losses: [0.028410953, 0.0016493098]\n",
      "Gen Loss: 4.47477 Disc Loss: 2.06241 Q Losses: [0.016103568, 8.1463935e-05]\n",
      "Gen Loss: 3.25882 Disc Loss: 0.443802 Q Losses: [0.015943367, 6.7100977e-05]\n",
      "Gen Loss: 4.19216 Disc Loss: 0.27454 Q Losses: [0.020358417, 1.1654833e-05]\n",
      "Gen Loss: 3.57989 Disc Loss: 0.254773 Q Losses: [0.017757837, 0.00026603692]\n",
      "Gen Loss: 4.77387 Disc Loss: 0.203048 Q Losses: [0.017546084, 0.0001945071]\n",
      "Gen Loss: 1.59582 Disc Loss: 0.76003 Q Losses: [0.028015021, 0.00048538361]\n",
      "Gen Loss: 3.64917 Disc Loss: 0.467954 Q Losses: [0.019117083, 0.00040858996]\n",
      "Gen Loss: 2.55282 Disc Loss: 0.351532 Q Losses: [0.022762334, 0.00018103441]\n",
      "Saved Model\n",
      "Gen Loss: 3.56035 Disc Loss: 0.297165 Q Losses: [0.017239355, 3.826835e-05]\n",
      "Gen Loss: -0.0980715 Disc Loss: 4.41325 Q Losses: [0.01851658, 0.00051496981]\n",
      "Gen Loss: 5.00348 Disc Loss: 0.356755 Q Losses: [0.016180862, 0.0011274552]\n",
      "Gen Loss: 3.53169 Disc Loss: 0.84835 Q Losses: [0.019245878, 3.841623e-05]\n",
      "Gen Loss: 1.97973 Disc Loss: 0.438303 Q Losses: [0.016207829, 0.0028112137]\n",
      "Gen Loss: 4.03205 Disc Loss: 0.817825 Q Losses: [0.019364495, 0.00021903947]\n",
      "Gen Loss: 4.20194 Disc Loss: 0.636988 Q Losses: [0.020818543, 0.00015891188]\n",
      "Gen Loss: -0.639564 Disc Loss: 1.25317 Q Losses: [0.01816728, 0.00019466494]\n",
      "Gen Loss: 2.10583 Disc Loss: 0.555788 Q Losses: [0.02272708, 3.6821919e-05]\n",
      "Gen Loss: 4.42008 Disc Loss: 0.293015 Q Losses: [0.015164205, 0.00019983455]\n",
      "Saved Model\n",
      "Gen Loss: 3.70113 Disc Loss: 0.16077 Q Losses: [0.024257351, 0.00014349593]\n",
      "Gen Loss: 3.51823 Disc Loss: 0.51481 Q Losses: [0.013109034, 0.0037541306]\n",
      "Gen Loss: 3.76057 Disc Loss: 0.224909 Q Losses: [0.017528921, 0.0012014736]\n",
      "Gen Loss: 2.27564 Disc Loss: 0.37588 Q Losses: [0.015189864, 0.0014538807]\n",
      "Gen Loss: 2.20963 Disc Loss: 0.635595 Q Losses: [0.021559276, 3.5949663e-05]\n",
      "Gen Loss: 3.33936 Disc Loss: 0.523066 Q Losses: [0.017532703, 0.055448283]\n",
      "Gen Loss: 2.29887 Disc Loss: 0.987578 Q Losses: [0.022085398, 0.0012361484]\n",
      "Gen Loss: 1.8214 Disc Loss: 0.52579 Q Losses: [0.017201718, 8.7639433e-05]\n",
      "Gen Loss: 4.97825 Disc Loss: 0.173788 Q Losses: [0.016473327, 0.00052868965]\n",
      "Gen Loss: 2.18552 Disc Loss: 0.406538 Q Losses: [0.014064018, 2.6373926e-05]\n",
      "Saved Model\n",
      "Gen Loss: 3.94503 Disc Loss: 0.295752 Q Losses: [0.023137212, 0.00015207619]\n",
      "Gen Loss: 2.69372 Disc Loss: 0.500578 Q Losses: [0.024264291, 0.0074960846]\n",
      "Gen Loss: 4.68893 Disc Loss: 0.153932 Q Losses: [0.030791484, 0.0099923164]\n",
      "Gen Loss: 4.22495 Disc Loss: 0.345203 Q Losses: [0.015868291, 0.00028953727]\n",
      "Gen Loss: 3.97386 Disc Loss: 0.238756 Q Losses: [0.02627462, 0.013161351]\n",
      "Gen Loss: 4.0145 Disc Loss: 0.193314 Q Losses: [0.019072238, 0.00019431132]\n",
      "Gen Loss: 3.8829 Disc Loss: 0.374586 Q Losses: [0.015310349, 8.2788647e-05]\n",
      "Gen Loss: 2.77579 Disc Loss: 0.240038 Q Losses: [0.01538611, 4.370229e-05]\n",
      "Gen Loss: 3.7346 Disc Loss: 0.197154 Q Losses: [0.013545761, 5.2500869e-05]\n",
      "Gen Loss: 4.6642 Disc Loss: 0.310331 Q Losses: [0.021068957, 0.000103959]\n",
      "Saved Model\n",
      "Gen Loss: 3.12014 Disc Loss: 0.334397 Q Losses: [0.015670255, 6.7108245e-05]\n",
      "Gen Loss: 3.4305 Disc Loss: 0.54166 Q Losses: [0.016272586, 0.00013155742]\n",
      "Gen Loss: 5.50117 Disc Loss: 0.686563 Q Losses: [0.020503841, 0.0075243404]\n",
      "Gen Loss: 2.30105 Disc Loss: 0.773097 Q Losses: [0.02250421, 2.9721785e-05]\n",
      "Gen Loss: 3.13883 Disc Loss: 0.32791 Q Losses: [0.016497357, 0.00010356102]\n",
      "Gen Loss: 2.76307 Disc Loss: 0.409334 Q Losses: [0.03101097, 0.021987416]\n",
      "Gen Loss: 3.44086 Disc Loss: 0.454973 Q Losses: [0.01544205, 9.898244e-05]\n",
      "Gen Loss: 3.66641 Disc Loss: 0.443665 Q Losses: [0.015176546, 0.00046995311]\n",
      "Gen Loss: 4.68327 Disc Loss: 0.634165 Q Losses: [0.020376207, 2.6381858e-05]\n",
      "Gen Loss: 3.65141 Disc Loss: 0.261158 Q Losses: [0.039731476, 0.0054566525]\n",
      "Saved Model\n",
      "Gen Loss: 2.6773 Disc Loss: 0.365041 Q Losses: [0.016137633, 0.00022198852]\n",
      "Gen Loss: 3.97772 Disc Loss: 0.276148 Q Losses: [0.015443776, 0.0010387533]\n",
      "Gen Loss: 4.23527 Disc Loss: 0.351367 Q Losses: [0.026776388, 6.3742489e-05]\n",
      "Gen Loss: 3.37075 Disc Loss: 0.293278 Q Losses: [0.018189508, 0.00068357348]\n",
      "Gen Loss: 3.42118 Disc Loss: 0.329756 Q Losses: [0.020054445, 0.00074892613]\n",
      "Gen Loss: 3.49614 Disc Loss: 0.374538 Q Losses: [0.017416768, 0.0021720098]\n",
      "Gen Loss: 4.25493 Disc Loss: 0.360637 Q Losses: [0.017636476, 0.0001800154]\n",
      "Gen Loss: 4.59113 Disc Loss: 0.181485 Q Losses: [0.020471357, 0.0024303852]\n",
      "Gen Loss: 4.23649 Disc Loss: 0.26035 Q Losses: [0.016342048, 0.00030044868]\n",
      "Gen Loss: 2.79493 Disc Loss: 0.493791 Q Losses: [0.022320783, 3.7613157e-05]\n",
      "Saved Model\n",
      "Gen Loss: 3.5305 Disc Loss: 0.233644 Q Losses: [0.01560724, 0.0018583873]\n",
      "Gen Loss: 3.81403 Disc Loss: 0.509033 Q Losses: [0.02427971, 0.00094260066]\n",
      "Gen Loss: 3.58396 Disc Loss: 0.272498 Q Losses: [0.025346834, 1.875122e-05]\n",
      "Gen Loss: 0.084079 Disc Loss: 0.947745 Q Losses: [0.031632051, 0.0062054619]\n",
      "Gen Loss: 4.69834 Disc Loss: 0.264148 Q Losses: [0.016309302, 8.3394378e-05]\n",
      "Gen Loss: 3.43046 Disc Loss: 0.397774 Q Losses: [0.017261144, 0.00026678958]\n",
      "Gen Loss: 4.44618 Disc Loss: 0.255602 Q Losses: [0.020831393, 0.00070911576]\n",
      "Gen Loss: 3.95976 Disc Loss: 0.386967 Q Losses: [0.016747419, 0.00045524313]\n",
      "Gen Loss: 4.03434 Disc Loss: 0.274246 Q Losses: [0.021312838, 4.0354142e-05]\n",
      "Gen Loss: 2.96919 Disc Loss: 0.182624 Q Losses: [0.017330099, 0.010857528]\n",
      "Saved Model\n",
      "Gen Loss: 3.42443 Disc Loss: 0.620882 Q Losses: [0.018574722, 0.064856403]\n",
      "Gen Loss: 3.69529 Disc Loss: 0.277653 Q Losses: [0.029023597, 0.00029627708]\n",
      "Gen Loss: 4.17958 Disc Loss: 0.427303 Q Losses: [0.027533913, 0.00012725726]\n",
      "Gen Loss: 4.70462 Disc Loss: 0.245763 Q Losses: [0.016531058, 0.00024224247]\n",
      "Gen Loss: 0.240693 Disc Loss: 0.805762 Q Losses: [0.018152962, 0.0002905736]\n",
      "Gen Loss: 4.5956 Disc Loss: 0.389209 Q Losses: [0.017951634, 7.2427334e-05]\n",
      "Gen Loss: 4.71558 Disc Loss: 0.290651 Q Losses: [0.030680075, 0.00050315203]\n",
      "Gen Loss: 3.45574 Disc Loss: 0.339011 Q Losses: [0.013271729, 0.0012108978]\n",
      "Gen Loss: 3.76705 Disc Loss: 0.254848 Q Losses: [0.020642247, 2.9337327e-05]\n",
      "Gen Loss: 5.75777 Disc Loss: 0.295866 Q Losses: [0.019852396, 0.00055729446]\n",
      "Saved Model\n",
      "Gen Loss: 3.45141 Disc Loss: 0.469646 Q Losses: [0.014262701, 5.3122196e-05]\n",
      "Gen Loss: 2.95483 Disc Loss: 0.404842 Q Losses: [0.016970748, 9.2898692e-05]\n",
      "Gen Loss: 5.07308 Disc Loss: 0.254217 Q Losses: [0.012975614, 6.427092e-05]\n",
      "Gen Loss: 5.83575 Disc Loss: 0.178736 Q Losses: [0.025532592, 6.6072229e-05]\n",
      "Gen Loss: 3.06497 Disc Loss: 0.432359 Q Losses: [0.018951757, 0.00017146047]\n",
      "Gen Loss: 4.60563 Disc Loss: 0.275636 Q Losses: [0.018072486, 6.3668507e-05]\n",
      "Gen Loss: 4.35434 Disc Loss: 0.239715 Q Losses: [0.023443762, 0.00012013198]\n",
      "Gen Loss: 4.09788 Disc Loss: 0.316948 Q Losses: [0.021391775, 5.3639011e-05]\n",
      "Gen Loss: 4.0904 Disc Loss: 0.195319 Q Losses: [0.02319492, 0.0061967992]\n",
      "Gen Loss: 3.05821 Disc Loss: 0.660638 Q Losses: [0.018347131, 0.00022104221]\n",
      "Saved Model\n",
      "Gen Loss: 5.21429 Disc Loss: 0.2776 Q Losses: [0.013518019, 0.00085625279]\n",
      "Gen Loss: 3.07445 Disc Loss: 0.393043 Q Losses: [0.016854282, 0.00077147805]\n",
      "Gen Loss: 3.29044 Disc Loss: 0.313972 Q Losses: [0.018699028, 0.002061333]\n",
      "Gen Loss: 4.03734 Disc Loss: 0.283208 Q Losses: [0.019740669, 7.444519e-05]\n",
      "Gen Loss: 4.54198 Disc Loss: 0.098049 Q Losses: [0.015389834, 7.6795302e-05]\n",
      "Gen Loss: 4.73733 Disc Loss: 0.456085 Q Losses: [0.017001435, 2.6804522e-05]\n",
      "Gen Loss: 3.26133 Disc Loss: 0.681401 Q Losses: [0.031491749, 5.4914002e-05]\n",
      "Gen Loss: 4.71605 Disc Loss: 0.318534 Q Losses: [0.023100071, 0.0050886776]\n",
      "Gen Loss: 4.34398 Disc Loss: 0.535356 Q Losses: [0.020246226, 0.00020829428]\n",
      "Gen Loss: 5.57479 Disc Loss: 0.238137 Q Losses: [0.025723765, 0.00016982944]\n",
      "Saved Model\n",
      "Gen Loss: 2.27663 Disc Loss: 0.551918 Q Losses: [0.034075893, 3.4671019e-05]\n",
      "Gen Loss: 3.32205 Disc Loss: 0.219489 Q Losses: [0.02087358, 1.3672777e-05]\n",
      "Gen Loss: 4.62647 Disc Loss: 0.269336 Q Losses: [0.019873932, 0.0001244131]\n",
      "Gen Loss: -0.428093 Disc Loss: 1.04792 Q Losses: [0.015277462, 3.3358301e-05]\n",
      "Gen Loss: 3.28333 Disc Loss: 0.243223 Q Losses: [0.013338005, 5.2757881e-05]\n",
      "Gen Loss: 3.08588 Disc Loss: 0.407223 Q Losses: [0.024189048, 0.00020409976]\n",
      "Gen Loss: 5.15921 Disc Loss: 0.347839 Q Losses: [0.022531606, 4.7000973e-05]\n",
      "Gen Loss: 2.07106 Disc Loss: 0.373658 Q Losses: [0.015539775, 0.0021684167]\n",
      "Gen Loss: 4.48773 Disc Loss: 0.229559 Q Losses: [0.021057814, 0.0081671681]\n",
      "Gen Loss: 9.68336 Disc Loss: 2.12897 Q Losses: [0.022675037, 0.00027546097]\n",
      "Saved Model\n",
      "Gen Loss: 4.41487 Disc Loss: 0.511992 Q Losses: [0.01968145, 5.7185905e-05]\n",
      "Gen Loss: 1.21414 Disc Loss: 0.612946 Q Losses: [0.025338588, 0.00022571978]\n",
      "Gen Loss: 2.70762 Disc Loss: 0.630687 Q Losses: [0.01503939, 0.0059342436]\n",
      "Gen Loss: 5.93036 Disc Loss: 0.291594 Q Losses: [0.019928038, 0.00010065443]\n",
      "Gen Loss: 5.07371 Disc Loss: 0.232579 Q Losses: [0.014012442, 0.00017238961]\n",
      "Gen Loss: 4.0571 Disc Loss: 0.236937 Q Losses: [0.033201575, 0.00013282662]\n",
      "Gen Loss: 4.57044 Disc Loss: 0.389941 Q Losses: [0.016425079, 0.00013980147]\n",
      "Gen Loss: 4.65791 Disc Loss: 0.22089 Q Losses: [0.015347269, 0.00032385314]\n",
      "Gen Loss: 2.36217 Disc Loss: 0.55521 Q Losses: [0.017408704, 0.0024083469]\n",
      "Gen Loss: 4.13003 Disc Loss: 0.294917 Q Losses: [0.017226372, 0.00034470379]\n",
      "Saved Model\n",
      "Gen Loss: 1.68139 Disc Loss: 0.402353 Q Losses: [0.019155094, 2.7327851e-05]\n",
      "Gen Loss: 3.20725 Disc Loss: 0.318294 Q Losses: [0.018680006, 4.946666e-05]\n",
      "Gen Loss: 3.40725 Disc Loss: 0.35129 Q Losses: [0.029485673, 0.00010363586]\n",
      "Gen Loss: 3.08331 Disc Loss: 0.319501 Q Losses: [0.027804885, 0.00013952823]\n",
      "Gen Loss: 5.40735 Disc Loss: 0.185186 Q Losses: [0.020742524, 0.00021000572]\n",
      "Gen Loss: 3.90379 Disc Loss: 0.396555 Q Losses: [0.01864874, 0.00053170224]\n",
      "Gen Loss: 5.59563 Disc Loss: 0.471758 Q Losses: [0.023247425, 0.00010128667]\n",
      "Gen Loss: 3.76708 Disc Loss: 0.451679 Q Losses: [0.016688213, 9.1025431e-05]\n",
      "Gen Loss: 5.04972 Disc Loss: 0.394109 Q Losses: [0.022298099, 0.00064099999]\n",
      "Gen Loss: 4.52249 Disc Loss: 0.353888 Q Losses: [0.021209821, 0.0002772577]\n",
      "Saved Model\n",
      "Gen Loss: 3.93747 Disc Loss: 0.310893 Q Losses: [0.014550921, 8.1233993e-05]\n",
      "Gen Loss: 4.03376 Disc Loss: 0.182987 Q Losses: [0.01729428, 0.0041765794]\n",
      "Gen Loss: 3.45356 Disc Loss: 0.577858 Q Losses: [0.015845031, 0.00015365175]\n",
      "Gen Loss: 3.60291 Disc Loss: 0.240708 Q Losses: [0.021507267, 0.0033540097]\n",
      "Gen Loss: 4.86178 Disc Loss: 0.270083 Q Losses: [0.016670959, 2.6838366e-05]\n",
      "Gen Loss: 5.91343 Disc Loss: 0.396718 Q Losses: [0.017204084, 0.020570576]\n",
      "Gen Loss: 3.98917 Disc Loss: 0.295034 Q Losses: [0.020887882, 5.657351e-05]\n",
      "Gen Loss: 3.24805 Disc Loss: 0.275754 Q Losses: [0.017587254, 0.00078222802]\n",
      "Gen Loss: 3.67566 Disc Loss: 0.423469 Q Losses: [0.017939001, 8.6590378e-05]\n",
      "Gen Loss: 3.19568 Disc Loss: 0.196991 Q Losses: [0.019013524, 2.5359841e-05]\n",
      "Saved Model\n",
      "Gen Loss: 4.92705 Disc Loss: 0.50715 Q Losses: [0.023640353, 9.0553076e-05]\n",
      "Gen Loss: 9.69447 Disc Loss: 2.14316 Q Losses: [0.023510043, 3.6351412e-05]\n",
      "Gen Loss: 1.61745 Disc Loss: 0.420563 Q Losses: [0.019989081, 9.8521617e-05]\n",
      "Gen Loss: 4.60928 Disc Loss: 0.636822 Q Losses: [0.018554173, 5.7007688e-05]\n",
      "Gen Loss: 4.35384 Disc Loss: 0.439861 Q Losses: [0.012241201, 0.00021367164]\n",
      "Gen Loss: 3.89989 Disc Loss: 0.405407 Q Losses: [0.012698549, 9.7946759e-05]\n",
      "Gen Loss: 5.03563 Disc Loss: 0.45564 Q Losses: [0.017885178, 0.0016108986]\n",
      "Gen Loss: 3.04263 Disc Loss: 0.31823 Q Losses: [0.017434783, 0.011384799]\n",
      "Gen Loss: 3.71888 Disc Loss: 0.34911 Q Losses: [0.018162427, 0.00041550826]\n",
      "Gen Loss: 3.13505 Disc Loss: 0.392057 Q Losses: [0.014081888, 0.00024828859]\n",
      "Saved Model\n",
      "Gen Loss: 4.85797 Disc Loss: 0.157879 Q Losses: [0.01656872, 0.00015712265]\n",
      "Gen Loss: 1.42414 Disc Loss: 0.309781 Q Losses: [0.019356679, 6.7639936e-05]\n",
      "Gen Loss: 2.99297 Disc Loss: 0.282768 Q Losses: [0.020462532, 0.00022214226]\n",
      "Gen Loss: 4.37542 Disc Loss: 0.270564 Q Losses: [0.017497532, 8.2818427e-05]\n",
      "Gen Loss: 5.92504 Disc Loss: 0.304809 Q Losses: [0.016248573, 0.00017581075]\n",
      "Gen Loss: 1.98764 Disc Loss: 0.569386 Q Losses: [0.016974088, 3.6631631e-05]\n",
      "Gen Loss: 4.14186 Disc Loss: 0.262039 Q Losses: [0.021452881, 0.0022918319]\n",
      "Gen Loss: 3.31258 Disc Loss: 0.228315 Q Losses: [0.014637435, 0.00021829817]\n",
      "Gen Loss: 6.67192 Disc Loss: 0.871434 Q Losses: [0.019984741, 0.0074821953]\n",
      "Gen Loss: 4.95015 Disc Loss: 0.380501 Q Losses: [0.026402153, 0.00017286566]\n",
      "Saved Model\n",
      "Gen Loss: 7.11167 Disc Loss: 1.73519 Q Losses: [0.021311231, 0.0201005]\n",
      "Gen Loss: 5.51609 Disc Loss: 0.176884 Q Losses: [0.018415995, 0.00010045818]\n",
      "Gen Loss: 3.60732 Disc Loss: 0.260284 Q Losses: [0.02059871, 4.9913258e-05]\n",
      "Gen Loss: -3.21407 Disc Loss: 1.49259 Q Losses: [0.021842644, 0.00010887624]\n",
      "Gen Loss: 7.01202 Disc Loss: 1.91949 Q Losses: [0.015724802, 5.2375864e-05]\n",
      "Gen Loss: 6.78112 Disc Loss: 0.726505 Q Losses: [0.016691657, 0.01076367]\n",
      "Gen Loss: 3.12335 Disc Loss: 0.664868 Q Losses: [0.015762363, 0.00014057949]\n",
      "Gen Loss: 4.01273 Disc Loss: 0.335823 Q Losses: [0.022375489, 0.00025909184]\n",
      "Gen Loss: 3.9869 Disc Loss: 0.308594 Q Losses: [0.019101778, 3.8989027e-05]\n",
      "Gen Loss: 3.13969 Disc Loss: 0.45589 Q Losses: [0.017963232, 2.7168535e-05]\n",
      "Saved Model\n",
      "Gen Loss: 4.47325 Disc Loss: 0.207628 Q Losses: [0.017963566, 5.6657587e-05]\n",
      "Gen Loss: 5.31403 Disc Loss: 0.456081 Q Losses: [0.013595875, 0.00030919901]\n",
      "Gen Loss: 3.68481 Disc Loss: 0.253115 Q Losses: [0.014230909, 0.00021513489]\n",
      "Gen Loss: 4.1914 Disc Loss: 0.250416 Q Losses: [0.016765662, 0.00012012669]\n",
      "Gen Loss: 4.74308 Disc Loss: 0.375451 Q Losses: [0.036815621, 0.00012978505]\n",
      "Gen Loss: 0.350708 Disc Loss: 0.432849 Q Losses: [0.027627245, 0.00029958217]\n",
      "Gen Loss: 5.13364 Disc Loss: 0.398382 Q Losses: [0.019235421, 1.977286e-05]\n",
      "Gen Loss: 4.04883 Disc Loss: 0.419877 Q Losses: [0.025165765, 3.921843e-05]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37617195, 2.3066013]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34770182, 2.3012104]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35136649, 2.3042116]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30870879, 2.3019052]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31174916, 2.3027959]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30871841, 2.3044991]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31914473, 2.3030994]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34686565, 2.3058383]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36116189, 2.3037789]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36430049, 2.3023546]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3588928, 2.3029413]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32713315, 2.3031306]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34356529, 2.3031723]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35557804, 2.3038197]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32184786, 2.3014846]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34061295, 2.3024611]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29847717, 2.3027513]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.26731467, 2.3036239]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37655136, 2.3025718]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3537381, 2.3051283]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35640055, 2.3022542]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32352066, 2.3034675]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3714883, 2.3025022]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36096418, 2.3007948]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33945772, 2.3029618]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38192359, 2.3003867]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30498719, 2.3030438]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34773606, 2.3011498]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35484737, 2.3022504]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37504363, 2.3031087]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35383338, 2.3025663]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32986677, 2.3020093]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36809754, 2.3025439]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32606643, 2.3029819]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30886465, 2.302844]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31098846, 2.3010697]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37023878, 2.3033564]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34481138, 2.3038106]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33415687, 2.303333]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33992094, 2.3028886]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3035152, 2.3037796]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31958902, 2.3039999]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33502588, 2.3032217]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35065717, 2.3004351]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35763863, 2.303762]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28051782, 2.3038929]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35275075, 2.3022194]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3123281, 2.3022244]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31509021, 2.3011479]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3098433, 2.3025284]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33048582, 2.3024442]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36446846, 2.3027654]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34999648, 2.3016088]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35327959, 2.3023286]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.25544351, 2.3035951]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32357359, 2.302763]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31538072, 2.3030281]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31841451, 2.3023868]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35869226, 2.3027167]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30522069, 2.3023887]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3445766, 2.3028343]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33920193, 2.3021288]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32425141, 2.3021808]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35242069, 2.3033171]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36331216, 2.3028157]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34294617, 2.3034945]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32212046, 2.3030894]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30533078, 2.3013906]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32403535, 2.3024621]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33320934, 2.301949]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33831614, 2.3023543]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33779332, 2.3033152]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33477339, 2.3028717]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35715511, 2.3032038]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33568364, 2.3014843]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32533333, 2.3022759]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29926899, 2.3032074]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30034047, 2.3020611]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28887469, 2.302515]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31487733, 2.3016825]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37306434, 2.3038301]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30180717, 2.3028798]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28597057, 2.302964]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34950775, 2.3030956]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31262553, 2.3016067]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37151319, 2.3027172]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35164934, 2.3033626]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34884745, 2.3012619]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34705183, 2.3022125]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29312384, 2.3026669]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33098239, 2.3021085]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28946978, 2.3031929]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33043242, 2.3036385]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30130613, 2.3037641]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.27170971, 2.3020492]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30189669, 2.3023767]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34148765, 2.3018477]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36175507, 2.3028796]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33805248, 2.3009167]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29281038, 2.302134]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31148329, 2.3023896]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32345974, 2.3017745]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34139216, 2.3027277]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30981672, 2.3026428]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33126706, 2.3024163]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32165229, 2.3011324]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31953812, 2.3012805]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33919221, 2.3016992]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35551208, 2.3031573]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32007599, 2.303472]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33713493, 2.3020518]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29058284, 2.302424]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34992188, 2.3028171]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33987698, 2.3020773]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.377664, 2.3024187]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37741524, 2.3018899]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29165089, 2.3026128]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38853586, 2.3030062]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32355571, 2.3017364]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30500919, 2.3023858]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32790226, 2.3027258]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3529017, 2.3019528]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30745083, 2.3021884]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.26138467, 2.3032401]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35030884, 2.3029697]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36834243, 2.3029215]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.40905988, 2.3020694]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32881981, 2.3033576]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35091838, 2.3030167]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35228264, 2.3029294]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32248211, 2.3029613]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30130386, 2.3032064]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38234323, 2.302763]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32444251, 2.3038495]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31585282, 2.3022041]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35088027, 2.3016739]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36841178, 2.3023212]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34187227, 2.3032384]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28756899, 2.3028555]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32751119, 2.3023288]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32449931, 2.302422]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35181186, 2.3033707]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3614403, 2.3021579]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35544339, 2.3031001]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3836177, 2.3036201]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33435506, 2.3014596]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36031049, 2.3033936]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32272685, 2.3026717]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32578906, 2.3022425]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29228407, 2.3022385]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34968692, 2.301487]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33880404, 2.3020182]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3397879, 2.3037577]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32444569, 2.3037543]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32715043, 2.302819]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29954746, 2.3034213]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37454164, 2.302943]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33295268, 2.3017402]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32715484, 2.3027968]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34340733, 2.3027964]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31712919, 2.3037086]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32720813, 2.3036318]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38987738, 2.3025613]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36924469, 2.301909]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33568898, 2.302834]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35443252, 2.304543]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33382571, 2.3030872]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28686941, 2.3023937]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31618106, 2.3022883]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31532115, 2.3042293]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29742634, 2.3019333]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37663439, 2.3018041]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33685291, 2.3025017]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30296069, 2.3024993]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34560657, 2.3024566]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35383201, 2.3029251]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36008063, 2.3027005]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3440766, 2.302238]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33986905, 2.3025639]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30655503, 2.3020396]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29618794, 2.3033111]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33041424, 2.3020227]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34102961, 2.3016467]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32328564, 2.3033466]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31629473, 2.3024635]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33958274, 2.3016362]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29109389, 2.3020856]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32454661, 2.3016639]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34296495, 2.302599]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33343241, 2.302382]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32143435, 2.3027704]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33032146, 2.3018615]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3405422, 2.3028848]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32744247, 2.3027861]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35227731, 2.302001]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.26926875, 2.3024812]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35292041, 2.3028016]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31480566, 2.3029075]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33934057, 2.3017437]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30941781, 2.3020325]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31438959, 2.3026683]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36561644, 2.3030744]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34517494, 2.3024812]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33559799, 2.3021874]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32551584, 2.303623]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30649519, 2.3024917]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31312346, 2.3028822]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33112633, 2.3027959]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28866196, 2.3020425]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32404181, 2.302459]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31010771, 2.3021412]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34603089, 2.3020823]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38056254, 2.3022325]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35333073, 2.3030081]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34410107, 2.3021417]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33971453, 2.301991]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35820821, 2.3017304]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36469239, 2.3020191]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32781959, 2.3032484]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34127641, 2.3018813]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.26565447, 2.3014302]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31282032, 2.3031082]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38467345, 2.301507]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33820814, 2.3031716]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3435424, 2.303688]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31560048, 2.3033655]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34330794, 2.302417]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.40817028, 2.3023508]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31695867, 2.3021529]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3186107, 2.3032384]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29855561, 2.3027794]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30761597, 2.303546]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34139025, 2.3039341]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31266242, 2.3026521]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34272155, 2.3025122]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33398715, 2.301646]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37692073, 2.3021834]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31276628, 2.3026109]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33824009, 2.3035374]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.2889818, 2.3025866]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33044314, 2.3018925]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33270514, 2.3028927]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30627343, 2.3033221]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32386935, 2.3014235]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31739211, 2.3022957]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34866098, 2.3024445]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33964768, 2.3028047]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31774345, 2.3012366]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29901373, 2.3025861]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33270866, 2.3028748]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33585191, 2.3020396]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37416703, 2.3023055]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31550956, 2.3024974]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37420917, 2.3020594]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3236528, 2.3024237]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38970011, 2.3021045]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30904889, 2.3039098]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3567031, 2.3031211]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29178065, 2.3030772]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.41369525, 2.3020282]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32147369, 2.3027472]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29742151, 2.303165]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30540699, 2.3013964]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32393318, 2.3022919]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34170476, 2.3033791]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30764818, 2.3030486]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.301534, 2.3037319]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37864426, 2.3028014]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34194559, 2.3024042]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.39758912, 2.3025367]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.26845112, 2.3024583]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35480028, 2.3020971]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31802475, 2.3029928]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31956196, 2.301851]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36271057, 2.3032479]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30798894, 2.3032665]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37179494, 2.3033633]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30628586, 2.3041179]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34648401, 2.3019056]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29707757, 2.3019204]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33302209, 2.3023918]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32047516, 2.3018432]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34258699, 2.3029656]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31979865, 2.3028464]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29191065, 2.3034654]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37320548, 2.3031056]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30462456, 2.3022299]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33003008, 2.302084]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36531106, 2.3025568]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35591555, 2.3012857]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31717962, 2.3023634]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33932149, 2.3035922]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30896851, 2.3024721]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30864277, 2.3025937]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36813027, 2.3019214]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34326142, 2.3029771]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35725853, 2.3026972]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33576316, 2.3030632]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28521156, 2.3029742]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31870303, 2.3020034]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31838512, 2.3029177]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30868009, 2.3033657]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28139925, 2.3032339]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37206149, 2.3037217]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.373705, 2.3035097]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34392905, 2.3028512]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29259723, 2.3028762]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34491163, 2.3028445]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3503201, 2.3015852]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33563295, 2.303102]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37534037, 2.3027802]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31291872, 2.3026116]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32281652, 2.3018665]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30869985, 2.3014836]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31260908, 2.3034806]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33899719, 2.3020823]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35985661, 2.3033376]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36416322, 2.3026786]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31685618, 2.3022165]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30555204, 2.3030183]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34806654, 2.3026395]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38346374, 2.3021722]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36326355, 2.3023796]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32488924, 2.3027134]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38960156, 2.3025639]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3195124, 2.302846]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35288718, 2.3021989]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34133437, 2.3024304]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35675317, 2.3014104]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31586665, 2.3020165]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31703874, 2.3036819]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31028906, 2.3045902]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32131591, 2.3017402]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3340975, 2.3014054]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32782006, 2.3031557]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31341428, 2.3021574]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34637409, 2.3029735]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3174921, 2.3021574]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.27079961, 2.3021226]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37154189, 2.3017993]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31541431, 2.3022561]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32208067, 2.302248]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35650888, 2.3024836]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33760911, 2.3038931]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32735381, 2.3021855]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33937639, 2.3029008]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32546061, 2.3027244]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32698458, 2.3027894]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33206147, 2.3033061]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31907558, 2.3022902]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.39135671, 2.3030376]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37070584, 2.3016226]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32945752, 2.301697]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34656394, 2.3014255]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30666345, 2.3013482]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34568083, 2.3009677]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34192222, 2.3031549]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33219671, 2.3047969]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30652347, 2.3033452]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32725322, 2.3023448]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34063804, 2.3022285]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30313227, 2.3028865]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36768705, 2.3036108]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36797675, 2.30249]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37525114, 2.3021193]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3257041, 2.3013415]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36138472, 2.3015645]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30149442, 2.3015237]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36470604, 2.301723]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29276448, 2.3026366]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32791251, 2.3029084]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33071083, 2.302685]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31424695, 2.3029041]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35406271, 2.3031974]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29312253, 2.3026061]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34048492, 2.3030884]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30279055, 2.3026564]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33316046, 2.3027902]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29764691, 2.3025446]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32795635, 2.3026292]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33187103, 2.301897]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34937394, 2.3031602]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36307177, 2.3027096]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3539488, 2.3029549]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34449714, 2.3026273]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34112701, 2.3038077]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35093671, 2.3015199]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32612813, 2.3033507]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29566392, 2.3031366]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3319062, 2.3036036]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33114779, 2.3021178]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3503001, 2.3026857]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32923821, 2.3013992]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31588793, 2.3020921]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35904646, 2.3009233]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34717774, 2.302269]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32663786, 2.3033106]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33236581, 2.303689]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29517293, 2.3024616]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31318146, 2.3025866]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37662068, 2.3020334]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29574034, 2.3029039]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3163406, 2.3032675]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.39081916, 2.3030019]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37091613, 2.3009491]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31882566, 2.3027949]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38703373, 2.30305]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35275668, 2.3021898]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33279145, 2.3044615]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34946123, 2.3014159]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32839757, 2.3025918]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37246788, 2.3017316]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33447352, 2.3035023]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28881094, 2.3026586]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29187262, 2.3027675]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35282809, 2.3003473]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3716864, 2.3026786]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37438476, 2.3026044]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33363801, 2.3025851]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33268356, 2.3036871]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29604304, 2.3030572]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32961488, 2.3020387]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31571519, 2.301017]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3344872, 2.3030705]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32775059, 2.302567]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35864973, 2.3000662]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37049347, 2.3026075]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.312489, 2.3017514]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32202935, 2.3036132]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29970956, 2.3013978]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.38014412, 2.3023322]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32253969, 2.3047152]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32389021, 2.3008585]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.29029334, 2.3013334]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.39139676, 2.3018751]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34526807, 2.3034487]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.3252992, 2.3017259]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.36259738, 2.3021817]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37238419, 2.3038001]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.30186707, 2.3017378]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.35581774, 2.3018904]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32791683, 2.3029053]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37268016, 2.3023386]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.37381369, 2.3012285]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.28628564, 2.3022134]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.32108557, 2.3021805]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31363541, 2.3044357]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.34103066, 2.3033824]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31276739, 2.3026116]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33367112, 2.3051107]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.31304553, 2.302542]\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33565134, 2.3021183]\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan Q Losses: [0.33254197, 2.3035789]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fe3bac870e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Pad the images so the are 32x32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_Q\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_cont_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_cat_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update to optimize mutual information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "#blow up after 81800\n",
    "    \n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 81000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "        lcont = np.random.uniform(-1,1,[batch_size,number_continuous]) #\n",
    "        \n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        \n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "        _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            b = np.reshape(a,[100,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-127000.cptk\n"
     ]
    }
   ],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    b = np.reshape(a,[100,1])\n",
    "    c = np.zeros_like(b)\n",
    "    lcont_sample = np.hstack([b,c])\n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35]",
   "language": "python",
   "name": "conda-env-dlpy35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
