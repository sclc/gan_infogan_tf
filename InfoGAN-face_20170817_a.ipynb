{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN CeleA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import os, glob, cv2, math, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.misc\n",
    "import scipy\n",
    "#from PIL import Image\n",
    "\n",
    "np.random.seed(1)\n",
    "#plt.ion()   # interactive mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load CeleA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html\n",
    "#img_rows, img_cols = 100, 100\n",
    "#img_rows, img_cols = 64, 64\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "def get_im(path):\n",
    "\n",
    "    #img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.imread(path)\n",
    "    #img = plt.imread(path)\n",
    "    #resized = img\n",
    "    resized = cv2.resize(img, (img_cols, img_rows))\n",
    "\n",
    "    return resized\n",
    "\n",
    "def read_train_data_fullname(path):\n",
    "\n",
    "    \n",
    "    files = glob.glob(path)\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_fullname_lfw(path):\n",
    "\n",
    "    root = path\n",
    "    all_folders = os.listdir(root)\n",
    "    path = []\n",
    "    for afolder in all_folders:\n",
    "        path.append(root+\"/\"+afolder+\"/*.jpg\")\n",
    "    #print(path)\n",
    "    files = []\n",
    "    for apath in path:\n",
    "        templist = glob.glob(apath)\n",
    "        for afile in templist:\n",
    "            files.append(afile)\n",
    "\n",
    "    # shuffling\n",
    "    filenames = permutation(files)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def read_train_data_batch(filenames, batchsize=5):\n",
    "    \n",
    "    end = min(len(filenames), batchsize)\n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[:end]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        #normalization\n",
    "        #img -= np.mean(img)\n",
    "        #img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    filenames = filenames[end:]\n",
    "    \n",
    "    return train_data, filenames\n",
    "\n",
    "def read_train_data_mini_batch(filenames, startpoint, batchsize=5):\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    for fl in filenames[startpoint:startpoint+batchsize]:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #print(type(img))        \n",
    "        #normalization\n",
    "        img -= np.mean(img)\n",
    "        img /= np.std(img)\n",
    "        #print(img.shape)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "#random mini-batch\n",
    "def read_train_data_random_batch(filenames, batchsize=5):\n",
    "    fullsize=len(filenames)\n",
    "    #http://qiita.com/hamukazu/items/ec1b4659df00f0ce43b1\n",
    "    idset = np.random.randint(0, high=fullsize, size=batchsize)\n",
    "    #print(idset) \n",
    "    train_data = []\n",
    "    \n",
    "    for fid in idset:\n",
    "        fl = filenames[fid]\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        #img[:,:,1] = 0\n",
    "        #img[:,:,2] = 0\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        #img = np.reshape(img,[img.shape[0],img.shape[1],1])\n",
    "        # normalization\n",
    "        # https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn\n",
    "        img -= np.mean(img)\n",
    "        img /= np.std(img)\n",
    "        \n",
    "        train_data.append(img)\n",
    "    \n",
    "    # list to np.array\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "\n",
    "    # dataid, height, width, #channel -> dataid, #channel, height, width\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    \n",
    "    return train_data\n",
    "# must be full path, ~/... not ok\n",
    "# train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data,train_data_filenames = read_train_data_batch(train_data_filenames)\n",
    "\n",
    "#random mini-batch\n",
    "#train_data = read_train_data_random_batch(train_data_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(train_data),len(train_data_filenames))\n",
    "#print(len(train_data[0][1]))\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # need this transpose if there is transpose in read_train_data_xx calls\n",
    "    #inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #inp = std * inp + mean\n",
    "    #plt.imshow(inp,cmap=\"Purples\",interpolation = 'bicubic')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.01)  # pause a bit so that plots are updated\n",
    "\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba/*.jpg\")\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "#train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "#print(len(train_data_filenames))\n",
    "#print(train_data_filenames[3])\n",
    "#train_data = read_train_data_mini_batch(train_data_filenames,0)  \n",
    "#print(train_data[0].shape)\n",
    "#print(train_data[43][:,:,0:2].shape)\n",
    "\n",
    "#imshow(train_data[2][:,:,0])\n",
    "#imshow(train_data[2][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge_color(images, size))\n",
    "    #return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return images\n",
    "    #return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def merge_color(images, size):\n",
    "    h, w, c = images.shape[1], images.shape[2],images.shape[3]\n",
    "    img = np.zeros((h * size[0], w * size[1],c))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image[:,:,:]\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/depth_to_space\n",
    "# http://qiita.com/tadOne/items/48302a399dcad44c69c8   Tensorflow - padding = VALID/SAMEの違いについて\n",
    "#     so 3 tf.depth_to_space(genX,2) gives 4x2^3 = 32\n",
    "# \n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*448,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,448])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=256,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=128,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=64,kernel_size=[4,4],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=3,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,64,[4,4],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,128,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,256,[4,4],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    #print(\"d_out\"+str(d_out))\n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/split\n",
    "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "# https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "# https://www.tensorflow.org/api_docs/python/tf/trainable_variables\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "# https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "# z_lat: one_hot_size + z_size + number_continuous = 10+64+2=76\n",
    "# g_loss def is interesting, my understanding: \n",
    "#        if Dg is the probablity to be told as feak data, then 1-Dg is the probabily of suceessfully cheating, \n",
    "#        so we cal KL(Dg/(1-Dg)), and readuce_mean works as sampling proceduce\n",
    "# \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "z_size = 128 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "#categorical_list = [10]*10 # Each entry in this list defines a categorical variable of a specific size.\n",
    "categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "# categorical_list = [10,10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 1 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "#print(\"latent_cat_in:\", latent_cat_in)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "#print(\"latent_cat_list: \",latent_cat_list)\n",
    "if number_continuous>0:\n",
    "    latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    #print(latent_cat_list[idx])\n",
    "    #print(latent_oh),  woundn't print anything in sess.run()\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "#print(\"1st z_lats: \", z_lats )\n",
    "z_lats.append(z_in)\n",
    "#print(\"2nd z_lats: \", z_lats )\n",
    "if number_continuous>0:\n",
    "    z_lats.append(latent_cont_in)\n",
    "#print(\"3rd z_lats: \", z_lats )\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "#print(\"z_lat: \", z_lat )\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "#print (Gz.shape)\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "#g_loss = -tf.reduce_mean(tf.log((Dg/(1.-Dg)))) #KL Divergence optimizer\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) \n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    #print (\"latent_var: \", latent_var)\n",
    "    #print (\"tf.log(QgCat[idx]): \",tf.log(QgCat[idx]))\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "#print (len(tvars))\n",
    "#for i in tvars:\n",
    "#    print(i)\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "#\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:1 Gen Loss: 8.2131 Disc Loss: 1.37209 Q Losses: [0.21092363, 2.3025088]\n",
      "epoch:0 batch_done:2 Gen Loss: 6.17389 Disc Loss: 0.304182 Q Losses: [0.15146941, 2.317749]\n",
      "epoch:0 batch_done:3 Gen Loss: 14.2564 Disc Loss: 0.789707 Q Losses: [0.14752175, 2.2731493]\n",
      "epoch:0 batch_done:4 Gen Loss: 7.94339 Disc Loss: 1.19521 Q Losses: [0.1438432, 2.2646496]\n",
      "epoch:0 batch_done:5 Gen Loss: 9.07372 Disc Loss: 0.443367 Q Losses: [0.12777501, 2.2597294]\n",
      "epoch:0 batch_done:6 Gen Loss: 12.1364 Disc Loss: 0.364585 Q Losses: [0.12539355, 2.2745388]\n",
      "epoch:0 batch_done:7 Gen Loss: 8.86962 Disc Loss: 0.354851 Q Losses: [0.10812667, 2.2230153]\n",
      "epoch:0 batch_done:8 Gen Loss: 13.2356 Disc Loss: 0.409395 Q Losses: [0.096717626, 2.2638135]\n",
      "epoch:0 batch_done:9 Gen Loss: 8.82091 Disc Loss: 0.319841 Q Losses: [0.10619939, 2.2623992]\n",
      "epoch:0 batch_done:10 Gen Loss: 22.57 Disc Loss: 2.33908 Q Losses: [0.13083716, 2.2167377]\n",
      "epoch:0 batch_done:11 Gen Loss: 14.3662 Disc Loss: 3.02198 Q Losses: [0.11850455, 2.2278762]\n",
      "epoch:0 batch_done:12 Gen Loss: 7.96228 Disc Loss: 0.43668 Q Losses: [0.088790506, 2.206841]\n",
      "epoch:0 batch_done:13 Gen Loss: 23.9813 Disc Loss: 1.79463 Q Losses: [0.10667296, 2.1995935]\n",
      "epoch:0 batch_done:14 Gen Loss: 26.3227 Disc Loss: 0.987315 Q Losses: [0.10025211, 2.2095854]\n",
      "epoch:0 batch_done:15 Gen Loss: 15.8073 Disc Loss: 1.15894 Q Losses: [0.1026714, 2.2142835]\n",
      "epoch:0 batch_done:16 Gen Loss: 9.81326 Disc Loss: 0.221045 Q Losses: [0.069543764, 2.2099369]\n",
      "epoch:0 batch_done:17 Gen Loss: 29.0569 Disc Loss: 2.53229 Q Losses: [0.071558625, 2.1707215]\n",
      "epoch:0 batch_done:18 Gen Loss: 29.6375 Disc Loss: 2.15455 Q Losses: [0.068551496, 2.1992359]\n",
      "epoch:0 batch_done:19 Gen Loss: 20.1754 Disc Loss: 1.22096 Q Losses: [0.0894823, 2.1685972]\n",
      "epoch:0 batch_done:20 Gen Loss: 7.50614 Disc Loss: 0.212324 Q Losses: [0.069285661, 2.1683676]\n",
      "epoch:0 batch_done:21 Gen Loss: 23.3232 Disc Loss: 5.0091 Q Losses: [0.067048408, 2.1598334]\n",
      "epoch:0 batch_done:22 Gen Loss: 24.2916 Disc Loss: 0.54961 Q Losses: [0.058152344, 2.1390419]\n",
      "epoch:0 batch_done:23 Gen Loss: 9.9585 Disc Loss: 0.609816 Q Losses: [0.073420569, 2.1656871]\n",
      "epoch:0 batch_done:24 Gen Loss: 19.6395 Disc Loss: 8.99465 Q Losses: [0.050480038, 2.170506]\n",
      "epoch:0 batch_done:25 Gen Loss: 19.2157 Disc Loss: 1.63647 Q Losses: [0.051920734, 2.1199965]\n",
      "epoch:0 batch_done:26 Gen Loss: 11.4445 Disc Loss: 0.442987 Q Losses: [0.069094114, 2.1416759]\n",
      "epoch:0 batch_done:27 Gen Loss: 7.14261 Disc Loss: 0.157883 Q Losses: [0.074258387, 2.155169]\n",
      "epoch:0 batch_done:28 Gen Loss: 11.6484 Disc Loss: 0.193724 Q Losses: [0.046200462, 2.1091938]\n",
      "epoch:0 batch_done:29 Gen Loss: 11.0865 Disc Loss: 0.0923543 Q Losses: [0.056252569, 2.0674887]\n",
      "epoch:0 batch_done:30 Gen Loss: 7.92726 Disc Loss: 0.090436 Q Losses: [0.049246904, 2.0895271]\n",
      "epoch:0 batch_done:31 Gen Loss: 5.29984 Disc Loss: 0.078852 Q Losses: [0.040481813, 2.0303323]\n",
      "epoch:0 batch_done:32 Gen Loss: 15.6708 Disc Loss: 0.36041 Q Losses: [0.057274193, 2.006567]\n",
      "epoch:0 batch_done:33 Gen Loss: 17.3126 Disc Loss: 0.190539 Q Losses: [0.03557983, 2.034544]\n",
      "epoch:0 batch_done:34 Gen Loss: 11.3316 Disc Loss: 0.329281 Q Losses: [0.0467076, 2.0168695]\n",
      "epoch:0 batch_done:35 Gen Loss: 5.68405 Disc Loss: 0.176213 Q Losses: [0.031162001, 1.9576405]\n",
      "epoch:0 batch_done:36 Gen Loss: 29.9856 Disc Loss: 2.32264 Q Losses: [0.033506453, 1.9768459]\n",
      "epoch:0 batch_done:37 Gen Loss: 32.9004 Disc Loss: 1.5557 Q Losses: [0.053848621, 1.9691892]\n",
      "epoch:0 batch_done:38 Gen Loss: 23.4902 Disc Loss: 0.369274 Q Losses: [0.04366868, 1.995024]\n",
      "epoch:0 batch_done:39 Gen Loss: 5.6198 Disc Loss: 0.171787 Q Losses: [0.047041982, 1.9689837]\n",
      "epoch:0 batch_done:40 Gen Loss: 23.906 Disc Loss: 2.36209 Q Losses: [0.048835181, 1.9770954]\n",
      "epoch:0 batch_done:41 Gen Loss: 29.414 Disc Loss: 0.343264 Q Losses: [0.049656194, 1.8584509]\n",
      "epoch:0 batch_done:42 Gen Loss: 26.313 Disc Loss: 0.243395 Q Losses: [0.045939371, 1.8738723]\n",
      "epoch:0 batch_done:43 Gen Loss: 13.7156 Disc Loss: 0.153849 Q Losses: [0.04726553, 1.8503759]\n",
      "epoch:0 batch_done:44 Gen Loss: 12.9791 Disc Loss: 0.324021 Q Losses: [0.046805099, 1.810775]\n",
      "epoch:0 batch_done:45 Gen Loss: 9.91749 Disc Loss: 0.219433 Q Losses: [0.045905504, 1.8230721]\n",
      "epoch:0 batch_done:46 Gen Loss: 9.08377 Disc Loss: 0.144168 Q Losses: [0.052112367, 1.7935263]\n",
      "epoch:0 batch_done:47 Gen Loss: 8.81994 Disc Loss: 0.0670087 Q Losses: [0.049816102, 1.736531]\n",
      "epoch:0 batch_done:48 Gen Loss: 7.34924 Disc Loss: 0.111869 Q Losses: [0.05244026, 1.6360621]\n",
      "epoch:0 batch_done:49 Gen Loss: 9.84142 Disc Loss: 0.229789 Q Losses: [0.060999386, 1.6111517]\n",
      "epoch:0 batch_done:50 Gen Loss: 10.9916 Disc Loss: 0.210858 Q Losses: [0.047029756, 1.6471615]\n",
      "epoch:0 batch_done:51 Gen Loss: 10.9292 Disc Loss: 0.202487 Q Losses: [0.040104069, 1.6198092]\n",
      "epoch:0 batch_done:52 Gen Loss: 6.31119 Disc Loss: 0.244826 Q Losses: [0.046350151, 1.6136621]\n",
      "epoch:0 batch_done:53 Gen Loss: 20.6219 Disc Loss: 0.467238 Q Losses: [0.041812874, 1.6412082]\n",
      "epoch:0 batch_done:54 Gen Loss: 19.6297 Disc Loss: 0.459974 Q Losses: [0.047886565, 1.5485398]\n",
      "epoch:0 batch_done:55 Gen Loss: 10.299 Disc Loss: 0.0241174 Q Losses: [0.04530444, 1.5713668]\n",
      "epoch:0 batch_done:56 Gen Loss: 31.9542 Disc Loss: 0.867483 Q Losses: [0.051947959, 1.5420208]\n",
      "epoch:0 batch_done:57 Gen Loss: 30.532 Disc Loss: 2.48734 Q Losses: [0.049531199, 1.4469681]\n",
      "epoch:0 batch_done:58 Gen Loss: 21.7344 Disc Loss: 0.0423298 Q Losses: [0.050643414, 1.5087633]\n",
      "epoch:0 batch_done:59 Gen Loss: 8.28595 Disc Loss: 0.0194447 Q Losses: [0.05308013, 1.4663074]\n",
      "epoch:0 batch_done:60 Gen Loss: 24.5826 Disc Loss: 0.522072 Q Losses: [0.047176853, 1.4255608]\n",
      "epoch:0 batch_done:61 Gen Loss: 27.9829 Disc Loss: 0.171055 Q Losses: [0.04897948, 1.3846014]\n",
      "epoch:0 batch_done:62 Gen Loss: 21.0747 Disc Loss: 0.306143 Q Losses: [0.062136736, 1.3476461]\n",
      "epoch:0 batch_done:63 Gen Loss: 6.86415 Disc Loss: 0.150085 Q Losses: [0.043540083, 1.3495703]\n",
      "epoch:0 batch_done:64 Gen Loss: 7.95856 Disc Loss: 0.257766 Q Losses: [0.04369849, 1.4140029]\n",
      "epoch:0 batch_done:65 Gen Loss: 7.6269 Disc Loss: 0.192782 Q Losses: [0.068716638, 1.2594326]\n",
      "epoch:0 batch_done:66 Gen Loss: 5.90426 Disc Loss: 0.115913 Q Losses: [0.058496997, 1.3538096]\n",
      "epoch:0 batch_done:67 Gen Loss: 8.80284 Disc Loss: 0.388565 Q Losses: [0.0554245, 1.2591343]\n",
      "epoch:0 batch_done:68 Gen Loss: 8.8596 Disc Loss: 0.0666915 Q Losses: [0.049186543, 1.3157618]\n",
      "epoch:0 batch_done:69 Gen Loss: 6.8614 Disc Loss: 0.184279 Q Losses: [0.047580868, 1.2414824]\n",
      "epoch:0 batch_done:70 Gen Loss: 26.6343 Disc Loss: 0.460579 Q Losses: [0.056646917, 1.2672201]\n",
      "epoch:0 batch_done:71 Gen Loss: 15.9382 Disc Loss: 2.52997 Q Losses: [0.038178638, 1.2754792]\n",
      "epoch:0 batch_done:72 Gen Loss: 3.77418 Disc Loss: 0.00823487 Q Losses: [0.062511809, 1.3112336]\n",
      "epoch:0 batch_done:73 Gen Loss: 47.7578 Disc Loss: 3.41565 Q Losses: [0.061586097, 1.2471048]\n",
      "epoch:0 batch_done:74 Gen Loss: 43.3351 Disc Loss: 11.569 Q Losses: [0.04160234, 1.1443833]\n",
      "epoch:0 batch_done:75 Gen Loss: 26.3807 Disc Loss: 1.94044 Q Losses: [0.058479972, 1.1849344]\n",
      "epoch:0 batch_done:76 Gen Loss: 11.6729 Disc Loss: 0.0339962 Q Losses: [0.063344911, 1.2164541]\n",
      "epoch:0 batch_done:77 Gen Loss: 2.78362 Disc Loss: 0.0279369 Q Losses: [0.062687337, 1.1464047]\n",
      "epoch:0 batch_done:78 Gen Loss: 33.7523 Disc Loss: 2.85767 Q Losses: [0.053254925, 1.1172292]\n",
      "epoch:0 batch_done:79 Gen Loss: 41.5593 Disc Loss: 0.686541 Q Losses: [0.057971343, 1.1008506]\n",
      "epoch:0 batch_done:80 Gen Loss: 34.3738 Disc Loss: 0.87223 Q Losses: [0.05425157, 1.1191339]\n",
      "epoch:0 batch_done:81 Gen Loss: 14.1585 Disc Loss: 0.342497 Q Losses: [0.051709764, 1.1638918]\n",
      "epoch:0 batch_done:82 Gen Loss: 28.9174 Disc Loss: 0.916126 Q Losses: [0.058251139, 1.0836142]\n",
      "epoch:0 batch_done:83 Gen Loss: 24.3333 Disc Loss: 1.34434 Q Losses: [0.059597604, 1.0754781]\n",
      "epoch:0 batch_done:84 Gen Loss: 8.27643 Disc Loss: 0.774339 Q Losses: [0.040934701, 1.0948457]\n",
      "epoch:0 batch_done:85 Gen Loss: 21.0002 Disc Loss: 0.818125 Q Losses: [0.047856614, 1.057561]\n",
      "epoch:0 batch_done:86 Gen Loss: 20.9348 Disc Loss: 0.586644 Q Losses: [0.055723906, 1.0274413]\n",
      "epoch:0 batch_done:87 Gen Loss: 13.7383 Disc Loss: 0.155986 Q Losses: [0.061559062, 1.010401]\n",
      "epoch:0 batch_done:88 Gen Loss: 5.46549 Disc Loss: 0.1218 Q Losses: [0.046541072, 0.98658448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:89 Gen Loss: 37.6401 Disc Loss: 2.23217 Q Losses: [0.04707266, 0.98472559]\n",
      "epoch:0 batch_done:90 Gen Loss: 42.3305 Disc Loss: 1.74594 Q Losses: [0.039180789, 1.0295266]\n",
      "epoch:0 batch_done:91 Gen Loss: 35.7975 Disc Loss: 0.846649 Q Losses: [0.0448916, 0.99979115]\n",
      "epoch:0 batch_done:92 Gen Loss: 23.7254 Disc Loss: 0.241562 Q Losses: [0.065734081, 0.99838662]\n",
      "epoch:0 batch_done:93 Gen Loss: 8.10042 Disc Loss: 0.0290089 Q Losses: [0.061919324, 0.9476403]\n",
      "epoch:0 batch_done:94 Gen Loss: 38.8562 Disc Loss: 3.10884 Q Losses: [0.063290648, 0.95101571]\n",
      "epoch:0 batch_done:95 Gen Loss: 42.266 Disc Loss: 2.27878 Q Losses: [0.053678751, 0.92767]\n",
      "epoch:0 batch_done:96 Gen Loss: 32.8427 Disc Loss: 2.11764 Q Losses: [0.064074911, 0.9326942]\n",
      "epoch:0 batch_done:97 Gen Loss: 21.1186 Disc Loss: 1.12305 Q Losses: [0.044609159, 0.87461638]\n",
      "epoch:0 batch_done:98 Gen Loss: 8.64448 Disc Loss: 0.282854 Q Losses: [0.055580292, 0.91853905]\n",
      "epoch:0 batch_done:99 Gen Loss: 6.57475 Disc Loss: 0.115622 Q Losses: [0.056076247, 0.86924136]\n",
      "epoch:0 batch_done:100 Gen Loss: 19.9581 Disc Loss: 0.420637 Q Losses: [0.042396665, 0.8550384]\n",
      "epoch:0 batch_done:101 Gen Loss: 21.8228 Disc Loss: 0.366721 Q Losses: [0.062441245, 0.82812142]\n",
      "epoch:0 batch_done:102 Gen Loss: 16.3322 Disc Loss: 0.796609 Q Losses: [0.051682778, 0.81581819]\n",
      "epoch:0 batch_done:103 Gen Loss: 6.86887 Disc Loss: 0.795609 Q Losses: [0.064513326, 0.80917788]\n",
      "epoch:0 batch_done:104 Gen Loss: 32.6363 Disc Loss: 1.75941 Q Losses: [0.05498869, 0.77292258]\n",
      "epoch:0 batch_done:105 Gen Loss: 37.4635 Disc Loss: 1.32628 Q Losses: [0.040718772, 0.81132472]\n",
      "epoch:0 batch_done:106 Gen Loss: 28.0361 Disc Loss: 2.9265 Q Losses: [0.062908083, 0.8457402]\n",
      "epoch:0 batch_done:107 Gen Loss: 12.3584 Disc Loss: 0.615701 Q Losses: [0.04149295, 0.81053334]\n",
      "epoch:0 batch_done:108 Gen Loss: 29.2387 Disc Loss: 1.12182 Q Losses: [0.055665333, 0.81454349]\n",
      "epoch:0 batch_done:109 Gen Loss: 30.4828 Disc Loss: 1.00326 Q Losses: [0.046639815, 0.78864479]\n",
      "epoch:0 batch_done:110 Gen Loss: 22.6395 Disc Loss: 1.22277 Q Losses: [0.057518803, 0.75988019]\n",
      "epoch:0 batch_done:111 Gen Loss: 13.6868 Disc Loss: 0.303163 Q Losses: [0.052424885, 0.74702406]\n",
      "epoch:0 batch_done:112 Gen Loss: 5.61126 Disc Loss: 0.0249667 Q Losses: [0.047533132, 0.77048707]\n",
      "epoch:0 batch_done:113 Gen Loss: 28.7834 Disc Loss: 0.817698 Q Losses: [0.051359668, 0.74513578]\n",
      "epoch:0 batch_done:114 Gen Loss: 32.5237 Disc Loss: 1.21857 Q Losses: [0.068749547, 0.78272033]\n",
      "epoch:0 batch_done:115 Gen Loss: 26.0655 Disc Loss: 1.22748 Q Losses: [0.048316836, 0.70627302]\n",
      "epoch:0 batch_done:116 Gen Loss: 16.6028 Disc Loss: 0.292708 Q Losses: [0.046320248, 0.72540736]\n",
      "epoch:0 batch_done:117 Gen Loss: 8.17205 Disc Loss: 0.00387455 Q Losses: [0.051551759, 0.69840479]\n",
      "epoch:0 batch_done:118 Gen Loss: 6.08075 Disc Loss: 0.0647509 Q Losses: [0.043661948, 0.69738859]\n",
      "epoch:0 batch_done:119 Gen Loss: 15.2292 Disc Loss: 0.269339 Q Losses: [0.042118318, 0.71403039]\n",
      "epoch:0 batch_done:120 Gen Loss: 14.9698 Disc Loss: 0.0777767 Q Losses: [0.057226311, 0.712659]\n",
      "epoch:0 batch_done:121 Gen Loss: 9.41418 Disc Loss: 0.213398 Q Losses: [0.049677871, 0.65239704]\n",
      "epoch:0 batch_done:122 Gen Loss: 15.912 Disc Loss: 0.452555 Q Losses: [0.052754618, 0.66676605]\n",
      "epoch:0 batch_done:123 Gen Loss: 10.6118 Disc Loss: 0.77301 Q Losses: [0.052971229, 0.66738689]\n",
      "epoch:0 batch_done:124 Gen Loss: 9.14015 Disc Loss: 0.200648 Q Losses: [0.042838782, 0.64395136]\n",
      "epoch:0 batch_done:125 Gen Loss: 10.6288 Disc Loss: 0.335453 Q Losses: [0.040027183, 0.63621217]\n",
      "epoch:0 batch_done:126 Gen Loss: 8.88748 Disc Loss: 0.0962057 Q Losses: [0.054093085, 0.64149773]\n",
      "epoch:0 batch_done:127 Gen Loss: 6.62322 Disc Loss: 0.0666021 Q Losses: [0.032911167, 0.63856608]\n",
      "epoch:0 batch_done:128 Gen Loss: 12.1603 Disc Loss: 0.191012 Q Losses: [0.038967177, 0.61792189]\n",
      "epoch:0 batch_done:129 Gen Loss: 11.142 Disc Loss: 0.262101 Q Losses: [0.054476582, 0.59888411]\n",
      "epoch:0 batch_done:130 Gen Loss: 7.75351 Disc Loss: 0.0659659 Q Losses: [0.038565073, 0.58722556]\n",
      "epoch:0 batch_done:131 Gen Loss: 6.99477 Disc Loss: 0.150134 Q Losses: [0.047786184, 0.59158516]\n",
      "epoch:0 batch_done:132 Gen Loss: 14.9954 Disc Loss: 0.209756 Q Losses: [0.049813811, 0.58555198]\n",
      "epoch:0 batch_done:133 Gen Loss: 14.4787 Disc Loss: 0.322642 Q Losses: [0.051297188, 0.57961595]\n",
      "epoch:0 batch_done:134 Gen Loss: 9.33015 Disc Loss: 0.256929 Q Losses: [0.040626045, 0.60043371]\n",
      "epoch:0 batch_done:135 Gen Loss: 4.89842 Disc Loss: 0.0479507 Q Losses: [0.043269031, 0.5673272]\n",
      "epoch:0 batch_done:136 Gen Loss: 33.3396 Disc Loss: 0.891975 Q Losses: [0.042596735, 0.5726791]\n",
      "epoch:0 batch_done:137 Gen Loss: 34.7679 Disc Loss: 1.75956 Q Losses: [0.042480238, 0.55589998]\n",
      "epoch:0 batch_done:138 Gen Loss: 27.3189 Disc Loss: 1.0489 Q Losses: [0.062403448, 0.53721726]\n",
      "epoch:0 batch_done:139 Gen Loss: 17.8145 Disc Loss: 0.0501757 Q Losses: [0.032377735, 0.58421993]\n",
      "epoch:0 batch_done:140 Gen Loss: 7.1376 Disc Loss: 0.00630224 Q Losses: [0.051939629, 0.54317886]\n",
      "epoch:0 batch_done:141 Gen Loss: 16.462 Disc Loss: 0.285118 Q Losses: [0.037782665, 0.5582428]\n",
      "epoch:0 batch_done:142 Gen Loss: 17.1241 Disc Loss: 0.22126 Q Losses: [0.04479263, 0.5433352]\n",
      "epoch:0 batch_done:143 Gen Loss: 12.4677 Disc Loss: 0.173648 Q Losses: [0.051747292, 0.56270564]\n",
      "epoch:0 batch_done:144 Gen Loss: 5.48733 Disc Loss: 0.106626 Q Losses: [0.057680793, 0.53194177]\n",
      "epoch:0 batch_done:145 Gen Loss: 22.6056 Disc Loss: 0.63364 Q Losses: [0.038100623, 0.51834869]\n",
      "epoch:0 batch_done:146 Gen Loss: 15.3184 Disc Loss: 2.15419 Q Losses: [0.031353027, 0.54452765]\n",
      "epoch:0 batch_done:147 Gen Loss: 8.32911 Disc Loss: 0.229463 Q Losses: [0.036663622, 0.51190042]\n",
      "epoch:0 batch_done:148 Gen Loss: 3.30376 Disc Loss: 0.0249899 Q Losses: [0.053944409, 0.50738204]\n",
      "epoch:0 batch_done:149 Gen Loss: 37.8201 Disc Loss: 1.20841 Q Losses: [0.042342745, 0.53851938]\n",
      "epoch:0 batch_done:150 Gen Loss: 32.7644 Disc Loss: 5.4701 Q Losses: [0.041876804, 0.51362467]\n",
      "epoch:0 batch_done:151 Gen Loss: 19.3282 Disc Loss: 2.15659 Q Losses: [0.044377893, 0.510113]\n",
      "epoch:0 batch_done:152 Gen Loss: 10.1288 Disc Loss: 0.0211902 Q Losses: [0.042468552, 0.5029251]\n",
      "epoch:0 batch_done:153 Gen Loss: 2.34103 Disc Loss: 0.00972714 Q Losses: [0.053967945, 0.49332082]\n",
      "epoch:0 batch_done:154 Gen Loss: 32.4196 Disc Loss: 2.01549 Q Losses: [0.038802654, 0.47835904]\n",
      "epoch:0 batch_done:155 Gen Loss: 32.9636 Disc Loss: 1.62692 Q Losses: [0.032135714, 0.47723675]\n",
      "epoch:0 batch_done:156 Gen Loss: 21.5311 Disc Loss: 0.884922 Q Losses: [0.0528633, 0.47385389]\n",
      "epoch:0 batch_done:157 Gen Loss: 9.05883 Disc Loss: 0.177287 Q Losses: [0.03967721, 0.48318264]\n",
      "epoch:0 batch_done:158 Gen Loss: 2.37388 Disc Loss: 0.0121524 Q Losses: [0.050179608, 0.48781979]\n",
      "epoch:0 batch_done:159 Gen Loss: 17.4054 Disc Loss: 0.281618 Q Losses: [0.039393842, 0.47057411]\n",
      "epoch:0 batch_done:160 Gen Loss: 15.2311 Disc Loss: 0.644478 Q Losses: [0.048016317, 0.43938062]\n",
      "epoch:0 batch_done:161 Gen Loss: 8.98339 Disc Loss: 0.257626 Q Losses: [0.036414571, 0.45991063]\n",
      "epoch:0 batch_done:162 Gen Loss: 3.15519 Disc Loss: 0.0948666 Q Losses: [0.042837769, 0.42739302]\n",
      "epoch:0 batch_done:163 Gen Loss: 27.2295 Disc Loss: 0.752076 Q Losses: [0.03729843, 0.44393888]\n",
      "epoch:0 batch_done:164 Gen Loss: 26.1203 Disc Loss: 2.07069 Q Losses: [0.036591701, 0.42441225]\n",
      "epoch:0 batch_done:165 Gen Loss: 12.1668 Disc Loss: 1.77156 Q Losses: [0.050784923, 0.45615011]\n",
      "epoch:0 batch_done:166 Gen Loss: 2.71207 Disc Loss: 0.0134452 Q Losses: [0.031968851, 0.443764]\n",
      "epoch:0 batch_done:167 Gen Loss: 33.2408 Disc Loss: 1.36407 Q Losses: [0.031037297, 0.42816985]\n",
      "epoch:0 batch_done:168 Gen Loss: 35.1567 Disc Loss: 1.23041 Q Losses: [0.040351376, 0.41750979]\n",
      "epoch:0 batch_done:169 Gen Loss: 26.822 Disc Loss: 0.906596 Q Losses: [0.031347204, 0.40512007]\n",
      "epoch:0 batch_done:170 Gen Loss: 17.1576 Disc Loss: 0.0990783 Q Losses: [0.042678788, 0.42378885]\n",
      "epoch:0 batch_done:171 Gen Loss: 8.63522 Disc Loss: 0.061142 Q Losses: [0.023948327, 0.40515006]\n",
      "epoch:0 batch_done:172 Gen Loss: 3.94623 Disc Loss: 0.0178102 Q Losses: [0.040258601, 0.39514697]\n",
      "epoch:0 batch_done:173 Gen Loss: 24.715 Disc Loss: 0.659247 Q Losses: [0.0393649, 0.39516473]\n",
      "epoch:0 batch_done:174 Gen Loss: 26.6393 Disc Loss: 0.908377 Q Losses: [0.047387674, 0.41041607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch_done:175 Gen Loss: 22.5981 Disc Loss: 0.263855 Q Losses: [0.036903683, 0.38455388]\n",
      "epoch:0 batch_done:176 Gen Loss: 17.2474 Disc Loss: 0.0936861 Q Losses: [0.048709266, 0.38559812]\n",
      "epoch:0 batch_done:177 Gen Loss: 10.3501 Disc Loss: 0.0857817 Q Losses: [0.030798223, 0.38668928]\n",
      "epoch:0 batch_done:178 Gen Loss: 4.59664 Disc Loss: 0.0113896 Q Losses: [0.037000649, 0.38251594]\n",
      "epoch:0 batch_done:179 Gen Loss: 25.4686 Disc Loss: 0.819543 Q Losses: [0.036630422, 0.37490487]\n",
      "epoch:0 batch_done:180 Gen Loss: 27.9771 Disc Loss: 0.718237 Q Losses: [0.030131966, 0.38719541]\n",
      "epoch:0 batch_done:181 Gen Loss: 22.0718 Disc Loss: 0.758681 Q Losses: [0.032661304, 0.35737544]\n",
      "epoch:0 batch_done:182 Gen Loss: 11.4459 Disc Loss: 0.26377 Q Losses: [0.028575521, 0.39053723]\n",
      "epoch:0 batch_done:183 Gen Loss: 7.80466 Disc Loss: 0.431643 Q Losses: [0.02241639, 0.35371113]\n",
      "epoch:0 batch_done:184 Gen Loss: 33.4672 Disc Loss: 0.839464 Q Losses: [0.037517965, 0.37651777]\n",
      "epoch:0 batch_done:185 Gen Loss: 34.6414 Disc Loss: 2.06561 Q Losses: [0.028823696, 0.3377777]\n",
      "epoch:0 batch_done:186 Gen Loss: 25.1977 Disc Loss: 1.36883 Q Losses: [0.028994605, 0.36004132]\n",
      "epoch:0 batch_done:187 Gen Loss: 14.1255 Disc Loss: 0.069496 Q Losses: [0.035269395, 0.35668927]\n",
      "epoch:0 batch_done:188 Gen Loss: 3.79838 Disc Loss: 0.0333954 Q Losses: [0.035315797, 0.36220831]\n",
      "epoch:0 batch_done:189 Gen Loss: 29.9183 Disc Loss: 0.83009 Q Losses: [0.031060735, 0.34528473]\n",
      "epoch:0 batch_done:190 Gen Loss: 28.0399 Disc Loss: 0.820402 Q Losses: [0.026827056, 0.36620253]\n",
      "epoch:0 batch_done:191 Gen Loss: 11.6745 Disc Loss: 1.78205 Q Losses: [0.026362456, 0.37927192]\n",
      "epoch:0 batch_done:192 Gen Loss: 0.309765 Disc Loss: 0.45494 Q Losses: [0.038260482, 0.38567096]\n",
      "epoch:0 batch_done:193 Gen Loss: 7.73955 Disc Loss: 0.228884 Q Losses: [0.031232692, 0.3565219]\n",
      "epoch:0 batch_done:194 Gen Loss: 12.0773 Disc Loss: 0.0979747 Q Losses: [0.032288726, 0.35038835]\n",
      "epoch:0 batch_done:195 Gen Loss: 8.81351 Disc Loss: 0.14097 Q Losses: [0.032410603, 0.38016593]\n",
      "epoch:0 batch_done:196 Gen Loss: 4.44958 Disc Loss: 0.113402 Q Losses: [0.037661247, 0.36092865]\n",
      "epoch:0 batch_done:197 Gen Loss: 41.0842 Disc Loss: 2.93039 Q Losses: [0.034431994, 0.3495371]\n",
      "epoch:0 batch_done:198 Gen Loss: 34.0034 Disc Loss: 6.06124 Q Losses: [0.032459117, 0.33207554]\n",
      "epoch:0 batch_done:199 Gen Loss: 14.8424 Disc Loss: 3.1611 Q Losses: [0.034437105, 0.36744812]\n",
      "epoch:0 batch_done:200 Gen Loss: 0.399678 Disc Loss: 0.519697 Q Losses: [0.03630551, 0.35582078]\n",
      "epoch:0 batch_done:201 Gen Loss: 26.486 Disc Loss: 4.44822 Q Losses: [0.039833877, 0.33493498]\n",
      "epoch:0 batch_done:202 Gen Loss: 31.0347 Disc Loss: 1.7291 Q Losses: [0.021833664, 0.32428348]\n",
      "epoch:0 batch_done:203 Gen Loss: 26.1261 Disc Loss: 0.599301 Q Losses: [0.031628355, 0.33512792]\n",
      "epoch:0 batch_done:204 Gen Loss: 18.0624 Disc Loss: 0.554388 Q Losses: [0.026351433, 0.31507939]\n",
      "epoch:0 batch_done:205 Gen Loss: 10.2526 Disc Loss: 0.28758 Q Losses: [0.03224289, 0.32431415]\n",
      "epoch:0 batch_done:206 Gen Loss: 5.13579 Disc Loss: 0.096388 Q Losses: [0.03170605, 0.33724511]\n",
      "epoch:0 batch_done:207 Gen Loss: 4.53678 Disc Loss: 0.194292 Q Losses: [0.023212625, 0.30461881]\n",
      "epoch:1 batch_done:1 Gen Loss: 8.69071 Disc Loss: 0.153208 Q Losses: [0.03027603, 0.31935909]\n",
      "epoch:1 batch_done:2 Gen Loss: 9.89248 Disc Loss: 0.0270121 Q Losses: [0.032413013, 0.2954706]\n",
      "epoch:1 batch_done:3 Gen Loss: 7.698 Disc Loss: 0.254623 Q Losses: [0.024052037, 0.29167703]\n",
      "epoch:1 batch_done:4 Gen Loss: 5.85021 Disc Loss: 0.00930285 Q Losses: [0.027163019, 0.28950182]\n",
      "epoch:1 batch_done:5 Gen Loss: 4.6098 Disc Loss: 0.109103 Q Losses: [0.030285278, 0.29864454]\n",
      "epoch:1 batch_done:6 Gen Loss: 6.48441 Disc Loss: 0.1205 Q Losses: [0.032464229, 0.27331406]\n",
      "epoch:1 batch_done:7 Gen Loss: 6.40471 Disc Loss: 0.065223 Q Losses: [0.028372217, 0.26997393]\n",
      "epoch:1 batch_done:8 Gen Loss: 6.02559 Disc Loss: 0.0410016 Q Losses: [0.026551204, 0.27072135]\n",
      "epoch:1 batch_done:9 Gen Loss: 6.60467 Disc Loss: 0.0650739 Q Losses: [0.023209276, 0.28397971]\n",
      "epoch:1 batch_done:10 Gen Loss: 6.83459 Disc Loss: 0.0861466 Q Losses: [0.032381404, 0.26208204]\n",
      "epoch:1 batch_done:11 Gen Loss: 6.11125 Disc Loss: 0.475648 Q Losses: [0.019617546, 0.27143106]\n",
      "epoch:1 batch_done:12 Gen Loss: 15.5941 Disc Loss: 0.6189 Q Losses: [0.026940808, 0.2864807]\n",
      "epoch:1 batch_done:13 Gen Loss: 14.7017 Disc Loss: 0.79982 Q Losses: [0.024713609, 0.28421634]\n",
      "epoch:1 batch_done:14 Gen Loss: 8.06874 Disc Loss: 0.549201 Q Losses: [0.027919512, 0.30170709]\n",
      "epoch:1 batch_done:15 Gen Loss: 21.1583 Disc Loss: 0.738859 Q Losses: [0.025346447, 0.26832399]\n",
      "epoch:1 batch_done:16 Gen Loss: 23.3907 Disc Loss: 0.44011 Q Losses: [0.024484893, 0.29585755]\n",
      "epoch:1 batch_done:17 Gen Loss: 19.0268 Disc Loss: 0.29065 Q Losses: [0.023876622, 0.27817786]\n",
      "epoch:1 batch_done:18 Gen Loss: 11.1561 Disc Loss: 0.0397589 Q Losses: [0.015499272, 0.27246025]\n",
      "epoch:1 batch_done:19 Gen Loss: 18.9762 Disc Loss: 0.414118 Q Losses: [0.027098853, 0.27061707]\n",
      "epoch:1 batch_done:20 Gen Loss: 20.7576 Disc Loss: 0.0449134 Q Losses: [0.026463069, 0.25043041]\n",
      "epoch:1 batch_done:21 Gen Loss: 18.2572 Disc Loss: 0.24948 Q Losses: [0.032459691, 0.25872153]\n",
      "epoch:1 batch_done:22 Gen Loss: 12.6154 Disc Loss: 0.543362 Q Losses: [0.022591939, 0.28967595]\n",
      "epoch:1 batch_done:23 Gen Loss: 7.60144 Disc Loss: 0.0868591 Q Losses: [0.025728852, 0.25824663]\n",
      "epoch:1 batch_done:24 Gen Loss: 4.12184 Disc Loss: 0.0315652 Q Losses: [0.031069627, 0.25487286]\n",
      "epoch:1 batch_done:25 Gen Loss: 19.0198 Disc Loss: 0.354466 Q Losses: [0.019439889, 0.24501146]\n",
      "epoch:1 batch_done:26 Gen Loss: 23.6013 Disc Loss: 0.113009 Q Losses: [0.022902885, 0.2839728]\n",
      "epoch:1 batch_done:27 Gen Loss: 22.2051 Disc Loss: 0.349119 Q Losses: [0.017517051, 0.25484782]\n",
      "epoch:1 batch_done:28 Gen Loss: 18.1807 Disc Loss: 0.128053 Q Losses: [0.028778676, 0.27209309]\n",
      "epoch:1 batch_done:29 Gen Loss: 12.8442 Disc Loss: 0.00947872 Q Losses: [0.024962317, 0.26455325]\n",
      "epoch:1 batch_done:30 Gen Loss: 5.62238 Disc Loss: 0.0178752 Q Losses: [0.027258452, 0.26994103]\n",
      "epoch:1 batch_done:31 Gen Loss: 38.1921 Disc Loss: 1.25636 Q Losses: [0.036117829, 0.26800168]\n",
      "epoch:1 batch_done:32 Gen Loss: 42.7365 Disc Loss: 1.93332 Q Losses: [0.026610056, 0.27259639]\n",
      "epoch:1 batch_done:33 Gen Loss: 37.8818 Disc Loss: 0.686809 Q Losses: [0.0253741, 0.25586694]\n",
      "epoch:1 batch_done:34 Gen Loss: 31.0026 Disc Loss: 0.505116 Q Losses: [0.032817028, 0.25802994]\n",
      "epoch:1 batch_done:35 Gen Loss: 24.7544 Disc Loss: 0.0394 Q Losses: [0.027781665, 0.23428127]\n",
      "epoch:1 batch_done:36 Gen Loss: 18.7228 Disc Loss: 0.00585437 Q Losses: [0.022597129, 0.22574115]\n",
      "epoch:1 batch_done:37 Gen Loss: 12.525 Disc Loss: 0.00312613 Q Losses: [0.026216408, 0.23284166]\n",
      "epoch:1 batch_done:38 Gen Loss: 6.44462 Disc Loss: 0.048671 Q Losses: [0.018721284, 0.23266993]\n",
      "epoch:1 batch_done:39 Gen Loss: 9.56534 Disc Loss: 0.13692 Q Losses: [0.024706006, 0.24940982]\n",
      "epoch:1 batch_done:40 Gen Loss: 9.68714 Disc Loss: 0.025602 Q Losses: [0.027208313, 0.21594018]\n",
      "epoch:1 batch_done:41 Gen Loss: 7.03516 Disc Loss: 0.0179726 Q Losses: [0.021303352, 0.22405055]\n",
      "epoch:1 batch_done:42 Gen Loss: 17.0268 Disc Loss: 0.285462 Q Losses: [0.020287797, 0.22715315]\n",
      "epoch:1 batch_done:43 Gen Loss: 17.8904 Disc Loss: 0.115793 Q Losses: [0.023067217, 0.22876227]\n",
      "epoch:1 batch_done:44 Gen Loss: 13.3287 Disc Loss: 0.0563269 Q Losses: [0.019219028, 0.2162635]\n",
      "epoch:1 batch_done:45 Gen Loss: 5.50772 Disc Loss: 0.282685 Q Losses: [0.034685552, 0.2188316]\n",
      "epoch:1 batch_done:46 Gen Loss: 41.2427 Disc Loss: 1.14553 Q Losses: [0.015508114, 0.26218861]\n",
      "epoch:1 batch_done:47 Gen Loss: 34.319 Disc Loss: 3.47168 Q Losses: [0.022387128, 0.26581025]\n",
      "epoch:1 batch_done:48 Gen Loss: 11.3761 Disc Loss: 0.779906 Q Losses: [0.026937265, 0.25168908]\n",
      "epoch:1 batch_done:49 Gen Loss: 17.4007 Disc Loss: 0.467137 Q Losses: [0.030846024, 0.27247143]\n",
      "epoch:1 batch_done:50 Gen Loss: 17.8751 Disc Loss: 0.42588 Q Losses: [0.022455685, 0.27713335]\n",
      "epoch:1 batch_done:51 Gen Loss: 16.0643 Disc Loss: 0.369773 Q Losses: [0.020926276, 0.25367773]\n",
      "epoch:1 batch_done:52 Gen Loss: 12.5097 Disc Loss: 0.130023 Q Losses: [0.036540769, 0.24124554]\n",
      "epoch:1 batch_done:53 Gen Loss: 7.85459 Disc Loss: 0.00347656 Q Losses: [0.032693911, 0.21338096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:54 Gen Loss: 4.83376 Disc Loss: 0.0192062 Q Losses: [0.02659083, 0.22239712]\n",
      "epoch:1 batch_done:55 Gen Loss: 17.8201 Disc Loss: 0.279992 Q Losses: [0.023991738, 0.20646657]\n",
      "epoch:1 batch_done:56 Gen Loss: 17.1679 Disc Loss: 0.344067 Q Losses: [0.01993876, 0.20467721]\n",
      "epoch:1 batch_done:57 Gen Loss: 11.6715 Disc Loss: 0.250682 Q Losses: [0.023655117, 0.21466273]\n",
      "epoch:1 batch_done:58 Gen Loss: 6.39291 Disc Loss: 0.0188015 Q Losses: [0.020906715, 0.20451525]\n",
      "epoch:1 batch_done:59 Gen Loss: 5.43899 Disc Loss: 0.0653448 Q Losses: [0.022125553, 0.23193613]\n",
      "epoch:1 batch_done:60 Gen Loss: 8.14117 Disc Loss: 0.111848 Q Losses: [0.021342019, 0.20347723]\n",
      "epoch:1 batch_done:61 Gen Loss: 7.46284 Disc Loss: 0.0333681 Q Losses: [0.018586241, 0.18574303]\n",
      "epoch:1 batch_done:62 Gen Loss: 5.65004 Disc Loss: 0.0739051 Q Losses: [0.025950348, 0.1952962]\n",
      "epoch:1 batch_done:63 Gen Loss: 5.32116 Disc Loss: 0.258503 Q Losses: [0.029974639, 0.18559192]\n",
      "epoch:1 batch_done:64 Gen Loss: 7.33843 Disc Loss: 0.158568 Q Losses: [0.018415641, 0.18820056]\n",
      "epoch:1 batch_done:65 Gen Loss: 5.631 Disc Loss: 0.138746 Q Losses: [0.020861877, 0.19289967]\n",
      "epoch:1 batch_done:66 Gen Loss: 6.20075 Disc Loss: 0.131128 Q Losses: [0.025293726, 0.20897776]\n",
      "epoch:1 batch_done:67 Gen Loss: 5.75448 Disc Loss: 0.183012 Q Losses: [0.020270029, 0.19040659]\n",
      "epoch:1 batch_done:68 Gen Loss: 7.46445 Disc Loss: 0.0601361 Q Losses: [0.016286073, 0.1887444]\n",
      "epoch:1 batch_done:69 Gen Loss: 6.34772 Disc Loss: 0.0818224 Q Losses: [0.022118099, 0.19124781]\n",
      "epoch:1 batch_done:70 Gen Loss: 6.38221 Disc Loss: 0.0815975 Q Losses: [0.024222065, 0.2184552]\n",
      "epoch:1 batch_done:71 Gen Loss: 6.67353 Disc Loss: 0.0423112 Q Losses: [0.01436954, 0.18187757]\n",
      "epoch:1 batch_done:72 Gen Loss: 5.80722 Disc Loss: 0.133039 Q Losses: [0.016463738, 0.19688261]\n",
      "epoch:1 batch_done:73 Gen Loss: 6.89197 Disc Loss: 0.159091 Q Losses: [0.021055656, 0.20848221]\n",
      "epoch:1 batch_done:74 Gen Loss: 6.00168 Disc Loss: 0.0914827 Q Losses: [0.023047252, 0.17979729]\n",
      "epoch:1 batch_done:75 Gen Loss: 6.0249 Disc Loss: 0.045356 Q Losses: [0.013781254, 0.17075688]\n",
      "epoch:1 batch_done:76 Gen Loss: 5.72778 Disc Loss: 0.0667678 Q Losses: [0.019472133, 0.17632005]\n",
      "epoch:1 batch_done:77 Gen Loss: 6.45279 Disc Loss: 0.0603679 Q Losses: [0.017478991, 0.16693768]\n",
      "epoch:1 batch_done:78 Gen Loss: 5.53808 Disc Loss: 0.106083 Q Losses: [0.020054311, 0.18273003]\n",
      "epoch:1 batch_done:79 Gen Loss: 5.65202 Disc Loss: 0.0663897 Q Losses: [0.019073442, 0.17936045]\n",
      "epoch:1 batch_done:80 Gen Loss: 5.81918 Disc Loss: 0.0404393 Q Losses: [0.013898246, 0.17601247]\n",
      "epoch:1 batch_done:81 Gen Loss: 5.74835 Disc Loss: 0.0334526 Q Losses: [0.015226132, 0.17488261]\n",
      "epoch:1 batch_done:82 Gen Loss: 5.79098 Disc Loss: 0.0230805 Q Losses: [0.018183099, 0.16775212]\n",
      "epoch:1 batch_done:83 Gen Loss: 5.74108 Disc Loss: 0.0393499 Q Losses: [0.026880149, 0.16513188]\n",
      "epoch:1 batch_done:84 Gen Loss: 6.17207 Disc Loss: 0.0269859 Q Losses: [0.014860967, 0.15952384]\n",
      "epoch:1 batch_done:85 Gen Loss: 6.02182 Disc Loss: 0.062326 Q Losses: [0.018138207, 0.16705365]\n",
      "epoch:1 batch_done:86 Gen Loss: 5.42859 Disc Loss: 0.0664638 Q Losses: [0.01345192, 0.17990255]\n",
      "epoch:1 batch_done:87 Gen Loss: 6.39993 Disc Loss: 0.0889021 Q Losses: [0.018379368, 0.16278857]\n",
      "epoch:1 batch_done:88 Gen Loss: 5.9724 Disc Loss: 0.0720413 Q Losses: [0.019690154, 0.16733763]\n",
      "epoch:1 batch_done:89 Gen Loss: 8.40813 Disc Loss: 0.135666 Q Losses: [0.020534746, 0.17711553]\n",
      "epoch:1 batch_done:90 Gen Loss: 8.99167 Disc Loss: 0.0122573 Q Losses: [0.016877437, 0.16607477]\n",
      "epoch:1 batch_done:91 Gen Loss: 6.62864 Disc Loss: 0.0756674 Q Losses: [0.016172908, 0.17597446]\n",
      "epoch:1 batch_done:92 Gen Loss: 5.03517 Disc Loss: 0.027486 Q Losses: [0.021081448, 0.16650677]\n",
      "epoch:1 batch_done:93 Gen Loss: 4.89355 Disc Loss: 0.0483588 Q Losses: [0.022348627, 0.17166564]\n",
      "epoch:1 batch_done:94 Gen Loss: 5.47651 Disc Loss: 0.0166277 Q Losses: [0.019757945, 0.16341206]\n",
      "epoch:1 batch_done:95 Gen Loss: 43.3371 Disc Loss: 1.3218 Q Losses: [0.016073775, 0.16989818]\n",
      "epoch:1 batch_done:96 Gen Loss: 38.8377 Disc Loss: 6.45462 Q Losses: [0.02040104, 0.16222852]\n",
      "epoch:1 batch_done:97 Gen Loss: 32.6159 Disc Loss: 0.144572 Q Losses: [0.01646233, 0.16808927]\n",
      "epoch:1 batch_done:98 Gen Loss: 25.4162 Disc Loss: 0.000637677 Q Losses: [0.022388758, 0.16310772]\n",
      "epoch:1 batch_done:99 Gen Loss: 15.276 Disc Loss: 0.00381237 Q Losses: [0.02525009, 0.18275654]\n",
      "epoch:1 batch_done:100 Gen Loss: 10.1754 Disc Loss: 9.71705e-05 Q Losses: [0.017003627, 0.18209065]\n",
      "epoch:1 batch_done:101 Gen Loss: 5.95763 Disc Loss: 0.00213144 Q Losses: [0.035788693, 0.17432204]\n",
      "epoch:1 batch_done:102 Gen Loss: 6.03298 Disc Loss: 0.00329144 Q Losses: [0.015774254, 0.18759829]\n",
      "epoch:1 batch_done:103 Gen Loss: 20.214 Disc Loss: 0.202178 Q Losses: [0.017492622, 0.1572167]\n",
      "epoch:1 batch_done:104 Gen Loss: 21.1226 Disc Loss: 0.0412047 Q Losses: [0.021259043, 0.16598457]\n",
      "epoch:1 batch_done:105 Gen Loss: 15.9805 Disc Loss: 0.0503649 Q Losses: [0.014610451, 0.15307969]\n",
      "epoch:1 batch_done:106 Gen Loss: 7.86766 Disc Loss: 0.160339 Q Losses: [0.023354433, 0.16125333]\n",
      "epoch:1 batch_done:107 Gen Loss: 49.4795 Disc Loss: 1.50916 Q Losses: [0.017642494, 0.16852152]\n",
      "epoch:1 batch_done:108 Gen Loss: 44.931 Disc Loss: 6.0337 Q Losses: [0.018596584, 0.17140353]\n",
      "epoch:1 batch_done:109 Gen Loss: 28.7756 Disc Loss: 1.5414 Q Losses: [0.028853133, 0.16529553]\n",
      "epoch:1 batch_done:110 Gen Loss: 16.1851 Disc Loss: 0.204759 Q Losses: [0.018739352, 0.15991509]\n",
      "epoch:1 batch_done:111 Gen Loss: 9.08526 Disc Loss: 8.74006e-05 Q Losses: [0.018731806, 0.16252021]\n",
      "epoch:1 batch_done:112 Gen Loss: 4.45146 Disc Loss: 0.00266909 Q Losses: [0.026474021, 0.15165058]\n",
      "epoch:1 batch_done:113 Gen Loss: 6.56399 Disc Loss: 0.0844374 Q Losses: [0.01889701, 0.16154462]\n",
      "epoch:1 batch_done:114 Gen Loss: 7.12634 Disc Loss: 0.0201863 Q Losses: [0.012891906, 0.20013094]\n",
      "epoch:1 batch_done:115 Gen Loss: 6.6964 Disc Loss: 0.0116783 Q Losses: [0.014926787, 0.16132888]\n",
      "epoch:1 batch_done:116 Gen Loss: 6.31764 Disc Loss: 0.062696 Q Losses: [0.016710352, 0.15560371]\n",
      "epoch:1 batch_done:117 Gen Loss: 7.55172 Disc Loss: 0.0555787 Q Losses: [0.025520412, 0.1444854]\n",
      "epoch:1 batch_done:118 Gen Loss: 6.93569 Disc Loss: 0.0410888 Q Losses: [0.01669671, 0.1560096]\n",
      "epoch:1 batch_done:119 Gen Loss: 7.03927 Disc Loss: 0.0465335 Q Losses: [0.015662074, 0.14759582]\n",
      "epoch:1 batch_done:120 Gen Loss: 7.83337 Disc Loss: 0.217983 Q Losses: [0.010705305, 0.13677087]\n",
      "epoch:1 batch_done:121 Gen Loss: 7.03487 Disc Loss: 0.231174 Q Losses: [0.017703317, 0.138304]\n",
      "epoch:1 batch_done:122 Gen Loss: 10.479 Disc Loss: 0.328021 Q Losses: [0.022625851, 0.15815312]\n",
      "epoch:1 batch_done:123 Gen Loss: 8.73635 Disc Loss: 0.126393 Q Losses: [0.015719946, 0.15410219]\n",
      "epoch:1 batch_done:124 Gen Loss: 7.047 Disc Loss: 0.265187 Q Losses: [0.019111887, 0.13966414]\n",
      "epoch:1 batch_done:125 Gen Loss: 9.98939 Disc Loss: 0.197085 Q Losses: [0.018602699, 0.15525174]\n",
      "epoch:1 batch_done:126 Gen Loss: 6.54782 Disc Loss: 0.661668 Q Losses: [0.02284348, 0.13882101]\n",
      "epoch:1 batch_done:127 Gen Loss: 8.00364 Disc Loss: 0.271733 Q Losses: [0.017039135, 0.13697445]\n",
      "epoch:1 batch_done:128 Gen Loss: 6.98716 Disc Loss: 0.0986126 Q Losses: [0.017230827, 0.15028274]\n",
      "epoch:1 batch_done:129 Gen Loss: 7.30412 Disc Loss: 0.0778812 Q Losses: [0.01466589, 0.13633174]\n",
      "epoch:1 batch_done:130 Gen Loss: 8.54833 Disc Loss: 0.0469434 Q Losses: [0.015341761, 0.14281219]\n",
      "epoch:1 batch_done:131 Gen Loss: 5.14071 Disc Loss: 0.0529052 Q Losses: [0.016013078, 0.14656658]\n",
      "epoch:1 batch_done:132 Gen Loss: 18.0462 Disc Loss: 0.31521 Q Losses: [0.014494481, 0.13404778]\n",
      "epoch:1 batch_done:133 Gen Loss: 17.7367 Disc Loss: 0.471555 Q Losses: [0.013993523, 0.13813645]\n",
      "epoch:1 batch_done:134 Gen Loss: 16.211 Disc Loss: 0.173282 Q Losses: [0.020082926, 0.12911476]\n",
      "epoch:1 batch_done:135 Gen Loss: 11.8337 Disc Loss: 0.0231425 Q Losses: [0.018947084, 0.12889478]\n",
      "epoch:1 batch_done:136 Gen Loss: 8.17294 Disc Loss: 0.0231452 Q Losses: [0.012103089, 0.15428197]\n",
      "epoch:1 batch_done:137 Gen Loss: 10.161 Disc Loss: 0.00216211 Q Losses: [0.014698992, 0.12294073]\n",
      "epoch:1 batch_done:138 Gen Loss: 5.82228 Disc Loss: 0.0352108 Q Losses: [0.017378092, 0.12559389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 batch_done:139 Gen Loss: 6.83508 Disc Loss: 0.00694096 Q Losses: [0.012174679, 0.13742769]\n",
      "epoch:1 batch_done:140 Gen Loss: 26.6268 Disc Loss: 0.462159 Q Losses: [0.017606799, 0.13781062]\n",
      "epoch:1 batch_done:141 Gen Loss: 25.1827 Disc Loss: 0.572105 Q Losses: [0.017610425, 0.15117589]\n",
      "epoch:1 batch_done:142 Gen Loss: 18.18 Disc Loss: 0.342289 Q Losses: [0.01922103, 0.12414168]\n",
      "epoch:1 batch_done:143 Gen Loss: 12.4271 Disc Loss: 0.00969673 Q Losses: [0.017137645, 0.13171029]\n",
      "epoch:1 batch_done:144 Gen Loss: 9.0906 Disc Loss: 0.0033722 Q Losses: [0.019204564, 0.14269292]\n",
      "epoch:1 batch_done:145 Gen Loss: 5.78753 Disc Loss: 0.0211001 Q Losses: [0.020754416, 0.13503754]\n",
      "epoch:1 batch_done:146 Gen Loss: 4.42157 Disc Loss: 0.061822 Q Losses: [0.017410165, 0.12977621]\n",
      "epoch:1 batch_done:147 Gen Loss: 5.23502 Disc Loss: 0.0273883 Q Losses: [0.015736038, 0.12045904]\n",
      "epoch:1 batch_done:148 Gen Loss: 6.08079 Disc Loss: 0.0129936 Q Losses: [0.013553713, 0.12020253]\n",
      "epoch:1 batch_done:149 Gen Loss: 6.57493 Disc Loss: 0.0113025 Q Losses: [0.020301677, 0.12233812]\n",
      "epoch:1 batch_done:150 Gen Loss: 5.72046 Disc Loss: 0.0198042 Q Losses: [0.012343505, 0.12848575]\n",
      "epoch:1 batch_done:151 Gen Loss: 5.03965 Disc Loss: 0.0783125 Q Losses: [0.015072368, 0.11531986]\n",
      "epoch:1 batch_done:152 Gen Loss: 6.99698 Disc Loss: 0.0734053 Q Losses: [0.016439829, 0.12852696]\n",
      "epoch:1 batch_done:153 Gen Loss: 7.11066 Disc Loss: 0.0281238 Q Losses: [0.01688515, 0.11633411]\n",
      "epoch:1 batch_done:154 Gen Loss: 6.23783 Disc Loss: 0.0800851 Q Losses: [0.018049426, 0.11648622]\n",
      "epoch:1 batch_done:155 Gen Loss: 4.85454 Disc Loss: 0.0433749 Q Losses: [0.012780746, 0.118035]\n",
      "epoch:1 batch_done:156 Gen Loss: 4.60803 Disc Loss: 0.0375143 Q Losses: [0.012455361, 0.11980265]\n",
      "epoch:1 batch_done:157 Gen Loss: 5.00087 Disc Loss: 0.097587 Q Losses: [0.012030162, 0.11627796]\n",
      "epoch:1 batch_done:158 Gen Loss: 5.06822 Disc Loss: 0.0434473 Q Losses: [0.012551589, 0.11699901]\n",
      "epoch:1 batch_done:159 Gen Loss: 5.51849 Disc Loss: 0.0919182 Q Losses: [0.018631395, 0.11112589]\n",
      "epoch:1 batch_done:160 Gen Loss: 4.97909 Disc Loss: 0.0287295 Q Losses: [0.01163571, 0.11158379]\n",
      "epoch:1 batch_done:161 Gen Loss: 7.95281 Disc Loss: 0.0635951 Q Losses: [0.020666441, 0.11088909]\n",
      "epoch:1 batch_done:162 Gen Loss: 6.71377 Disc Loss: 0.0895725 Q Losses: [0.021089612, 0.10922514]\n",
      "epoch:1 batch_done:163 Gen Loss: 6.12564 Disc Loss: 0.0342835 Q Losses: [0.018271172, 0.1193977]\n",
      "epoch:1 batch_done:164 Gen Loss: 5.69254 Disc Loss: 0.0573637 Q Losses: [0.0066927392, 0.10066424]\n",
      "epoch:1 batch_done:165 Gen Loss: 5.11968 Disc Loss: 0.0845357 Q Losses: [0.011030206, 0.11195128]\n",
      "epoch:1 batch_done:166 Gen Loss: 6.37587 Disc Loss: 0.0536749 Q Losses: [0.0088912118, 0.11280548]\n",
      "epoch:1 batch_done:167 Gen Loss: 7.33472 Disc Loss: 0.0178658 Q Losses: [0.009808748, 0.10794906]\n",
      "epoch:1 batch_done:168 Gen Loss: 7.5402 Disc Loss: 0.101083 Q Losses: [0.013376292, 0.10573623]\n",
      "epoch:1 batch_done:169 Gen Loss: 4.47804 Disc Loss: 0.101128 Q Losses: [0.015545796, 0.11648631]\n",
      "epoch:1 batch_done:170 Gen Loss: 6.1235 Disc Loss: 0.0335948 Q Losses: [0.013480045, 0.11609314]\n",
      "epoch:1 batch_done:171 Gen Loss: 6.67953 Disc Loss: 0.0147466 Q Losses: [0.013320242, 0.11135174]\n",
      "epoch:1 batch_done:172 Gen Loss: 6.6989 Disc Loss: 0.00721371 Q Losses: [0.012897294, 0.11151437]\n",
      "epoch:1 batch_done:173 Gen Loss: 8.02771 Disc Loss: 0.0880435 Q Losses: [0.017302906, 0.1092044]\n",
      "epoch:1 batch_done:174 Gen Loss: 7.68445 Disc Loss: 0.0237947 Q Losses: [0.013603421, 0.10040925]\n",
      "epoch:1 batch_done:175 Gen Loss: 5.8198 Disc Loss: 0.0753203 Q Losses: [0.016212899, 0.098836511]\n",
      "epoch:1 batch_done:176 Gen Loss: 5.22032 Disc Loss: 0.0503713 Q Losses: [0.008787021, 0.11350726]\n",
      "epoch:1 batch_done:177 Gen Loss: 7.85476 Disc Loss: 0.0647255 Q Losses: [0.014620748, 0.10011342]\n",
      "epoch:1 batch_done:178 Gen Loss: 8.7874 Disc Loss: 0.00692359 Q Losses: [0.010297872, 0.11933513]\n",
      "epoch:1 batch_done:179 Gen Loss: 10.3867 Disc Loss: 0.102183 Q Losses: [0.01476117, 0.10766519]\n",
      "epoch:1 batch_done:180 Gen Loss: 10.2126 Disc Loss: 0.00898225 Q Losses: [0.0089320261, 0.12558731]\n",
      "epoch:1 batch_done:181 Gen Loss: 5.99066 Disc Loss: 0.00359139 Q Losses: [0.011856606, 0.10605612]\n",
      "epoch:1 batch_done:182 Gen Loss: 15.8879 Disc Loss: 0.185782 Q Losses: [0.011032374, 0.11189701]\n",
      "epoch:1 batch_done:183 Gen Loss: 16.4287 Disc Loss: 0.0756334 Q Losses: [0.0095610041, 0.12302312]\n",
      "epoch:1 batch_done:184 Gen Loss: 15.1171 Disc Loss: 0.126573 Q Losses: [0.010627839, 0.11961494]\n",
      "epoch:1 batch_done:185 Gen Loss: 13.0614 Disc Loss: 0.022416 Q Losses: [0.018686898, 0.12285495]\n",
      "epoch:1 batch_done:186 Gen Loss: 7.60404 Disc Loss: 0.0196176 Q Losses: [0.022480413, 0.11378375]\n",
      "epoch:1 batch_done:187 Gen Loss: 5.36862 Disc Loss: 0.0287582 Q Losses: [0.01218227, 0.11160253]\n",
      "epoch:1 batch_done:188 Gen Loss: 6.16193 Disc Loss: 0.0194596 Q Losses: [0.015645701, 0.099077448]\n",
      "epoch:1 batch_done:189 Gen Loss: 8.16801 Disc Loss: 0.00315289 Q Losses: [0.013654333, 0.10700969]\n",
      "epoch:1 batch_done:190 Gen Loss: 7.82417 Disc Loss: 0.0642269 Q Losses: [0.011929622, 0.1022415]\n",
      "epoch:1 batch_done:191 Gen Loss: 7.92947 Disc Loss: 0.0398743 Q Losses: [0.026567226, 0.10697948]\n",
      "epoch:1 batch_done:192 Gen Loss: 8.36726 Disc Loss: 0.00357484 Q Losses: [0.020273149, 0.0942204]\n",
      "epoch:1 batch_done:193 Gen Loss: 10.0923 Disc Loss: 0.111513 Q Losses: [0.011008426, 0.09708859]\n",
      "epoch:1 batch_done:194 Gen Loss: 11.7169 Disc Loss: 0.135215 Q Losses: [0.013093338, 0.09354683]\n",
      "epoch:1 batch_done:195 Gen Loss: 8.31256 Disc Loss: 0.0892187 Q Losses: [0.010498237, 0.096705019]\n",
      "epoch:1 batch_done:196 Gen Loss: 7.21333 Disc Loss: 0.00633345 Q Losses: [0.010189129, 0.11391822]\n",
      "epoch:1 batch_done:197 Gen Loss: 7.57066 Disc Loss: 0.00334067 Q Losses: [0.015089252, 0.10616464]\n",
      "epoch:1 batch_done:198 Gen Loss: 5.44661 Disc Loss: 0.0120226 Q Losses: [0.022792161, 0.10469392]\n",
      "epoch:1 batch_done:199 Gen Loss: 6.69614 Disc Loss: 0.0536544 Q Losses: [0.013763258, 0.099364161]\n",
      "epoch:1 batch_done:200 Gen Loss: 11.019 Disc Loss: 0.0841422 Q Losses: [0.011938578, 0.099692523]\n",
      "epoch:1 batch_done:201 Gen Loss: 17.9086 Disc Loss: 0.0419285 Q Losses: [0.016083937, 0.093341365]\n",
      "epoch:1 batch_done:202 Gen Loss: 13.9876 Disc Loss: 0.112163 Q Losses: [0.016437417, 0.09152735]\n",
      "epoch:1 batch_done:203 Gen Loss: 11.3727 Disc Loss: 0.0467633 Q Losses: [0.0086124865, 0.092953749]\n",
      "epoch:1 batch_done:204 Gen Loss: 12.9308 Disc Loss: 0.0111872 Q Losses: [0.0088200625, 0.10236537]\n",
      "epoch:1 batch_done:205 Gen Loss: 4.27366 Disc Loss: 0.0857011 Q Losses: [0.013171915, 0.098218352]\n",
      "epoch:1 batch_done:206 Gen Loss: 9.26895 Disc Loss: 0.000734934 Q Losses: [0.010320228, 0.10120484]\n",
      "epoch:1 batch_done:207 Gen Loss: 6.61144 Disc Loss: 0.00245857 Q Losses: [0.026044007, 0.10803532]\n",
      "epoch:2 batch_done:1 Gen Loss: 23.1155 Disc Loss: 0.27059 Q Losses: [0.017210744, 0.12835127]\n",
      "epoch:2 batch_done:2 Gen Loss: 30.1753 Disc Loss: 0.0965789 Q Losses: [0.011253103, 0.098449007]\n",
      "epoch:2 batch_done:3 Gen Loss: 21.0324 Disc Loss: 0.408515 Q Losses: [0.023861237, 0.1023073]\n",
      "epoch:2 batch_done:4 Gen Loss: 23.5925 Disc Loss: 0.0412655 Q Losses: [0.017860759, 0.098311484]\n",
      "epoch:2 batch_done:5 Gen Loss: 14.4244 Disc Loss: 0.00401026 Q Losses: [0.015681889, 0.1182233]\n",
      "epoch:2 batch_done:6 Gen Loss: 15.9526 Disc Loss: 3.73539e-05 Q Losses: [0.011883165, 0.088783517]\n",
      "epoch:2 batch_done:7 Gen Loss: 14.9422 Disc Loss: 0.000150326 Q Losses: [0.011258885, 0.097048685]\n",
      "epoch:2 batch_done:8 Gen Loss: 8.37703 Disc Loss: 0.000203289 Q Losses: [0.0084594795, 0.13130775]\n",
      "epoch:2 batch_done:9 Gen Loss: 6.2628 Disc Loss: 0.00223371 Q Losses: [0.0096697882, 0.10961983]\n",
      "epoch:2 batch_done:10 Gen Loss: 5.81787 Disc Loss: 0.00630032 Q Losses: [0.012453012, 0.098826416]\n",
      "epoch:2 batch_done:11 Gen Loss: 16.4569 Disc Loss: 4.40118e-05 Q Losses: [0.018384106, 0.1091915]\n",
      "epoch:2 batch_done:12 Gen Loss: 5.83326 Disc Loss: 0.0105846 Q Losses: [0.020368623, 0.11043572]\n",
      "epoch:2 batch_done:13 Gen Loss: 12.6089 Disc Loss: 0.000277085 Q Losses: [0.014539354, 0.088157684]\n",
      "epoch:2 batch_done:14 Gen Loss: 7.56812 Disc Loss: 0.0385627 Q Losses: [0.020580081, 0.097929269]\n",
      "epoch:2 batch_done:15 Gen Loss: 9.18893 Disc Loss: 0.000836847 Q Losses: [0.014954275, 0.10611475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:16 Gen Loss: 11.8806 Disc Loss: 0.000197947 Q Losses: [0.013452483, 0.092561476]\n",
      "epoch:2 batch_done:17 Gen Loss: 6.5434 Disc Loss: 0.00352527 Q Losses: [0.009706283, 0.097292185]\n",
      "epoch:2 batch_done:18 Gen Loss: 7.96548 Disc Loss: 0.00104677 Q Losses: [0.035451014, 0.10020071]\n",
      "epoch:2 batch_done:19 Gen Loss: 6.45048 Disc Loss: 0.00683874 Q Losses: [0.012133654, 0.10846195]\n",
      "epoch:2 batch_done:20 Gen Loss: 9.11629 Disc Loss: 0.000945519 Q Losses: [0.019700263, 0.093038663]\n",
      "epoch:2 batch_done:21 Gen Loss: 43.9464 Disc Loss: 0.77511 Q Losses: [0.013173433, 0.094633803]\n",
      "epoch:2 batch_done:22 Gen Loss: 36.8138 Disc Loss: 5.97198 Q Losses: [0.016123997, 0.11619222]\n",
      "epoch:2 batch_done:23 Gen Loss: 26.3569 Disc Loss: 0.101485 Q Losses: [0.013957071, 0.10035029]\n",
      "epoch:2 batch_done:24 Gen Loss: 19.4368 Disc Loss: 1.96142e-05 Q Losses: [0.015666578, 0.10530168]\n",
      "epoch:2 batch_done:25 Gen Loss: 10.2548 Disc Loss: 1.40146e-05 Q Losses: [0.02123859, 0.11154191]\n",
      "epoch:2 batch_done:26 Gen Loss: 5.64076 Disc Loss: 0.0184706 Q Losses: [0.019971902, 0.10542292]\n",
      "epoch:2 batch_done:27 Gen Loss: 6.84396 Disc Loss: 0.0115436 Q Losses: [0.015507322, 0.11100613]\n",
      "epoch:2 batch_done:28 Gen Loss: 8.39861 Disc Loss: 0.000707814 Q Losses: [0.017338771, 0.089399241]\n",
      "epoch:2 batch_done:29 Gen Loss: 6.97337 Disc Loss: 0.0152101 Q Losses: [0.017496871, 0.090855703]\n",
      "epoch:2 batch_done:30 Gen Loss: 7.04646 Disc Loss: 0.00777145 Q Losses: [0.013314387, 0.08712846]\n",
      "epoch:2 batch_done:31 Gen Loss: 6.78533 Disc Loss: 0.00946785 Q Losses: [0.010354939, 0.091511354]\n",
      "epoch:2 batch_done:32 Gen Loss: 8.39853 Disc Loss: 0.000632504 Q Losses: [0.011935749, 0.094984137]\n",
      "epoch:2 batch_done:33 Gen Loss: 6.52921 Disc Loss: 0.0208246 Q Losses: [0.012517741, 0.088118747]\n",
      "epoch:2 batch_done:34 Gen Loss: 14.6678 Disc Loss: 0.123898 Q Losses: [0.019086048, 0.092735507]\n",
      "epoch:2 batch_done:35 Gen Loss: 15.7672 Disc Loss: 0.000677407 Q Losses: [0.015377561, 0.087238222]\n",
      "epoch:2 batch_done:36 Gen Loss: 15.1244 Disc Loss: 0.0595771 Q Losses: [0.012475766, 0.082939699]\n",
      "epoch:2 batch_done:37 Gen Loss: 13.3212 Disc Loss: 0.00707535 Q Losses: [0.010873837, 0.084360197]\n",
      "epoch:2 batch_done:38 Gen Loss: 10.8476 Disc Loss: 0.00241884 Q Losses: [0.0078757405, 0.085481517]\n",
      "epoch:2 batch_done:39 Gen Loss: 7.82274 Disc Loss: 0.0168539 Q Losses: [0.0119303, 0.092407599]\n",
      "epoch:2 batch_done:40 Gen Loss: 5.19352 Disc Loss: 0.0359159 Q Losses: [0.017377332, 0.086338505]\n",
      "epoch:2 batch_done:41 Gen Loss: 8.34511 Disc Loss: 0.0751459 Q Losses: [0.0089614885, 0.085249439]\n",
      "epoch:2 batch_done:42 Gen Loss: 9.59015 Disc Loss: 0.00273135 Q Losses: [0.012983099, 0.085918553]\n",
      "epoch:2 batch_done:43 Gen Loss: 6.2611 Disc Loss: 0.101839 Q Losses: [0.015916567, 0.087257273]\n",
      "epoch:2 batch_done:44 Gen Loss: 7.55429 Disc Loss: 0.0589809 Q Losses: [0.013672621, 0.077701598]\n",
      "epoch:2 batch_done:45 Gen Loss: 7.52496 Disc Loss: 0.0184216 Q Losses: [0.012131579, 0.079908043]\n",
      "epoch:2 batch_done:46 Gen Loss: 6.76494 Disc Loss: 0.0322058 Q Losses: [0.0092366263, 0.080009252]\n",
      "epoch:2 batch_done:47 Gen Loss: 6.58591 Disc Loss: 0.0194786 Q Losses: [0.0086062709, 0.10054553]\n",
      "epoch:2 batch_done:48 Gen Loss: 6.92463 Disc Loss: 0.018754 Q Losses: [0.010630644, 0.083342701]\n",
      "epoch:2 batch_done:49 Gen Loss: 6.60836 Disc Loss: 0.0223762 Q Losses: [0.036600403, 0.08531782]\n",
      "epoch:2 batch_done:50 Gen Loss: 6.14429 Disc Loss: 0.0260531 Q Losses: [0.012626709, 0.087071799]\n",
      "epoch:2 batch_done:51 Gen Loss: 6.92808 Disc Loss: 0.0304969 Q Losses: [0.015587001, 0.084487915]\n",
      "epoch:2 batch_done:52 Gen Loss: 12.905 Disc Loss: 0.112825 Q Losses: [0.010978309, 0.077692695]\n",
      "epoch:2 batch_done:53 Gen Loss: 14.2518 Disc Loss: 0.0507076 Q Losses: [0.015095256, 0.079050913]\n",
      "epoch:2 batch_done:54 Gen Loss: 11.64 Disc Loss: 0.100146 Q Losses: [0.018940832, 0.084382065]\n",
      "epoch:2 batch_done:55 Gen Loss: 12.684 Disc Loss: 0.0474067 Q Losses: [0.011216233, 0.10103077]\n",
      "epoch:2 batch_done:56 Gen Loss: 5.49803 Disc Loss: 0.0121897 Q Losses: [0.011018956, 0.097424805]\n",
      "epoch:2 batch_done:57 Gen Loss: 6.07332 Disc Loss: 0.0381366 Q Losses: [0.0091792075, 0.083600856]\n",
      "epoch:2 batch_done:58 Gen Loss: 9.8676 Disc Loss: 0.000337638 Q Losses: [0.011257187, 0.088734046]\n",
      "epoch:2 batch_done:59 Gen Loss: 52.1314 Disc Loss: 1.53944 Q Losses: [0.013723772, 0.093205966]\n",
      "epoch:2 batch_done:60 Gen Loss: 48.9765 Disc Loss: 3.7327 Q Losses: [0.014608966, 0.08426784]\n",
      "epoch:2 batch_done:61 Gen Loss: 38.4576 Disc Loss: 0.093886 Q Losses: [0.012432966, 0.096246585]\n",
      "epoch:2 batch_done:62 Gen Loss: 27.4046 Disc Loss: 0.00630335 Q Losses: [0.011092118, 0.10600869]\n",
      "epoch:2 batch_done:63 Gen Loss: 19.7524 Disc Loss: 0.00103598 Q Losses: [0.012135988, 0.090631656]\n",
      "epoch:2 batch_done:64 Gen Loss: 13.1525 Disc Loss: 0.00602209 Q Losses: [0.017300421, 0.10852319]\n",
      "epoch:2 batch_done:65 Gen Loss: 8.44431 Disc Loss: 0.000167857 Q Losses: [0.024400841, 0.104931]\n",
      "epoch:2 batch_done:66 Gen Loss: 5.76985 Disc Loss: 0.00410653 Q Losses: [0.024013385, 0.099515319]\n",
      "epoch:2 batch_done:67 Gen Loss: 6.00994 Disc Loss: 0.00539753 Q Losses: [0.01659026, 0.097226687]\n",
      "epoch:2 batch_done:68 Gen Loss: 5.75714 Disc Loss: 0.0094665 Q Losses: [0.020225, 0.098001406]\n",
      "epoch:2 batch_done:69 Gen Loss: 5.28632 Disc Loss: 0.0401349 Q Losses: [0.011852259, 0.08416976]\n",
      "epoch:2 batch_done:70 Gen Loss: 6.36095 Disc Loss: 0.0787361 Q Losses: [0.010707264, 0.095492668]\n",
      "epoch:2 batch_done:71 Gen Loss: 6.57378 Disc Loss: 0.0418965 Q Losses: [0.013872697, 0.079699725]\n",
      "epoch:2 batch_done:72 Gen Loss: 5.9251 Disc Loss: 0.0797884 Q Losses: [0.016941641, 0.086276568]\n",
      "epoch:2 batch_done:73 Gen Loss: 5.62911 Disc Loss: 0.0701189 Q Losses: [0.010942796, 0.08974348]\n",
      "epoch:2 batch_done:74 Gen Loss: 1.99846 Disc Loss: 0.187837 Q Losses: [0.010358508, 0.084701717]\n",
      "epoch:2 batch_done:75 Gen Loss: 33.6465 Disc Loss: 0.489574 Q Losses: [0.011960913, 0.076292902]\n",
      "epoch:2 batch_done:76 Gen Loss: 20.5806 Disc Loss: 2.57502 Q Losses: [0.013025185, 0.081181943]\n",
      "epoch:2 batch_done:77 Gen Loss: 5.3813 Disc Loss: 0.293018 Q Losses: [0.012430783, 0.086208135]\n",
      "epoch:2 batch_done:78 Gen Loss: 37.7369 Disc Loss: 0.760167 Q Losses: [0.0091568464, 0.083775505]\n",
      "epoch:2 batch_done:79 Gen Loss: 28.5038 Disc Loss: 0.920447 Q Losses: [0.016055416, 0.090057269]\n",
      "epoch:2 batch_done:80 Gen Loss: 17.2529 Disc Loss: 0.135338 Q Losses: [0.013904389, 0.079997689]\n",
      "epoch:2 batch_done:81 Gen Loss: 9.6325 Disc Loss: 0.0603554 Q Losses: [0.015064731, 0.086764619]\n",
      "epoch:2 batch_done:82 Gen Loss: 5.68337 Disc Loss: 0.00361334 Q Losses: [0.020549659, 0.075289905]\n",
      "epoch:2 batch_done:83 Gen Loss: 4.37268 Disc Loss: 0.0182287 Q Losses: [0.011687146, 0.11241776]\n",
      "epoch:2 batch_done:84 Gen Loss: 5.93347 Disc Loss: 0.0359911 Q Losses: [0.012793302, 0.073995277]\n",
      "epoch:2 batch_done:85 Gen Loss: 5.79929 Disc Loss: 0.0522739 Q Losses: [0.012562733, 0.072641127]\n",
      "epoch:2 batch_done:86 Gen Loss: 6.03902 Disc Loss: 0.0179124 Q Losses: [0.011998763, 0.080121756]\n",
      "epoch:2 batch_done:87 Gen Loss: 4.87055 Disc Loss: 0.0634992 Q Losses: [0.012391842, 0.094945922]\n",
      "epoch:2 batch_done:88 Gen Loss: 5.19353 Disc Loss: 0.0514029 Q Losses: [0.009684341, 0.076329157]\n",
      "epoch:2 batch_done:89 Gen Loss: 5.16591 Disc Loss: 0.0553668 Q Losses: [0.014090853, 0.064220294]\n",
      "epoch:2 batch_done:90 Gen Loss: 5.55837 Disc Loss: 0.030352 Q Losses: [0.016040467, 0.064777628]\n",
      "epoch:2 batch_done:91 Gen Loss: 5.47427 Disc Loss: 0.0367637 Q Losses: [0.010197083, 0.062421009]\n",
      "epoch:2 batch_done:92 Gen Loss: 4.76151 Disc Loss: 0.0589112 Q Losses: [0.015549425, 0.070121542]\n",
      "epoch:2 batch_done:93 Gen Loss: 5.12507 Disc Loss: 0.0178715 Q Losses: [0.013492214, 0.06196072]\n",
      "epoch:2 batch_done:94 Gen Loss: 4.06655 Disc Loss: 0.0990431 Q Losses: [0.0066800043, 0.060416855]\n",
      "epoch:2 batch_done:95 Gen Loss: 8.60993 Disc Loss: 0.0689968 Q Losses: [0.009909844, 0.066070929]\n",
      "epoch:2 batch_done:96 Gen Loss: 7.78998 Disc Loss: 0.0380799 Q Losses: [0.0096055353, 0.067016914]\n",
      "epoch:2 batch_done:97 Gen Loss: 5.41062 Disc Loss: 0.0708988 Q Losses: [0.011636622, 0.061880305]\n",
      "epoch:2 batch_done:98 Gen Loss: 8.66973 Disc Loss: 0.106914 Q Losses: [0.012508302, 0.059862934]\n",
      "epoch:2 batch_done:99 Gen Loss: 7.1904 Disc Loss: 0.0610925 Q Losses: [0.013208602, 0.059301071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:100 Gen Loss: 3.92377 Disc Loss: 0.0785133 Q Losses: [0.011843737, 0.069846898]\n",
      "epoch:2 batch_done:101 Gen Loss: 18.4227 Disc Loss: 0.206798 Q Losses: [0.0069042509, 0.063026354]\n",
      "epoch:2 batch_done:102 Gen Loss: 15.2178 Disc Loss: 0.447069 Q Losses: [0.011163343, 0.05450334]\n",
      "epoch:2 batch_done:103 Gen Loss: 8.42313 Disc Loss: 0.147782 Q Losses: [0.011632754, 0.058878586]\n",
      "epoch:2 batch_done:104 Gen Loss: 3.80373 Disc Loss: 0.00780242 Q Losses: [0.0094258152, 0.066873431]\n",
      "epoch:2 batch_done:105 Gen Loss: 18.6484 Disc Loss: 0.228847 Q Losses: [0.0060983994, 0.057839856]\n",
      "epoch:2 batch_done:106 Gen Loss: 23.3117 Disc Loss: 0.0467739 Q Losses: [0.010373767, 0.057101496]\n",
      "epoch:2 batch_done:107 Gen Loss: 7.02574 Disc Loss: 1.71783 Q Losses: [0.011225177, 0.06527327]\n",
      "epoch:2 batch_done:108 Gen Loss: 0.218494 Disc Loss: 0.00534058 Q Losses: [0.0083267894, 0.058592692]\n",
      "epoch:2 batch_done:109 Gen Loss: 47.9199 Disc Loss: 1.76526 Q Losses: [0.0096548079, 0.061855651]\n",
      "epoch:2 batch_done:110 Gen Loss: 40.4878 Disc Loss: 6.28297 Q Losses: [0.0090775825, 0.066110075]\n",
      "epoch:2 batch_done:111 Gen Loss: 25.2615 Disc Loss: 1.92228 Q Losses: [0.0061451024, 0.057804149]\n",
      "epoch:2 batch_done:112 Gen Loss: 14.809 Disc Loss: 0.000331537 Q Losses: [0.0086179245, 0.07842949]\n",
      "epoch:2 batch_done:113 Gen Loss: 7.11904 Disc Loss: 5.60455e-05 Q Losses: [0.0072476175, 0.068148442]\n",
      "epoch:2 batch_done:114 Gen Loss: 7.25603 Disc Loss: 0.0743294 Q Losses: [0.0099608097, 0.073090911]\n",
      "epoch:2 batch_done:115 Gen Loss: 7.6534 Disc Loss: 0.0177031 Q Losses: [0.0085843988, 0.063364573]\n",
      "epoch:2 batch_done:116 Gen Loss: 7.05872 Disc Loss: 0.00907763 Q Losses: [0.011309275, 0.065861993]\n",
      "epoch:2 batch_done:117 Gen Loss: 6.52554 Disc Loss: 0.02032 Q Losses: [0.013043304, 0.063851878]\n",
      "epoch:2 batch_done:118 Gen Loss: 6.69225 Disc Loss: 0.023789 Q Losses: [0.009491384, 0.072143964]\n",
      "epoch:2 batch_done:119 Gen Loss: 6.79876 Disc Loss: 0.0234191 Q Losses: [0.0091607347, 0.057078324]\n",
      "epoch:2 batch_done:120 Gen Loss: 7.22863 Disc Loss: 0.0356141 Q Losses: [0.0097557316, 0.053276762]\n",
      "epoch:2 batch_done:121 Gen Loss: 7.30832 Disc Loss: 0.0246174 Q Losses: [0.010326694, 0.061746515]\n",
      "epoch:2 batch_done:122 Gen Loss: 7.00739 Disc Loss: 0.0579347 Q Losses: [0.010983034, 0.063414477]\n",
      "epoch:2 batch_done:123 Gen Loss: 6.99917 Disc Loss: 0.0509995 Q Losses: [0.010177513, 0.052234828]\n",
      "epoch:2 batch_done:124 Gen Loss: 7.27213 Disc Loss: 0.0514091 Q Losses: [0.010445534, 0.068402588]\n",
      "epoch:2 batch_done:125 Gen Loss: 15.532 Disc Loss: 0.237073 Q Losses: [0.013215908, 0.061068602]\n",
      "epoch:2 batch_done:126 Gen Loss: 16.4923 Disc Loss: 0.0889624 Q Losses: [0.026758943, 0.058201648]\n",
      "epoch:2 batch_done:127 Gen Loss: 11.3859 Disc Loss: 0.392767 Q Losses: [0.012466982, 0.058989417]\n",
      "epoch:2 batch_done:128 Gen Loss: 5.10223 Disc Loss: 0.142612 Q Losses: [0.0086371833, 0.0598322]\n",
      "epoch:2 batch_done:129 Gen Loss: 18.3275 Disc Loss: 0.297257 Q Losses: [0.0076355059, 0.059407678]\n",
      "epoch:2 batch_done:130 Gen Loss: 19.7327 Disc Loss: 0.177408 Q Losses: [0.0084123854, 0.058878865]\n",
      "epoch:2 batch_done:131 Gen Loss: 15.6092 Disc Loss: 0.317894 Q Losses: [0.02301365, 0.06214083]\n",
      "epoch:2 batch_done:132 Gen Loss: 11.6333 Disc Loss: 0.086773 Q Losses: [0.012051177, 0.055898055]\n",
      "epoch:2 batch_done:133 Gen Loss: 8.13948 Disc Loss: 0.00428387 Q Losses: [0.0046060653, 0.062261026]\n",
      "epoch:2 batch_done:134 Gen Loss: 5.50023 Disc Loss: 0.00774469 Q Losses: [0.013213227, 0.060565133]\n",
      "epoch:2 batch_done:135 Gen Loss: 6.41007 Disc Loss: 0.0493986 Q Losses: [0.021080345, 0.066598266]\n",
      "epoch:2 batch_done:136 Gen Loss: 6.86348 Disc Loss: 0.0220762 Q Losses: [0.012198058, 0.059369493]\n",
      "epoch:2 batch_done:137 Gen Loss: 8.79656 Disc Loss: 0.00788423 Q Losses: [0.01425554, 0.053707704]\n",
      "epoch:2 batch_done:138 Gen Loss: 5.94679 Disc Loss: 0.0121563 Q Losses: [0.0081986412, 0.059784446]\n",
      "epoch:2 batch_done:139 Gen Loss: 12.7433 Disc Loss: 0.127559 Q Losses: [0.013171969, 0.049243271]\n",
      "epoch:2 batch_done:140 Gen Loss: 14.0985 Disc Loss: 0.0141067 Q Losses: [0.01395423, 0.055029683]\n",
      "epoch:2 batch_done:141 Gen Loss: 11.8363 Disc Loss: 0.120837 Q Losses: [0.009864904, 0.056547962]\n",
      "epoch:2 batch_done:142 Gen Loss: 8.25314 Disc Loss: 0.119756 Q Losses: [0.0095481947, 0.059326198]\n",
      "epoch:2 batch_done:143 Gen Loss: 6.66533 Disc Loss: 0.0151965 Q Losses: [0.020978484, 0.05454864]\n",
      "epoch:2 batch_done:144 Gen Loss: 5.13114 Disc Loss: 0.0171588 Q Losses: [0.013528651, 0.052133672]\n",
      "epoch:2 batch_done:145 Gen Loss: 58.2781 Disc Loss: 2.38468 Q Losses: [0.012594133, 0.054720111]\n",
      "epoch:2 batch_done:146 Gen Loss: 58.8783 Disc Loss: 7.3189 Q Losses: [0.013087932, 0.063028589]\n",
      "epoch:2 batch_done:147 Gen Loss: 47.0524 Disc Loss: 2.17758 Q Losses: [0.011264442, 0.052735671]\n",
      "epoch:2 batch_done:148 Gen Loss: 34.925 Disc Loss: 0.300006 Q Losses: [0.0084360875, 0.057169538]\n",
      "epoch:2 batch_done:149 Gen Loss: 26.3039 Disc Loss: 0.111276 Q Losses: [0.017991124, 0.051929321]\n",
      "epoch:2 batch_done:150 Gen Loss: 20.1382 Disc Loss: 0.0405841 Q Losses: [0.031833354, 0.054650966]\n",
      "epoch:2 batch_done:151 Gen Loss: 13.7014 Disc Loss: 0.00407416 Q Losses: [0.011162424, 0.07032197]\n",
      "epoch:2 batch_done:152 Gen Loss: 8.23635 Disc Loss: 0.0527884 Q Losses: [0.012848552, 0.06085822]\n",
      "epoch:2 batch_done:153 Gen Loss: 5.16674 Disc Loss: 0.0320219 Q Losses: [0.009187011, 0.074443638]\n",
      "epoch:2 batch_done:154 Gen Loss: 29.9279 Disc Loss: 0.686574 Q Losses: [0.027820433, 0.058191746]\n",
      "epoch:2 batch_done:155 Gen Loss: 38.5511 Disc Loss: 0.00364301 Q Losses: [0.0093185026, 0.063937001]\n",
      "epoch:2 batch_done:156 Gen Loss: 38.5627 Disc Loss: 0.111108 Q Losses: [0.011173245, 0.068162844]\n",
      "epoch:2 batch_done:157 Gen Loss: 35.0247 Disc Loss: 0.0339339 Q Losses: [0.0084584597, 0.059433006]\n",
      "epoch:2 batch_done:158 Gen Loss: 29.4101 Disc Loss: 0.0841466 Q Losses: [0.0079887025, 0.061227053]\n",
      "epoch:2 batch_done:159 Gen Loss: 24.7558 Disc Loss: 0.0196829 Q Losses: [0.016484497, 0.065701768]\n",
      "epoch:2 batch_done:160 Gen Loss: 19.0909 Disc Loss: 0.212099 Q Losses: [0.011483935, 0.05860927]\n",
      "epoch:2 batch_done:161 Gen Loss: 13.8084 Disc Loss: 0.0974351 Q Losses: [0.012774808, 0.058354639]\n",
      "epoch:2 batch_done:162 Gen Loss: 7.75939 Disc Loss: 0.000824068 Q Losses: [0.0097788619, 0.060113259]\n",
      "epoch:2 batch_done:163 Gen Loss: 8.48357 Disc Loss: 0.10143 Q Losses: [0.015712079, 0.061070278]\n",
      "epoch:2 batch_done:164 Gen Loss: 12.2499 Disc Loss: 0.202787 Q Losses: [0.0089584254, 0.049858328]\n",
      "epoch:2 batch_done:165 Gen Loss: 9.90338 Disc Loss: 0.0128251 Q Losses: [0.0082723983, 0.050060242]\n",
      "epoch:2 batch_done:166 Gen Loss: 12.4757 Disc Loss: 0.383624 Q Losses: [0.0065121716, 0.056981638]\n",
      "epoch:2 batch_done:167 Gen Loss: 10.9699 Disc Loss: 0.0893437 Q Losses: [0.014657662, 0.068217002]\n",
      "epoch:2 batch_done:168 Gen Loss: 6.65092 Disc Loss: 0.147371 Q Losses: [0.010088471, 0.066404141]\n",
      "epoch:2 batch_done:169 Gen Loss: 9.86237 Disc Loss: 0.131489 Q Losses: [0.007878907, 0.056060344]\n",
      "epoch:2 batch_done:170 Gen Loss: 7.31046 Disc Loss: 0.242292 Q Losses: [0.010204684, 0.065412104]\n",
      "epoch:2 batch_done:171 Gen Loss: 6.03466 Disc Loss: 0.0436558 Q Losses: [0.011936649, 0.057500839]\n",
      "epoch:2 batch_done:172 Gen Loss: 6.56675 Disc Loss: 0.0696893 Q Losses: [0.010617007, 0.053961165]\n",
      "epoch:2 batch_done:173 Gen Loss: 7.04761 Disc Loss: 0.0126888 Q Losses: [0.017354138, 0.05138652]\n",
      "epoch:2 batch_done:174 Gen Loss: 6.96678 Disc Loss: 0.00887854 Q Losses: [0.011891641, 0.047956459]\n",
      "epoch:2 batch_done:175 Gen Loss: 5.27485 Disc Loss: 0.0461568 Q Losses: [0.010925871, 0.05701489]\n",
      "epoch:2 batch_done:176 Gen Loss: 5.82268 Disc Loss: 0.0695714 Q Losses: [0.012410384, 0.048166476]\n",
      "epoch:2 batch_done:177 Gen Loss: 5.63959 Disc Loss: 0.0674672 Q Losses: [0.0073550697, 0.047789074]\n",
      "epoch:2 batch_done:178 Gen Loss: 6.0509 Disc Loss: 0.0364968 Q Losses: [0.0060059624, 0.049743637]\n",
      "epoch:2 batch_done:179 Gen Loss: 6.59974 Disc Loss: 0.0167037 Q Losses: [0.0098092798, 0.052213449]\n",
      "epoch:2 batch_done:180 Gen Loss: 6.51251 Disc Loss: 0.0391028 Q Losses: [0.0083458237, 0.053678285]\n",
      "epoch:2 batch_done:181 Gen Loss: 5.75042 Disc Loss: 0.03014 Q Losses: [0.0091189556, 0.05392839]\n",
      "epoch:2 batch_done:182 Gen Loss: 5.98601 Disc Loss: 0.0173303 Q Losses: [0.017544182, 0.057795934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 batch_done:183 Gen Loss: 7.46407 Disc Loss: 0.00393597 Q Losses: [0.0068123802, 0.052176982]\n",
      "epoch:2 batch_done:184 Gen Loss: 10.6564 Disc Loss: 0.0192348 Q Losses: [0.010901267, 0.04635603]\n",
      "epoch:2 batch_done:185 Gen Loss: 12.2669 Disc Loss: 0.132274 Q Losses: [0.016211439, 0.050055549]\n",
      "epoch:2 batch_done:186 Gen Loss: 16.8337 Disc Loss: 0.0693544 Q Losses: [0.0085657183, 0.047357433]\n",
      "epoch:2 batch_done:187 Gen Loss: 15.8452 Disc Loss: 0.0380307 Q Losses: [0.017687187, 0.052696623]\n",
      "epoch:2 batch_done:188 Gen Loss: 10.308 Disc Loss: 0.0513371 Q Losses: [0.0086062048, 0.047935452]\n",
      "epoch:2 batch_done:189 Gen Loss: 7.45341 Disc Loss: 0.039681 Q Losses: [0.013679765, 0.055166192]\n",
      "epoch:2 batch_done:190 Gen Loss: 5.93324 Disc Loss: 0.00221889 Q Losses: [0.0077989018, 0.051677201]\n",
      "epoch:2 batch_done:191 Gen Loss: 7.8478 Disc Loss: 0.000794862 Q Losses: [0.0074484963, 0.04587809]\n",
      "epoch:2 batch_done:192 Gen Loss: 8.25426 Disc Loss: 0.0645491 Q Losses: [0.014557078, 0.048314836]\n",
      "epoch:2 batch_done:193 Gen Loss: 9.31746 Disc Loss: 0.0184422 Q Losses: [0.015187846, 0.047113568]\n",
      "epoch:2 batch_done:194 Gen Loss: 10.5721 Disc Loss: 0.0229797 Q Losses: [0.007208277, 0.053392027]\n",
      "epoch:2 batch_done:195 Gen Loss: 4.84947 Disc Loss: 0.0420619 Q Losses: [0.0060861618, 0.059949595]\n",
      "epoch:2 batch_done:196 Gen Loss: 13.4214 Disc Loss: 0.154329 Q Losses: [0.012687793, 0.055181436]\n",
      "epoch:2 batch_done:197 Gen Loss: 18.0112 Disc Loss: 0.0707127 Q Losses: [0.011785242, 0.049802333]\n",
      "epoch:2 batch_done:198 Gen Loss: 15.5139 Disc Loss: 0.342421 Q Losses: [0.0097842924, 0.057690036]\n",
      "epoch:2 batch_done:199 Gen Loss: 10.0738 Disc Loss: 0.00293721 Q Losses: [0.0077497233, 0.053637642]\n",
      "epoch:2 batch_done:200 Gen Loss: 8.80152 Disc Loss: 8.26011e-05 Q Losses: [0.0095304139, 0.042532358]\n",
      "epoch:2 batch_done:201 Gen Loss: 5.75713 Disc Loss: 0.037092 Q Losses: [0.0060764705, 0.049745735]\n",
      "epoch:2 batch_done:202 Gen Loss: 10.8388 Disc Loss: 0.000145986 Q Losses: [0.016068824, 0.052301612]\n",
      "epoch:2 batch_done:203 Gen Loss: 7.8021 Disc Loss: 0.0560503 Q Losses: [0.0083822059, 0.049481366]\n",
      "epoch:2 batch_done:204 Gen Loss: 11.1925 Disc Loss: 0.00258234 Q Losses: [0.020771462, 0.052061249]\n",
      "epoch:2 batch_done:205 Gen Loss: 12.4232 Disc Loss: 0.00190099 Q Losses: [0.010453313, 0.049993031]\n",
      "epoch:2 batch_done:206 Gen Loss: 7.45782 Disc Loss: 0.00317827 Q Losses: [0.015382096, 0.048464634]\n",
      "epoch:2 batch_done:207 Gen Loss: 8.27342 Disc Loss: 0.000588766 Q Losses: [0.0067263418, 0.052239854]\n",
      "epoch:3 batch_done:1 Gen Loss: 9.8761 Disc Loss: 0.106013 Q Losses: [0.010613646, 0.050886482]\n",
      "epoch:3 batch_done:2 Gen Loss: 14.2998 Disc Loss: 0.00455 Q Losses: [0.0084170382, 0.054505482]\n",
      "epoch:3 batch_done:3 Gen Loss: 10.5975 Disc Loss: 0.0347913 Q Losses: [0.010763941, 0.050551437]\n",
      "epoch:3 batch_done:4 Gen Loss: 12.4795 Disc Loss: 0.116439 Q Losses: [0.0099750571, 0.052291397]\n",
      "epoch:3 batch_done:5 Gen Loss: 4.95362 Disc Loss: 0.0112763 Q Losses: [0.0087920241, 0.048934378]\n",
      "epoch:3 batch_done:6 Gen Loss: 11.2328 Disc Loss: 0.111709 Q Losses: [0.0084970146, 0.055173129]\n",
      "epoch:3 batch_done:7 Gen Loss: 18.4668 Disc Loss: 0.0463293 Q Losses: [0.0093771052, 0.052709356]\n",
      "epoch:3 batch_done:8 Gen Loss: 18.2753 Disc Loss: 0.0330698 Q Losses: [0.0098960008, 0.050448433]\n",
      "epoch:3 batch_done:9 Gen Loss: 14.7126 Disc Loss: 0.00278846 Q Losses: [0.0082742581, 0.047850661]\n",
      "epoch:3 batch_done:10 Gen Loss: 11.9827 Disc Loss: 0.000377709 Q Losses: [0.0073959674, 0.046074241]\n",
      "epoch:3 batch_done:11 Gen Loss: 11.6333 Disc Loss: 0.000711054 Q Losses: [0.01049102, 0.056202367]\n",
      "epoch:3 batch_done:12 Gen Loss: 15.3848 Disc Loss: 0.00303931 Q Losses: [0.0093436828, 0.051144056]\n",
      "epoch:3 batch_done:13 Gen Loss: 10.9465 Disc Loss: 0.000726875 Q Losses: [0.0075047025, 0.050203566]\n",
      "epoch:3 batch_done:14 Gen Loss: 11.2038 Disc Loss: 0.00259414 Q Losses: [0.012301569, 0.057220507]\n",
      "epoch:3 batch_done:15 Gen Loss: 8.03329 Disc Loss: 0.00145198 Q Losses: [0.0062386049, 0.043310158]\n",
      "epoch:3 batch_done:16 Gen Loss: 6.55811 Disc Loss: 0.00213608 Q Losses: [0.013260363, 0.048348092]\n",
      "epoch:3 batch_done:17 Gen Loss: 6.56805 Disc Loss: 0.003701 Q Losses: [0.012250913, 0.049275499]\n",
      "epoch:3 batch_done:18 Gen Loss: 8.75641 Disc Loss: 0.069673 Q Losses: [0.014802201, 0.043870136]\n",
      "epoch:3 batch_done:19 Gen Loss: 13.1847 Disc Loss: 0.001707 Q Losses: [0.0089546554, 0.045529298]\n",
      "epoch:3 batch_done:20 Gen Loss: 7.89758 Disc Loss: 0.00462184 Q Losses: [0.018341804, 0.053089332]\n",
      "epoch:3 batch_done:21 Gen Loss: 7.87217 Disc Loss: 0.00473404 Q Losses: [0.005606283, 0.053554211]\n",
      "epoch:3 batch_done:22 Gen Loss: 14.6211 Disc Loss: 0.00340363 Q Losses: [0.016855683, 0.043492049]\n",
      "epoch:3 batch_done:23 Gen Loss: 6.442 Disc Loss: 0.0112524 Q Losses: [0.008649271, 0.042668968]\n",
      "epoch:3 batch_done:24 Gen Loss: 17.4352 Disc Loss: 0.00261095 Q Losses: [0.024486948, 0.043239899]\n",
      "epoch:3 batch_done:25 Gen Loss: 9.96759 Disc Loss: 0.00871952 Q Losses: [0.017121702, 0.045254156]\n",
      "epoch:3 batch_done:26 Gen Loss: 6.90343 Disc Loss: 0.0136031 Q Losses: [0.011423947, 0.050668672]\n",
      "epoch:3 batch_done:27 Gen Loss: 42.0877 Disc Loss: 0.654297 Q Losses: [0.0048276461, 0.043584485]\n",
      "epoch:3 batch_done:28 Gen Loss: 29.3138 Disc Loss: 2.49925 Q Losses: [0.010371067, 0.055094317]\n",
      "epoch:3 batch_done:29 Gen Loss: 9.24297 Disc Loss: 2.59975e-05 Q Losses: [0.015102916, 0.072208062]\n",
      "epoch:3 batch_done:30 Gen Loss: 49.3056 Disc Loss: 0.437538 Q Losses: [0.014737912, 0.058123965]\n",
      "epoch:3 batch_done:31 Gen Loss: 45.9415 Disc Loss: 0.495832 Q Losses: [0.016693655, 0.066265531]\n",
      "epoch:3 batch_done:32 Gen Loss: 35.7276 Disc Loss: 0.407367 Q Losses: [0.0098660206, 0.055882666]\n",
      "epoch:3 batch_done:33 Gen Loss: 26.7389 Disc Loss: 0.0380388 Q Losses: [0.014154782, 0.053562317]\n",
      "epoch:3 batch_done:34 Gen Loss: 20.1953 Disc Loss: 0.00202089 Q Losses: [0.01271891, 0.04909952]\n",
      "epoch:3 batch_done:35 Gen Loss: 15.4119 Disc Loss: 4.88313e-05 Q Losses: [0.010072261, 0.051539034]\n",
      "epoch:3 batch_done:36 Gen Loss: 11.7441 Disc Loss: 0.00063786 Q Losses: [0.0093003269, 0.049312968]\n",
      "epoch:3 batch_done:37 Gen Loss: 8.47747 Disc Loss: 0.000298077 Q Losses: [0.017948475, 0.04773239]\n",
      "epoch:3 batch_done:38 Gen Loss: 5.87805 Disc Loss: 0.00535354 Q Losses: [0.0099298405, 0.055554491]\n",
      "epoch:3 batch_done:39 Gen Loss: 6.45037 Disc Loss: 0.0162832 Q Losses: [0.018466499, 0.047382474]\n",
      "epoch:3 batch_done:40 Gen Loss: 7.8613 Disc Loss: 0.019433 Q Losses: [0.010904841, 0.05403994]\n",
      "epoch:3 batch_done:41 Gen Loss: 7.8948 Disc Loss: 0.00725649 Q Losses: [0.0067003733, 0.053765588]\n",
      "epoch:3 batch_done:42 Gen Loss: 7.61544 Disc Loss: 0.0154617 Q Losses: [0.014252042, 0.049549147]\n",
      "epoch:3 batch_done:43 Gen Loss: 7.66534 Disc Loss: 0.108499 Q Losses: [0.020074008, 0.048364334]\n",
      "epoch:3 batch_done:44 Gen Loss: 17.1975 Disc Loss: 0.11676 Q Losses: [0.011825649, 0.04713434]\n",
      "epoch:3 batch_done:45 Gen Loss: 10.6447 Disc Loss: 0.437193 Q Losses: [0.017033786, 0.04246784]\n",
      "epoch:3 batch_done:46 Gen Loss: 4.87901 Disc Loss: 0.0182721 Q Losses: [0.01167218, 0.060739074]\n",
      "epoch:3 batch_done:47 Gen Loss: 10.9319 Disc Loss: 0.0610463 Q Losses: [0.0093737785, 0.043807313]\n",
      "epoch:3 batch_done:48 Gen Loss: 9.34521 Disc Loss: 0.0079141 Q Losses: [0.010129776, 0.052884467]\n",
      "epoch:3 batch_done:49 Gen Loss: 13.8125 Disc Loss: 0.07007 Q Losses: [0.018808275, 0.040552787]\n",
      "epoch:3 batch_done:50 Gen Loss: 15.6376 Disc Loss: 0.0556118 Q Losses: [0.00985891, 0.045095898]\n",
      "epoch:3 batch_done:51 Gen Loss: 11.9301 Disc Loss: 0.205751 Q Losses: [0.011230378, 0.048293322]\n",
      "epoch:3 batch_done:52 Gen Loss: 11.6282 Disc Loss: 0.00467726 Q Losses: [0.011327994, 0.054286696]\n",
      "epoch:3 batch_done:53 Gen Loss: 7.76343 Disc Loss: 0.0426957 Q Losses: [0.012004901, 0.045693178]\n",
      "epoch:3 batch_done:54 Gen Loss: 8.21593 Disc Loss: 0.00625228 Q Losses: [0.014078668, 0.045649439]\n",
      "epoch:3 batch_done:55 Gen Loss: 11.9898 Disc Loss: 0.00170508 Q Losses: [0.010798829, 0.052558243]\n",
      "epoch:3 batch_done:56 Gen Loss: 7.02945 Disc Loss: 0.0202344 Q Losses: [0.012161857, 0.043967023]\n",
      "epoch:3 batch_done:57 Gen Loss: 6.0344 Disc Loss: 0.0110923 Q Losses: [0.011339046, 0.043520696]\n",
      "epoch:3 batch_done:58 Gen Loss: 8.83005 Disc Loss: 0.040055 Q Losses: [0.013911242, 0.043585613]\n",
      "epoch:3 batch_done:59 Gen Loss: 8.35019 Disc Loss: 0.0143527 Q Losses: [0.025951548, 0.039765432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 batch_done:60 Gen Loss: 9.45431 Disc Loss: 0.030537 Q Losses: [0.005979945, 0.048576757]\n",
      "epoch:3 batch_done:61 Gen Loss: 9.75419 Disc Loss: 0.027053 Q Losses: [0.015830882, 0.042541154]\n",
      "epoch:3 batch_done:62 Gen Loss: 6.61548 Disc Loss: 0.0384143 Q Losses: [0.0082066674, 0.046494711]\n",
      "epoch:3 batch_done:63 Gen Loss: 19.691 Disc Loss: 0.18141 Q Losses: [0.010070532, 0.037848227]\n",
      "epoch:3 batch_done:64 Gen Loss: 16.8626 Disc Loss: 0.534971 Q Losses: [0.0096437288, 0.043023907]\n",
      "epoch:3 batch_done:65 Gen Loss: 14.5164 Disc Loss: 0.00590743 Q Losses: [0.014850939, 0.04586916]\n",
      "epoch:3 batch_done:66 Gen Loss: 12.2637 Disc Loss: 0.00109079 Q Losses: [0.0087715387, 0.044791643]\n",
      "epoch:3 batch_done:67 Gen Loss: 10.5854 Disc Loss: 0.000812038 Q Losses: [0.009509379, 0.044123642]\n",
      "epoch:3 batch_done:68 Gen Loss: 10.8446 Disc Loss: 5.53096e-05 Q Losses: [0.0068881647, 0.043662883]\n",
      "epoch:3 batch_done:69 Gen Loss: 12.0085 Disc Loss: 7.91069e-05 Q Losses: [0.0096641723, 0.046504576]\n",
      "epoch:3 batch_done:70 Gen Loss: 6.94814 Disc Loss: 0.00118259 Q Losses: [0.0038181017, 0.035650492]\n",
      "epoch:3 batch_done:71 Gen Loss: 7.07743 Disc Loss: 0.00122031 Q Losses: [0.0077126231, 0.042881981]\n",
      "epoch:3 batch_done:72 Gen Loss: 6.06475 Disc Loss: 0.0170531 Q Losses: [0.013703722, 0.040018216]\n",
      "epoch:3 batch_done:73 Gen Loss: 8.48836 Disc Loss: 0.0385182 Q Losses: [0.0090284068, 0.036675759]\n",
      "epoch:3 batch_done:74 Gen Loss: 8.5996 Disc Loss: 0.0201856 Q Losses: [0.011446424, 0.038908862]\n",
      "epoch:3 batch_done:75 Gen Loss: 10.9932 Disc Loss: 0.00302811 Q Losses: [0.0098046139, 0.036687721]\n",
      "epoch:3 batch_done:76 Gen Loss: 13.3996 Disc Loss: 0.000130002 Q Losses: [0.0099860057, 0.038314857]\n",
      "epoch:3 batch_done:77 Gen Loss: 6.77737 Disc Loss: 0.0117887 Q Losses: [0.0081461556, 0.044087298]\n",
      "epoch:3 batch_done:78 Gen Loss: 5.75661 Disc Loss: 0.00764934 Q Losses: [0.0058753984, 0.045225222]\n",
      "epoch:3 batch_done:79 Gen Loss: 6.5122 Disc Loss: 0.00701781 Q Losses: [0.018923065, 0.042819116]\n",
      "epoch:3 batch_done:80 Gen Loss: 6.12784 Disc Loss: 0.00650651 Q Losses: [0.0075336564, 0.040661555]\n",
      "epoch:3 batch_done:81 Gen Loss: 7.34779 Disc Loss: 0.00635366 Q Losses: [0.009002028, 0.035608575]\n",
      "epoch:3 batch_done:82 Gen Loss: 6.18898 Disc Loss: 0.00469785 Q Losses: [0.0084941704, 0.040499978]\n",
      "epoch:3 batch_done:83 Gen Loss: 33.3399 Disc Loss: 0.497496 Q Losses: [0.015670538, 0.034409061]\n",
      "epoch:3 batch_done:84 Gen Loss: 21.2272 Disc Loss: 3.0166 Q Losses: [0.0080023911, 0.034691714]\n",
      "epoch:3 batch_done:85 Gen Loss: 11.6986 Disc Loss: 0.0142026 Q Losses: [0.005915029, 0.039151765]\n",
      "epoch:3 batch_done:86 Gen Loss: 4.61697 Disc Loss: 0.000205629 Q Losses: [0.0087105762, 0.042969681]\n",
      "epoch:3 batch_done:87 Gen Loss: 8.454 Disc Loss: 0.0740806 Q Losses: [0.012147809, 0.036154695]\n",
      "epoch:3 batch_done:88 Gen Loss: 9.98914 Disc Loss: 0.000929971 Q Losses: [0.0080215372, 0.043802895]\n",
      "epoch:3 batch_done:89 Gen Loss: 11.4441 Disc Loss: 4.74278e-05 Q Losses: [0.011118399, 0.045748442]\n",
      "epoch:3 batch_done:90 Gen Loss: 11.0241 Disc Loss: 0.00067828 Q Losses: [0.007214997, 0.037010401]\n",
      "epoch:3 batch_done:91 Gen Loss: 9.29808 Disc Loss: 0.000156677 Q Losses: [0.0097780274, 0.043709207]\n",
      "epoch:3 batch_done:92 Gen Loss: 6.19708 Disc Loss: 0.00389495 Q Losses: [0.012309465, 0.03773994]\n",
      "epoch:3 batch_done:93 Gen Loss: 6.74634 Disc Loss: 0.022634 Q Losses: [0.01276767, 0.043782398]\n",
      "epoch:3 batch_done:94 Gen Loss: 7.24612 Disc Loss: 0.00740805 Q Losses: [0.0092174802, 0.044250317]\n",
      "epoch:3 batch_done:95 Gen Loss: 13.5116 Disc Loss: 6.72882e-05 Q Losses: [0.0088910162, 0.036538381]\n",
      "epoch:3 batch_done:96 Gen Loss: 7.4283 Disc Loss: 0.00124354 Q Losses: [0.0090354476, 0.043278307]\n",
      "epoch:3 batch_done:97 Gen Loss: 10.9668 Disc Loss: 0.000980565 Q Losses: [0.018755784, 0.038293984]\n",
      "epoch:3 batch_done:98 Gen Loss: 16.9468 Disc Loss: 0.0289606 Q Losses: [0.0089775659, 0.037301205]\n",
      "epoch:3 batch_done:99 Gen Loss: 11.2603 Disc Loss: 0.000318124 Q Losses: [0.0072604255, 0.041999757]\n",
      "epoch:3 batch_done:100 Gen Loss: 7.05897 Disc Loss: 0.0247155 Q Losses: [0.0062193321, 0.03956873]\n",
      "epoch:3 batch_done:101 Gen Loss: 19.493 Disc Loss: 0.231772 Q Losses: [0.0062677287, 0.03692548]\n",
      "epoch:3 batch_done:102 Gen Loss: 25.09 Disc Loss: 0.243144 Q Losses: [0.0071267961, 0.037091941]\n",
      "epoch:3 batch_done:103 Gen Loss: 23.2362 Disc Loss: 0.0977678 Q Losses: [0.0079788612, 0.036632236]\n",
      "epoch:3 batch_done:104 Gen Loss: 20.1395 Disc Loss: 0.00626935 Q Losses: [0.0078422092, 0.032879155]\n",
      "epoch:3 batch_done:105 Gen Loss: 20.2287 Disc Loss: 0.0389582 Q Losses: [0.0062781349, 0.040222637]\n",
      "epoch:3 batch_done:106 Gen Loss: 14.0383 Disc Loss: 0.000670122 Q Losses: [0.007371902, 0.035804175]\n",
      "epoch:3 batch_done:107 Gen Loss: 20.559 Disc Loss: 8.34807e-05 Q Losses: [0.0068153352, 0.05585235]\n",
      "epoch:3 batch_done:108 Gen Loss: 10.6138 Disc Loss: 0.000185857 Q Losses: [0.0096670939, 0.038980074]\n",
      "epoch:3 batch_done:109 Gen Loss: 12.2718 Disc Loss: 9.4383e-05 Q Losses: [0.010167048, 0.037806056]\n",
      "epoch:3 batch_done:110 Gen Loss: 12.2435 Disc Loss: 0.000178292 Q Losses: [0.0087804068, 0.04901357]\n",
      "epoch:3 batch_done:111 Gen Loss: 5.76595 Disc Loss: 0.00561402 Q Losses: [0.012155128, 0.039905518]\n",
      "epoch:3 batch_done:112 Gen Loss: 14.2017 Disc Loss: 0.000511136 Q Losses: [0.0093735158, 0.054727893]\n",
      "epoch:3 batch_done:113 Gen Loss: 12.5824 Disc Loss: 0.0950673 Q Losses: [0.0098258518, 0.040747304]\n",
      "epoch:3 batch_done:114 Gen Loss: 20.8322 Disc Loss: 0.0735849 Q Losses: [0.011790942, 0.041809507]\n",
      "epoch:3 batch_done:115 Gen Loss: 15.7357 Disc Loss: 0.00621927 Q Losses: [0.031371403, 0.035336513]\n",
      "epoch:3 batch_done:116 Gen Loss: 16.2416 Disc Loss: 0.0351628 Q Losses: [0.011645687, 0.03680674]\n",
      "epoch:3 batch_done:117 Gen Loss: 12.9569 Disc Loss: 0.0175225 Q Losses: [0.0056024599, 0.056864291]\n",
      "epoch:3 batch_done:118 Gen Loss: 9.74831 Disc Loss: 0.000564808 Q Losses: [0.01124922, 0.041141707]\n",
      "epoch:3 batch_done:119 Gen Loss: 9.77684 Disc Loss: 0.0046409 Q Losses: [0.012497041, 0.051109463]\n",
      "epoch:3 batch_done:120 Gen Loss: 7.27296 Disc Loss: 0.00180002 Q Losses: [0.010830089, 0.034958031]\n",
      "epoch:3 batch_done:121 Gen Loss: 5.70594 Disc Loss: 0.00907963 Q Losses: [0.0097794943, 0.039930955]\n",
      "epoch:3 batch_done:122 Gen Loss: 4.85485 Disc Loss: 0.102651 Q Losses: [0.010130098, 0.047593255]\n",
      "epoch:3 batch_done:123 Gen Loss: 7.95858 Disc Loss: 0.0195531 Q Losses: [0.0091493297, 0.037255421]\n",
      "epoch:3 batch_done:124 Gen Loss: 39.3464 Disc Loss: 0.533636 Q Losses: [0.011385096, 0.037650958]\n",
      "epoch:3 batch_done:125 Gen Loss: 20.1375 Disc Loss: 3.29115 Q Losses: [0.0084620938, 0.038915291]\n",
      "epoch:3 batch_done:126 Gen Loss: 2.0982 Disc Loss: 0.124472 Q Losses: [0.0089766458, 0.054366313]\n",
      "epoch:3 batch_done:127 Gen Loss: 10.4459 Disc Loss: 0.00193036 Q Losses: [0.010229144, 0.050153852]\n",
      "epoch:3 batch_done:128 Gen Loss: 34.7057 Disc Loss: 0.222853 Q Losses: [0.0060233222, 0.040523585]\n",
      "epoch:3 batch_done:129 Gen Loss: 37.8997 Disc Loss: 0.0578694 Q Losses: [0.01228504, 0.042713366]\n",
      "epoch:3 batch_done:130 Gen Loss: 36.2199 Disc Loss: 0.0792321 Q Losses: [0.010899644, 0.043568961]\n",
      "epoch:3 batch_done:131 Gen Loss: 34.2253 Disc Loss: 0.0579889 Q Losses: [0.030006247, 0.038183622]\n",
      "epoch:3 batch_done:132 Gen Loss: 31.2512 Disc Loss: 0.00159785 Q Losses: [0.013743137, 0.038043149]\n",
      "epoch:3 batch_done:133 Gen Loss: 28.8796 Disc Loss: 0.00079809 Q Losses: [0.010380154, 0.040914048]\n",
      "epoch:3 batch_done:134 Gen Loss: 25.2777 Disc Loss: 0.0783589 Q Losses: [0.010646453, 0.039423548]\n",
      "epoch:3 batch_done:135 Gen Loss: 21.6901 Disc Loss: 0.000697792 Q Losses: [0.0084915543, 0.034708299]\n",
      "epoch:3 batch_done:136 Gen Loss: 17.8593 Disc Loss: 2.11894e-05 Q Losses: [0.0077089779, 0.039346501]\n",
      "epoch:3 batch_done:137 Gen Loss: 13.4286 Disc Loss: 0.000495821 Q Losses: [0.011030702, 0.032825127]\n",
      "epoch:3 batch_done:138 Gen Loss: 8.28815 Disc Loss: 0.00024518 Q Losses: [0.0083605675, 0.037107293]\n",
      "epoch:3 batch_done:139 Gen Loss: 5.88768 Disc Loss: 0.05883 Q Losses: [0.017878395, 0.04100477]\n",
      "epoch:3 batch_done:140 Gen Loss: 17.1085 Disc Loss: 0.194605 Q Losses: [0.0060283327, 0.036868788]\n",
      "epoch:3 batch_done:141 Gen Loss: 19.7767 Disc Loss: 0.0677182 Q Losses: [0.0069555733, 0.031702466]\n",
      "epoch:3 batch_done:142 Gen Loss: 17.1853 Disc Loss: 0.13084 Q Losses: [0.0063779177, 0.033589095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 batch_done:143 Gen Loss: 12.9817 Disc Loss: 0.119492 Q Losses: [0.0074349847, 0.031913541]\n",
      "epoch:3 batch_done:144 Gen Loss: 7.70457 Disc Loss: 0.114554 Q Losses: [0.025484338, 0.029728677]\n",
      "epoch:3 batch_done:145 Gen Loss: 3.91634 Disc Loss: 0.023758 Q Losses: [0.0077299532, 0.035258304]\n",
      "epoch:3 batch_done:146 Gen Loss: 23.954 Disc Loss: 0.282093 Q Losses: [0.012528456, 0.039527088]\n",
      "epoch:3 batch_done:147 Gen Loss: 27.4192 Disc Loss: 0.429301 Q Losses: [0.0064031528, 0.038068701]\n",
      "epoch:3 batch_done:148 Gen Loss: 23.0103 Disc Loss: 0.505432 Q Losses: [0.009650874, 0.034387387]\n",
      "epoch:3 batch_done:149 Gen Loss: 17.7617 Disc Loss: 0.00260457 Q Losses: [0.010571674, 0.033992577]\n",
      "epoch:3 batch_done:150 Gen Loss: 13.9653 Disc Loss: 0.00142405 Q Losses: [0.020429648, 0.032166287]\n",
      "epoch:3 batch_done:151 Gen Loss: 13.6372 Disc Loss: 0.00101511 Q Losses: [0.0072987671, 0.031735603]\n",
      "epoch:3 batch_done:152 Gen Loss: 11.687 Disc Loss: 0.00236401 Q Losses: [0.0077999029, 0.034539275]\n",
      "epoch:3 batch_done:153 Gen Loss: 5.84132 Disc Loss: 0.00389552 Q Losses: [0.011275766, 0.038573012]\n",
      "epoch:3 batch_done:154 Gen Loss: 5.99513 Disc Loss: 0.00470356 Q Losses: [0.0085005648, 0.033129565]\n",
      "epoch:3 batch_done:155 Gen Loss: 65.4354 Disc Loss: 2.91019 Q Losses: [0.0067277644, 0.039483383]\n",
      "epoch:3 batch_done:156 Gen Loss: 49.6492 Disc Loss: 8.66176 Q Losses: [0.0096555576, 0.037707411]\n",
      "epoch:3 batch_done:157 Gen Loss: 16.0593 Disc Loss: 1.09076 Q Losses: [0.010324979, 0.035063136]\n",
      "epoch:3 batch_done:158 Gen Loss: 50.5446 Disc Loss: 0.979496 Q Losses: [0.011870069, 0.041265845]\n",
      "epoch:3 batch_done:159 Gen Loss: 46.7649 Disc Loss: 0.626008 Q Losses: [0.01198158, 0.058789063]\n",
      "epoch:3 batch_done:160 Gen Loss: 35.3151 Disc Loss: 1.02859 Q Losses: [0.017011778, 0.044500243]\n",
      "epoch:3 batch_done:161 Gen Loss: 24.1056 Disc Loss: 0.236002 Q Losses: [0.010053197, 0.039647475]\n",
      "epoch:3 batch_done:162 Gen Loss: 14.2192 Disc Loss: 0.125625 Q Losses: [0.01318437, 0.042985585]\n",
      "epoch:3 batch_done:163 Gen Loss: 5.29255 Disc Loss: 0.0021482 Q Losses: [0.017841324, 0.046952523]\n",
      "epoch:3 batch_done:164 Gen Loss: 13.0962 Disc Loss: 0.187642 Q Losses: [0.039453857, 0.041137584]\n",
      "epoch:3 batch_done:165 Gen Loss: 13.8113 Disc Loss: 0.112729 Q Losses: [0.031923924, 0.054920144]\n",
      "epoch:3 batch_done:166 Gen Loss: 13.2617 Disc Loss: 0.0331178 Q Losses: [0.016093411, 0.04460866]\n",
      "epoch:3 batch_done:167 Gen Loss: 11.6859 Disc Loss: 0.0141839 Q Losses: [0.010110393, 0.033986684]\n",
      "epoch:3 batch_done:168 Gen Loss: 9.60401 Disc Loss: 0.0104872 Q Losses: [0.015987657, 0.042306855]\n",
      "epoch:3 batch_done:169 Gen Loss: 6.95687 Disc Loss: 0.0970649 Q Losses: [0.0086417533, 0.047720555]\n",
      "epoch:3 batch_done:170 Gen Loss: 5.18391 Disc Loss: 0.0110947 Q Losses: [0.011774624, 0.034389779]\n",
      "epoch:3 batch_done:171 Gen Loss: 4.0447 Disc Loss: 0.110389 Q Losses: [0.010615402, 0.041001495]\n",
      "epoch:3 batch_done:172 Gen Loss: 6.82771 Disc Loss: 0.0724882 Q Losses: [0.0064469459, 0.031030934]\n",
      "epoch:3 batch_done:173 Gen Loss: 6.90179 Disc Loss: 0.0338691 Q Losses: [0.0087326644, 0.031518094]\n",
      "epoch:3 batch_done:174 Gen Loss: 5.60958 Disc Loss: 0.176034 Q Losses: [0.0098175332, 0.030043002]\n",
      "epoch:3 batch_done:175 Gen Loss: 5.71993 Disc Loss: 0.0376154 Q Losses: [0.0073112091, 0.033965051]\n",
      "epoch:3 batch_done:176 Gen Loss: 5.99479 Disc Loss: 0.129655 Q Losses: [0.0070867445, 0.033036079]\n",
      "epoch:3 batch_done:177 Gen Loss: 6.33394 Disc Loss: 0.036381 Q Losses: [0.0082355756, 0.028664924]\n",
      "epoch:3 batch_done:178 Gen Loss: 6.08572 Disc Loss: 0.0342701 Q Losses: [0.0069430023, 0.03064495]\n",
      "epoch:3 batch_done:179 Gen Loss: 4.94329 Disc Loss: 0.14248 Q Losses: [0.0040671285, 0.028263278]\n",
      "epoch:3 batch_done:180 Gen Loss: 7.06877 Disc Loss: 0.0950939 Q Losses: [0.010062538, 0.032483634]\n",
      "epoch:3 batch_done:181 Gen Loss: 7.04204 Disc Loss: 0.0286002 Q Losses: [0.011136421, 0.032499053]\n",
      "epoch:3 batch_done:182 Gen Loss: 5.59058 Disc Loss: 0.0381781 Q Losses: [0.00959168, 0.031757556]\n",
      "epoch:3 batch_done:183 Gen Loss: 4.36317 Disc Loss: 0.217655 Q Losses: [0.0082017668, 0.031551603]\n",
      "epoch:3 batch_done:184 Gen Loss: 10.6742 Disc Loss: 0.135551 Q Losses: [0.010686241, 0.025449825]\n",
      "epoch:3 batch_done:185 Gen Loss: 13.5198 Disc Loss: 0.0580658 Q Losses: [0.0082115689, 0.029366422]\n",
      "epoch:3 batch_done:186 Gen Loss: 9.53593 Disc Loss: 0.174206 Q Losses: [0.0079569966, 0.047503121]\n",
      "epoch:3 batch_done:187 Gen Loss: 7.24034 Disc Loss: 0.0224015 Q Losses: [0.006414332, 0.025821662]\n",
      "epoch:3 batch_done:188 Gen Loss: 9.02838 Disc Loss: 0.0157359 Q Losses: [0.0065306653, 0.043526042]\n",
      "epoch:3 batch_done:189 Gen Loss: 4.74456 Disc Loss: 0.0302843 Q Losses: [0.0071425694, 0.026083592]\n",
      "epoch:3 batch_done:190 Gen Loss: 37.3796 Disc Loss: 0.887865 Q Losses: [0.0045425035, 0.031684633]\n",
      "epoch:3 batch_done:191 Gen Loss: 35.6152 Disc Loss: 3.67499 Q Losses: [0.011794034, 0.028458964]\n",
      "epoch:3 batch_done:192 Gen Loss: 26.1483 Disc Loss: 0.770986 Q Losses: [0.0098313596, 0.02570487]\n",
      "epoch:3 batch_done:193 Gen Loss: 18.9925 Disc Loss: 0.0019059 Q Losses: [0.0061851675, 0.030623749]\n",
      "epoch:3 batch_done:194 Gen Loss: 13.9819 Disc Loss: 0.00139007 Q Losses: [0.013464905, 0.029370653]\n",
      "epoch:3 batch_done:195 Gen Loss: 8.45115 Disc Loss: 0.000163018 Q Losses: [0.0068153804, 0.034341782]\n",
      "epoch:3 batch_done:196 Gen Loss: 9.34602 Disc Loss: 7.56167e-05 Q Losses: [0.010345905, 0.037523545]\n",
      "epoch:3 batch_done:197 Gen Loss: 15.4723 Disc Loss: 0.151617 Q Losses: [0.013998996, 0.026275933]\n",
      "epoch:3 batch_done:198 Gen Loss: 19.5615 Disc Loss: 0.0159129 Q Losses: [0.0093044937, 0.034429021]\n",
      "epoch:3 batch_done:199 Gen Loss: 19.8724 Disc Loss: 0.00731381 Q Losses: [0.005953989, 0.029647052]\n",
      "epoch:3 batch_done:200 Gen Loss: 17.5043 Disc Loss: 0.000333476 Q Losses: [0.010583402, 0.038907703]\n",
      "epoch:3 batch_done:201 Gen Loss: 16.9864 Disc Loss: 0.00366542 Q Losses: [0.0077186837, 0.028447304]\n",
      "epoch:3 batch_done:202 Gen Loss: 12.5168 Disc Loss: 0.107174 Q Losses: [0.0083697308, 0.034622796]\n",
      "epoch:3 batch_done:203 Gen Loss: 9.85797 Disc Loss: 0.000969892 Q Losses: [0.012125689, 0.027997237]\n",
      "epoch:3 batch_done:204 Gen Loss: 7.05273 Disc Loss: 0.00092987 Q Losses: [0.020166282, 0.029483851]\n",
      "epoch:3 batch_done:205 Gen Loss: 6.12285 Disc Loss: 0.0110068 Q Losses: [0.0075689191, 0.038613513]\n",
      "epoch:3 batch_done:206 Gen Loss: 5.6149 Disc Loss: 0.0136546 Q Losses: [0.0066119609, 0.034293927]\n",
      "epoch:3 batch_done:207 Gen Loss: 45.0545 Disc Loss: 0.859794 Q Losses: [0.0078177322, 0.02866222]\n",
      "epoch:4 batch_done:1 Gen Loss: 42.201 Disc Loss: 3.92338 Q Losses: [0.0095528811, 0.026468173]\n",
      "epoch:4 batch_done:2 Gen Loss: 32.4605 Disc Loss: 0.460098 Q Losses: [0.011542729, 0.02912645]\n",
      "epoch:4 batch_done:3 Gen Loss: 23.0765 Disc Loss: 0.253484 Q Losses: [0.012059363, 0.032741018]\n",
      "epoch:4 batch_done:4 Gen Loss: 15.1728 Disc Loss: 0.0529247 Q Losses: [0.0069854092, 0.03074527]\n",
      "epoch:4 batch_done:5 Gen Loss: 10.0521 Disc Loss: 0.00397622 Q Losses: [0.012721464, 0.038736977]\n",
      "epoch:4 batch_done:6 Gen Loss: 6.54446 Disc Loss: 0.00187059 Q Losses: [0.0090150461, 0.034732454]\n",
      "epoch:4 batch_done:7 Gen Loss: 6.49474 Disc Loss: 0.025735 Q Losses: [0.010115801, 0.037539423]\n",
      "epoch:4 batch_done:8 Gen Loss: 7.25367 Disc Loss: 0.0189725 Q Losses: [0.010356804, 0.044407256]\n",
      "epoch:4 batch_done:9 Gen Loss: 7.39481 Disc Loss: 0.00521117 Q Losses: [0.0085428767, 0.034352858]\n",
      "epoch:4 batch_done:10 Gen Loss: 7.45715 Disc Loss: 0.0370546 Q Losses: [0.0060250191, 0.029739572]\n",
      "epoch:4 batch_done:11 Gen Loss: 8.04331 Disc Loss: 0.0297901 Q Losses: [0.0091902893, 0.030359022]\n",
      "epoch:4 batch_done:12 Gen Loss: 7.69312 Disc Loss: 0.0104072 Q Losses: [0.0089184046, 0.029248117]\n",
      "epoch:4 batch_done:13 Gen Loss: 7.29761 Disc Loss: 0.00474024 Q Losses: [0.010364696, 0.028802754]\n",
      "epoch:4 batch_done:14 Gen Loss: 15.9225 Disc Loss: 0.14154 Q Losses: [0.0056651463, 0.026990339]\n",
      "epoch:4 batch_done:15 Gen Loss: 18.3056 Disc Loss: 0.030698 Q Losses: [0.0064105489, 0.030955136]\n",
      "epoch:4 batch_done:16 Gen Loss: 15.0051 Disc Loss: 0.153244 Q Losses: [0.0084948372, 0.027209494]\n",
      "epoch:4 batch_done:17 Gen Loss: 4.70774 Disc Loss: 0.283675 Q Losses: [0.0076899114, 0.030268539]\n",
      "epoch:4 batch_done:18 Gen Loss: 14.6622 Disc Loss: 0.145727 Q Losses: [0.0055654221, 0.026871257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:19 Gen Loss: 13.3046 Disc Loss: 0.0635628 Q Losses: [0.0060394742, 0.035183191]\n",
      "epoch:4 batch_done:20 Gen Loss: 10.3333 Disc Loss: 0.0842475 Q Losses: [0.007869442, 0.029867353]\n",
      "epoch:4 batch_done:21 Gen Loss: 6.15065 Disc Loss: 0.0310025 Q Losses: [0.0087430095, 0.027193511]\n",
      "epoch:4 batch_done:22 Gen Loss: 6.07519 Disc Loss: 0.0187998 Q Losses: [0.0076252907, 0.038470179]\n",
      "epoch:4 batch_done:23 Gen Loss: 8.39556 Disc Loss: 0.0425076 Q Losses: [0.0065127006, 0.029388074]\n",
      "epoch:4 batch_done:24 Gen Loss: 8.26201 Disc Loss: 0.00726266 Q Losses: [0.0078040082, 0.027993556]\n",
      "epoch:4 batch_done:25 Gen Loss: 7.03397 Disc Loss: 0.0309396 Q Losses: [0.0093066432, 0.036001958]\n",
      "epoch:4 batch_done:26 Gen Loss: 6.64008 Disc Loss: 0.0166305 Q Losses: [0.006976312, 0.02798618]\n",
      "epoch:4 batch_done:27 Gen Loss: 59.7875 Disc Loss: 1.34006 Q Losses: [0.013749868, 0.031068208]\n",
      "epoch:4 batch_done:28 Gen Loss: 34.068 Disc Loss: 9.64606 Q Losses: [0.016286889, 0.032121189]\n",
      "epoch:4 batch_done:29 Gen Loss: 19.3451 Disc Loss: 0.567523 Q Losses: [0.0099194786, 0.042381063]\n",
      "epoch:4 batch_done:30 Gen Loss: 5.28737 Disc Loss: 0.0192764 Q Losses: [0.011028254, 0.038609687]\n",
      "epoch:4 batch_done:31 Gen Loss: 49.811 Disc Loss: 1.33894 Q Losses: [0.010574762, 0.032250337]\n",
      "epoch:4 batch_done:32 Gen Loss: 55.1049 Disc Loss: 1.42663 Q Losses: [0.0099665392, 0.049444627]\n",
      "epoch:4 batch_done:33 Gen Loss: 48.4922 Disc Loss: 1.09341 Q Losses: [0.015216039, 0.039228]\n",
      "epoch:4 batch_done:34 Gen Loss: 33.1581 Disc Loss: 1.03998 Q Losses: [0.010634904, 0.033950936]\n",
      "epoch:4 batch_done:35 Gen Loss: 17.0614 Disc Loss: 0.0237352 Q Losses: [0.016786186, 0.038991034]\n",
      "epoch:4 batch_done:36 Gen Loss: 6.18642 Disc Loss: 0.0526887 Q Losses: [0.020371471, 0.044692762]\n",
      "epoch:4 batch_done:37 Gen Loss: 33.2291 Disc Loss: 0.759333 Q Losses: [0.017469373, 0.042652391]\n",
      "epoch:4 batch_done:38 Gen Loss: 32.974 Disc Loss: 1.01991 Q Losses: [0.018010132, 0.037881501]\n",
      "epoch:4 batch_done:39 Gen Loss: 23.4338 Disc Loss: 0.807016 Q Losses: [0.014630761, 0.037841436]\n",
      "epoch:4 batch_done:40 Gen Loss: 12.2974 Disc Loss: 0.139585 Q Losses: [0.0073411441, 0.034882121]\n",
      "epoch:4 batch_done:41 Gen Loss: 4.45 Disc Loss: 0.00402726 Q Losses: [0.013319802, 0.037685838]\n",
      "epoch:4 batch_done:42 Gen Loss: 41.1071 Disc Loss: 1.29518 Q Losses: [0.0086298604, 0.031386767]\n",
      "epoch:4 batch_done:43 Gen Loss: 32.1291 Disc Loss: 2.51178 Q Losses: [0.015145979, 0.04128575]\n",
      "epoch:4 batch_done:44 Gen Loss: 14.3748 Disc Loss: 1.76408 Q Losses: [0.0081810029, 0.034894463]\n",
      "epoch:4 batch_done:45 Gen Loss: 1.12728 Disc Loss: 0.290029 Q Losses: [0.0077692061, 0.052308209]\n",
      "epoch:4 batch_done:46 Gen Loss: 44.6265 Disc Loss: 3.94635 Q Losses: [0.0077065639, 0.043714833]\n",
      "epoch:4 batch_done:47 Gen Loss: 43.8802 Disc Loss: 1.92036 Q Losses: [0.0062063225, 0.039798997]\n",
      "epoch:4 batch_done:48 Gen Loss: 34.7634 Disc Loss: 1.00514 Q Losses: [0.010395955, 0.036742959]\n",
      "epoch:4 batch_done:49 Gen Loss: 23.4173 Disc Loss: 0.55143 Q Losses: [0.0090853302, 0.03749387]\n",
      "epoch:4 batch_done:50 Gen Loss: 14.4045 Disc Loss: 0.235296 Q Losses: [0.0086178891, 0.036415696]\n",
      "epoch:4 batch_done:51 Gen Loss: 7.00873 Disc Loss: 0.00225449 Q Losses: [0.01012742, 0.045913555]\n",
      "epoch:4 batch_done:52 Gen Loss: 4.26141 Disc Loss: 0.0421709 Q Losses: [0.0085039716, 0.039752841]\n",
      "epoch:4 batch_done:53 Gen Loss: 11.9272 Disc Loss: 0.285969 Q Losses: [0.0090924874, 0.038257509]\n",
      "epoch:4 batch_done:54 Gen Loss: 13.6027 Disc Loss: 0.0462362 Q Losses: [0.0090741273, 0.032966465]\n",
      "epoch:4 batch_done:55 Gen Loss: 11.5552 Disc Loss: 0.122399 Q Losses: [0.0050098654, 0.037365805]\n",
      "epoch:4 batch_done:56 Gen Loss: 9.27244 Disc Loss: 0.124751 Q Losses: [0.0080987271, 0.030326733]\n",
      "epoch:4 batch_done:57 Gen Loss: 6.97419 Disc Loss: 0.00843054 Q Losses: [0.0089185303, 0.057259373]\n",
      "epoch:4 batch_done:58 Gen Loss: 6.40447 Disc Loss: 0.120128 Q Losses: [0.0084814243, 0.028233955]\n",
      "epoch:4 batch_done:59 Gen Loss: 4.91918 Disc Loss: 0.00676065 Q Losses: [0.0096018529, 0.030345373]\n",
      "epoch:4 batch_done:60 Gen Loss: 4.87052 Disc Loss: 0.156818 Q Losses: [0.0076615904, 0.027102379]\n",
      "epoch:4 batch_done:61 Gen Loss: 5.85665 Disc Loss: 0.0311942 Q Losses: [0.007954401, 0.028808493]\n",
      "epoch:4 batch_done:62 Gen Loss: 5.35194 Disc Loss: 0.0584094 Q Losses: [0.0063114464, 0.030277472]\n",
      "epoch:4 batch_done:63 Gen Loss: 5.70048 Disc Loss: 0.0202771 Q Losses: [0.0090645608, 0.031750929]\n",
      "epoch:4 batch_done:64 Gen Loss: 4.88092 Disc Loss: 0.11339 Q Losses: [0.0044877846, 0.023552276]\n",
      "epoch:4 batch_done:65 Gen Loss: 6.07866 Disc Loss: 0.0463011 Q Losses: [0.009592928, 0.029297501]\n",
      "epoch:4 batch_done:66 Gen Loss: 5.49634 Disc Loss: 0.108247 Q Losses: [0.025145642, 0.023626201]\n",
      "epoch:4 batch_done:67 Gen Loss: 4.90689 Disc Loss: 0.0520424 Q Losses: [0.01802149, 0.025638178]\n",
      "epoch:4 batch_done:68 Gen Loss: 5.53774 Disc Loss: 0.0104647 Q Losses: [0.0098245628, 0.03138376]\n",
      "epoch:4 batch_done:69 Gen Loss: 5.63656 Disc Loss: 0.0182496 Q Losses: [0.011508737, 0.027218632]\n",
      "epoch:4 batch_done:70 Gen Loss: 5.28469 Disc Loss: 0.0843903 Q Losses: [0.0058550164, 0.024686966]\n",
      "epoch:4 batch_done:71 Gen Loss: 6.11867 Disc Loss: 0.0357978 Q Losses: [0.013764985, 0.021836931]\n",
      "epoch:4 batch_done:72 Gen Loss: 3.56649 Disc Loss: 0.181955 Q Losses: [0.0053763734, 0.026669033]\n",
      "epoch:4 batch_done:73 Gen Loss: 8.44217 Disc Loss: 0.279636 Q Losses: [0.011234898, 0.032734819]\n",
      "epoch:4 batch_done:74 Gen Loss: 9.46804 Disc Loss: 0.0524188 Q Losses: [0.010884127, 0.022137668]\n",
      "epoch:4 batch_done:75 Gen Loss: 7.40738 Disc Loss: 0.0372581 Q Losses: [0.0054074558, 0.020593934]\n",
      "epoch:4 batch_done:76 Gen Loss: 4.37505 Disc Loss: 0.167278 Q Losses: [0.0075292918, 0.025683556]\n",
      "epoch:4 batch_done:77 Gen Loss: 21.0904 Disc Loss: 0.420244 Q Losses: [0.0066362182, 0.024276935]\n",
      "epoch:4 batch_done:78 Gen Loss: 21.6197 Disc Loss: 0.565613 Q Losses: [0.0057450389, 0.022277266]\n",
      "epoch:4 batch_done:79 Gen Loss: 16.3838 Disc Loss: 0.283377 Q Losses: [0.0066626519, 0.021438792]\n",
      "epoch:4 batch_done:80 Gen Loss: 11.3722 Disc Loss: 0.076996 Q Losses: [0.010040785, 0.01991367]\n",
      "epoch:4 batch_done:81 Gen Loss: 8.0817 Disc Loss: 0.00102191 Q Losses: [0.0043159407, 0.024245854]\n",
      "epoch:4 batch_done:82 Gen Loss: 6.41761 Disc Loss: 0.0451202 Q Losses: [0.0074800691, 0.022084983]\n",
      "epoch:4 batch_done:83 Gen Loss: 8.20414 Disc Loss: 0.0475909 Q Losses: [0.011588527, 0.02532899]\n",
      "epoch:4 batch_done:84 Gen Loss: 7.5286 Disc Loss: 0.0278491 Q Losses: [0.0090748537, 0.023805862]\n",
      "epoch:4 batch_done:85 Gen Loss: 6.74617 Disc Loss: 0.136662 Q Losses: [0.0061065876, 0.022806525]\n",
      "epoch:4 batch_done:86 Gen Loss: 10.2601 Disc Loss: 0.146196 Q Losses: [0.0062379539, 0.019894917]\n",
      "epoch:4 batch_done:87 Gen Loss: 9.1046 Disc Loss: 0.0723645 Q Losses: [0.0055624484, 0.022953294]\n",
      "epoch:4 batch_done:88 Gen Loss: 6.19816 Disc Loss: 0.109483 Q Losses: [0.0069623888, 0.024324432]\n",
      "epoch:4 batch_done:89 Gen Loss: 5.69708 Disc Loss: 0.0201798 Q Losses: [0.0069601373, 0.021200974]\n",
      "epoch:4 batch_done:90 Gen Loss: 7.16325 Disc Loss: 0.0546183 Q Losses: [0.010204128, 0.032835059]\n",
      "epoch:4 batch_done:91 Gen Loss: 27.0622 Disc Loss: 0.326468 Q Losses: [0.0080996938, 0.02413962]\n",
      "epoch:4 batch_done:92 Gen Loss: 26.9452 Disc Loss: 0.268534 Q Losses: [0.0076868762, 0.02276499]\n",
      "epoch:4 batch_done:93 Gen Loss: 20.1239 Disc Loss: 0.11208 Q Losses: [0.010052377, 0.029361367]\n",
      "epoch:4 batch_done:94 Gen Loss: 13.4829 Disc Loss: 0.0057903 Q Losses: [0.005243863, 0.024402838]\n",
      "epoch:4 batch_done:95 Gen Loss: 12.6498 Disc Loss: 0.0118901 Q Losses: [0.007418612, 0.027186947]\n",
      "epoch:4 batch_done:96 Gen Loss: 11.1148 Disc Loss: 0.0941689 Q Losses: [0.0092626922, 0.029569689]\n",
      "epoch:4 batch_done:97 Gen Loss: 10.097 Disc Loss: 0.0474666 Q Losses: [0.013341095, 0.02524478]\n",
      "epoch:4 batch_done:98 Gen Loss: 9.95165 Disc Loss: 0.0357765 Q Losses: [0.013683459, 0.025328988]\n",
      "epoch:4 batch_done:99 Gen Loss: 7.39037 Disc Loss: 0.0430085 Q Losses: [0.005795876, 0.02350862]\n",
      "epoch:4 batch_done:100 Gen Loss: 4.94594 Disc Loss: 0.00799661 Q Losses: [0.0093335491, 0.025616057]\n",
      "epoch:4 batch_done:101 Gen Loss: 5.59905 Disc Loss: 0.0163726 Q Losses: [0.0055185826, 0.022078097]\n",
      "epoch:4 batch_done:102 Gen Loss: 6.61674 Disc Loss: 0.0401355 Q Losses: [0.0078322804, 0.022493571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:103 Gen Loss: 6.19995 Disc Loss: 0.0433611 Q Losses: [0.0073911659, 0.031243203]\n",
      "epoch:4 batch_done:104 Gen Loss: 6.52055 Disc Loss: 0.0524944 Q Losses: [0.0091646546, 0.026277818]\n",
      "epoch:4 batch_done:105 Gen Loss: 6.78408 Disc Loss: 0.0224158 Q Losses: [0.010206357, 0.021852475]\n",
      "epoch:4 batch_done:106 Gen Loss: 9.71582 Disc Loss: 0.00534363 Q Losses: [0.0053689987, 0.019540131]\n",
      "epoch:4 batch_done:107 Gen Loss: 6.04723 Disc Loss: 0.00897131 Q Losses: [0.0098437751, 0.022153139]\n",
      "epoch:4 batch_done:108 Gen Loss: 7.33899 Disc Loss: 0.036366 Q Losses: [0.010633467, 0.026497684]\n",
      "epoch:4 batch_done:109 Gen Loss: 9.81307 Disc Loss: 0.00427509 Q Losses: [0.0058050943, 0.026478261]\n",
      "epoch:4 batch_done:110 Gen Loss: 8.17032 Disc Loss: 0.099668 Q Losses: [0.0064084278, 0.031325839]\n",
      "epoch:4 batch_done:111 Gen Loss: 8.98091 Disc Loss: 0.0326402 Q Losses: [0.0061392765, 0.022823472]\n",
      "epoch:4 batch_done:112 Gen Loss: 5.96937 Disc Loss: 0.128904 Q Losses: [0.018020542, 0.018778361]\n",
      "epoch:4 batch_done:113 Gen Loss: 8.48259 Disc Loss: 0.0039066 Q Losses: [0.0080855517, 0.02589684]\n",
      "epoch:4 batch_done:114 Gen Loss: 6.08753 Disc Loss: 0.0281506 Q Losses: [0.011418596, 0.0230202]\n",
      "epoch:4 batch_done:115 Gen Loss: 6.22209 Disc Loss: 0.0222201 Q Losses: [0.0078572175, 0.019947184]\n",
      "epoch:4 batch_done:116 Gen Loss: 11.4353 Disc Loss: 0.00054923 Q Losses: [0.0078305276, 0.020437296]\n",
      "epoch:4 batch_done:117 Gen Loss: 13.5088 Disc Loss: 0.122237 Q Losses: [0.0060540382, 0.022900382]\n",
      "epoch:4 batch_done:118 Gen Loss: 18.5675 Disc Loss: 0.00571065 Q Losses: [0.0073663043, 0.024709681]\n",
      "epoch:4 batch_done:119 Gen Loss: 17.4361 Disc Loss: 0.0571845 Q Losses: [0.0090194754, 0.020474151]\n",
      "epoch:4 batch_done:120 Gen Loss: 16.1191 Disc Loss: 0.00257387 Q Losses: [0.013816187, 0.030073646]\n",
      "epoch:4 batch_done:121 Gen Loss: 15.439 Disc Loss: 0.00956901 Q Losses: [0.01934127, 0.022496706]\n",
      "epoch:4 batch_done:122 Gen Loss: 14.3424 Disc Loss: 0.00121915 Q Losses: [0.0089799408, 0.024795186]\n",
      "epoch:4 batch_done:123 Gen Loss: 15.4955 Disc Loss: 0.00707038 Q Losses: [0.0065951888, 0.024155317]\n",
      "epoch:4 batch_done:124 Gen Loss: 10.3009 Disc Loss: 0.00633806 Q Losses: [0.0086952914, 0.018832579]\n",
      "epoch:4 batch_done:125 Gen Loss: 6.86785 Disc Loss: 0.00537884 Q Losses: [0.0085633965, 0.020997208]\n",
      "epoch:4 batch_done:126 Gen Loss: 6.69198 Disc Loss: 0.0402499 Q Losses: [0.012979038, 0.023883173]\n",
      "epoch:4 batch_done:127 Gen Loss: 14.4277 Disc Loss: 0.0161572 Q Losses: [0.0086154565, 0.028913615]\n",
      "epoch:4 batch_done:128 Gen Loss: 6.21797 Disc Loss: 0.0299445 Q Losses: [0.0092657246, 0.022320163]\n",
      "epoch:4 batch_done:129 Gen Loss: 8.73324 Disc Loss: 0.00786927 Q Losses: [0.0088506378, 0.019942489]\n",
      "epoch:4 batch_done:130 Gen Loss: 30.0296 Disc Loss: 0.300564 Q Losses: [0.0084662819, 0.02376036]\n",
      "epoch:4 batch_done:131 Gen Loss: 32.2276 Disc Loss: 0.89119 Q Losses: [0.0074929856, 0.021718113]\n",
      "epoch:4 batch_done:132 Gen Loss: 22.9273 Disc Loss: 0.0294634 Q Losses: [0.0085450457, 0.026014943]\n",
      "epoch:4 batch_done:133 Gen Loss: 21.9053 Disc Loss: 0.000487174 Q Losses: [0.014786451, 0.028487479]\n",
      "epoch:4 batch_done:134 Gen Loss: 23.3042 Disc Loss: 0.000165591 Q Losses: [0.012165781, 0.032868259]\n",
      "epoch:4 batch_done:135 Gen Loss: 13.2067 Disc Loss: 0.00019604 Q Losses: [0.01024808, 0.028555382]\n",
      "epoch:4 batch_done:136 Gen Loss: 7.5562 Disc Loss: 0.042315 Q Losses: [0.0092299487, 0.036663428]\n",
      "epoch:4 batch_done:137 Gen Loss: 7.89714 Disc Loss: 0.00117255 Q Losses: [0.012578049, 0.053692885]\n",
      "epoch:4 batch_done:138 Gen Loss: 47.9776 Disc Loss: 0.411807 Q Losses: [0.015501887, 0.027353797]\n",
      "epoch:4 batch_done:139 Gen Loss: 48.0729 Disc Loss: 0.0493327 Q Losses: [0.015551487, 0.042165622]\n",
      "epoch:4 batch_done:140 Gen Loss: 36.3829 Disc Loss: 0.46587 Q Losses: [0.011067657, 0.028169036]\n",
      "epoch:4 batch_done:141 Gen Loss: 23.6022 Disc Loss: 0.019984 Q Losses: [0.0094458051, 0.028822599]\n",
      "epoch:4 batch_done:142 Gen Loss: 14.2655 Disc Loss: 0.000357478 Q Losses: [0.0078737848, 0.03378737]\n",
      "epoch:4 batch_done:143 Gen Loss: 6.67892 Disc Loss: 0.036099 Q Losses: [0.01271539, 0.032471716]\n",
      "epoch:4 batch_done:144 Gen Loss: 76.6499 Disc Loss: 3.37293 Q Losses: [0.011804287, 0.029033042]\n",
      "epoch:4 batch_done:145 Gen Loss: 54.145 Disc Loss: 17.6616 Q Losses: [0.019091513, 0.032996915]\n",
      "epoch:4 batch_done:146 Gen Loss: 30.1971 Disc Loss: 2.01942 Q Losses: [0.015613516, 0.032965552]\n",
      "epoch:4 batch_done:147 Gen Loss: 15.0653 Disc Loss: 0.0159464 Q Losses: [0.011707118, 0.040027488]\n",
      "epoch:4 batch_done:148 Gen Loss: 5.28016 Disc Loss: 0.000170263 Q Losses: [0.0082948478, 0.041261546]\n",
      "epoch:4 batch_done:149 Gen Loss: 19.1374 Disc Loss: 0.575534 Q Losses: [0.0098662525, 0.043820608]\n",
      "epoch:4 batch_done:150 Gen Loss: 21.5867 Disc Loss: 0.000117857 Q Losses: [0.013292757, 0.036831133]\n",
      "epoch:4 batch_done:151 Gen Loss: 19.628 Disc Loss: 2.43816e-05 Q Losses: [0.0082033975, 0.045895595]\n",
      "epoch:4 batch_done:152 Gen Loss: 16.6835 Disc Loss: 0.00887742 Q Losses: [0.014415147, 0.032955848]\n",
      "epoch:4 batch_done:153 Gen Loss: 12.7364 Disc Loss: 0.00631789 Q Losses: [0.013063924, 0.030672051]\n",
      "epoch:4 batch_done:154 Gen Loss: 8.67377 Disc Loss: 0.0414722 Q Losses: [0.010381827, 0.039614029]\n",
      "epoch:4 batch_done:155 Gen Loss: 4.6465 Disc Loss: 0.0746387 Q Losses: [0.014572086, 0.05284594]\n",
      "epoch:4 batch_done:156 Gen Loss: 4.81804 Disc Loss: 0.138555 Q Losses: [0.018526267, 0.040804766]\n",
      "epoch:4 batch_done:157 Gen Loss: 5.25964 Disc Loss: 0.0260957 Q Losses: [0.026012845, 0.032691516]\n",
      "epoch:4 batch_done:158 Gen Loss: 4.47976 Disc Loss: 0.184343 Q Losses: [0.018279552, 0.036680773]\n",
      "epoch:4 batch_done:159 Gen Loss: 5.15058 Disc Loss: 0.0551646 Q Losses: [0.0093140593, 0.030256592]\n",
      "epoch:4 batch_done:160 Gen Loss: 7.77341 Disc Loss: 0.155183 Q Losses: [0.011644822, 0.02928219]\n",
      "epoch:4 batch_done:161 Gen Loss: 5.67535 Disc Loss: 0.223055 Q Losses: [0.0069199391, 0.024918869]\n",
      "epoch:4 batch_done:162 Gen Loss: 8.34239 Disc Loss: 0.237122 Q Losses: [0.0080551682, 0.023977049]\n",
      "epoch:4 batch_done:163 Gen Loss: 7.10641 Disc Loss: 0.113241 Q Losses: [0.022793053, 0.029900786]\n",
      "epoch:4 batch_done:164 Gen Loss: 7.35901 Disc Loss: 0.209005 Q Losses: [0.015011954, 0.023131896]\n",
      "epoch:4 batch_done:165 Gen Loss: 5.71586 Disc Loss: 0.187873 Q Losses: [0.012597512, 0.024812326]\n",
      "epoch:4 batch_done:166 Gen Loss: 10.6912 Disc Loss: 0.166977 Q Losses: [0.0092093991, 0.020526001]\n",
      "epoch:4 batch_done:167 Gen Loss: 9.67423 Disc Loss: 0.0909492 Q Losses: [0.0075970897, 0.032651547]\n",
      "epoch:4 batch_done:168 Gen Loss: 5.3684 Disc Loss: 0.177648 Q Losses: [0.0074601853, 0.026201595]\n",
      "epoch:4 batch_done:169 Gen Loss: 7.18902 Disc Loss: 0.131282 Q Losses: [0.0083510783, 0.022753295]\n",
      "epoch:4 batch_done:170 Gen Loss: 5.93094 Disc Loss: 0.217651 Q Losses: [0.0098555414, 0.023094699]\n",
      "epoch:4 batch_done:171 Gen Loss: 5.49911 Disc Loss: 0.0143244 Q Losses: [0.010912188, 0.028719418]\n",
      "epoch:4 batch_done:172 Gen Loss: 5.50084 Disc Loss: 0.0420447 Q Losses: [0.0062096426, 0.026719604]\n",
      "epoch:4 batch_done:173 Gen Loss: 31.2218 Disc Loss: 0.783872 Q Losses: [0.015824685, 0.020570135]\n",
      "epoch:4 batch_done:174 Gen Loss: 31.6313 Disc Loss: 2.12111 Q Losses: [0.0071769152, 0.024618722]\n",
      "epoch:4 batch_done:175 Gen Loss: 25.2156 Disc Loss: 0.538171 Q Losses: [0.010271309, 0.029554635]\n",
      "epoch:4 batch_done:176 Gen Loss: 18.9641 Disc Loss: 0.0955638 Q Losses: [0.0078625157, 0.02750032]\n",
      "epoch:4 batch_done:177 Gen Loss: 13.2963 Disc Loss: 0.00236042 Q Losses: [0.016299371, 0.022171609]\n",
      "epoch:4 batch_done:178 Gen Loss: 8.84906 Disc Loss: 0.000856212 Q Losses: [0.0075910636, 0.024856545]\n",
      "epoch:4 batch_done:179 Gen Loss: 6.19629 Disc Loss: 0.00349814 Q Losses: [0.010217146, 0.02265808]\n",
      "epoch:4 batch_done:180 Gen Loss: 8.02883 Disc Loss: 0.10531 Q Losses: [0.0087859295, 0.019072084]\n",
      "epoch:4 batch_done:181 Gen Loss: 10.7493 Disc Loss: 0.0975151 Q Losses: [0.0097553376, 0.020442663]\n",
      "epoch:4 batch_done:182 Gen Loss: 9.35206 Disc Loss: 0.0414122 Q Losses: [0.0094114831, 0.024233587]\n",
      "epoch:4 batch_done:183 Gen Loss: 7.08785 Disc Loss: 0.0924699 Q Losses: [0.0054090051, 0.021003654]\n",
      "epoch:4 batch_done:184 Gen Loss: 6.38442 Disc Loss: 0.0480035 Q Losses: [0.0074229809, 0.020706479]\n",
      "epoch:4 batch_done:185 Gen Loss: 6.72148 Disc Loss: 0.08074 Q Losses: [0.0072513344, 0.020054724]\n",
      "epoch:4 batch_done:186 Gen Loss: 6.41347 Disc Loss: 0.0388402 Q Losses: [0.0073671769, 0.02691273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 batch_done:187 Gen Loss: 5.74636 Disc Loss: 0.0673696 Q Losses: [0.0098709418, 0.020083318]\n",
      "epoch:4 batch_done:188 Gen Loss: 5.85346 Disc Loss: 0.119004 Q Losses: [0.011751089, 0.02403765]\n",
      "epoch:4 batch_done:189 Gen Loss: 9.00249 Disc Loss: 0.00701323 Q Losses: [0.0076736668, 0.020028096]\n",
      "epoch:4 batch_done:190 Gen Loss: 9.55288 Disc Loss: 0.164419 Q Losses: [0.0073470701, 0.029691894]\n",
      "epoch:4 batch_done:191 Gen Loss: 8.92698 Disc Loss: 0.00926991 Q Losses: [0.010237096, 0.019587521]\n",
      "epoch:4 batch_done:192 Gen Loss: 7.87152 Disc Loss: 0.0260731 Q Losses: [0.0093355421, 0.020154472]\n",
      "epoch:4 batch_done:193 Gen Loss: 10.4842 Disc Loss: 0.168799 Q Losses: [0.010043226, 0.023827318]\n",
      "epoch:4 batch_done:194 Gen Loss: 6.81865 Disc Loss: 0.112379 Q Losses: [0.0099370237, 0.021470755]\n",
      "epoch:4 batch_done:195 Gen Loss: 7.50556 Disc Loss: 0.00904412 Q Losses: [0.012018226, 0.019833511]\n",
      "epoch:4 batch_done:196 Gen Loss: 16.6397 Disc Loss: 0.0259877 Q Losses: [0.0076929289, 0.021261327]\n",
      "epoch:4 batch_done:197 Gen Loss: 8.39849 Disc Loss: 0.0997793 Q Losses: [0.0064360513, 0.021111071]\n",
      "epoch:4 batch_done:198 Gen Loss: 13.1513 Disc Loss: 0.000881854 Q Losses: [0.0077172997, 0.01980057]\n",
      "epoch:4 batch_done:199 Gen Loss: 4.97439 Disc Loss: 0.0241579 Q Losses: [0.0085454565, 0.021982824]\n",
      "epoch:4 batch_done:200 Gen Loss: 5.43254 Disc Loss: 0.0140319 Q Losses: [0.0067345346, 0.021548267]\n",
      "epoch:4 batch_done:201 Gen Loss: 5.67002 Disc Loss: 0.0177765 Q Losses: [0.0077826018, 0.02581851]\n",
      "epoch:4 batch_done:202 Gen Loss: 9.28381 Disc Loss: 0.090038 Q Losses: [0.010123143, 0.022356072]\n",
      "epoch:4 batch_done:203 Gen Loss: 10.7593 Disc Loss: 0.0584534 Q Losses: [0.0052555031, 0.01908798]\n",
      "epoch:4 batch_done:204 Gen Loss: 7.75377 Disc Loss: 0.0417342 Q Losses: [0.0055625974, 0.021554815]\n",
      "epoch:4 batch_done:205 Gen Loss: 15.9306 Disc Loss: 0.00243686 Q Losses: [0.0054265084, 0.020773239]\n",
      "epoch:4 batch_done:206 Gen Loss: 5.93568 Disc Loss: 0.0072694 Q Losses: [0.0067240335, 0.022239501]\n",
      "epoch:4 batch_done:207 Gen Loss: 17.3422 Disc Loss: 0.0266914 Q Losses: [0.018609755, 0.021204602]\n",
      "epoch:5 batch_done:1 Gen Loss: 7.73418 Disc Loss: 0.00290083 Q Losses: [0.010506909, 0.021718614]\n",
      "epoch:5 batch_done:2 Gen Loss: 7.81819 Disc Loss: 0.00368671 Q Losses: [0.016023058, 0.018659782]\n",
      "epoch:5 batch_done:3 Gen Loss: 10.7156 Disc Loss: 0.136186 Q Losses: [0.0057629296, 0.021695718]\n",
      "epoch:5 batch_done:4 Gen Loss: 19.1107 Disc Loss: 0.0044638 Q Losses: [0.0076558986, 0.020573329]\n",
      "epoch:5 batch_done:5 Gen Loss: 11.0457 Disc Loss: 0.04039 Q Losses: [0.0082191313, 0.024126837]\n",
      "epoch:5 batch_done:6 Gen Loss: 13.1621 Disc Loss: 0.0147722 Q Losses: [0.0082882028, 0.017806664]\n",
      "epoch:5 batch_done:7 Gen Loss: 12.7274 Disc Loss: 0.00134594 Q Losses: [0.006816186, 0.024005115]\n",
      "epoch:5 batch_done:8 Gen Loss: 5.51293 Disc Loss: 0.0639652 Q Losses: [0.0084929885, 0.017746862]\n",
      "epoch:5 batch_done:9 Gen Loss: 7.8875 Disc Loss: 0.0301526 Q Losses: [0.010070359, 0.019547934]\n",
      "epoch:5 batch_done:10 Gen Loss: 14.73 Disc Loss: 0.0268067 Q Losses: [0.0078107007, 0.019980704]\n",
      "epoch:5 batch_done:11 Gen Loss: 6.20123 Disc Loss: 0.00241844 Q Losses: [0.0067448211, 0.021621048]\n",
      "epoch:5 batch_done:12 Gen Loss: 14.3979 Disc Loss: 0.000758884 Q Losses: [0.0099222306, 0.023802485]\n",
      "epoch:5 batch_done:13 Gen Loss: 7.35333 Disc Loss: 0.00286074 Q Losses: [0.0083795395, 0.021153839]\n",
      "epoch:5 batch_done:14 Gen Loss: 14.1005 Disc Loss: 0.160003 Q Losses: [0.0070427498, 0.020690408]\n",
      "epoch:5 batch_done:15 Gen Loss: 19.2947 Disc Loss: 0.0716332 Q Losses: [0.026947336, 0.01691675]\n",
      "epoch:5 batch_done:16 Gen Loss: 14.9718 Disc Loss: 0.0660383 Q Losses: [0.0074156485, 0.021488579]\n",
      "epoch:5 batch_done:17 Gen Loss: 18.9315 Disc Loss: 0.0106929 Q Losses: [0.0085683968, 0.020160813]\n",
      "epoch:5 batch_done:18 Gen Loss: 19.093 Disc Loss: 0.00322775 Q Losses: [0.0097196866, 0.024604548]\n",
      "epoch:5 batch_done:19 Gen Loss: 13.4287 Disc Loss: 0.00820354 Q Losses: [0.013050362, 0.020688431]\n",
      "epoch:5 batch_done:20 Gen Loss: 14.879 Disc Loss: 0.00043263 Q Losses: [0.0095408065, 0.022795847]\n",
      "epoch:5 batch_done:21 Gen Loss: 15.0827 Disc Loss: 0.0159456 Q Losses: [0.0085075395, 0.020019425]\n",
      "epoch:5 batch_done:22 Gen Loss: 9.14781 Disc Loss: 0.00473914 Q Losses: [0.0088726003, 0.019545425]\n",
      "epoch:5 batch_done:23 Gen Loss: 9.54701 Disc Loss: 0.000590701 Q Losses: [0.008573235, 0.021004781]\n",
      "epoch:5 batch_done:24 Gen Loss: 15.098 Disc Loss: 0.000520539 Q Losses: [0.008402192, 0.018763864]\n",
      "epoch:5 batch_done:25 Gen Loss: 12.0451 Disc Loss: 0.00267886 Q Losses: [0.0070070084, 0.018224964]\n",
      "epoch:5 batch_done:26 Gen Loss: 7.09909 Disc Loss: 0.00114606 Q Losses: [0.0057602152, 0.019266652]\n",
      "epoch:5 batch_done:27 Gen Loss: 9.77544 Disc Loss: 0.0973534 Q Losses: [0.010179397, 0.02306179]\n",
      "epoch:5 batch_done:28 Gen Loss: 20.4064 Disc Loss: 0.0380421 Q Losses: [0.0077292435, 0.023574598]\n",
      "epoch:5 batch_done:29 Gen Loss: 11.0311 Disc Loss: 0.0152708 Q Losses: [0.0086778346, 0.023976916]\n",
      "epoch:5 batch_done:30 Gen Loss: 20.5756 Disc Loss: 0.00608149 Q Losses: [0.0095584989, 0.021873107]\n",
      "epoch:5 batch_done:31 Gen Loss: 14.3542 Disc Loss: 0.00288426 Q Losses: [0.0066121463, 0.031469926]\n",
      "epoch:5 batch_done:32 Gen Loss: 10.1648 Disc Loss: 0.0439166 Q Losses: [0.0075014094, 0.020406727]\n",
      "epoch:5 batch_done:33 Gen Loss: 22.5742 Disc Loss: 0.00177941 Q Losses: [0.0095055364, 0.023611533]\n",
      "epoch:5 batch_done:34 Gen Loss: 20.6771 Disc Loss: 0.000707693 Q Losses: [0.0073672817, 0.024995785]\n",
      "epoch:5 batch_done:35 Gen Loss: 13.7018 Disc Loss: 0.000349538 Q Losses: [0.017136913, 0.027349338]\n",
      "epoch:5 batch_done:36 Gen Loss: 10.1496 Disc Loss: 0.00184004 Q Losses: [0.0089345947, 0.024152398]\n",
      "epoch:5 batch_done:37 Gen Loss: 6.64589 Disc Loss: 0.00239186 Q Losses: [0.0061509237, 0.020527456]\n",
      "epoch:5 batch_done:38 Gen Loss: 9.29278 Disc Loss: 0.0629829 Q Losses: [0.0097583923, 0.028897239]\n",
      "epoch:5 batch_done:39 Gen Loss: 8.8292 Disc Loss: 0.0141251 Q Losses: [0.007534463, 0.025673971]\n",
      "epoch:5 batch_done:40 Gen Loss: 13.4684 Disc Loss: 0.0181871 Q Losses: [0.013706245, 0.024686638]\n",
      "epoch:5 batch_done:41 Gen Loss: 19.0832 Disc Loss: 0.00322644 Q Losses: [0.0052001253, 0.022078682]\n",
      "epoch:5 batch_done:42 Gen Loss: 15.4066 Disc Loss: 0.0191505 Q Losses: [0.0076039536, 0.020912193]\n",
      "epoch:5 batch_done:43 Gen Loss: 6.5573 Disc Loss: 0.0696127 Q Losses: [0.0057465122, 0.017749067]\n",
      "epoch:5 batch_done:44 Gen Loss: 8.4963 Disc Loss: 0.00100162 Q Losses: [0.0094657, 0.022110149]\n",
      "epoch:5 batch_done:45 Gen Loss: 10.2555 Disc Loss: 0.0004285 Q Losses: [0.0068322527, 0.021867363]\n",
      "epoch:5 batch_done:46 Gen Loss: 7.01625 Disc Loss: 0.00233431 Q Losses: [0.019813696, 0.027596191]\n",
      "epoch:5 batch_done:47 Gen Loss: 5.91438 Disc Loss: 0.0201798 Q Losses: [0.0052065779, 0.017582837]\n",
      "epoch:5 batch_done:48 Gen Loss: 6.59216 Disc Loss: 0.00883233 Q Losses: [0.0092336368, 0.03019912]\n",
      "epoch:5 batch_done:49 Gen Loss: 10.1613 Disc Loss: 0.010671 Q Losses: [0.014189053, 0.027592879]\n",
      "epoch:5 batch_done:50 Gen Loss: 6.19604 Disc Loss: 0.00566846 Q Losses: [0.0093524763, 0.022242619]\n",
      "epoch:5 batch_done:51 Gen Loss: 9.38183 Disc Loss: 0.000671653 Q Losses: [0.021988487, 0.027656902]\n",
      "epoch:5 batch_done:52 Gen Loss: 9.69249 Disc Loss: 0.0101156 Q Losses: [0.011626991, 0.020995263]\n",
      "epoch:5 batch_done:53 Gen Loss: 5.68017 Disc Loss: 0.00764166 Q Losses: [0.0093226787, 0.019594025]\n",
      "epoch:5 batch_done:54 Gen Loss: 7.80553 Disc Loss: 0.00358358 Q Losses: [0.01233873, 0.024387041]\n",
      "epoch:5 batch_done:55 Gen Loss: 58.1064 Disc Loss: 1.53817 Q Losses: [0.007511002, 0.020279754]\n",
      "epoch:5 batch_done:56 Gen Loss: 38.7581 Disc Loss: 13.4917 Q Losses: [0.010151354, 0.021384675]\n",
      "epoch:5 batch_done:57 Gen Loss: 5.01853 Disc Loss: 0.0136732 Q Losses: [0.012997898, 0.040318087]\n",
      "epoch:5 batch_done:58 Gen Loss: 57.9953 Disc Loss: 5.36794 Q Losses: [0.011731511, 0.03056681]\n",
      "epoch:5 batch_done:59 Gen Loss: 58.926 Disc Loss: 1.89455 Q Losses: [0.013439078, 0.03213685]\n",
      "epoch:5 batch_done:60 Gen Loss: 44.6957 Disc Loss: 2.25816 Q Losses: [0.0097579435, 0.030968066]\n",
      "epoch:5 batch_done:61 Gen Loss: 34.8332 Disc Loss: 0.059717 Q Losses: [0.0096881306, 0.030456778]\n",
      "epoch:5 batch_done:62 Gen Loss: 27.5761 Disc Loss: 0.0114004 Q Losses: [0.011900255, 0.025801988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch_done:63 Gen Loss: 21.84 Disc Loss: 0.000280536 Q Losses: [0.012772675, 0.026760556]\n",
      "epoch:5 batch_done:64 Gen Loss: 17.0043 Disc Loss: 0.00198741 Q Losses: [0.006780623, 0.028725648]\n",
      "epoch:5 batch_done:65 Gen Loss: 12.5954 Disc Loss: 0.000111357 Q Losses: [0.0075317277, 0.026426248]\n",
      "epoch:5 batch_done:66 Gen Loss: 8.54451 Disc Loss: 0.000364677 Q Losses: [0.0088490658, 0.025772456]\n",
      "epoch:5 batch_done:67 Gen Loss: 5.12501 Disc Loss: 0.00970135 Q Losses: [0.0081538409, 0.024889329]\n",
      "epoch:5 batch_done:68 Gen Loss: 7.12772 Disc Loss: 0.0870605 Q Losses: [0.0073507959, 0.020487726]\n",
      "epoch:5 batch_done:69 Gen Loss: 7.31753 Disc Loss: 0.0175194 Q Losses: [0.0069438028, 0.022574216]\n",
      "epoch:5 batch_done:70 Gen Loss: 6.55982 Disc Loss: 0.0964055 Q Losses: [0.0083048381, 0.02235803]\n",
      "epoch:5 batch_done:71 Gen Loss: 5.89525 Disc Loss: 0.0822044 Q Losses: [0.013543971, 0.01965614]\n",
      "epoch:5 batch_done:72 Gen Loss: 6.55662 Disc Loss: 0.0314247 Q Losses: [0.010231342, 0.023781389]\n",
      "epoch:5 batch_done:73 Gen Loss: 6.57329 Disc Loss: 0.0738584 Q Losses: [0.0078889895, 0.017202817]\n",
      "epoch:5 batch_done:74 Gen Loss: 6.65076 Disc Loss: 0.0168123 Q Losses: [0.009357973, 0.017381601]\n",
      "epoch:5 batch_done:75 Gen Loss: 6.35834 Disc Loss: 0.0251189 Q Losses: [0.0080527794, 0.018591441]\n",
      "epoch:5 batch_done:76 Gen Loss: 6.55115 Disc Loss: 0.0633819 Q Losses: [0.0042688642, 0.016448738]\n",
      "epoch:5 batch_done:77 Gen Loss: 6.63015 Disc Loss: 0.0266408 Q Losses: [0.0085270498, 0.013464218]\n",
      "epoch:5 batch_done:78 Gen Loss: 6.12262 Disc Loss: 0.0823679 Q Losses: [0.0095516685, 0.015686039]\n",
      "epoch:5 batch_done:79 Gen Loss: 5.99324 Disc Loss: 0.0449591 Q Losses: [0.0072377892, 0.019302629]\n",
      "epoch:5 batch_done:80 Gen Loss: 5.63324 Disc Loss: 0.0608454 Q Losses: [0.0059395954, 0.017907711]\n",
      "epoch:5 batch_done:81 Gen Loss: 6.10544 Disc Loss: 0.0975144 Q Losses: [0.0087682875, 0.019453179]\n",
      "epoch:5 batch_done:82 Gen Loss: 5.32483 Disc Loss: 0.19151 Q Losses: [0.0062828958, 0.019186527]\n",
      "epoch:5 batch_done:83 Gen Loss: 7.01224 Disc Loss: 0.0803541 Q Losses: [0.0042975554, 0.017824091]\n",
      "epoch:5 batch_done:84 Gen Loss: 7.14433 Disc Loss: 0.0540766 Q Losses: [0.0065054325, 0.015927663]\n",
      "epoch:5 batch_done:85 Gen Loss: 6.25879 Disc Loss: 0.0698464 Q Losses: [0.0073294127, 0.013540036]\n",
      "epoch:5 batch_done:86 Gen Loss: 5.77565 Disc Loss: 0.0366672 Q Losses: [0.0069930488, 0.015319391]\n",
      "epoch:5 batch_done:87 Gen Loss: 31.14 Disc Loss: 0.958462 Q Losses: [0.01731172, 0.018012363]\n",
      "epoch:5 batch_done:88 Gen Loss: 31.0472 Disc Loss: 2.29908 Q Losses: [0.007636243, 0.017081715]\n",
      "epoch:5 batch_done:89 Gen Loss: 22.9993 Disc Loss: 1.15606 Q Losses: [0.010550618, 0.015052001]\n",
      "epoch:5 batch_done:90 Gen Loss: 16.7977 Disc Loss: 0.0210368 Q Losses: [0.0069731362, 0.014369459]\n",
      "epoch:5 batch_done:91 Gen Loss: 11.7377 Disc Loss: 0.000128209 Q Losses: [0.0063920347, 0.01974901]\n",
      "epoch:5 batch_done:92 Gen Loss: 9.79721 Disc Loss: 0.00350172 Q Losses: [0.0063356292, 0.016921077]\n",
      "epoch:5 batch_done:93 Gen Loss: 4.68927 Disc Loss: 0.0115619 Q Losses: [0.011756847, 0.020423748]\n",
      "epoch:5 batch_done:94 Gen Loss: 10.309 Disc Loss: 0.195958 Q Losses: [0.013742179, 0.016981298]\n",
      "epoch:5 batch_done:95 Gen Loss: 12.9838 Disc Loss: 0.0269944 Q Losses: [0.0083553512, 0.018423352]\n",
      "epoch:5 batch_done:96 Gen Loss: 13.2487 Disc Loss: 0.00765706 Q Losses: [0.0077130632, 0.019050119]\n",
      "epoch:5 batch_done:97 Gen Loss: 11.2724 Disc Loss: 0.0581739 Q Losses: [0.012708566, 0.015991051]\n",
      "epoch:5 batch_done:98 Gen Loss: 9.68697 Disc Loss: 0.022586 Q Losses: [0.00735129, 0.016263276]\n",
      "epoch:5 batch_done:99 Gen Loss: 5.66965 Disc Loss: 0.0216158 Q Losses: [0.006780955, 0.021200974]\n",
      "epoch:5 batch_done:100 Gen Loss: 5.30721 Disc Loss: 0.0608301 Q Losses: [0.0099619096, 0.019242015]\n",
      "epoch:5 batch_done:101 Gen Loss: 6.0903 Disc Loss: 0.0213123 Q Losses: [0.0071102539, 0.019948099]\n",
      "epoch:5 batch_done:102 Gen Loss: 34.1305 Disc Loss: 1.33867 Q Losses: [0.0093968809, 0.017682791]\n",
      "epoch:5 batch_done:103 Gen Loss: 35.1047 Disc Loss: 3.27281 Q Losses: [0.012102064, 0.017761946]\n",
      "epoch:5 batch_done:104 Gen Loss: 28.8003 Disc Loss: 1.26278 Q Losses: [0.0091054961, 0.028682016]\n",
      "epoch:5 batch_done:105 Gen Loss: 21.7164 Disc Loss: 0.0527496 Q Losses: [0.0093120076, 0.020671943]\n",
      "epoch:5 batch_done:106 Gen Loss: 18.6491 Disc Loss: 0.000111344 Q Losses: [0.0084593771, 0.015646851]\n",
      "epoch:5 batch_done:107 Gen Loss: 12.4925 Disc Loss: 8.69979e-05 Q Losses: [0.010477485, 0.018499561]\n",
      "epoch:5 batch_done:108 Gen Loss: 6.7672 Disc Loss: 0.000720931 Q Losses: [0.007759931, 0.018177252]\n",
      "epoch:5 batch_done:109 Gen Loss: 5.82624 Disc Loss: 0.0397272 Q Losses: [0.0089130485, 0.018774996]\n",
      "epoch:5 batch_done:110 Gen Loss: 8.22461 Disc Loss: 0.009223 Q Losses: [0.025198523, 0.019107355]\n",
      "epoch:5 batch_done:111 Gen Loss: 19.1927 Disc Loss: 0.239365 Q Losses: [0.0078267194, 0.020766038]\n",
      "epoch:5 batch_done:112 Gen Loss: 23.2214 Disc Loss: 0.129526 Q Losses: [0.0088915471, 0.017509419]\n",
      "epoch:5 batch_done:113 Gen Loss: 17.9182 Disc Loss: 0.13593 Q Losses: [0.010721308, 0.022869654]\n",
      "epoch:5 batch_done:114 Gen Loss: 13.1257 Disc Loss: 0.0248609 Q Losses: [0.013741576, 0.020235877]\n",
      "epoch:5 batch_done:115 Gen Loss: 11.0009 Disc Loss: 0.00755438 Q Losses: [0.0070604929, 0.022751976]\n",
      "epoch:5 batch_done:116 Gen Loss: 9.52199 Disc Loss: 0.0150766 Q Losses: [0.0081497468, 0.020950889]\n",
      "epoch:5 batch_done:117 Gen Loss: 4.98811 Disc Loss: 0.012411 Q Losses: [0.0054960675, 0.022363607]\n",
      "epoch:5 batch_done:118 Gen Loss: 5.95543 Disc Loss: 0.0565008 Q Losses: [0.013855764, 0.017494572]\n",
      "epoch:5 batch_done:119 Gen Loss: 8.52662 Disc Loss: 0.0650552 Q Losses: [0.0090582352, 0.021920787]\n",
      "epoch:5 batch_done:120 Gen Loss: 11.3935 Disc Loss: 0.0226991 Q Losses: [0.015513563, 0.016833132]\n",
      "epoch:5 batch_done:121 Gen Loss: 6.65952 Disc Loss: 0.43767 Q Losses: [0.012456574, 0.020740263]\n",
      "epoch:5 batch_done:122 Gen Loss: 3.63176 Disc Loss: 0.022027 Q Losses: [0.010159804, 0.016712822]\n",
      "epoch:5 batch_done:123 Gen Loss: 4.5393 Disc Loss: 0.013596 Q Losses: [0.0094093308, 0.017699026]\n",
      "epoch:5 batch_done:124 Gen Loss: 6.56301 Disc Loss: 0.0466833 Q Losses: [0.011653263, 0.016175732]\n",
      "epoch:5 batch_done:125 Gen Loss: 6.49744 Disc Loss: 0.0418071 Q Losses: [0.015158553, 0.015805595]\n",
      "epoch:5 batch_done:126 Gen Loss: 5.67831 Disc Loss: 0.0140416 Q Losses: [0.0057765171, 0.016909964]\n",
      "epoch:5 batch_done:127 Gen Loss: 6.00159 Disc Loss: 0.0200596 Q Losses: [0.013899995, 0.019889195]\n",
      "epoch:5 batch_done:128 Gen Loss: 6.01229 Disc Loss: 0.037727 Q Losses: [0.0094771888, 0.0144894]\n",
      "epoch:5 batch_done:129 Gen Loss: 6.03188 Disc Loss: 0.0314947 Q Losses: [0.0053375298, 0.015697937]\n",
      "epoch:5 batch_done:130 Gen Loss: 7.08064 Disc Loss: 0.0184366 Q Losses: [0.0055332836, 0.01578106]\n",
      "epoch:5 batch_done:131 Gen Loss: 6.04165 Disc Loss: 0.0113259 Q Losses: [0.0068251067, 0.012881207]\n",
      "epoch:5 batch_done:132 Gen Loss: 5.18416 Disc Loss: 0.0317734 Q Losses: [0.0055216081, 0.017550971]\n",
      "epoch:5 batch_done:133 Gen Loss: 4.84272 Disc Loss: 0.0332757 Q Losses: [0.007694168, 0.015650157]\n",
      "epoch:5 batch_done:134 Gen Loss: 5.30133 Disc Loss: 0.0502064 Q Losses: [0.00660777, 0.017326046]\n",
      "epoch:5 batch_done:135 Gen Loss: 5.94267 Disc Loss: 0.0150639 Q Losses: [0.010340817, 0.014180392]\n",
      "epoch:5 batch_done:136 Gen Loss: 8.1659 Disc Loss: 0.00825962 Q Losses: [0.01363362, 0.018614002]\n",
      "epoch:5 batch_done:137 Gen Loss: 7.52737 Disc Loss: 0.0798075 Q Losses: [0.0069668959, 0.017815286]\n",
      "epoch:5 batch_done:138 Gen Loss: 7.48125 Disc Loss: 0.0191825 Q Losses: [0.0089896787, 0.016701315]\n",
      "epoch:5 batch_done:139 Gen Loss: 7.25442 Disc Loss: 0.0147703 Q Losses: [0.010794642, 0.016167451]\n",
      "epoch:5 batch_done:140 Gen Loss: 9.47119 Disc Loss: 0.00874379 Q Losses: [0.010896414, 0.012771692]\n",
      "epoch:5 batch_done:141 Gen Loss: 5.34714 Disc Loss: 0.0612047 Q Losses: [0.006323386, 0.014788803]\n",
      "epoch:5 batch_done:142 Gen Loss: 9.08468 Disc Loss: 0.00787372 Q Losses: [0.0086886184, 0.019945763]\n",
      "epoch:5 batch_done:143 Gen Loss: 5.24975 Disc Loss: 0.0290851 Q Losses: [0.011087406, 0.01528412]\n",
      "epoch:5 batch_done:144 Gen Loss: 5.89265 Disc Loss: 0.0245043 Q Losses: [0.010198344, 0.014965459]\n",
      "epoch:5 batch_done:145 Gen Loss: 6.93687 Disc Loss: 0.00663222 Q Losses: [0.01325025, 0.018216781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch_done:146 Gen Loss: 7.47949 Disc Loss: 0.0617739 Q Losses: [0.0086725429, 0.01297467]\n",
      "epoch:5 batch_done:147 Gen Loss: 9.7752 Disc Loss: 0.11153 Q Losses: [0.016869484, 0.020635011]\n",
      "epoch:5 batch_done:148 Gen Loss: 4.91193 Disc Loss: 0.0298013 Q Losses: [0.0064505953, 0.016407654]\n",
      "epoch:5 batch_done:149 Gen Loss: 4.6399 Disc Loss: 0.0175776 Q Losses: [0.0053160787, 0.013435255]\n",
      "epoch:5 batch_done:150 Gen Loss: 12.6272 Disc Loss: 0.163669 Q Losses: [0.01122196, 0.018442377]\n",
      "epoch:5 batch_done:151 Gen Loss: 15.7824 Disc Loss: 0.0634452 Q Losses: [0.011812322, 0.015498128]\n",
      "epoch:5 batch_done:152 Gen Loss: 16.5299 Disc Loss: 0.0529576 Q Losses: [0.021552723, 0.016277663]\n",
      "epoch:5 batch_done:153 Gen Loss: 16.0087 Disc Loss: 0.0263576 Q Losses: [0.01818312, 0.013994822]\n",
      "epoch:5 batch_done:154 Gen Loss: 13.5899 Disc Loss: 0.00662768 Q Losses: [0.0076994617, 0.018630646]\n",
      "epoch:5 batch_done:155 Gen Loss: 17.1554 Disc Loss: 0.00308855 Q Losses: [0.0074448921, 0.014617158]\n",
      "epoch:5 batch_done:156 Gen Loss: 10.2911 Disc Loss: 0.000753646 Q Losses: [0.0063703884, 0.020431772]\n",
      "epoch:5 batch_done:157 Gen Loss: 16.1848 Disc Loss: 0.000680898 Q Losses: [0.0066461861, 0.014814129]\n",
      "epoch:5 batch_done:158 Gen Loss: 6.93086 Disc Loss: 0.0243013 Q Losses: [0.0071064159, 0.014320403]\n",
      "epoch:5 batch_done:159 Gen Loss: 7.81387 Disc Loss: 0.000795657 Q Losses: [0.0081740636, 0.015212741]\n",
      "epoch:5 batch_done:160 Gen Loss: 7.42951 Disc Loss: 0.00198433 Q Losses: [0.0075800996, 0.016140828]\n",
      "epoch:5 batch_done:161 Gen Loss: 8.94668 Disc Loss: 0.000318265 Q Losses: [0.0082493713, 0.017739877]\n",
      "epoch:5 batch_done:162 Gen Loss: 7.84874 Disc Loss: 0.0713293 Q Losses: [0.0067279814, 0.013360568]\n",
      "epoch:5 batch_done:163 Gen Loss: 12.8184 Disc Loss: 0.00527111 Q Losses: [0.0075217667, 0.014045539]\n",
      "epoch:5 batch_done:164 Gen Loss: 12.3816 Disc Loss: 0.00656219 Q Losses: [0.0062912959, 0.021403512]\n",
      "epoch:5 batch_done:165 Gen Loss: 14.5353 Disc Loss: 0.00719726 Q Losses: [0.0061402242, 0.019207675]\n",
      "epoch:5 batch_done:166 Gen Loss: 6.63216 Disc Loss: 0.101329 Q Losses: [0.0051499223, 0.020069584]\n",
      "epoch:5 batch_done:167 Gen Loss: 10.9746 Disc Loss: 0.0189049 Q Losses: [0.013456249, 0.014695121]\n",
      "epoch:5 batch_done:168 Gen Loss: 5.91783 Disc Loss: 0.00176263 Q Losses: [0.01570414, 0.016604714]\n",
      "epoch:5 batch_done:169 Gen Loss: 7.40736 Disc Loss: 0.000817034 Q Losses: [0.0056199171, 0.016967602]\n",
      "epoch:5 batch_done:170 Gen Loss: 40.3138 Disc Loss: 1.23376 Q Losses: [0.0083442451, 0.015356295]\n",
      "epoch:5 batch_done:171 Gen Loss: 24.6285 Disc Loss: 7.61962 Q Losses: [0.00645022, 0.018996624]\n",
      "epoch:5 batch_done:172 Gen Loss: 8.12955 Disc Loss: 0.00501759 Q Losses: [0.0068696621, 0.022430059]\n",
      "epoch:5 batch_done:173 Gen Loss: 1.18518 Disc Loss: 0.00877501 Q Losses: [0.0083649959, 0.025744]\n",
      "epoch:5 batch_done:174 Gen Loss: 11.8398 Disc Loss: 0.00011902 Q Losses: [0.0094620418, 0.018456118]\n",
      "epoch:5 batch_done:175 Gen Loss: 10.5919 Disc Loss: 0.00106308 Q Losses: [0.0073237102, 0.016474836]\n",
      "epoch:5 batch_done:176 Gen Loss: 16.8074 Disc Loss: 0.000292397 Q Losses: [0.011804342, 0.015274094]\n",
      "epoch:5 batch_done:177 Gen Loss: 11.4063 Disc Loss: 0.00723709 Q Losses: [0.023006462, 0.024460228]\n",
      "epoch:5 batch_done:178 Gen Loss: 13.1909 Disc Loss: 0.0358867 Q Losses: [0.028034076, 0.029708866]\n",
      "epoch:5 batch_done:179 Gen Loss: 10.1311 Disc Loss: 0.0941758 Q Losses: [0.014151547, 0.021399723]\n",
      "epoch:5 batch_done:180 Gen Loss: 6.61617 Disc Loss: 0.0606977 Q Losses: [0.011491131, 0.020803168]\n",
      "epoch:5 batch_done:181 Gen Loss: 9.9429 Disc Loss: 0.0742728 Q Losses: [0.01643642, 0.022570074]\n",
      "epoch:5 batch_done:182 Gen Loss: 11.4978 Disc Loss: 0.00162368 Q Losses: [0.017273314, 0.020989468]\n",
      "epoch:5 batch_done:183 Gen Loss: 17.1962 Disc Loss: 0.0427477 Q Losses: [0.019711744, 0.02981128]\n",
      "epoch:5 batch_done:184 Gen Loss: 16.9445 Disc Loss: 0.120406 Q Losses: [0.0095609268, 0.024112333]\n",
      "epoch:5 batch_done:185 Gen Loss: 12.4529 Disc Loss: 0.0318319 Q Losses: [0.01676061, 0.024211518]\n",
      "epoch:5 batch_done:186 Gen Loss: 7.49154 Disc Loss: 0.0144633 Q Losses: [0.025072299, 0.02356017]\n",
      "epoch:5 batch_done:187 Gen Loss: 6.17723 Disc Loss: 0.00745351 Q Losses: [0.013785563, 0.030409394]\n",
      "epoch:5 batch_done:188 Gen Loss: 6.44745 Disc Loss: 0.0125663 Q Losses: [0.010583512, 0.025474099]\n",
      "epoch:5 batch_done:189 Gen Loss: 5.60983 Disc Loss: 0.0846799 Q Losses: [0.0085667074, 0.023499057]\n",
      "epoch:5 batch_done:190 Gen Loss: 7.17979 Disc Loss: 0.0304016 Q Losses: [0.0083578769, 0.02058593]\n",
      "epoch:5 batch_done:191 Gen Loss: 7.05193 Disc Loss: 0.0550594 Q Losses: [0.007409784, 0.019184347]\n",
      "epoch:5 batch_done:192 Gen Loss: 12.1806 Disc Loss: 0.00327131 Q Losses: [0.0072808946, 0.017852629]\n",
      "epoch:5 batch_done:193 Gen Loss: 8.28188 Disc Loss: 0.0589154 Q Losses: [0.016892448, 0.020071477]\n",
      "epoch:5 batch_done:194 Gen Loss: 5.73535 Disc Loss: 0.0135947 Q Losses: [0.011543971, 0.013782527]\n",
      "epoch:5 batch_done:195 Gen Loss: 13.7119 Disc Loss: 0.200736 Q Losses: [0.0070598125, 0.016669951]\n",
      "epoch:5 batch_done:196 Gen Loss: 13.885 Disc Loss: 0.060952 Q Losses: [0.0070089693, 0.017397895]\n",
      "epoch:5 batch_done:197 Gen Loss: 12.4015 Disc Loss: 0.0199519 Q Losses: [0.007165669, 0.018332873]\n",
      "epoch:5 batch_done:198 Gen Loss: 10.7643 Disc Loss: 0.0143487 Q Losses: [0.0050784335, 0.014760269]\n",
      "epoch:5 batch_done:199 Gen Loss: 18.3482 Disc Loss: 0.0210984 Q Losses: [0.0061932057, 0.013907063]\n",
      "epoch:5 batch_done:200 Gen Loss: 6.53571 Disc Loss: 0.0190173 Q Losses: [0.0056943195, 0.015328292]\n",
      "epoch:5 batch_done:201 Gen Loss: 17.5507 Disc Loss: 0.00141149 Q Losses: [0.0074514886, 0.013323085]\n",
      "epoch:5 batch_done:202 Gen Loss: 6.66962 Disc Loss: 0.00266846 Q Losses: [0.0065155253, 0.018424541]\n",
      "epoch:5 batch_done:203 Gen Loss: 16.6419 Disc Loss: 0.00314524 Q Losses: [0.0062428163, 0.011053128]\n",
      "epoch:5 batch_done:204 Gen Loss: 18.8688 Disc Loss: 0.220251 Q Losses: [0.0076075876, 0.015743604]\n",
      "epoch:5 batch_done:205 Gen Loss: 18.3115 Disc Loss: 0.100917 Q Losses: [0.017870873, 0.022213563]\n",
      "epoch:5 batch_done:206 Gen Loss: 25.3829 Disc Loss: 0.104065 Q Losses: [0.006979173, 0.020169545]\n",
      "epoch:5 batch_done:207 Gen Loss: 14.689 Disc Loss: 0.00979836 Q Losses: [0.0091501381, 0.016842693]\n",
      "epoch:6 batch_done:1 Gen Loss: 26.5413 Disc Loss: 0.0223447 Q Losses: [0.010095, 0.015974132]\n",
      "epoch:6 batch_done:2 Gen Loss: 22.7339 Disc Loss: 0.00281623 Q Losses: [0.011716496, 0.020146172]\n",
      "epoch:6 batch_done:3 Gen Loss: 12.6482 Disc Loss: 0.00125532 Q Losses: [0.013170912, 0.0156173]\n",
      "epoch:6 batch_done:4 Gen Loss: 8.05884 Disc Loss: 0.00211877 Q Losses: [0.015230207, 0.0167583]\n",
      "epoch:6 batch_done:5 Gen Loss: 14.8987 Disc Loss: 0.154142 Q Losses: [0.0092370752, 0.016973557]\n",
      "epoch:6 batch_done:6 Gen Loss: 21.0435 Disc Loss: 0.0158681 Q Losses: [0.010500138, 0.013453502]\n",
      "epoch:6 batch_done:7 Gen Loss: 21.0859 Disc Loss: 0.0594934 Q Losses: [0.006092221, 0.020071557]\n",
      "epoch:6 batch_done:8 Gen Loss: 16.6326 Disc Loss: 0.121171 Q Losses: [0.010559543, 0.027292978]\n",
      "epoch:6 batch_done:9 Gen Loss: 16.5952 Disc Loss: 0.00235615 Q Losses: [0.0086573884, 0.014189994]\n",
      "epoch:6 batch_done:10 Gen Loss: 9.28559 Disc Loss: 0.00136824 Q Losses: [0.0078519741, 0.014577619]\n",
      "epoch:6 batch_done:11 Gen Loss: 8.91797 Disc Loss: 0.00205979 Q Losses: [0.0057768021, 0.015757978]\n",
      "epoch:6 batch_done:12 Gen Loss: 25.1683 Disc Loss: 0.000599261 Q Losses: [0.015782494, 0.016996609]\n",
      "epoch:6 batch_done:13 Gen Loss: 24.09 Disc Loss: 0.000600082 Q Losses: [0.0080817919, 0.013648641]\n",
      "epoch:6 batch_done:14 Gen Loss: 20.1118 Disc Loss: 0.00319591 Q Losses: [0.011684053, 0.017653856]\n",
      "epoch:6 batch_done:15 Gen Loss: 10.6493 Disc Loss: 0.000416648 Q Losses: [0.012153539, 0.016216598]\n",
      "epoch:6 batch_done:16 Gen Loss: 18.2896 Disc Loss: 0.00212224 Q Losses: [0.010555139, 0.01626217]\n",
      "epoch:6 batch_done:17 Gen Loss: 6.28458 Disc Loss: 0.00283448 Q Losses: [0.0065607796, 0.018952735]\n",
      "epoch:6 batch_done:18 Gen Loss: 16.2188 Disc Loss: 0.000778385 Q Losses: [0.0091833659, 0.01345606]\n",
      "epoch:6 batch_done:19 Gen Loss: 5.2315 Disc Loss: 0.0257141 Q Losses: [0.0053927428, 0.019463817]\n",
      "epoch:6 batch_done:20 Gen Loss: 22.9513 Disc Loss: 0.000729226 Q Losses: [0.0053717261, 0.021383416]\n",
      "epoch:6 batch_done:21 Gen Loss: 20.7092 Disc Loss: 0.000300254 Q Losses: [0.014387164, 0.019880636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:22 Gen Loss: 7.74853 Disc Loss: 0.0579722 Q Losses: [0.0055020549, 0.019230161]\n",
      "epoch:6 batch_done:23 Gen Loss: 26.5339 Disc Loss: 0.00120508 Q Losses: [0.010254559, 0.018500488]\n",
      "epoch:6 batch_done:24 Gen Loss: 26.6606 Disc Loss: 0.0160866 Q Losses: [0.010852981, 0.024331026]\n",
      "epoch:6 batch_done:25 Gen Loss: 23.2102 Disc Loss: 0.022958 Q Losses: [0.0072791865, 0.023493458]\n",
      "epoch:6 batch_done:26 Gen Loss: 19.727 Disc Loss: 0.00115134 Q Losses: [0.009368117, 0.016415009]\n",
      "epoch:6 batch_done:27 Gen Loss: 17.2771 Disc Loss: 0.00295719 Q Losses: [0.006941637, 0.015898572]\n",
      "epoch:6 batch_done:28 Gen Loss: 15.1177 Disc Loss: 0.00760687 Q Losses: [0.0078790579, 0.015121412]\n",
      "epoch:6 batch_done:29 Gen Loss: 12.5309 Disc Loss: 0.00143446 Q Losses: [0.0095826332, 0.019100849]\n",
      "epoch:6 batch_done:30 Gen Loss: 9.55183 Disc Loss: 0.00703849 Q Losses: [0.012889825, 0.016149435]\n",
      "epoch:6 batch_done:31 Gen Loss: 7.55016 Disc Loss: 0.000709535 Q Losses: [0.012472567, 0.02447886]\n",
      "epoch:6 batch_done:32 Gen Loss: 5.40982 Disc Loss: 0.00974812 Q Losses: [0.011005001, 0.017659811]\n",
      "epoch:6 batch_done:33 Gen Loss: 7.24324 Disc Loss: 0.0491702 Q Losses: [0.0088997558, 0.016338916]\n",
      "epoch:6 batch_done:34 Gen Loss: 8.47051 Disc Loss: 0.00324249 Q Losses: [0.0065598548, 0.015781559]\n",
      "epoch:6 batch_done:35 Gen Loss: 12.3556 Disc Loss: 0.0041794 Q Losses: [0.0091998996, 0.017630514]\n",
      "epoch:6 batch_done:36 Gen Loss: 7.78703 Disc Loss: 0.0072005 Q Losses: [0.0049081696, 0.015981166]\n",
      "epoch:6 batch_done:37 Gen Loss: 10.5642 Disc Loss: 0.0139163 Q Losses: [0.0084405616, 0.01234812]\n",
      "epoch:6 batch_done:38 Gen Loss: 7.1335 Disc Loss: 0.00105382 Q Losses: [0.012059822, 0.023617886]\n",
      "epoch:6 batch_done:39 Gen Loss: 12.832 Disc Loss: 0.00251187 Q Losses: [0.0082857236, 0.017409034]\n",
      "epoch:6 batch_done:40 Gen Loss: 5.49353 Disc Loss: 0.0185157 Q Losses: [0.016069226, 0.015526556]\n",
      "epoch:6 batch_done:41 Gen Loss: 11.4215 Disc Loss: 0.0080328 Q Losses: [0.0079705808, 0.021550041]\n",
      "epoch:6 batch_done:42 Gen Loss: 11.3492 Disc Loss: 0.000475241 Q Losses: [0.0062881215, 0.012480205]\n",
      "epoch:6 batch_done:43 Gen Loss: 6.05097 Disc Loss: 0.00384323 Q Losses: [0.0099917818, 0.014569337]\n",
      "epoch:6 batch_done:44 Gen Loss: 11.5324 Disc Loss: 0.120682 Q Losses: [0.0091029294, 0.01441985]\n",
      "epoch:6 batch_done:45 Gen Loss: 23.8737 Disc Loss: 0.031865 Q Losses: [0.022446483, 0.013352706]\n",
      "epoch:6 batch_done:46 Gen Loss: 20.6694 Disc Loss: 0.0213135 Q Losses: [0.0059129596, 0.016500488]\n",
      "epoch:6 batch_done:47 Gen Loss: 17.6503 Disc Loss: 0.00725684 Q Losses: [0.01627237, 0.013584638]\n",
      "epoch:6 batch_done:48 Gen Loss: 24.1536 Disc Loss: 0.0149566 Q Losses: [0.0090986723, 0.013350328]\n",
      "epoch:6 batch_done:49 Gen Loss: 13.4864 Disc Loss: 0.00131409 Q Losses: [0.013043878, 0.011680029]\n",
      "epoch:6 batch_done:50 Gen Loss: 20.5019 Disc Loss: 0.0172747 Q Losses: [0.027960483, 0.013660651]\n",
      "epoch:6 batch_done:51 Gen Loss: 15.0215 Disc Loss: 0.00098708 Q Losses: [0.010291921, 0.014608813]\n",
      "epoch:6 batch_done:52 Gen Loss: 11.4508 Disc Loss: 0.0017482 Q Losses: [0.0080837766, 0.017608367]\n",
      "epoch:6 batch_done:53 Gen Loss: 21.8774 Disc Loss: 0.00283206 Q Losses: [0.013750141, 0.018502429]\n",
      "epoch:6 batch_done:54 Gen Loss: 13.5571 Disc Loss: 0.000435656 Q Losses: [0.0076121483, 0.015145941]\n",
      "epoch:6 batch_done:55 Gen Loss: 19.0655 Disc Loss: 0.000440628 Q Losses: [0.0058853743, 0.024180377]\n",
      "epoch:6 batch_done:56 Gen Loss: 14.4014 Disc Loss: 0.0203406 Q Losses: [0.0086666215, 0.033842772]\n",
      "epoch:6 batch_done:57 Gen Loss: 25.3893 Disc Loss: 0.0241273 Q Losses: [0.0088755256, 0.020145625]\n",
      "epoch:6 batch_done:58 Gen Loss: 21.5231 Disc Loss: 0.00636243 Q Losses: [0.009892622, 0.033812016]\n",
      "epoch:6 batch_done:59 Gen Loss: 19.6176 Disc Loss: 0.000694636 Q Losses: [0.0073079746, 0.046292558]\n",
      "epoch:6 batch_done:60 Gen Loss: 17.6142 Disc Loss: 0.000319574 Q Losses: [0.01034497, 0.037666369]\n",
      "epoch:6 batch_done:61 Gen Loss: 13.946 Disc Loss: 0.000339776 Q Losses: [0.0073999665, 0.036022872]\n",
      "epoch:6 batch_done:62 Gen Loss: 11.6015 Disc Loss: 0.0012098 Q Losses: [0.0075687878, 0.034139208]\n",
      "epoch:6 batch_done:63 Gen Loss: 9.27045 Disc Loss: 0.00543724 Q Losses: [0.0092994906, 0.025301766]\n",
      "epoch:6 batch_done:64 Gen Loss: 6.80167 Disc Loss: 0.0130545 Q Losses: [0.012580623, 0.02532794]\n",
      "epoch:6 batch_done:65 Gen Loss: 9.14107 Disc Loss: 0.00244356 Q Losses: [0.0073366622, 0.025734827]\n",
      "epoch:6 batch_done:66 Gen Loss: 12.0924 Disc Loss: 0.00048123 Q Losses: [0.0062562195, 0.033216827]\n",
      "epoch:6 batch_done:67 Gen Loss: 6.95019 Disc Loss: 0.00747589 Q Losses: [0.010546176, 0.033235464]\n",
      "epoch:6 batch_done:68 Gen Loss: 6.7654 Disc Loss: 0.00320905 Q Losses: [0.0068280539, 0.020753212]\n",
      "epoch:6 batch_done:69 Gen Loss: 5.8641 Disc Loss: 0.0229147 Q Losses: [0.0074748923, 0.01967762]\n",
      "epoch:6 batch_done:70 Gen Loss: 8.23116 Disc Loss: 0.00193149 Q Losses: [0.0070413817, 0.029288651]\n",
      "epoch:6 batch_done:71 Gen Loss: 15.9899 Disc Loss: 0.0116651 Q Losses: [0.0076974812, 0.016410593]\n",
      "epoch:6 batch_done:72 Gen Loss: 6.33626 Disc Loss: 0.0033357 Q Losses: [0.011899079, 0.022921614]\n",
      "epoch:6 batch_done:73 Gen Loss: 8.83014 Disc Loss: 0.0332329 Q Losses: [0.017254546, 0.016647711]\n",
      "epoch:6 batch_done:74 Gen Loss: 20.0945 Disc Loss: 0.00439043 Q Losses: [0.0068082381, 0.015507943]\n",
      "epoch:6 batch_done:75 Gen Loss: 5.76954 Disc Loss: 0.00498412 Q Losses: [0.0079718493, 0.020837342]\n",
      "epoch:6 batch_done:76 Gen Loss: 24.8243 Disc Loss: 0.000963007 Q Losses: [0.0067912028, 0.016167272]\n",
      "epoch:6 batch_done:77 Gen Loss: 24.2898 Disc Loss: 0.000631061 Q Losses: [0.0064044679, 0.018791854]\n",
      "epoch:6 batch_done:78 Gen Loss: 12.5794 Disc Loss: 0.00245627 Q Losses: [0.015657036, 0.014978774]\n",
      "epoch:6 batch_done:79 Gen Loss: 10.8332 Disc Loss: 0.000178477 Q Losses: [0.006864964, 0.013050657]\n",
      "epoch:6 batch_done:80 Gen Loss: 11.8012 Disc Loss: 0.10236 Q Losses: [0.015912171, 0.015221847]\n",
      "epoch:6 batch_done:81 Gen Loss: 14.5423 Disc Loss: 0.00424544 Q Losses: [0.0063721878, 0.014556987]\n",
      "epoch:6 batch_done:82 Gen Loss: 16.708 Disc Loss: 0.00401796 Q Losses: [0.007195815, 0.013606351]\n",
      "epoch:6 batch_done:83 Gen Loss: 24.204 Disc Loss: 0.00395402 Q Losses: [0.0065661343, 0.012405167]\n",
      "epoch:6 batch_done:84 Gen Loss: 9.70744 Disc Loss: 0.0352159 Q Losses: [0.012188632, 0.021150205]\n",
      "epoch:6 batch_done:85 Gen Loss: 21.8344 Disc Loss: 0.0139041 Q Losses: [0.0069985953, 0.012715168]\n",
      "epoch:6 batch_done:86 Gen Loss: 14.86 Disc Loss: 0.000841976 Q Losses: [0.010341997, 0.014261464]\n",
      "epoch:6 batch_done:87 Gen Loss: 23.4149 Disc Loss: 0.0001448 Q Losses: [0.0072083608, 0.010329688]\n",
      "epoch:6 batch_done:88 Gen Loss: 14.4123 Disc Loss: 0.000342217 Q Losses: [0.0057768272, 0.013716063]\n",
      "epoch:6 batch_done:89 Gen Loss: 21.1898 Disc Loss: 0.000122827 Q Losses: [0.013792519, 0.016019028]\n",
      "epoch:6 batch_done:90 Gen Loss: 11.2509 Disc Loss: 0.000143779 Q Losses: [0.020894226, 0.012354275]\n",
      "epoch:6 batch_done:91 Gen Loss: 14.7482 Disc Loss: 0.00288845 Q Losses: [0.0074157994, 0.011352533]\n",
      "epoch:6 batch_done:92 Gen Loss: 5.7907 Disc Loss: 0.0106887 Q Losses: [0.0058216313, 0.013436162]\n",
      "epoch:6 batch_done:93 Gen Loss: 18.0971 Disc Loss: 0.00399513 Q Losses: [0.0072797784, 0.013122054]\n",
      "epoch:6 batch_done:94 Gen Loss: 8.50875 Disc Loss: 0.00499965 Q Losses: [0.0059563145, 0.016289212]\n",
      "epoch:6 batch_done:95 Gen Loss: 16.4299 Disc Loss: 0.000977892 Q Losses: [0.011835414, 0.012433451]\n",
      "epoch:6 batch_done:96 Gen Loss: 17.446 Disc Loss: 0.164231 Q Losses: [0.0062174434, 0.01187966]\n",
      "epoch:6 batch_done:97 Gen Loss: 30.3851 Disc Loss: 0.0313017 Q Losses: [0.0068532256, 0.013300956]\n",
      "epoch:6 batch_done:98 Gen Loss: 28.7491 Disc Loss: 0.0841749 Q Losses: [0.0058561536, 0.015523557]\n",
      "epoch:6 batch_done:99 Gen Loss: 27.8279 Disc Loss: 0.211894 Q Losses: [0.0078934254, 0.011581009]\n",
      "epoch:6 batch_done:100 Gen Loss: 18.0712 Disc Loss: 0.000769931 Q Losses: [0.0098521486, 0.011545188]\n",
      "epoch:6 batch_done:101 Gen Loss: 14.9295 Disc Loss: 4.97721e-06 Q Losses: [0.0061152643, 0.013864805]\n",
      "epoch:6 batch_done:102 Gen Loss: 26.3418 Disc Loss: 4.53018e-06 Q Losses: [0.01909494, 0.01168512]\n",
      "epoch:6 batch_done:103 Gen Loss: 21.7723 Disc Loss: 6.57474e-06 Q Losses: [0.0051654358, 0.010804594]\n",
      "epoch:6 batch_done:104 Gen Loss: 32.1033 Disc Loss: 3.53911e-06 Q Losses: [0.019599229, 0.014339887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:105 Gen Loss: 27.4163 Disc Loss: 2.17747e-06 Q Losses: [0.0065350006, 0.011223446]\n",
      "epoch:6 batch_done:106 Gen Loss: 17.6013 Disc Loss: 2.44756e-06 Q Losses: [0.0061010709, 0.014706844]\n",
      "epoch:6 batch_done:107 Gen Loss: 29.4495 Disc Loss: 8.86628e-07 Q Losses: [0.0094757825, 0.011670329]\n",
      "epoch:6 batch_done:108 Gen Loss: 19.2566 Disc Loss: 3.71986e-06 Q Losses: [0.01293704, 0.013587818]\n",
      "epoch:6 batch_done:109 Gen Loss: 31.3567 Disc Loss: 6.82186e-06 Q Losses: [0.0077541517, 0.013906224]\n",
      "epoch:6 batch_done:110 Gen Loss: 30.8933 Disc Loss: 1.63542e-06 Q Losses: [0.0087075112, 0.013102865]\n",
      "epoch:6 batch_done:111 Gen Loss: 25.6756 Disc Loss: 2.5854e-06 Q Losses: [0.0078541469, 0.014117986]\n",
      "epoch:6 batch_done:112 Gen Loss: 20.3434 Disc Loss: 2.29854e-06 Q Losses: [0.0078779142, 0.013443852]\n",
      "epoch:6 batch_done:113 Gen Loss: 18.339 Disc Loss: 1.2207e-05 Q Losses: [0.0076565947, 0.010075855]\n",
      "epoch:6 batch_done:114 Gen Loss: 14.0126 Disc Loss: 1.08462e-05 Q Losses: [0.0060477513, 0.010059594]\n",
      "epoch:6 batch_done:115 Gen Loss: 11.6301 Disc Loss: 1.68301e-05 Q Losses: [0.0087489802, 0.01183521]\n",
      "epoch:6 batch_done:116 Gen Loss: 15.912 Disc Loss: 7.71757e-06 Q Losses: [0.010302591, 0.012532183]\n",
      "epoch:6 batch_done:117 Gen Loss: 19.636 Disc Loss: 4.10538e-06 Q Losses: [0.0085954992, 0.009729431]\n",
      "epoch:6 batch_done:118 Gen Loss: 9.0066 Disc Loss: 0.000143673 Q Losses: [0.0077854781, 0.0098400116]\n",
      "epoch:6 batch_done:119 Gen Loss: 26.6024 Disc Loss: 2.95419e-06 Q Losses: [0.0059353551, 0.011280929]\n",
      "epoch:6 batch_done:120 Gen Loss: 22.1986 Disc Loss: 6.97155e-06 Q Losses: [0.0068972334, 0.011980493]\n",
      "epoch:6 batch_done:121 Gen Loss: 12.5029 Disc Loss: 5.47341e-06 Q Losses: [0.0071885241, 0.013355613]\n",
      "epoch:6 batch_done:122 Gen Loss: 8.52614 Disc Loss: 0.000322488 Q Losses: [0.0069919396, 0.011222547]\n",
      "epoch:6 batch_done:123 Gen Loss: 21.1409 Disc Loss: 2.47867e-05 Q Losses: [0.011843428, 0.0092994114]\n",
      "epoch:6 batch_done:124 Gen Loss: 13.0996 Disc Loss: 1.53087e-05 Q Losses: [0.011353264, 0.012931485]\n",
      "epoch:6 batch_done:125 Gen Loss: 9.31795 Disc Loss: 0.000170774 Q Losses: [0.0066249543, 0.016394483]\n",
      "epoch:6 batch_done:126 Gen Loss: 11.3234 Disc Loss: 1.82691e-05 Q Losses: [0.0081301248, 0.011681202]\n",
      "epoch:6 batch_done:127 Gen Loss: 17.2907 Disc Loss: 1.05247e-05 Q Losses: [0.0060397498, 0.01249495]\n",
      "epoch:6 batch_done:128 Gen Loss: 9.50662 Disc Loss: 8.42536e-05 Q Losses: [0.007042069, 0.011316692]\n",
      "epoch:6 batch_done:129 Gen Loss: 24.5533 Disc Loss: 7.83472e-06 Q Losses: [0.0068502421, 0.015210339]\n",
      "epoch:6 batch_done:130 Gen Loss: 23.5214 Disc Loss: 6.53682e-05 Q Losses: [0.0097538857, 0.010311173]\n",
      "epoch:6 batch_done:131 Gen Loss: 16.908 Disc Loss: 5.93274e-06 Q Losses: [0.0071828337, 0.012761148]\n",
      "epoch:6 batch_done:132 Gen Loss: 11.1438 Disc Loss: 0.000167123 Q Losses: [0.0091997674, 0.012504889]\n",
      "epoch:6 batch_done:133 Gen Loss: 9.6669 Disc Loss: 8.17959e-05 Q Losses: [0.0087695867, 0.013296594]\n",
      "epoch:6 batch_done:134 Gen Loss: 6.66912 Disc Loss: 0.00160453 Q Losses: [0.010368677, 0.012166291]\n",
      "epoch:6 batch_done:135 Gen Loss: 5.82121 Disc Loss: 0.00860623 Q Losses: [0.006357688, 0.011081921]\n",
      "epoch:6 batch_done:136 Gen Loss: 13.9665 Disc Loss: 0.000120703 Q Losses: [0.013431432, 0.011564702]\n",
      "epoch:6 batch_done:137 Gen Loss: 6.84235 Disc Loss: 0.00172331 Q Losses: [0.0068395021, 0.015138146]\n",
      "epoch:6 batch_done:138 Gen Loss: 17.686 Disc Loss: 0.000283868 Q Losses: [0.0083399434, 0.01386218]\n",
      "epoch:6 batch_done:139 Gen Loss: 8.33619 Disc Loss: 0.000824679 Q Losses: [0.016567461, 0.016102988]\n",
      "epoch:6 batch_done:140 Gen Loss: 7.21331 Disc Loss: 0.00187441 Q Losses: [0.0096506793, 0.012399393]\n",
      "epoch:6 batch_done:141 Gen Loss: 17.7901 Disc Loss: 0.00151317 Q Losses: [0.0056619686, 0.010114479]\n",
      "epoch:6 batch_done:142 Gen Loss: 10.8611 Disc Loss: 7.27741e-05 Q Losses: [0.0093981605, 0.013883155]\n",
      "epoch:6 batch_done:143 Gen Loss: 14.5613 Disc Loss: 0.000310583 Q Losses: [0.009173682, 0.012210486]\n",
      "epoch:6 batch_done:144 Gen Loss: 11.8187 Disc Loss: 0.000407229 Q Losses: [0.0092468206, 0.011009178]\n",
      "epoch:6 batch_done:145 Gen Loss: 6.79944 Disc Loss: 0.00571954 Q Losses: [0.0058913901, 0.0139846]\n",
      "epoch:6 batch_done:146 Gen Loss: 11.6733 Disc Loss: 0.00173828 Q Losses: [0.0094004152, 0.011368033]\n",
      "epoch:6 batch_done:147 Gen Loss: 13.3384 Disc Loss: 0.00102082 Q Losses: [0.0075475997, 0.017916605]\n",
      "epoch:6 batch_done:148 Gen Loss: 10.3883 Disc Loss: 0.000604629 Q Losses: [0.010564, 0.012729846]\n",
      "epoch:6 batch_done:149 Gen Loss: 5.77948 Disc Loss: 0.0115888 Q Losses: [0.0072806375, 0.012422025]\n",
      "epoch:6 batch_done:150 Gen Loss: 8.6188 Disc Loss: 0.00574291 Q Losses: [0.0078785876, 0.014700175]\n",
      "epoch:6 batch_done:151 Gen Loss: 17.2686 Disc Loss: 0.00161621 Q Losses: [0.0079197465, 0.012776213]\n",
      "epoch:6 batch_done:152 Gen Loss: 11.6311 Disc Loss: 0.00221335 Q Losses: [0.0076591736, 0.013087984]\n",
      "epoch:6 batch_done:153 Gen Loss: 6.41251 Disc Loss: 0.00422002 Q Losses: [0.012843828, 0.0099065769]\n",
      "epoch:6 batch_done:154 Gen Loss: 6.188 Disc Loss: 0.00785831 Q Losses: [0.0081954151, 0.015329609]\n",
      "epoch:6 batch_done:155 Gen Loss: 10.3074 Disc Loss: 0.00346236 Q Losses: [0.0083450191, 0.013124283]\n",
      "epoch:6 batch_done:156 Gen Loss: 10.2746 Disc Loss: 0.000438541 Q Losses: [0.011672202, 0.011729161]\n",
      "epoch:6 batch_done:157 Gen Loss: 6.29435 Disc Loss: 0.00338731 Q Losses: [0.0065220697, 0.012600282]\n",
      "epoch:6 batch_done:158 Gen Loss: 11.6126 Disc Loss: 0.000247586 Q Losses: [0.0077302563, 0.015621029]\n",
      "epoch:6 batch_done:159 Gen Loss: 6.58228 Disc Loss: 0.00214324 Q Losses: [0.0076416419, 0.011505191]\n",
      "epoch:6 batch_done:160 Gen Loss: 5.68813 Disc Loss: 0.02688 Q Losses: [0.013129432, 0.013782563]\n",
      "epoch:6 batch_done:161 Gen Loss: 7.62217 Disc Loss: 0.00146785 Q Losses: [0.0075438214, 0.014909726]\n",
      "epoch:6 batch_done:162 Gen Loss: 8.24629 Disc Loss: 0.00140801 Q Losses: [0.011348873, 0.016415339]\n",
      "epoch:6 batch_done:163 Gen Loss: 8.21121 Disc Loss: 0.000897825 Q Losses: [0.0061033368, 0.014769715]\n",
      "epoch:6 batch_done:164 Gen Loss: 5.90559 Disc Loss: 0.0116714 Q Losses: [0.0098162368, 0.012739319]\n",
      "epoch:6 batch_done:165 Gen Loss: 8.48365 Disc Loss: 0.000616228 Q Losses: [0.0085682226, 0.013176324]\n",
      "epoch:6 batch_done:166 Gen Loss: 5.95633 Disc Loss: 0.0135179 Q Losses: [0.0088794492, 0.012843376]\n",
      "epoch:6 batch_done:167 Gen Loss: 6.00532 Disc Loss: 0.01385 Q Losses: [0.0081834011, 0.010114421]\n",
      "epoch:6 batch_done:168 Gen Loss: 8.33535 Disc Loss: 0.100649 Q Losses: [0.018236693, 0.01090064]\n",
      "epoch:6 batch_done:169 Gen Loss: 15.027 Disc Loss: 0.0427896 Q Losses: [0.0061050742, 0.019104227]\n",
      "epoch:6 batch_done:170 Gen Loss: 24.0071 Disc Loss: 0.0443753 Q Losses: [0.0092515154, 0.011953]\n",
      "epoch:6 batch_done:171 Gen Loss: 16.9186 Disc Loss: 0.00033412 Q Losses: [0.0084873587, 0.013693636]\n",
      "epoch:6 batch_done:172 Gen Loss: 19.4073 Disc Loss: 0.000768656 Q Losses: [0.014777866, 0.014467579]\n",
      "epoch:6 batch_done:173 Gen Loss: 11.9059 Disc Loss: 0.000317308 Q Losses: [0.0071979202, 0.012873959]\n",
      "epoch:6 batch_done:174 Gen Loss: 8.33004 Disc Loss: 0.000578435 Q Losses: [0.01492389, 0.019195743]\n",
      "epoch:6 batch_done:175 Gen Loss: 12.7409 Disc Loss: 0.000503559 Q Losses: [0.010694613, 0.012077701]\n",
      "epoch:6 batch_done:176 Gen Loss: 16.3385 Disc Loss: 0.000453011 Q Losses: [0.0065487283, 0.0099896733]\n",
      "epoch:6 batch_done:177 Gen Loss: 6.52479 Disc Loss: 0.00380998 Q Losses: [0.0079361768, 0.010935717]\n",
      "epoch:6 batch_done:178 Gen Loss: 15.4374 Disc Loss: 0.0140568 Q Losses: [0.0086480891, 0.010914873]\n",
      "epoch:6 batch_done:179 Gen Loss: 16.5836 Disc Loss: 0.00160184 Q Losses: [0.0083948802, 0.015394356]\n",
      "epoch:6 batch_done:180 Gen Loss: 10.346 Disc Loss: 0.00171242 Q Losses: [0.0057837609, 0.014133926]\n",
      "epoch:6 batch_done:181 Gen Loss: 15.5267 Disc Loss: 0.001977 Q Losses: [0.0094460212, 0.013311399]\n",
      "epoch:6 batch_done:182 Gen Loss: 13.4532 Disc Loss: 0.00500774 Q Losses: [0.0088136708, 0.0091837216]\n",
      "epoch:6 batch_done:183 Gen Loss: 9.24431 Disc Loss: 0.00048124 Q Losses: [0.0094787451, 0.012508251]\n",
      "epoch:6 batch_done:184 Gen Loss: 13.8778 Disc Loss: 0.00023428 Q Losses: [0.007563632, 0.022823187]\n",
      "epoch:6 batch_done:185 Gen Loss: 12.6013 Disc Loss: 0.000571872 Q Losses: [0.009081563, 0.025550809]\n",
      "epoch:6 batch_done:186 Gen Loss: 5.51002 Disc Loss: 0.0300297 Q Losses: [0.0088163, 0.017603483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 batch_done:187 Gen Loss: 5.42607 Disc Loss: 0.0125404 Q Losses: [0.0041331802, 0.026133325]\n",
      "epoch:6 batch_done:188 Gen Loss: 20.099 Disc Loss: 0.000930515 Q Losses: [0.013439046, 0.024340978]\n",
      "epoch:6 batch_done:189 Gen Loss: 8.33665 Disc Loss: 0.000641793 Q Losses: [0.011209752, 0.023367291]\n",
      "epoch:6 batch_done:190 Gen Loss: 22.93 Disc Loss: 0.002635 Q Losses: [0.01093493, 0.035221197]\n",
      "epoch:6 batch_done:191 Gen Loss: 14.5872 Disc Loss: 0.0460927 Q Losses: [0.0076855901, 0.025922533]\n",
      "epoch:6 batch_done:192 Gen Loss: 7.82404 Disc Loss: 0.00243326 Q Losses: [0.011266829, 0.04653028]\n",
      "epoch:6 batch_done:193 Gen Loss: 12.5835 Disc Loss: 0.00106029 Q Losses: [0.0084126526, 0.039569318]\n",
      "epoch:6 batch_done:194 Gen Loss: 13.122 Disc Loss: 0.00243597 Q Losses: [0.0083760787, 0.067093298]\n",
      "epoch:6 batch_done:195 Gen Loss: 8.50619 Disc Loss: 0.000767732 Q Losses: [0.0085589821, 0.040436298]\n",
      "epoch:6 batch_done:196 Gen Loss: 7.29719 Disc Loss: 0.00203488 Q Losses: [0.01343602, 0.069393501]\n",
      "epoch:6 batch_done:197 Gen Loss: 6.08897 Disc Loss: 0.0045644 Q Losses: [0.025847431, 0.082601547]\n",
      "epoch:6 batch_done:198 Gen Loss: 21.0814 Disc Loss: 0.00011135 Q Losses: [0.0090559907, 0.070500426]\n",
      "epoch:6 batch_done:199 Gen Loss: 22.9013 Disc Loss: 0.000137159 Q Losses: [0.010400867, 0.14056107]\n",
      "epoch:6 batch_done:200 Gen Loss: 19.0578 Disc Loss: 0.000833298 Q Losses: [0.01068397, 0.23797405]\n",
      "epoch:6 batch_done:201 Gen Loss: 8.07128 Disc Loss: 0.000823025 Q Losses: [0.020228636, 0.15752494]\n",
      "epoch:6 batch_done:202 Gen Loss: 27.9999 Disc Loss: 0.000968641 Q Losses: [0.01964052, 0.14574042]\n",
      "epoch:6 batch_done:203 Gen Loss: 49.7042 Disc Loss: 2.94152e-05 Q Losses: [0.033054098, 0.49472851]\n",
      "epoch:6 batch_done:204 Gen Loss: 38.9031 Disc Loss: 0.000176639 Q Losses: [0.032680936, 0.39123783]\n",
      "epoch:6 batch_done:205 Gen Loss: 25.459 Disc Loss: 6.70652e-05 Q Losses: [0.04308591, 0.1937453]\n",
      "epoch:6 batch_done:206 Gen Loss: 20.7657 Disc Loss: 0.000117168 Q Losses: [0.014769208, 0.10219617]\n",
      "epoch:6 batch_done:207 Gen Loss: 18.3516 Disc Loss: 0.000330206 Q Losses: [0.024249606, 0.29894149]\n",
      "epoch:7 batch_done:1 Gen Loss: 20.3951 Disc Loss: 0.000191135 Q Losses: [0.017178141, 0.32075918]\n",
      "epoch:7 batch_done:2 Gen Loss: 22.5968 Disc Loss: 0.000711349 Q Losses: [0.066387385, 0.80180025]\n",
      "epoch:7 batch_done:3 Gen Loss: 9.46775 Disc Loss: 0.000249857 Q Losses: [0.02949575, 0.36635739]\n",
      "epoch:7 batch_done:4 Gen Loss: 6.14559 Disc Loss: 0.00363988 Q Losses: [0.026234318, 0.18876003]\n",
      "epoch:7 batch_done:5 Gen Loss: 6.96054 Disc Loss: 0.00647243 Q Losses: [0.027977198, 0.20731571]\n",
      "epoch:7 batch_done:6 Gen Loss: 6.82727 Disc Loss: 0.0113496 Q Losses: [0.026201386, 0.17659286]\n",
      "epoch:7 batch_done:7 Gen Loss: 7.8117 Disc Loss: 0.00211837 Q Losses: [0.01453351, 0.078394905]\n",
      "epoch:7 batch_done:8 Gen Loss: 5.96154 Disc Loss: 0.00390338 Q Losses: [0.031610761, 0.091736607]\n",
      "epoch:7 batch_done:9 Gen Loss: 7.34426 Disc Loss: 0.00104016 Q Losses: [0.030064465, 0.066781946]\n",
      "epoch:7 batch_done:10 Gen Loss: 6.31314 Disc Loss: 0.002781 Q Losses: [0.029543476, 0.059809938]\n",
      "epoch:7 batch_done:11 Gen Loss: 6.04205 Disc Loss: 0.0197508 Q Losses: [0.021803986, 0.030748103]\n",
      "epoch:7 batch_done:12 Gen Loss: 6.98025 Disc Loss: 0.0219782 Q Losses: [0.028095182, 0.038485594]\n",
      "epoch:7 batch_done:13 Gen Loss: 7.31499 Disc Loss: 0.00556884 Q Losses: [0.012194446, 0.032181416]\n",
      "epoch:7 batch_done:14 Gen Loss: 7.36163 Disc Loss: 0.00243509 Q Losses: [0.0091596581, 0.050891846]\n",
      "epoch:7 batch_done:15 Gen Loss: 7.89757 Disc Loss: 0.00315207 Q Losses: [0.011895001, 0.044553578]\n",
      "epoch:7 batch_done:16 Gen Loss: 6.98876 Disc Loss: 0.00393786 Q Losses: [0.013842445, 0.026080877]\n",
      "epoch:7 batch_done:17 Gen Loss: 6.22365 Disc Loss: 0.00370042 Q Losses: [0.01054468, 0.02200616]\n",
      "epoch:7 batch_done:18 Gen Loss: 5.91688 Disc Loss: 0.00697253 Q Losses: [0.010931024, 0.02197117]\n",
      "epoch:7 batch_done:19 Gen Loss: 6.01706 Disc Loss: 0.00987994 Q Losses: [0.0089823883, 0.022603331]\n",
      "epoch:7 batch_done:20 Gen Loss: 6.29221 Disc Loss: 0.00929565 Q Losses: [0.0076872362, 0.016822768]\n",
      "epoch:7 batch_done:21 Gen Loss: 6.45033 Disc Loss: 0.00864252 Q Losses: [0.015082404, 0.024035888]\n",
      "epoch:7 batch_done:22 Gen Loss: 6.6011 Disc Loss: 0.00445812 Q Losses: [0.0092898812, 0.011349631]\n",
      "epoch:7 batch_done:23 Gen Loss: 6.60071 Disc Loss: 0.00555438 Q Losses: [0.017199848, 0.016514879]\n",
      "epoch:7 batch_done:24 Gen Loss: 6.35167 Disc Loss: 0.00618755 Q Losses: [0.0092728473, 0.024650885]\n",
      "epoch:7 batch_done:25 Gen Loss: 6.14182 Disc Loss: 0.00542341 Q Losses: [0.0074252244, 0.017535768]\n",
      "epoch:7 batch_done:26 Gen Loss: 6.71609 Disc Loss: 0.00329781 Q Losses: [0.0088747442, 0.018344186]\n",
      "epoch:7 batch_done:27 Gen Loss: 6.80073 Disc Loss: 0.00403516 Q Losses: [0.0058826413, 0.022281833]\n",
      "epoch:7 batch_done:28 Gen Loss: 6.84647 Disc Loss: 0.0022364 Q Losses: [0.010517604, 0.011165822]\n",
      "epoch:7 batch_done:29 Gen Loss: 6.5916 Disc Loss: 0.00695164 Q Losses: [0.011409111, 0.012361726]\n",
      "epoch:7 batch_done:30 Gen Loss: 6.34704 Disc Loss: 0.00319093 Q Losses: [0.0091168787, 0.011493394]\n",
      "epoch:7 batch_done:31 Gen Loss: 6.04964 Disc Loss: 0.00980734 Q Losses: [0.01190429, 0.01219004]\n",
      "epoch:7 batch_done:32 Gen Loss: 6.09737 Disc Loss: 0.00571318 Q Losses: [0.0092491992, 0.010875551]\n",
      "epoch:7 batch_done:33 Gen Loss: 6.31935 Disc Loss: 0.00577679 Q Losses: [0.014429821, 0.010609365]\n",
      "epoch:7 batch_done:34 Gen Loss: 6.54879 Disc Loss: 0.00369775 Q Losses: [0.0076116649, 0.012814874]\n",
      "epoch:7 batch_done:35 Gen Loss: 6.55419 Disc Loss: 0.00419605 Q Losses: [0.010318348, 0.009696289]\n",
      "epoch:7 batch_done:36 Gen Loss: 6.67125 Disc Loss: 0.00304648 Q Losses: [0.0056075864, 0.010828048]\n",
      "epoch:7 batch_done:37 Gen Loss: 6.72711 Disc Loss: 0.0029076 Q Losses: [0.0056615784, 0.010609657]\n",
      "epoch:7 batch_done:38 Gen Loss: 6.66732 Disc Loss: 0.00282337 Q Losses: [0.0065681664, 0.0089735612]\n",
      "epoch:7 batch_done:39 Gen Loss: 6.50888 Disc Loss: 0.00580998 Q Losses: [0.0079399254, 0.010825293]\n",
      "epoch:7 batch_done:40 Gen Loss: 6.14703 Disc Loss: 0.00976202 Q Losses: [0.015344833, 0.0088768899]\n",
      "epoch:7 batch_done:41 Gen Loss: 6.08084 Disc Loss: 0.00547226 Q Losses: [0.0080897417, 0.0079197781]\n",
      "epoch:7 batch_done:42 Gen Loss: 6.3406 Disc Loss: 0.00526879 Q Losses: [0.0057554748, 0.010871743]\n",
      "epoch:7 batch_done:43 Gen Loss: 6.65148 Disc Loss: 0.00334472 Q Losses: [0.0045505539, 0.0091895517]\n",
      "epoch:7 batch_done:44 Gen Loss: 6.70273 Disc Loss: 0.00457411 Q Losses: [0.0072977194, 0.010936975]\n",
      "epoch:7 batch_done:45 Gen Loss: 6.66923 Disc Loss: 0.00743923 Q Losses: [0.0086447513, 0.0083850101]\n",
      "epoch:7 batch_done:46 Gen Loss: 6.55536 Disc Loss: 0.00331702 Q Losses: [0.0053358576, 0.0095389709]\n",
      "epoch:7 batch_done:47 Gen Loss: 6.32694 Disc Loss: 0.00450864 Q Losses: [0.014302143, 0.010542828]\n",
      "epoch:7 batch_done:48 Gen Loss: 6.28362 Disc Loss: 0.00453333 Q Losses: [0.0077751009, 0.0067203399]\n",
      "epoch:7 batch_done:49 Gen Loss: 6.34146 Disc Loss: 0.00457551 Q Losses: [0.0053367764, 0.0076222727]\n",
      "epoch:7 batch_done:50 Gen Loss: 6.25494 Disc Loss: 0.0102966 Q Losses: [0.006193907, 0.0083192857]\n",
      "epoch:7 batch_done:51 Gen Loss: 6.2477 Disc Loss: 0.0066147 Q Losses: [0.009325265, 0.0083530396]\n",
      "epoch:7 batch_done:52 Gen Loss: 6.23477 Disc Loss: 0.00841553 Q Losses: [0.0075186812, 0.010913084]\n",
      "epoch:7 batch_done:53 Gen Loss: 6.13649 Disc Loss: 0.009102 Q Losses: [0.0062545729, 0.0089305174]\n",
      "epoch:7 batch_done:54 Gen Loss: 6.32088 Disc Loss: 0.005889 Q Losses: [0.0060273493, 0.0076049641]\n",
      "epoch:7 batch_done:55 Gen Loss: 6.26498 Disc Loss: 0.0109598 Q Losses: [0.0043134457, 0.011893736]\n",
      "epoch:7 batch_done:56 Gen Loss: 6.55549 Disc Loss: 0.00332181 Q Losses: [0.0084471162, 0.0079531567]\n",
      "epoch:7 batch_done:57 Gen Loss: 6.90693 Disc Loss: 0.00245422 Q Losses: [0.0066444143, 0.0071667493]\n",
      "epoch:7 batch_done:58 Gen Loss: 6.90826 Disc Loss: 0.00398307 Q Losses: [0.0058889501, 0.0099543976]\n",
      "epoch:7 batch_done:59 Gen Loss: 6.29426 Disc Loss: 0.00790031 Q Losses: [0.0054133618, 0.007209274]\n",
      "epoch:7 batch_done:60 Gen Loss: 6.07889 Disc Loss: 0.00749491 Q Losses: [0.004085843, 0.0072743027]\n",
      "epoch:7 batch_done:61 Gen Loss: 6.42517 Disc Loss: 0.0115341 Q Losses: [0.0059799408, 0.0085750818]\n",
      "epoch:7 batch_done:62 Gen Loss: 6.59984 Disc Loss: 0.0111276 Q Losses: [0.011943496, 0.0092511829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 batch_done:63 Gen Loss: 6.54377 Disc Loss: 0.00996231 Q Losses: [0.011124777, 0.0081482176]\n",
      "epoch:7 batch_done:64 Gen Loss: 6.41953 Disc Loss: 0.00773055 Q Losses: [0.0073925434, 0.0068876543]\n",
      "epoch:7 batch_done:65 Gen Loss: 6.2854 Disc Loss: 0.0110174 Q Losses: [0.0066657872, 0.0077065961]\n",
      "epoch:7 batch_done:66 Gen Loss: 6.2291 Disc Loss: 0.0106788 Q Losses: [0.0039625186, 0.0086774193]\n",
      "epoch:7 batch_done:67 Gen Loss: 6.36004 Disc Loss: 0.00939927 Q Losses: [0.012010009, 0.0086628795]\n",
      "epoch:7 batch_done:68 Gen Loss: 6.64454 Disc Loss: 0.00707092 Q Losses: [0.0064816345, 0.01130811]\n",
      "epoch:7 batch_done:69 Gen Loss: 6.5078 Disc Loss: 0.0068295 Q Losses: [0.013757396, 0.0083280886]\n",
      "epoch:7 batch_done:70 Gen Loss: 5.99963 Disc Loss: 0.0179216 Q Losses: [0.006090573, 0.0097723976]\n",
      "epoch:7 batch_done:71 Gen Loss: 6.90451 Disc Loss: 0.0169249 Q Losses: [0.0076336395, 0.0075993794]\n",
      "epoch:7 batch_done:72 Gen Loss: 6.58306 Disc Loss: 0.0229078 Q Losses: [0.0064937547, 0.0081956778]\n",
      "epoch:7 batch_done:73 Gen Loss: 6.33987 Disc Loss: 0.00926158 Q Losses: [0.018303659, 0.0082732216]\n",
      "epoch:7 batch_done:74 Gen Loss: 8.01118 Disc Loss: 0.0270019 Q Losses: [0.009331543, 0.013662794]\n",
      "epoch:7 batch_done:75 Gen Loss: 8.14569 Disc Loss: 0.00566259 Q Losses: [0.010852271, 0.0079833427]\n",
      "epoch:7 batch_done:76 Gen Loss: 5.9481 Disc Loss: 0.033358 Q Losses: [0.015529074, 0.0086444207]\n",
      "epoch:7 batch_done:77 Gen Loss: 8.38325 Disc Loss: 0.0295274 Q Losses: [0.0058824774, 0.0090236682]\n",
      "epoch:7 batch_done:78 Gen Loss: 8.51235 Disc Loss: 0.00696656 Q Losses: [0.0073129865, 0.0070746797]\n",
      "epoch:7 batch_done:79 Gen Loss: 7.59826 Disc Loss: 0.00564733 Q Losses: [0.0055866078, 0.0095996261]\n",
      "epoch:7 batch_done:80 Gen Loss: 7.11896 Disc Loss: 0.0111884 Q Losses: [0.007002994, 0.0078732967]\n",
      "epoch:7 batch_done:81 Gen Loss: 8.01039 Disc Loss: 0.0266575 Q Losses: [0.009148024, 0.0076674214]\n",
      "epoch:7 batch_done:82 Gen Loss: 8.04214 Disc Loss: 0.00872047 Q Losses: [0.0067934757, 0.014516067]\n",
      "epoch:7 batch_done:83 Gen Loss: 6.86854 Disc Loss: 0.0224911 Q Losses: [0.0048939474, 0.0072847093]\n",
      "epoch:7 batch_done:84 Gen Loss: 6.69003 Disc Loss: 0.0110777 Q Losses: [0.0044390671, 0.0069350735]\n",
      "epoch:7 batch_done:85 Gen Loss: 7.10966 Disc Loss: 0.00819904 Q Losses: [0.0070054866, 0.006601139]\n",
      "epoch:7 batch_done:86 Gen Loss: 7.23113 Disc Loss: 0.00622777 Q Losses: [0.011843335, 0.0072027929]\n",
      "epoch:7 batch_done:87 Gen Loss: 7.08022 Disc Loss: 0.00488566 Q Losses: [0.0091246134, 0.0090679694]\n",
      "epoch:7 batch_done:88 Gen Loss: 7.05988 Disc Loss: 0.00487318 Q Losses: [0.010744357, 0.0070186541]\n",
      "epoch:7 batch_done:89 Gen Loss: 6.65454 Disc Loss: 0.0138811 Q Losses: [0.007185542, 0.0089042028]\n",
      "epoch:7 batch_done:90 Gen Loss: 6.72539 Disc Loss: 0.00911707 Q Losses: [0.0072802403, 0.0091655925]\n",
      "epoch:7 batch_done:91 Gen Loss: 7.14456 Disc Loss: 0.00566056 Q Losses: [0.010389656, 0.010028454]\n",
      "epoch:7 batch_done:92 Gen Loss: 7.22668 Disc Loss: 0.00481849 Q Losses: [0.0048175729, 0.01000292]\n",
      "epoch:7 batch_done:93 Gen Loss: 6.93658 Disc Loss: 0.00771967 Q Losses: [0.0071196151, 0.0083650444]\n",
      "epoch:7 batch_done:94 Gen Loss: 6.76105 Disc Loss: 0.00528152 Q Losses: [0.0059636151, 0.0077461791]\n",
      "epoch:7 batch_done:95 Gen Loss: 6.67191 Disc Loss: 0.00580178 Q Losses: [0.0063052587, 0.0068469876]\n",
      "epoch:7 batch_done:96 Gen Loss: 6.73848 Disc Loss: 0.00452124 Q Losses: [0.022978462, 0.009271292]\n",
      "epoch:7 batch_done:97 Gen Loss: 6.68464 Disc Loss: 0.00545022 Q Losses: [0.0051846118, 0.0061629778]\n",
      "epoch:7 batch_done:98 Gen Loss: 6.54922 Disc Loss: 0.00736202 Q Losses: [0.01203729, 0.0061703268]\n",
      "epoch:7 batch_done:99 Gen Loss: 6.54192 Disc Loss: 0.00819917 Q Losses: [0.012964648, 0.0092445444]\n",
      "epoch:7 batch_done:100 Gen Loss: 6.76374 Disc Loss: 0.00595706 Q Losses: [0.0065343287, 0.01162043]\n",
      "epoch:7 batch_done:101 Gen Loss: 6.72502 Disc Loss: 0.00660027 Q Losses: [0.0084802713, 0.0066886973]\n",
      "epoch:7 batch_done:102 Gen Loss: 6.77587 Disc Loss: 0.00666816 Q Losses: [0.0090033468, 0.0070347684]\n",
      "epoch:7 batch_done:103 Gen Loss: 6.69183 Disc Loss: 0.00589442 Q Losses: [0.0064843013, 0.006272668]\n",
      "epoch:7 batch_done:104 Gen Loss: 6.69396 Disc Loss: 0.0038779 Q Losses: [0.0060115233, 0.006325624]\n",
      "epoch:7 batch_done:105 Gen Loss: 6.54654 Disc Loss: 0.00651368 Q Losses: [0.011963522, 0.0060345465]\n",
      "epoch:7 batch_done:106 Gen Loss: 6.77028 Disc Loss: 0.00466449 Q Losses: [0.0057615838, 0.0072049652]\n",
      "epoch:7 batch_done:107 Gen Loss: 6.82226 Disc Loss: 0.00356769 Q Losses: [0.0073314612, 0.0075492198]\n",
      "epoch:7 batch_done:108 Gen Loss: 6.41992 Disc Loss: 0.00840002 Q Losses: [0.012277648, 0.0089097377]\n",
      "epoch:7 batch_done:109 Gen Loss: 6.2557 Disc Loss: 0.00793733 Q Losses: [0.0065583242, 0.0061955415]\n",
      "epoch:7 batch_done:110 Gen Loss: 6.43107 Disc Loss: 0.00639107 Q Losses: [0.011770174, 0.007126064]\n",
      "epoch:7 batch_done:111 Gen Loss: 5.91114 Disc Loss: 0.0232292 Q Losses: [0.0068251081, 0.0057979017]\n",
      "epoch:7 batch_done:112 Gen Loss: 6.08058 Disc Loss: 0.00912657 Q Losses: [0.0060099508, 0.0069091814]\n",
      "epoch:7 batch_done:113 Gen Loss: 6.51934 Disc Loss: 0.00854679 Q Losses: [0.0096316598, 0.0069260774]\n",
      "epoch:7 batch_done:114 Gen Loss: 6.94149 Disc Loss: 0.00418451 Q Losses: [0.0081414692, 0.0084301606]\n",
      "epoch:7 batch_done:115 Gen Loss: 7.2138 Disc Loss: 0.00263322 Q Losses: [0.005247565, 0.0073002172]\n",
      "epoch:7 batch_done:116 Gen Loss: 7.10124 Disc Loss: 0.00226428 Q Losses: [0.0070361355, 0.0066166865]\n",
      "epoch:7 batch_done:117 Gen Loss: 7.11148 Disc Loss: 0.00225444 Q Losses: [0.0067294128, 0.0082171736]\n",
      "epoch:7 batch_done:118 Gen Loss: 6.62884 Disc Loss: 0.00867098 Q Losses: [0.018999042, 0.0078671016]\n",
      "epoch:7 batch_done:119 Gen Loss: 6.58131 Disc Loss: 0.0032586 Q Losses: [0.011768054, 0.0072656004]\n",
      "epoch:7 batch_done:120 Gen Loss: 6.5605 Disc Loss: 0.00378453 Q Losses: [0.0078432392, 0.010902892]\n",
      "epoch:7 batch_done:121 Gen Loss: 6.65203 Disc Loss: 0.00376888 Q Losses: [0.0089635719, 0.0070855874]\n",
      "epoch:7 batch_done:122 Gen Loss: 6.60751 Disc Loss: 0.00621062 Q Losses: [0.008145676, 0.0092910677]\n",
      "epoch:7 batch_done:123 Gen Loss: 6.49812 Disc Loss: 0.0052335 Q Losses: [0.0096706953, 0.0066814069]\n",
      "epoch:7 batch_done:124 Gen Loss: 6.74965 Disc Loss: 0.00346777 Q Losses: [0.0083895363, 0.0063622394]\n",
      "epoch:7 batch_done:125 Gen Loss: 6.83348 Disc Loss: 0.00427387 Q Losses: [0.0083471779, 0.0057249293]\n",
      "epoch:7 batch_done:126 Gen Loss: 6.28781 Disc Loss: 0.0125996 Q Losses: [0.0061785029, 0.0061857323]\n",
      "epoch:7 batch_done:127 Gen Loss: 7.34754 Disc Loss: 0.0142291 Q Losses: [0.0093276687, 0.0064708041]\n",
      "epoch:7 batch_done:128 Gen Loss: 7.3132 Disc Loss: 0.011982 Q Losses: [0.0091442131, 0.008287074]\n",
      "epoch:7 batch_done:129 Gen Loss: 7.26392 Disc Loss: 0.0127072 Q Losses: [0.0098984819, 0.0084722824]\n",
      "epoch:7 batch_done:130 Gen Loss: 5.56516 Disc Loss: 0.0283284 Q Losses: [0.0076955305, 0.0069282912]\n",
      "epoch:7 batch_done:131 Gen Loss: 11.0398 Disc Loss: 0.0471558 Q Losses: [0.0060992688, 0.009688668]\n",
      "epoch:7 batch_done:132 Gen Loss: 14.3421 Disc Loss: 0.0100636 Q Losses: [0.0084985234, 0.0098794717]\n",
      "epoch:7 batch_done:133 Gen Loss: 6.16935 Disc Loss: 0.0614915 Q Losses: [0.0083216736, 0.0075437613]\n",
      "epoch:7 batch_done:134 Gen Loss: 7.74355 Disc Loss: 0.0314028 Q Losses: [0.0075804945, 0.0061456989]\n",
      "epoch:7 batch_done:135 Gen Loss: 7.87156 Disc Loss: 0.0103907 Q Losses: [0.011090795, 0.0071381237]\n",
      "epoch:7 batch_done:136 Gen Loss: 7.9481 Disc Loss: 0.00114323 Q Losses: [0.011700436, 0.0096357344]\n",
      "epoch:7 batch_done:137 Gen Loss: 12.5856 Disc Loss: 0.00188831 Q Losses: [0.0061730407, 0.0071239825]\n",
      "epoch:7 batch_done:138 Gen Loss: 6.8512 Disc Loss: 0.00277568 Q Losses: [0.0054752869, 0.0083788335]\n",
      "epoch:7 batch_done:139 Gen Loss: 12.8398 Disc Loss: 0.00237445 Q Losses: [0.016712926, 0.0084010521]\n",
      "epoch:7 batch_done:140 Gen Loss: 6.76367 Disc Loss: 0.0134016 Q Losses: [0.012136502, 0.0062766811]\n",
      "epoch:7 batch_done:141 Gen Loss: 6.27463 Disc Loss: 0.0211106 Q Losses: [0.010556965, 0.0062469505]\n",
      "epoch:7 batch_done:142 Gen Loss: 8.7373 Disc Loss: 0.00771619 Q Losses: [0.0084343283, 0.009520499]\n",
      "epoch:7 batch_done:143 Gen Loss: 7.58614 Disc Loss: 0.00496397 Q Losses: [0.0045286813, 0.0058355583]\n",
      "epoch:7 batch_done:144 Gen Loss: 46.8077 Disc Loss: 0.343994 Q Losses: [0.0042843292, 0.0093470505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 batch_done:145 Gen Loss: 3.79473 Disc Loss: 12.4536 Q Losses: [0.0065572495, 0.0072026416]\n",
      "epoch:7 batch_done:146 Gen Loss: 12.8021 Disc Loss: 0.32326 Q Losses: [0.0076647671, 0.0066845687]\n",
      "epoch:7 batch_done:147 Gen Loss: 14.2852 Disc Loss: 5.73654e-05 Q Losses: [0.008026043, 0.011227276]\n",
      "epoch:7 batch_done:148 Gen Loss: 14.1531 Disc Loss: 7.49654e-05 Q Losses: [0.0077482704, 0.0069923121]\n",
      "epoch:7 batch_done:149 Gen Loss: 12.4947 Disc Loss: 1.28126e-05 Q Losses: [0.01108959, 0.0092780367]\n",
      "epoch:7 batch_done:150 Gen Loss: 10.2801 Disc Loss: 0.000178898 Q Losses: [0.0087740645, 0.011455745]\n",
      "epoch:7 batch_done:151 Gen Loss: 6.57559 Disc Loss: 0.0234682 Q Losses: [0.011012164, 0.0086228065]\n",
      "epoch:7 batch_done:152 Gen Loss: 6.79852 Disc Loss: 0.0603585 Q Losses: [0.013559533, 0.0079830987]\n",
      "epoch:7 batch_done:153 Gen Loss: 8.96465 Disc Loss: 0.0632834 Q Losses: [0.018385716, 0.0070410045]\n",
      "epoch:7 batch_done:154 Gen Loss: 8.44616 Disc Loss: 0.0103431 Q Losses: [0.0070217643, 0.0093460474]\n",
      "epoch:7 batch_done:155 Gen Loss: 6.59331 Disc Loss: 0.0376811 Q Losses: [0.0089019667, 0.010941658]\n",
      "epoch:7 batch_done:156 Gen Loss: 9.46074 Disc Loss: 0.181667 Q Losses: [0.01091993, 0.010926338]\n",
      "epoch:7 batch_done:157 Gen Loss: 8.32022 Disc Loss: 0.181774 Q Losses: [0.0090615172, 0.011752577]\n",
      "epoch:7 batch_done:158 Gen Loss: 5.06238 Disc Loss: 0.293727 Q Losses: [0.0092371423, 0.0082231425]\n",
      "epoch:7 batch_done:159 Gen Loss: 27.0998 Disc Loss: 0.411293 Q Losses: [0.007027017, 0.0091149574]\n",
      "epoch:7 batch_done:160 Gen Loss: 32.2383 Disc Loss: 0.558152 Q Losses: [0.0091655534, 0.011604149]\n",
      "epoch:7 batch_done:161 Gen Loss: 29.562 Disc Loss: 0.51994 Q Losses: [0.0092472471, 0.014207803]\n",
      "epoch:7 batch_done:162 Gen Loss: 26.2873 Disc Loss: 0.0151625 Q Losses: [0.0077463109, 0.0077680703]\n",
      "epoch:7 batch_done:163 Gen Loss: 21.912 Disc Loss: 0.0201903 Q Losses: [0.010584449, 0.0078413244]\n",
      "epoch:7 batch_done:164 Gen Loss: 17.398 Disc Loss: 3.18526e-05 Q Losses: [0.0086800065, 0.0097761163]\n",
      "epoch:7 batch_done:165 Gen Loss: 12.3735 Disc Loss: 1.14534e-05 Q Losses: [0.0079087717, 0.010576133]\n",
      "epoch:7 batch_done:166 Gen Loss: 7.20914 Disc Loss: 0.000708514 Q Losses: [0.0076788114, 0.01205883]\n",
      "epoch:7 batch_done:167 Gen Loss: 6.10362 Disc Loss: 0.0140739 Q Losses: [0.0073162713, 0.0094486978]\n",
      "epoch:7 batch_done:168 Gen Loss: 8.70195 Disc Loss: 0.0328941 Q Losses: [0.0072995042, 0.009088858]\n",
      "epoch:7 batch_done:169 Gen Loss: 9.51288 Disc Loss: 0.00112246 Q Losses: [0.011906665, 0.009550632]\n",
      "epoch:7 batch_done:170 Gen Loss: 8.98398 Disc Loss: 0.000520802 Q Losses: [0.0092642996, 0.0099130776]\n",
      "epoch:7 batch_done:171 Gen Loss: 7.79517 Disc Loss: 0.000986451 Q Losses: [0.0089454874, 0.010252301]\n",
      "epoch:7 batch_done:172 Gen Loss: 6.48685 Disc Loss: 0.0044233 Q Losses: [0.0076536778, 0.0098004034]\n",
      "epoch:7 batch_done:173 Gen Loss: 7.3617 Disc Loss: 0.0270658 Q Losses: [0.012347159, 0.010731733]\n",
      "epoch:7 batch_done:174 Gen Loss: 9.29078 Disc Loss: 0.0324551 Q Losses: [0.0082757231, 0.011264356]\n",
      "epoch:7 batch_done:175 Gen Loss: 9.79069 Disc Loss: 0.0255985 Q Losses: [0.0080103083, 0.011459486]\n",
      "epoch:7 batch_done:176 Gen Loss: 8.96239 Disc Loss: 0.0130345 Q Losses: [0.0077357111, 0.0099112205]\n",
      "epoch:7 batch_done:177 Gen Loss: 12.4647 Disc Loss: 0.0606954 Q Losses: [0.010121982, 0.014330661]\n",
      "epoch:7 batch_done:178 Gen Loss: 12.2502 Disc Loss: 0.0334003 Q Losses: [0.0063758357, 0.0096921548]\n",
      "epoch:7 batch_done:179 Gen Loss: 9.94602 Disc Loss: 0.0230085 Q Losses: [0.009575407, 0.012099839]\n",
      "epoch:7 batch_done:180 Gen Loss: 6.53953 Disc Loss: 0.119874 Q Losses: [0.0097656474, 0.0095129777]\n",
      "epoch:7 batch_done:181 Gen Loss: 4.67228 Disc Loss: 0.0635706 Q Losses: [0.0097654313, 0.012751626]\n",
      "epoch:7 batch_done:182 Gen Loss: 6.29202 Disc Loss: 0.0222362 Q Losses: [0.0073859245, 0.0090127001]\n",
      "epoch:7 batch_done:183 Gen Loss: 7.14613 Disc Loss: 0.0108754 Q Losses: [0.0058592055, 0.010601052]\n",
      "epoch:7 batch_done:184 Gen Loss: 6.97842 Disc Loss: 0.0230605 Q Losses: [0.010257437, 0.00824623]\n",
      "epoch:7 batch_done:185 Gen Loss: 6.53604 Disc Loss: 0.01683 Q Losses: [0.010994611, 0.010654796]\n",
      "epoch:7 batch_done:186 Gen Loss: 6.54276 Disc Loss: 0.0044281 Q Losses: [0.0083616171, 0.0085736113]\n",
      "epoch:7 batch_done:187 Gen Loss: 6.38151 Disc Loss: 0.00506437 Q Losses: [0.0066656573, 0.0090706851]\n",
      "epoch:7 batch_done:188 Gen Loss: 6.38685 Disc Loss: 0.0122779 Q Losses: [0.007369095, 0.0097912634]\n",
      "epoch:7 batch_done:189 Gen Loss: 7.14803 Disc Loss: 0.00347548 Q Losses: [0.014253918, 0.0070356121]\n",
      "epoch:7 batch_done:190 Gen Loss: 6.65075 Disc Loss: 0.00472928 Q Losses: [0.017635066, 0.0064473245]\n",
      "epoch:7 batch_done:191 Gen Loss: 6.18073 Disc Loss: 0.0227265 Q Losses: [0.010493847, 0.0081698503]\n",
      "epoch:7 batch_done:192 Gen Loss: 6.54847 Disc Loss: 0.00455194 Q Losses: [0.0080163861, 0.01050679]\n",
      "epoch:7 batch_done:193 Gen Loss: 6.29563 Disc Loss: 0.00740874 Q Losses: [0.0074983984, 0.0096546076]\n",
      "epoch:7 batch_done:194 Gen Loss: 6.1517 Disc Loss: 0.0254241 Q Losses: [0.013683916, 0.0095588006]\n",
      "epoch:7 batch_done:195 Gen Loss: 5.92004 Disc Loss: 0.0229624 Q Losses: [0.0066841664, 0.0094339363]\n",
      "epoch:7 batch_done:196 Gen Loss: 6.38511 Disc Loss: 0.00928229 Q Losses: [0.010804014, 0.012981931]\n",
      "epoch:7 batch_done:197 Gen Loss: 6.74574 Disc Loss: 0.00537341 Q Losses: [0.0069687036, 0.0077454969]\n",
      "epoch:7 batch_done:198 Gen Loss: 7.0364 Disc Loss: 0.00252769 Q Losses: [0.010317831, 0.0097362697]\n",
      "epoch:7 batch_done:199 Gen Loss: 6.40093 Disc Loss: 0.00734743 Q Losses: [0.0070664519, 0.0099457819]\n",
      "epoch:7 batch_done:200 Gen Loss: 6.25488 Disc Loss: 0.00981365 Q Losses: [0.0058760382, 0.0071184179]\n",
      "epoch:7 batch_done:201 Gen Loss: 6.21657 Disc Loss: 0.00680093 Q Losses: [0.007718151, 0.0085295746]\n",
      "epoch:7 batch_done:202 Gen Loss: 6.25816 Disc Loss: 0.00910774 Q Losses: [0.0094788587, 0.0084180757]\n",
      "epoch:7 batch_done:203 Gen Loss: 6.59563 Disc Loss: 0.00698502 Q Losses: [0.00775903, 0.0081488909]\n",
      "epoch:7 batch_done:204 Gen Loss: 6.66594 Disc Loss: 0.00627276 Q Losses: [0.0072306599, 0.0093970234]\n",
      "epoch:7 batch_done:205 Gen Loss: 6.81754 Disc Loss: 0.00487466 Q Losses: [0.0058246381, 0.010044104]\n",
      "epoch:7 batch_done:206 Gen Loss: 6.47439 Disc Loss: 0.0099254 Q Losses: [0.007764983, 0.0084291277]\n",
      "epoch:7 batch_done:207 Gen Loss: 5.68354 Disc Loss: 0.0568347 Q Losses: [0.0073734415, 0.011482001]\n",
      "epoch:8 batch_done:1 Gen Loss: 7.47384 Disc Loss: 0.0336605 Q Losses: [0.010706826, 0.011120515]\n",
      "epoch:8 batch_done:2 Gen Loss: 8.28208 Disc Loss: 0.00244212 Q Losses: [0.010513655, 0.011719154]\n",
      "epoch:8 batch_done:3 Gen Loss: 7.16923 Disc Loss: 0.00442712 Q Losses: [0.0082135517, 0.012147022]\n",
      "epoch:8 batch_done:4 Gen Loss: 5.85849 Disc Loss: 0.0581377 Q Losses: [0.011037068, 0.0095882397]\n",
      "epoch:8 batch_done:5 Gen Loss: 42.1663 Disc Loss: 0.474903 Q Losses: [0.011900899, 0.010789383]\n",
      "epoch:8 batch_done:6 Gen Loss: 44.4918 Disc Loss: 1.74808 Q Losses: [0.0054374505, 0.0080596562]\n",
      "epoch:8 batch_done:7 Gen Loss: 38.6479 Disc Loss: 0.188398 Q Losses: [0.010863975, 0.010931071]\n",
      "epoch:8 batch_done:8 Gen Loss: 33.4668 Disc Loss: 0.00178022 Q Losses: [0.007872697, 0.0082611684]\n",
      "epoch:8 batch_done:9 Gen Loss: 26.4514 Disc Loss: 5.71798e-06 Q Losses: [0.027619157, 0.0083352895]\n",
      "epoch:8 batch_done:10 Gen Loss: 20.8119 Disc Loss: 2.98024e-07 Q Losses: [0.0084369462, 0.0097648967]\n",
      "epoch:8 batch_done:11 Gen Loss: 16.0558 Disc Loss: 9.96531e-07 Q Losses: [0.012086371, 0.010330495]\n",
      "epoch:8 batch_done:12 Gen Loss: 11.3055 Disc Loss: 4.09972e-05 Q Losses: [0.0094980663, 0.011068994]\n",
      "epoch:8 batch_done:13 Gen Loss: 7.18029 Disc Loss: 0.000782402 Q Losses: [0.010261231, 0.010018991]\n",
      "epoch:8 batch_done:14 Gen Loss: 6.06267 Disc Loss: 0.0236181 Q Losses: [0.011542234, 0.0083755245]\n",
      "epoch:8 batch_done:15 Gen Loss: 7.48893 Disc Loss: 0.0291551 Q Losses: [0.012364522, 0.0093418509]\n",
      "epoch:8 batch_done:16 Gen Loss: 7.64064 Disc Loss: 0.00567065 Q Losses: [0.017193735, 0.014331374]\n",
      "epoch:8 batch_done:17 Gen Loss: 7.28322 Disc Loss: 0.00309048 Q Losses: [0.012653152, 0.013295149]\n",
      "epoch:8 batch_done:18 Gen Loss: 6.30982 Disc Loss: 0.00751341 Q Losses: [0.012339924, 0.010983236]\n",
      "epoch:8 batch_done:19 Gen Loss: 6.54989 Disc Loss: 0.0230617 Q Losses: [0.012503007, 0.010182604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:20 Gen Loss: 7.16928 Disc Loss: 0.0222632 Q Losses: [0.0088825971, 0.0088644791]\n",
      "epoch:8 batch_done:21 Gen Loss: 7.29909 Disc Loss: 0.0063523 Q Losses: [0.012192954, 0.0089560365]\n",
      "epoch:8 batch_done:22 Gen Loss: 6.68418 Disc Loss: 0.00932973 Q Losses: [0.0084598009, 0.010696625]\n",
      "epoch:8 batch_done:23 Gen Loss: 6.54879 Disc Loss: 0.00768546 Q Losses: [0.0095433574, 0.011074834]\n",
      "epoch:8 batch_done:24 Gen Loss: 7.58922 Disc Loss: 0.0358366 Q Losses: [0.0082004378, 0.0081311958]\n",
      "epoch:8 batch_done:25 Gen Loss: 7.4526 Disc Loss: 0.0185006 Q Losses: [0.007461017, 0.008216884]\n",
      "epoch:8 batch_done:26 Gen Loss: 7.11804 Disc Loss: 0.00972204 Q Losses: [0.008390516, 0.0069869356]\n",
      "epoch:8 batch_done:27 Gen Loss: 6.02549 Disc Loss: 0.0492631 Q Losses: [0.0091126822, 0.015105816]\n",
      "epoch:8 batch_done:28 Gen Loss: 6.1606 Disc Loss: 0.00814144 Q Losses: [0.0081669223, 0.010062976]\n",
      "epoch:8 batch_done:29 Gen Loss: 5.17835 Disc Loss: 0.0786631 Q Losses: [0.0099015441, 0.0095366966]\n",
      "epoch:8 batch_done:30 Gen Loss: 60.3661 Disc Loss: 1.42246 Q Losses: [0.010790206, 0.01644668]\n",
      "epoch:8 batch_done:31 Gen Loss: 49.0228 Disc Loss: 16.4036 Q Losses: [0.0057480559, 0.0080568492]\n",
      "epoch:8 batch_done:32 Gen Loss: 34.4326 Disc Loss: 1.09946 Q Losses: [0.009776935, 0.0086005656]\n",
      "epoch:8 batch_done:33 Gen Loss: 24.448 Disc Loss: 0.000920782 Q Losses: [0.0093346685, 0.011388976]\n",
      "epoch:8 batch_done:34 Gen Loss: 16.9351 Disc Loss: 2.58919e-06 Q Losses: [0.0082312385, 0.0077431626]\n",
      "epoch:8 batch_done:35 Gen Loss: 8.8278 Disc Loss: 3.47716e-05 Q Losses: [0.011384464, 0.0093176784]\n",
      "epoch:8 batch_done:36 Gen Loss: 32.2519 Disc Loss: 0.64605 Q Losses: [0.0071397098, 0.0095202597]\n",
      "epoch:8 batch_done:37 Gen Loss: 39.2771 Disc Loss: 0.00815361 Q Losses: [0.013392751, 0.01435431]\n",
      "epoch:8 batch_done:38 Gen Loss: 36.4674 Disc Loss: 0.309208 Q Losses: [0.010840897, 0.01302829]\n",
      "epoch:8 batch_done:39 Gen Loss: 28.8355 Disc Loss: 0.122171 Q Losses: [0.012446701, 0.013231239]\n",
      "epoch:8 batch_done:40 Gen Loss: 20.0525 Disc Loss: 0.129717 Q Losses: [0.011665345, 0.016141286]\n",
      "epoch:8 batch_done:41 Gen Loss: 12.6251 Disc Loss: 0.00844637 Q Losses: [0.0075060413, 0.010515792]\n",
      "epoch:8 batch_done:42 Gen Loss: 8.36349 Disc Loss: 0.00766768 Q Losses: [0.0103658, 0.018821623]\n",
      "epoch:8 batch_done:43 Gen Loss: 5.12666 Disc Loss: 0.00806582 Q Losses: [0.010414176, 0.021342427]\n",
      "epoch:8 batch_done:44 Gen Loss: 11.1947 Disc Loss: 0.137652 Q Losses: [0.01707495, 0.016246917]\n",
      "epoch:8 batch_done:45 Gen Loss: 12.9309 Disc Loss: 0.000763167 Q Losses: [0.010863658, 0.010274453]\n",
      "epoch:8 batch_done:46 Gen Loss: 12.5958 Disc Loss: 0.0165882 Q Losses: [0.011395887, 0.014720932]\n",
      "epoch:8 batch_done:47 Gen Loss: 12.6198 Disc Loss: 0.00854132 Q Losses: [0.0093756858, 0.016381875]\n",
      "epoch:8 batch_done:48 Gen Loss: 10.1922 Disc Loss: 0.0172512 Q Losses: [0.025504556, 0.011231091]\n",
      "epoch:8 batch_done:49 Gen Loss: 6.67339 Disc Loss: 0.00590886 Q Losses: [0.0080981161, 0.013846738]\n",
      "epoch:8 batch_done:50 Gen Loss: 29.9072 Disc Loss: 0.636588 Q Losses: [0.0081985258, 0.016450495]\n",
      "epoch:8 batch_done:51 Gen Loss: 36.3677 Disc Loss: 0.261043 Q Losses: [0.0079057664, 0.018856676]\n",
      "epoch:8 batch_done:52 Gen Loss: 32.4853 Disc Loss: 0.737727 Q Losses: [0.007432065, 0.014713554]\n",
      "epoch:8 batch_done:53 Gen Loss: 26.5601 Disc Loss: 0.0402339 Q Losses: [0.014075489, 0.013861037]\n",
      "epoch:8 batch_done:54 Gen Loss: 20.4089 Disc Loss: 0.000508264 Q Losses: [0.0073069679, 0.014153639]\n",
      "epoch:8 batch_done:55 Gen Loss: 15.9291 Disc Loss: 0.000511361 Q Losses: [0.0093723014, 0.011510652]\n",
      "epoch:8 batch_done:56 Gen Loss: 12.2298 Disc Loss: 0.00157833 Q Losses: [0.016105233, 0.010560458]\n",
      "epoch:8 batch_done:57 Gen Loss: 10.2348 Disc Loss: 0.000201639 Q Losses: [0.023297273, 0.011729073]\n",
      "epoch:8 batch_done:58 Gen Loss: 7.71657 Disc Loss: 0.000795859 Q Losses: [0.0059886398, 0.011713863]\n",
      "epoch:8 batch_done:59 Gen Loss: 6.66864 Disc Loss: 0.00156742 Q Losses: [0.0097663039, 0.011973256]\n",
      "epoch:8 batch_done:60 Gen Loss: 5.57539 Disc Loss: 0.00602857 Q Losses: [0.012099301, 0.0093441103]\n",
      "epoch:8 batch_done:61 Gen Loss: 7.82008 Disc Loss: 0.0700759 Q Losses: [0.0083755711, 0.0071723219]\n",
      "epoch:8 batch_done:62 Gen Loss: 8.59312 Disc Loss: 0.00317288 Q Losses: [0.020616597, 0.012373297]\n",
      "epoch:8 batch_done:63 Gen Loss: 8.26658 Disc Loss: 0.0012371 Q Losses: [0.0068861218, 0.010889812]\n",
      "epoch:8 batch_done:64 Gen Loss: 7.48192 Disc Loss: 0.101056 Q Losses: [0.0091054961, 0.010847298]\n",
      "epoch:8 batch_done:65 Gen Loss: 4.5302 Disc Loss: 0.0374776 Q Losses: [0.0056648599, 0.008918047]\n",
      "epoch:8 batch_done:66 Gen Loss: 6.56999 Disc Loss: 0.04877 Q Losses: [0.011055041, 0.008205005]\n",
      "epoch:8 batch_done:67 Gen Loss: 8.10123 Disc Loss: 0.00554873 Q Losses: [0.034129515, 0.010190536]\n",
      "epoch:8 batch_done:68 Gen Loss: 5.99836 Disc Loss: 0.0394707 Q Losses: [0.01706838, 0.0070407852]\n",
      "epoch:8 batch_done:69 Gen Loss: 7.41826 Disc Loss: 0.00560687 Q Losses: [0.0071502356, 0.0090028364]\n",
      "epoch:8 batch_done:70 Gen Loss: 5.04154 Disc Loss: 0.0484628 Q Losses: [0.0082215387, 0.0082749519]\n",
      "epoch:8 batch_done:71 Gen Loss: 5.61771 Disc Loss: 0.0228166 Q Losses: [0.0073362906, 0.0094654597]\n",
      "epoch:8 batch_done:72 Gen Loss: 16.8759 Disc Loss: 0.242395 Q Losses: [0.0098233875, 0.0076991147]\n",
      "epoch:8 batch_done:73 Gen Loss: 19.8526 Disc Loss: 0.218593 Q Losses: [0.0085319178, 0.0074165962]\n",
      "epoch:8 batch_done:74 Gen Loss: 15.249 Disc Loss: 0.532267 Q Losses: [0.012028926, 0.0059426087]\n",
      "epoch:8 batch_done:75 Gen Loss: 13.0434 Disc Loss: 0.238773 Q Losses: [0.010644903, 0.0067403228]\n",
      "epoch:8 batch_done:76 Gen Loss: 13.9914 Disc Loss: 0.000227709 Q Losses: [0.014282318, 0.0063583106]\n",
      "epoch:8 batch_done:77 Gen Loss: 5.72941 Disc Loss: 0.00281556 Q Losses: [0.0058200988, 0.0099778064]\n",
      "epoch:8 batch_done:78 Gen Loss: 5.65541 Disc Loss: 0.00468621 Q Losses: [0.0075739184, 0.010671684]\n",
      "epoch:8 batch_done:79 Gen Loss: 9.20023 Disc Loss: 0.000207059 Q Losses: [0.0051436662, 0.007856912]\n",
      "epoch:8 batch_done:80 Gen Loss: 49.7931 Disc Loss: 1.93918 Q Losses: [0.0079021715, 0.0094607305]\n",
      "epoch:8 batch_done:81 Gen Loss: 43.2993 Disc Loss: 6.59931 Q Losses: [0.0061532324, 0.0097561516]\n",
      "epoch:8 batch_done:82 Gen Loss: 27.4167 Disc Loss: 2.55005 Q Losses: [0.01250366, 0.01516457]\n",
      "epoch:8 batch_done:83 Gen Loss: 17.0975 Disc Loss: 0.147232 Q Losses: [0.0099262008, 0.010901757]\n",
      "epoch:8 batch_done:84 Gen Loss: 7.5413 Disc Loss: 0.00248192 Q Losses: [0.011959862, 0.011823156]\n",
      "epoch:8 batch_done:85 Gen Loss: 14.1381 Disc Loss: 0.331119 Q Losses: [0.0075269793, 0.01295138]\n",
      "epoch:8 batch_done:86 Gen Loss: 13.9373 Disc Loss: 0.0278791 Q Losses: [0.0078489389, 0.0110876]\n",
      "epoch:8 batch_done:87 Gen Loss: 9.85276 Disc Loss: 0.108316 Q Losses: [0.010728581, 0.016247375]\n",
      "epoch:8 batch_done:88 Gen Loss: 5.77729 Disc Loss: 0.0547349 Q Losses: [0.0077459607, 0.022543544]\n",
      "epoch:8 batch_done:89 Gen Loss: 17.1337 Disc Loss: 0.357303 Q Losses: [0.010904942, 0.011952899]\n",
      "epoch:8 batch_done:90 Gen Loss: 18.285 Disc Loss: 0.406796 Q Losses: [0.00964751, 0.019622833]\n",
      "epoch:8 batch_done:91 Gen Loss: 14.0538 Disc Loss: 0.591282 Q Losses: [0.010386758, 0.013966563]\n",
      "epoch:8 batch_done:92 Gen Loss: 9.31353 Disc Loss: 0.180063 Q Losses: [0.010661565, 0.013764342]\n",
      "epoch:8 batch_done:93 Gen Loss: 3.63724 Disc Loss: 0.0215258 Q Losses: [0.012980796, 0.017699111]\n",
      "epoch:8 batch_done:94 Gen Loss: 22.8195 Disc Loss: 0.514092 Q Losses: [0.0079468274, 0.012133129]\n",
      "epoch:8 batch_done:95 Gen Loss: 28.1271 Disc Loss: 0.238279 Q Losses: [0.0084746229, 0.016093869]\n",
      "epoch:8 batch_done:96 Gen Loss: 19.691 Disc Loss: 1.23463 Q Losses: [0.008481564, 0.0076107392]\n",
      "epoch:8 batch_done:97 Gen Loss: 13.5651 Disc Loss: 0.0946938 Q Losses: [0.0057760389, 0.0094209444]\n",
      "epoch:8 batch_done:98 Gen Loss: 8.11554 Disc Loss: 0.0302895 Q Losses: [0.0090379445, 0.01122155]\n",
      "epoch:8 batch_done:99 Gen Loss: 4.41562 Disc Loss: 0.0210591 Q Losses: [0.0075004608, 0.01227431]\n",
      "epoch:8 batch_done:100 Gen Loss: 5.86521 Disc Loss: 0.059639 Q Losses: [0.015218208, 0.010978423]\n",
      "epoch:8 batch_done:101 Gen Loss: 7.38504 Disc Loss: 0.0417164 Q Losses: [0.00923004, 0.0098048002]\n",
      "epoch:8 batch_done:102 Gen Loss: 8.43751 Disc Loss: 0.00309756 Q Losses: [0.0065460419, 0.013271896]\n",
      "epoch:8 batch_done:103 Gen Loss: 6.91425 Disc Loss: 0.0378804 Q Losses: [0.01081701, 0.011174923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:104 Gen Loss: 5.40605 Disc Loss: 0.0187369 Q Losses: [0.009076939, 0.010607049]\n",
      "epoch:8 batch_done:105 Gen Loss: 12.0996 Disc Loss: 0.148273 Q Losses: [0.0090543767, 0.010353264]\n",
      "epoch:8 batch_done:106 Gen Loss: 10.5145 Disc Loss: 0.264192 Q Losses: [0.011900667, 0.010534648]\n",
      "epoch:8 batch_done:107 Gen Loss: 7.6673 Disc Loss: 0.0998873 Q Losses: [0.0092481785, 0.0081430702]\n",
      "epoch:8 batch_done:108 Gen Loss: 4.48686 Disc Loss: 0.058723 Q Losses: [0.0080614146, 0.0094144363]\n",
      "epoch:8 batch_done:109 Gen Loss: 23.8766 Disc Loss: 0.39069 Q Losses: [0.0053035859, 0.0092103165]\n",
      "epoch:8 batch_done:110 Gen Loss: 26.07 Disc Loss: 1.15488 Q Losses: [0.0081203412, 0.010156326]\n",
      "epoch:8 batch_done:111 Gen Loss: 16.3712 Disc Loss: 1.52678 Q Losses: [0.010646285, 0.0090273693]\n",
      "epoch:8 batch_done:112 Gen Loss: 9.79595 Disc Loss: 0.00717102 Q Losses: [0.0070585893, 0.011517795]\n",
      "epoch:8 batch_done:113 Gen Loss: 4.39317 Disc Loss: 0.0434681 Q Losses: [0.014483437, 0.0082131205]\n",
      "epoch:8 batch_done:114 Gen Loss: 9.86518 Disc Loss: 0.124803 Q Losses: [0.0044062217, 0.0077633178]\n",
      "epoch:8 batch_done:115 Gen Loss: 12.1555 Disc Loss: 0.00153738 Q Losses: [0.0087455334, 0.0077591212]\n",
      "epoch:8 batch_done:116 Gen Loss: 8.34173 Disc Loss: 0.00363259 Q Losses: [0.0067576943, 0.0070187449]\n",
      "epoch:8 batch_done:117 Gen Loss: 6.45034 Disc Loss: 0.0110859 Q Losses: [0.0039682807, 0.0088990713]\n",
      "epoch:8 batch_done:118 Gen Loss: 31.1625 Disc Loss: 0.47877 Q Losses: [0.0057439404, 0.0094611365]\n",
      "epoch:8 batch_done:119 Gen Loss: 29.2908 Disc Loss: 1.43883 Q Losses: [0.0056089303, 0.017982721]\n",
      "epoch:8 batch_done:120 Gen Loss: 18.6611 Disc Loss: 1.25591 Q Losses: [0.0089084348, 0.01068932]\n",
      "epoch:8 batch_done:121 Gen Loss: 8.46182 Disc Loss: 0.000162201 Q Losses: [0.0078734122, 0.015675584]\n",
      "epoch:8 batch_done:122 Gen Loss: 6.41462 Disc Loss: 0.000483226 Q Losses: [0.0061042872, 0.028476188]\n",
      "epoch:8 batch_done:123 Gen Loss: 6.79292 Disc Loss: 0.00041627 Q Losses: [0.0081159212, 0.016945198]\n",
      "epoch:8 batch_done:124 Gen Loss: 18.6866 Disc Loss: 0.268898 Q Losses: [0.0078626405, 0.050300412]\n",
      "epoch:8 batch_done:125 Gen Loss: 23.1479 Disc Loss: 0.000756837 Q Losses: [0.0060818451, 0.035447493]\n",
      "epoch:8 batch_done:126 Gen Loss: 21.0013 Disc Loss: 0.00069471 Q Losses: [0.0098105241, 0.035571963]\n",
      "epoch:8 batch_done:127 Gen Loss: 16.1573 Disc Loss: 0.0165618 Q Losses: [0.0066854693, 0.058694214]\n",
      "epoch:8 batch_done:128 Gen Loss: 9.12948 Disc Loss: 0.00764363 Q Losses: [0.010434545, 0.037511408]\n",
      "epoch:8 batch_done:129 Gen Loss: 11.3514 Disc Loss: 0.0918185 Q Losses: [0.013699062, 0.031624511]\n",
      "epoch:8 batch_done:130 Gen Loss: 12.9675 Disc Loss: 0.0287022 Q Losses: [0.010832421, 0.069507651]\n",
      "epoch:8 batch_done:131 Gen Loss: 18.0583 Disc Loss: 0.00190196 Q Losses: [0.010568537, 0.12453512]\n",
      "epoch:8 batch_done:132 Gen Loss: 20.1618 Disc Loss: 0.0203439 Q Losses: [0.015452756, 0.17579919]\n",
      "epoch:8 batch_done:133 Gen Loss: 26.0661 Disc Loss: 0.0177403 Q Losses: [0.01003086, 0.15730977]\n",
      "epoch:8 batch_done:134 Gen Loss: 24.9448 Disc Loss: 0.00031736 Q Losses: [0.011965432, 0.17536344]\n",
      "epoch:8 batch_done:135 Gen Loss: 27.484 Disc Loss: 0.0192359 Q Losses: [0.018324168, 0.26272517]\n",
      "epoch:8 batch_done:136 Gen Loss: 28.96 Disc Loss: 0.0223892 Q Losses: [0.017690726, 0.53979146]\n",
      "epoch:8 batch_done:137 Gen Loss: 25.4588 Disc Loss: 0.000125904 Q Losses: [0.031376902, 0.22247893]\n",
      "epoch:8 batch_done:138 Gen Loss: 23.8665 Disc Loss: 0.000741873 Q Losses: [0.025536228, 0.082130961]\n",
      "epoch:8 batch_done:139 Gen Loss: 20.5275 Disc Loss: 0.000178198 Q Losses: [0.021666985, 0.18912898]\n",
      "epoch:8 batch_done:140 Gen Loss: 14.6537 Disc Loss: 2.46026e-05 Q Losses: [0.027219357, 0.25859404]\n",
      "epoch:8 batch_done:141 Gen Loss: 16.624 Disc Loss: 0.0002747 Q Losses: [0.037303101, 0.21063849]\n",
      "epoch:8 batch_done:142 Gen Loss: 15.7517 Disc Loss: 0.00196944 Q Losses: [0.066978, 0.68039596]\n",
      "epoch:8 batch_done:143 Gen Loss: 40.4647 Disc Loss: 0.603086 Q Losses: [0.042078368, 0.58818483]\n",
      "epoch:8 batch_done:144 Gen Loss: 25.2483 Disc Loss: 1.14481 Q Losses: [0.037894577, 0.24260426]\n",
      "epoch:8 batch_done:145 Gen Loss: 14.1823 Disc Loss: 0.0502326 Q Losses: [0.019448839, 0.26469526]\n",
      "epoch:8 batch_done:146 Gen Loss: 13.3071 Disc Loss: 0.000310642 Q Losses: [0.070861295, 0.41689733]\n",
      "epoch:8 batch_done:147 Gen Loss: 11.0499 Disc Loss: 0.00151129 Q Losses: [0.069657147, 0.55478311]\n",
      "epoch:8 batch_done:148 Gen Loss: 6.17998 Disc Loss: 0.00329007 Q Losses: [0.053721286, 0.18676184]\n",
      "epoch:8 batch_done:149 Gen Loss: 7.80644 Disc Loss: 0.000559201 Q Losses: [0.065550216, 0.1034822]\n",
      "epoch:8 batch_done:150 Gen Loss: 10.3073 Disc Loss: 0.000541819 Q Losses: [0.059250325, 0.067801811]\n",
      "epoch:8 batch_done:151 Gen Loss: 8.8007 Disc Loss: 0.00018406 Q Losses: [0.042204358, 0.10865983]\n",
      "epoch:8 batch_done:152 Gen Loss: 6.74762 Disc Loss: 0.00166875 Q Losses: [0.055181868, 0.050461311]\n",
      "epoch:8 batch_done:153 Gen Loss: 6.22358 Disc Loss: 0.00293193 Q Losses: [0.041370362, 0.067620002]\n",
      "epoch:8 batch_done:154 Gen Loss: 6.00209 Disc Loss: 0.00560907 Q Losses: [0.02327379, 0.056696281]\n",
      "epoch:8 batch_done:155 Gen Loss: 5.70572 Disc Loss: 0.00824799 Q Losses: [0.022417093, 0.042423822]\n",
      "epoch:8 batch_done:156 Gen Loss: 6.08114 Disc Loss: 0.00597277 Q Losses: [0.01733399, 0.030456435]\n",
      "epoch:8 batch_done:157 Gen Loss: 6.20687 Disc Loss: 0.00474448 Q Losses: [0.018743843, 0.022182323]\n",
      "epoch:8 batch_done:158 Gen Loss: 6.29244 Disc Loss: 0.00530859 Q Losses: [0.016688427, 0.016154183]\n",
      "epoch:8 batch_done:159 Gen Loss: 6.31788 Disc Loss: 0.00346856 Q Losses: [0.0080133937, 0.014297011]\n",
      "epoch:8 batch_done:160 Gen Loss: 6.32322 Disc Loss: 0.00337708 Q Losses: [0.012895019, 0.029710686]\n",
      "epoch:8 batch_done:161 Gen Loss: 6.24496 Disc Loss: 0.00358036 Q Losses: [0.0076532876, 0.014447151]\n",
      "epoch:8 batch_done:162 Gen Loss: 6.26625 Disc Loss: 0.00622482 Q Losses: [0.012240999, 0.017708777]\n",
      "epoch:8 batch_done:163 Gen Loss: 6.3723 Disc Loss: 0.00287657 Q Losses: [0.011553034, 0.010104842]\n",
      "epoch:8 batch_done:164 Gen Loss: 6.37063 Disc Loss: 0.00292911 Q Losses: [0.010609167, 0.012177513]\n",
      "epoch:8 batch_done:165 Gen Loss: 6.06393 Disc Loss: 0.0044352 Q Losses: [0.0087462775, 0.010039275]\n",
      "epoch:8 batch_done:166 Gen Loss: 5.81894 Disc Loss: 0.00767241 Q Losses: [0.011341919, 0.011088481]\n",
      "epoch:8 batch_done:167 Gen Loss: 5.94064 Disc Loss: 0.0155305 Q Losses: [0.0073179398, 0.0079963114]\n",
      "epoch:8 batch_done:168 Gen Loss: 7.55476 Disc Loss: 0.0414849 Q Losses: [0.0094169267, 0.015992686]\n",
      "epoch:8 batch_done:169 Gen Loss: 7.63895 Disc Loss: 0.0100336 Q Losses: [0.0081679728, 0.013798297]\n",
      "epoch:8 batch_done:170 Gen Loss: 7.36203 Disc Loss: 0.00920356 Q Losses: [0.0096915225, 0.011406661]\n",
      "epoch:8 batch_done:171 Gen Loss: 5.87238 Disc Loss: 0.0431953 Q Losses: [0.01104508, 0.0092227291]\n",
      "epoch:8 batch_done:172 Gen Loss: 7.24071 Disc Loss: 0.0917951 Q Losses: [0.0071745981, 0.016434709]\n",
      "epoch:8 batch_done:173 Gen Loss: 7.66325 Disc Loss: 0.0168051 Q Losses: [0.0086997338, 0.0087782182]\n",
      "epoch:8 batch_done:174 Gen Loss: 7.37135 Disc Loss: 0.0119582 Q Losses: [0.0086640082, 0.01115812]\n",
      "epoch:8 batch_done:175 Gen Loss: 6.54415 Disc Loss: 0.0227941 Q Losses: [0.0068983319, 0.0081883688]\n",
      "epoch:8 batch_done:176 Gen Loss: 6.13721 Disc Loss: 0.00638942 Q Losses: [0.016079748, 0.0079496838]\n",
      "epoch:8 batch_done:177 Gen Loss: 6.01509 Disc Loss: 0.00948195 Q Losses: [0.0065674651, 0.0086696856]\n",
      "epoch:8 batch_done:178 Gen Loss: 5.32488 Disc Loss: 0.0440677 Q Losses: [0.0065223174, 0.0080412356]\n",
      "epoch:8 batch_done:179 Gen Loss: 5.67173 Disc Loss: 0.0121462 Q Losses: [0.0070590046, 0.010150386]\n",
      "epoch:8 batch_done:180 Gen Loss: 5.93643 Disc Loss: 0.0209406 Q Losses: [0.0094899386, 0.016312491]\n",
      "epoch:8 batch_done:181 Gen Loss: 6.40049 Disc Loss: 0.0053167 Q Losses: [0.0066673798, 0.010732148]\n",
      "epoch:8 batch_done:182 Gen Loss: 5.94729 Disc Loss: 0.0350563 Q Losses: [0.0081106843, 0.0098597575]\n",
      "epoch:8 batch_done:183 Gen Loss: 5.8333 Disc Loss: 0.00618069 Q Losses: [0.0059609534, 0.011936467]\n",
      "epoch:8 batch_done:184 Gen Loss: 5.87693 Disc Loss: 0.00987077 Q Losses: [0.0056670904, 0.0077200821]\n",
      "epoch:8 batch_done:185 Gen Loss: 5.771 Disc Loss: 0.0177445 Q Losses: [0.014464471, 0.0081402482]\n",
      "epoch:8 batch_done:186 Gen Loss: 6.05697 Disc Loss: 0.00761066 Q Losses: [0.0094940802, 0.01232212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 batch_done:187 Gen Loss: 6.27995 Disc Loss: 0.00537932 Q Losses: [0.0088093216, 0.0096323695]\n",
      "epoch:8 batch_done:188 Gen Loss: 6.39763 Disc Loss: 0.00419973 Q Losses: [0.0046811178, 0.012515877]\n",
      "epoch:8 batch_done:189 Gen Loss: 6.44165 Disc Loss: 0.00371171 Q Losses: [0.006783857, 0.006368591]\n",
      "epoch:8 batch_done:190 Gen Loss: 6.35193 Disc Loss: 0.00459993 Q Losses: [0.0055228211, 0.0060924436]\n",
      "epoch:8 batch_done:191 Gen Loss: 6.31671 Disc Loss: 0.00444473 Q Losses: [0.0062445384, 0.0064608068]\n",
      "epoch:8 batch_done:192 Gen Loss: 6.18901 Disc Loss: 0.0094083 Q Losses: [0.008121049, 0.0060351724]\n",
      "epoch:8 batch_done:193 Gen Loss: 6.15601 Disc Loss: 0.00753843 Q Losses: [0.0056083654, 0.0084736254]\n",
      "epoch:8 batch_done:194 Gen Loss: 6.26929 Disc Loss: 0.00378562 Q Losses: [0.0053698393, 0.0081590479]\n",
      "epoch:8 batch_done:195 Gen Loss: 6.37795 Disc Loss: 0.00336163 Q Losses: [0.0044698757, 0.0057625356]\n",
      "epoch:8 batch_done:196 Gen Loss: 6.19473 Disc Loss: 0.00545895 Q Losses: [0.0055559003, 0.0092559159]\n",
      "epoch:8 batch_done:197 Gen Loss: 5.98728 Disc Loss: 0.0146257 Q Losses: [0.0037982394, 0.0058845384]\n",
      "epoch:8 batch_done:198 Gen Loss: 6.06521 Disc Loss: 0.006516 Q Losses: [0.006005289, 0.0060959775]\n",
      "epoch:8 batch_done:199 Gen Loss: 5.62533 Disc Loss: 0.0602596 Q Losses: [0.0072418693, 0.0062511377]\n",
      "epoch:8 batch_done:200 Gen Loss: 5.33183 Disc Loss: 0.0195303 Q Losses: [0.011096883, 0.0057613002]\n",
      "epoch:8 batch_done:201 Gen Loss: 5.7321 Disc Loss: 0.00896548 Q Losses: [0.0097273849, 0.006953842]\n",
      "epoch:8 batch_done:202 Gen Loss: 6.17818 Disc Loss: 0.0087458 Q Losses: [0.0056614513, 0.015117364]\n",
      "epoch:8 batch_done:203 Gen Loss: 6.32355 Disc Loss: 0.00839335 Q Losses: [0.0051902905, 0.0051607387]\n",
      "epoch:8 batch_done:204 Gen Loss: 6.1876 Disc Loss: 0.0107925 Q Losses: [0.010746587, 0.0094390567]\n",
      "epoch:8 batch_done:205 Gen Loss: 5.97899 Disc Loss: 0.0120654 Q Losses: [0.0083517479, 0.01000772]\n",
      "epoch:8 batch_done:206 Gen Loss: 6.13682 Disc Loss: 0.00628508 Q Losses: [0.0062088529, 0.0073391595]\n",
      "epoch:8 batch_done:207 Gen Loss: 6.25988 Disc Loss: 0.0081587 Q Losses: [0.0068386514, 0.0052609099]\n",
      "epoch:9 batch_done:1 Gen Loss: 6.31279 Disc Loss: 0.0048074 Q Losses: [0.0083313501, 0.0074598501]\n",
      "epoch:9 batch_done:2 Gen Loss: 6.26541 Disc Loss: 0.00700828 Q Losses: [0.0063128388, 0.0064965114]\n",
      "epoch:9 batch_done:3 Gen Loss: 6.32634 Disc Loss: 0.00486373 Q Losses: [0.0060042404, 0.0045338916]\n",
      "epoch:9 batch_done:4 Gen Loss: 6.1147 Disc Loss: 0.00978376 Q Losses: [0.005102938, 0.005779034]\n",
      "epoch:9 batch_done:5 Gen Loss: 6.11252 Disc Loss: 0.00873368 Q Losses: [0.014040483, 0.0057308218]\n",
      "epoch:9 batch_done:6 Gen Loss: 5.57263 Disc Loss: 0.0781463 Q Losses: [0.004468902, 0.0055071968]\n",
      "epoch:9 batch_done:7 Gen Loss: 6.0763 Disc Loss: 0.0141932 Q Losses: [0.010899742, 0.0058739297]\n",
      "epoch:9 batch_done:8 Gen Loss: 6.24515 Disc Loss: 0.0193973 Q Losses: [0.0050663706, 0.0057183579]\n",
      "epoch:9 batch_done:9 Gen Loss: 6.25159 Disc Loss: 0.0156615 Q Losses: [0.006047694, 0.0051552495]\n",
      "epoch:9 batch_done:10 Gen Loss: 6.45086 Disc Loss: 0.0114425 Q Losses: [0.0055490965, 0.005044763]\n",
      "epoch:9 batch_done:11 Gen Loss: 6.6759 Disc Loss: 0.00979282 Q Losses: [0.007654184, 0.009551255]\n",
      "epoch:9 batch_done:12 Gen Loss: 6.12847 Disc Loss: 0.0301749 Q Losses: [0.006781077, 0.0062701236]\n",
      "epoch:9 batch_done:13 Gen Loss: 5.94908 Disc Loss: 0.0305936 Q Losses: [0.0072843684, 0.0046713259]\n",
      "epoch:9 batch_done:14 Gen Loss: 6.77506 Disc Loss: 0.0175245 Q Losses: [0.0065642716, 0.0053105685]\n",
      "epoch:9 batch_done:15 Gen Loss: 6.6417 Disc Loss: 0.01974 Q Losses: [0.006552442, 0.0062329182]\n",
      "epoch:9 batch_done:16 Gen Loss: 6.47764 Disc Loss: 0.00804317 Q Losses: [0.0058590351, 0.0043084947]\n",
      "epoch:9 batch_done:17 Gen Loss: 6.35529 Disc Loss: 0.0156067 Q Losses: [0.0049808687, 0.0042125676]\n",
      "epoch:9 batch_done:18 Gen Loss: 6.43032 Disc Loss: 0.0136518 Q Losses: [0.0038636336, 0.0066400599]\n",
      "epoch:9 batch_done:19 Gen Loss: 6.58026 Disc Loss: 0.00767219 Q Losses: [0.026136694, 0.0041073207]\n",
      "epoch:9 batch_done:20 Gen Loss: 6.11533 Disc Loss: 0.0186137 Q Losses: [0.0060297968, 0.0046159793]\n",
      "epoch:9 batch_done:21 Gen Loss: 6.27831 Disc Loss: 0.0110056 Q Losses: [0.016450975, 0.0058807144]\n",
      "epoch:9 batch_done:22 Gen Loss: 5.91555 Disc Loss: 0.0239261 Q Losses: [0.0071208095, 0.0048534409]\n",
      "epoch:9 batch_done:23 Gen Loss: 5.91502 Disc Loss: 0.019518 Q Losses: [0.00551588, 0.0046134903]\n",
      "epoch:9 batch_done:24 Gen Loss: 5.59098 Disc Loss: 0.0473592 Q Losses: [0.0048137046, 0.0065508564]\n",
      "epoch:9 batch_done:25 Gen Loss: 6.3487 Disc Loss: 0.0122452 Q Losses: [0.005781739, 0.004754941]\n",
      "epoch:9 batch_done:26 Gen Loss: 6.52049 Disc Loss: 0.0145965 Q Losses: [0.0062103109, 0.0052314997]\n",
      "epoch:9 batch_done:27 Gen Loss: 6.01854 Disc Loss: 0.0221889 Q Losses: [0.01322226, 0.0047205151]\n",
      "epoch:9 batch_done:28 Gen Loss: 5.04426 Disc Loss: 0.0516834 Q Losses: [0.0078862738, 0.0044507952]\n",
      "epoch:9 batch_done:29 Gen Loss: 5.76734 Disc Loss: 0.0362 Q Losses: [0.0048663202, 0.0042520287]\n",
      "epoch:9 batch_done:30 Gen Loss: 6.65846 Disc Loss: 0.00964921 Q Losses: [0.0060252855, 0.004225702]\n",
      "epoch:9 batch_done:31 Gen Loss: 6.90744 Disc Loss: 0.00736375 Q Losses: [0.0092783393, 0.00829631]\n",
      "epoch:9 batch_done:32 Gen Loss: 6.71747 Disc Loss: 0.00533704 Q Losses: [0.0056622918, 0.0043504545]\n",
      "epoch:9 batch_done:33 Gen Loss: 6.4549 Disc Loss: 0.00727446 Q Losses: [0.0049114046, 0.0040786816]\n",
      "epoch:9 batch_done:34 Gen Loss: 5.59641 Disc Loss: 0.0322371 Q Losses: [0.010220347, 0.0046376502]\n",
      "epoch:9 batch_done:35 Gen Loss: 5.90684 Disc Loss: 0.01139 Q Losses: [0.0070443107, 0.004562838]\n",
      "epoch:9 batch_done:36 Gen Loss: 6.5042 Disc Loss: 0.0077611 Q Losses: [0.0056745717, 0.0047050351]\n",
      "epoch:9 batch_done:37 Gen Loss: 5.85831 Disc Loss: 0.0980275 Q Losses: [0.008903685, 0.0061049992]\n",
      "epoch:9 batch_done:38 Gen Loss: 5.83597 Disc Loss: 0.0180142 Q Losses: [0.0059191776, 0.0043375045]\n",
      "epoch:9 batch_done:39 Gen Loss: 6.44395 Disc Loss: 0.0131976 Q Losses: [0.0051715244, 0.0065460568]\n",
      "epoch:9 batch_done:40 Gen Loss: 6.92121 Disc Loss: 0.00499874 Q Losses: [0.0072857114, 0.0040012035]\n",
      "epoch:9 batch_done:41 Gen Loss: 6.06746 Disc Loss: 0.0320991 Q Losses: [0.0083936732, 0.0046365759]\n",
      "epoch:9 batch_done:42 Gen Loss: 5.97963 Disc Loss: 0.00689979 Q Losses: [0.0080430992, 0.0052989256]\n",
      "epoch:9 batch_done:43 Gen Loss: 6.37623 Disc Loss: 0.00699427 Q Losses: [0.0070854062, 0.0042825532]\n",
      "epoch:9 batch_done:44 Gen Loss: 6.55021 Disc Loss: 0.00837185 Q Losses: [0.0063538956, 0.0048562489]\n",
      "epoch:9 batch_done:45 Gen Loss: 5.86871 Disc Loss: 0.0355473 Q Losses: [0.01174522, 0.0067166528]\n",
      "epoch:9 batch_done:46 Gen Loss: 6.01321 Disc Loss: 0.00824385 Q Losses: [0.01458215, 0.0058091702]\n",
      "epoch:9 batch_done:47 Gen Loss: 6.1914 Disc Loss: 0.0154717 Q Losses: [0.0054393187, 0.0055736024]\n",
      "epoch:9 batch_done:48 Gen Loss: 6.29348 Disc Loss: 0.0086826 Q Losses: [0.0052213343, 0.0051921043]\n",
      "epoch:9 batch_done:49 Gen Loss: 6.30484 Disc Loss: 0.0083868 Q Losses: [0.0065471223, 0.0041748229]\n",
      "epoch:9 batch_done:50 Gen Loss: 6.42008 Disc Loss: 0.00547456 Q Losses: [0.0095694289, 0.0051607802]\n",
      "epoch:9 batch_done:51 Gen Loss: 6.4969 Disc Loss: 0.00575521 Q Losses: [0.0076820143, 0.0047908938]\n",
      "epoch:9 batch_done:52 Gen Loss: 6.85746 Disc Loss: 0.00319723 Q Losses: [0.0072171255, 0.0056544142]\n",
      "epoch:9 batch_done:53 Gen Loss: 6.29237 Disc Loss: 0.0113935 Q Losses: [0.0056538964, 0.0064479569]\n",
      "epoch:9 batch_done:54 Gen Loss: 6.6508 Disc Loss: 0.00723569 Q Losses: [0.0051892698, 0.0047943052]\n",
      "epoch:9 batch_done:55 Gen Loss: 6.83967 Disc Loss: 0.00477133 Q Losses: [0.017694987, 0.0049976856]\n",
      "epoch:9 batch_done:56 Gen Loss: 7.05118 Disc Loss: 0.0061493 Q Losses: [0.007980939, 0.0058457432]\n",
      "epoch:9 batch_done:57 Gen Loss: 6.17301 Disc Loss: 0.0111397 Q Losses: [0.013982706, 0.0061277817]\n",
      "epoch:9 batch_done:58 Gen Loss: 5.33069 Disc Loss: 0.0236607 Q Losses: [0.0045982921, 0.0054507307]\n",
      "epoch:9 batch_done:59 Gen Loss: 6.89135 Disc Loss: 0.0383844 Q Losses: [0.0080657378, 0.005587847]\n",
      "epoch:9 batch_done:60 Gen Loss: 7.26415 Disc Loss: 0.0125269 Q Losses: [0.013556779, 0.005439382]\n",
      "epoch:9 batch_done:61 Gen Loss: 7.46036 Disc Loss: 0.00337878 Q Losses: [0.0059450846, 0.0071847765]\n",
      "epoch:9 batch_done:62 Gen Loss: 7.34589 Disc Loss: 0.0154021 Q Losses: [0.0080355434, 0.007694908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 batch_done:63 Gen Loss: 7.2788 Disc Loss: 0.00701393 Q Losses: [0.0072039934, 0.0063978387]\n",
      "epoch:9 batch_done:64 Gen Loss: 4.80038 Disc Loss: 0.0654029 Q Losses: [0.012582003, 0.0053411038]\n",
      "epoch:9 batch_done:65 Gen Loss: 14.4195 Disc Loss: 0.11126 Q Losses: [0.0063143107, 0.0052376939]\n",
      "epoch:9 batch_done:66 Gen Loss: 17.7613 Disc Loss: 0.0462397 Q Losses: [0.0055052433, 0.0061001629]\n",
      "epoch:9 batch_done:67 Gen Loss: 15.4176 Disc Loss: 0.122887 Q Losses: [0.0081677511, 0.006337679]\n",
      "epoch:9 batch_done:68 Gen Loss: 13.8592 Disc Loss: 0.00654852 Q Losses: [0.0082931072, 0.008448964]\n",
      "epoch:9 batch_done:69 Gen Loss: 14.1088 Disc Loss: 0.00863757 Q Losses: [0.0063483827, 0.0079827979]\n",
      "epoch:9 batch_done:70 Gen Loss: 14.5362 Disc Loss: 0.000152636 Q Losses: [0.0066753943, 0.0061071282]\n",
      "epoch:9 batch_done:71 Gen Loss: 10.2202 Disc Loss: 9.68291e-05 Q Losses: [0.0068712486, 0.0067584272]\n",
      "epoch:9 batch_done:72 Gen Loss: 8.67554 Disc Loss: 0.00196454 Q Losses: [0.008680365, 0.0059222458]\n",
      "epoch:9 batch_done:73 Gen Loss: 8.2647 Disc Loss: 0.000360636 Q Losses: [0.0072696786, 0.0056331311]\n",
      "epoch:9 batch_done:74 Gen Loss: 11.2903 Disc Loss: 2.43167e-05 Q Losses: [0.012015137, 0.005606309]\n",
      "epoch:9 batch_done:75 Gen Loss: 7.67882 Disc Loss: 0.000505002 Q Losses: [0.010958824, 0.0057849153]\n",
      "epoch:9 batch_done:76 Gen Loss: 9.00386 Disc Loss: 0.000142998 Q Losses: [0.0084056901, 0.0057070102]\n",
      "epoch:9 batch_done:77 Gen Loss: 5.94429 Disc Loss: 0.00544796 Q Losses: [0.016262123, 0.0081733176]\n",
      "epoch:9 batch_done:78 Gen Loss: 6.29954 Disc Loss: 0.00409596 Q Losses: [0.0079279756, 0.0061644921]\n",
      "epoch:9 batch_done:79 Gen Loss: 8.10135 Disc Loss: 0.000535899 Q Losses: [0.0092574377, 0.005033358]\n",
      "epoch:9 batch_done:80 Gen Loss: 6.43323 Disc Loss: 0.00297174 Q Losses: [0.010612526, 0.0076468741]\n",
      "epoch:9 batch_done:81 Gen Loss: 6.18037 Disc Loss: 0.0151193 Q Losses: [0.014377685, 0.0054272674]\n",
      "epoch:9 batch_done:82 Gen Loss: 6.7087 Disc Loss: 0.00684387 Q Losses: [0.006918055, 0.01182677]\n",
      "epoch:9 batch_done:83 Gen Loss: 10.4297 Disc Loss: 0.000448388 Q Losses: [0.0063029882, 0.0070935246]\n",
      "epoch:9 batch_done:84 Gen Loss: 6.2342 Disc Loss: 0.00731008 Q Losses: [0.0078868084, 0.0066248849]\n",
      "epoch:9 batch_done:85 Gen Loss: 7.02986 Disc Loss: 0.0238278 Q Losses: [0.013426052, 0.0048791766]\n",
      "epoch:9 batch_done:86 Gen Loss: 7.73553 Disc Loss: 0.00532128 Q Losses: [0.013095537, 0.0066093905]\n",
      "epoch:9 batch_done:87 Gen Loss: 11.9164 Disc Loss: 0.00208071 Q Losses: [0.012220721, 0.0069318661]\n",
      "epoch:9 batch_done:88 Gen Loss: 8.62611 Disc Loss: 0.00190138 Q Losses: [0.0096273366, 0.0079166479]\n",
      "epoch:9 batch_done:89 Gen Loss: 11.5847 Disc Loss: 0.0149934 Q Losses: [0.020353453, 0.0052869706]\n",
      "epoch:9 batch_done:90 Gen Loss: 5.71501 Disc Loss: 0.0126292 Q Losses: [0.011258285, 0.0096829254]\n",
      "epoch:9 batch_done:91 Gen Loss: 8.82838 Disc Loss: 0.0188805 Q Losses: [0.017237809, 0.005900735]\n",
      "epoch:9 batch_done:92 Gen Loss: 9.06748 Disc Loss: 0.000627796 Q Losses: [0.0085616289, 0.0082564577]\n",
      "epoch:9 batch_done:93 Gen Loss: 14.9417 Disc Loss: 0.121442 Q Losses: [0.0073652314, 0.0077186385]\n",
      "epoch:9 batch_done:94 Gen Loss: 21.9626 Disc Loss: 0.0741358 Q Losses: [0.0088456376, 0.0069139251]\n",
      "epoch:9 batch_done:95 Gen Loss: 18.6199 Disc Loss: 0.0767352 Q Losses: [0.015901886, 0.0081208348]\n",
      "epoch:9 batch_done:96 Gen Loss: 15.1485 Disc Loss: 0.00400569 Q Losses: [0.0080919694, 0.006304346]\n",
      "epoch:9 batch_done:97 Gen Loss: 14.4317 Disc Loss: 0.000784423 Q Losses: [0.011724498, 0.01227459]\n",
      "epoch:9 batch_done:98 Gen Loss: 15.2735 Disc Loss: 0.00024924 Q Losses: [0.007465547, 0.0090946276]\n",
      "epoch:9 batch_done:99 Gen Loss: 17.882 Disc Loss: 0.00137317 Q Losses: [0.012538169, 0.0067215376]\n",
      "epoch:9 batch_done:100 Gen Loss: 13.3917 Disc Loss: 7.2268e-05 Q Losses: [0.0088726934, 0.007877823]\n",
      "epoch:9 batch_done:101 Gen Loss: 11.895 Disc Loss: 0.000176768 Q Losses: [0.0078311292, 0.0083557479]\n",
      "epoch:9 batch_done:102 Gen Loss: 9.36051 Disc Loss: 0.000175711 Q Losses: [0.010191651, 0.0076968092]\n",
      "epoch:9 batch_done:103 Gen Loss: 9.67557 Disc Loss: 0.00175841 Q Losses: [0.012133939, 0.0063628135]\n",
      "epoch:9 batch_done:104 Gen Loss: 10.8943 Disc Loss: 0.00010111 Q Losses: [0.013540929, 0.0080866572]\n",
      "epoch:9 batch_done:105 Gen Loss: 11.2117 Disc Loss: 0.000567015 Q Losses: [0.0065921163, 0.007957004]\n",
      "epoch:9 batch_done:106 Gen Loss: 5.67469 Disc Loss: 0.00910481 Q Losses: [0.0087111425, 0.0092493482]\n",
      "epoch:9 batch_done:107 Gen Loss: 6.30493 Disc Loss: 0.00581157 Q Losses: [0.012281022, 0.0068463534]\n",
      "epoch:9 batch_done:108 Gen Loss: 9.4951 Disc Loss: 0.000248628 Q Losses: [0.011765713, 0.0094208084]\n",
      "epoch:9 batch_done:109 Gen Loss: 9.62075 Disc Loss: 0.000520663 Q Losses: [0.01475575, 0.0075914711]\n",
      "epoch:9 batch_done:110 Gen Loss: 12.7277 Disc Loss: 0.102097 Q Losses: [0.018846208, 0.010247711]\n",
      "epoch:9 batch_done:111 Gen Loss: 15.1443 Disc Loss: 0.0384469 Q Losses: [0.0060033482, 0.0080678202]\n",
      "epoch:9 batch_done:112 Gen Loss: 16.913 Disc Loss: 0.0131946 Q Losses: [0.0088283438, 0.010671346]\n",
      "epoch:9 batch_done:113 Gen Loss: 20.5734 Disc Loss: 0.0322511 Q Losses: [0.0088949995, 0.0077054873]\n",
      "epoch:9 batch_done:114 Gen Loss: 11.4988 Disc Loss: 0.00398018 Q Losses: [0.012459747, 0.0069820965]\n",
      "epoch:9 batch_done:115 Gen Loss: 16.7679 Disc Loss: 0.0030416 Q Losses: [0.022961333, 0.0064244121]\n",
      "epoch:9 batch_done:116 Gen Loss: 10.5626 Disc Loss: 0.00395737 Q Losses: [0.0090598352, 0.0077019585]\n",
      "epoch:9 batch_done:117 Gen Loss: 10.7244 Disc Loss: 0.00127537 Q Losses: [0.0070938943, 0.0065531917]\n",
      "epoch:9 batch_done:118 Gen Loss: 10.3773 Disc Loss: 0.00254268 Q Losses: [0.01296875, 0.0074363016]\n",
      "epoch:9 batch_done:119 Gen Loss: 7.90498 Disc Loss: 0.0434022 Q Losses: [0.010086792, 0.0092887469]\n",
      "epoch:9 batch_done:120 Gen Loss: 8.4704 Disc Loss: 0.00479621 Q Losses: [0.01378008, 0.0099354852]\n",
      "epoch:9 batch_done:121 Gen Loss: 8.25893 Disc Loss: 0.0221601 Q Losses: [0.0085461829, 0.011629246]\n",
      "epoch:9 batch_done:122 Gen Loss: 10.9417 Disc Loss: 0.0229022 Q Losses: [0.012170475, 0.010399066]\n",
      "epoch:9 batch_done:123 Gen Loss: 5.82042 Disc Loss: 0.00435187 Q Losses: [0.012777058, 0.0079339016]\n",
      "epoch:9 batch_done:124 Gen Loss: 5.89633 Disc Loss: 0.00679667 Q Losses: [0.012529291, 0.008013498]\n",
      "epoch:9 batch_done:125 Gen Loss: 6.25495 Disc Loss: 0.014357 Q Losses: [0.010110637, 0.0096153989]\n",
      "epoch:9 batch_done:126 Gen Loss: 7.37909 Disc Loss: 0.0048779 Q Losses: [0.012487544, 0.0080656577]\n",
      "epoch:9 batch_done:127 Gen Loss: 32.1489 Disc Loss: 0.322235 Q Losses: [0.013568139, 0.0078891963]\n",
      "epoch:9 batch_done:128 Gen Loss: 17.1529 Disc Loss: 3.34555 Q Losses: [0.010728687, 0.009418508]\n",
      "epoch:9 batch_done:129 Gen Loss: 5.45534 Disc Loss: 9.36947e-06 Q Losses: [0.0080326665, 0.0090954332]\n",
      "epoch:9 batch_done:130 Gen Loss: 6.53068 Disc Loss: 0.0716498 Q Losses: [0.013457043, 0.0088657811]\n",
      "epoch:9 batch_done:131 Gen Loss: 20.3994 Disc Loss: 0.154625 Q Losses: [0.010793492, 0.033973705]\n",
      "epoch:9 batch_done:132 Gen Loss: 23.6392 Disc Loss: 2.81086e-06 Q Losses: [0.00872338, 0.035607737]\n",
      "epoch:9 batch_done:133 Gen Loss: 23.2914 Disc Loss: 0.000106437 Q Losses: [0.011426486, 0.056476396]\n",
      "epoch:9 batch_done:134 Gen Loss: 18.0229 Disc Loss: 9.09826e-05 Q Losses: [0.014776206, 0.046837877]\n",
      "epoch:9 batch_done:135 Gen Loss: 16.9457 Disc Loss: 0.000313757 Q Losses: [0.011696901, 0.11523851]\n",
      "epoch:9 batch_done:136 Gen Loss: 15.2821 Disc Loss: 0.00686795 Q Losses: [0.013767126, 0.12035875]\n",
      "epoch:9 batch_done:137 Gen Loss: 11.1641 Disc Loss: 8.85467e-05 Q Losses: [0.018630525, 0.13937202]\n",
      "epoch:9 batch_done:138 Gen Loss: 6.75139 Disc Loss: 0.0279991 Q Losses: [0.020768251, 0.26253444]\n",
      "epoch:9 batch_done:139 Gen Loss: 12.8297 Disc Loss: 0.0324493 Q Losses: [0.013435406, 0.22396904]\n",
      "epoch:9 batch_done:140 Gen Loss: 12.7012 Disc Loss: 9.17801e-05 Q Losses: [0.020600922, 0.16476119]\n",
      "epoch:9 batch_done:141 Gen Loss: 13.7138 Disc Loss: 0.00281206 Q Losses: [0.01595333, 0.11965767]\n",
      "epoch:9 batch_done:142 Gen Loss: 12.5452 Disc Loss: 0.064951 Q Losses: [0.017438248, 0.10067015]\n",
      "epoch:9 batch_done:143 Gen Loss: 5.83508 Disc Loss: 0.0700654 Q Losses: [0.016565833, 0.065484531]\n",
      "epoch:9 batch_done:144 Gen Loss: 52.0885 Disc Loss: 0.857725 Q Losses: [0.01865622, 0.08384718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 batch_done:145 Gen Loss: 37.8244 Disc Loss: 6.66723 Q Losses: [0.011655685, 0.056860581]\n",
      "epoch:9 batch_done:146 Gen Loss: 25.4813 Disc Loss: 3.53903e-08 Q Losses: [0.025982991, 0.086227208]\n",
      "epoch:9 batch_done:147 Gen Loss: 10.5899 Disc Loss: 2.84053e-07 Q Losses: [0.017934937, 0.032803558]\n",
      "epoch:9 batch_done:148 Gen Loss: 5.66344 Disc Loss: 0.0455412 Q Losses: [0.012451663, 0.072566435]\n",
      "epoch:9 batch_done:149 Gen Loss: 34.6416 Disc Loss: 0.421503 Q Losses: [0.016589776, 0.030576553]\n",
      "epoch:9 batch_done:150 Gen Loss: 42.151 Disc Loss: 7.45059e-08 Q Losses: [0.014332025, 0.036939621]\n",
      "epoch:9 batch_done:151 Gen Loss: 42.385 Disc Loss: 9.56894e-06 Q Losses: [0.012848741, 0.025357768]\n",
      "epoch:9 batch_done:152 Gen Loss: 41.3832 Disc Loss: 0.000119033 Q Losses: [0.010709639, 0.018213496]\n",
      "epoch:9 batch_done:153 Gen Loss: 35.3763 Disc Loss: 0.0711736 Q Losses: [0.01426388, 0.018809423]\n",
      "epoch:9 batch_done:154 Gen Loss: 27.4208 Disc Loss: 0.0317273 Q Losses: [0.015468718, 0.018615797]\n",
      "epoch:9 batch_done:155 Gen Loss: 21.4758 Disc Loss: 0.00494567 Q Losses: [0.010193074, 0.011357027]\n",
      "epoch:9 batch_done:156 Gen Loss: 14.5479 Disc Loss: 0.0398443 Q Losses: [0.0089271823, 0.011354055]\n",
      "epoch:9 batch_done:157 Gen Loss: 7.45074 Disc Loss: 0.0810253 Q Losses: [0.0074562752, 0.01418457]\n",
      "epoch:9 batch_done:158 Gen Loss: 5.5938 Disc Loss: 0.0291476 Q Losses: [0.0067367563, 0.011032682]\n",
      "epoch:9 batch_done:159 Gen Loss: 13.1119 Disc Loss: 0.0934894 Q Losses: [0.0091926176, 0.015844008]\n",
      "epoch:9 batch_done:160 Gen Loss: 14.0653 Disc Loss: 0.0832327 Q Losses: [0.012886124, 0.014049158]\n",
      "epoch:9 batch_done:161 Gen Loss: 11.0858 Disc Loss: 0.0579998 Q Losses: [0.009196586, 0.01009081]\n",
      "epoch:9 batch_done:162 Gen Loss: 8.00646 Disc Loss: 0.00165314 Q Losses: [0.0078234728, 0.026439276]\n",
      "epoch:9 batch_done:163 Gen Loss: 5.837 Disc Loss: 0.00954413 Q Losses: [0.0080628367, 0.0069338707]\n",
      "epoch:9 batch_done:164 Gen Loss: 18.1711 Disc Loss: 0.154992 Q Losses: [0.011025205, 0.020059202]\n",
      "epoch:9 batch_done:165 Gen Loss: 13.3067 Disc Loss: 0.942087 Q Losses: [0.0067092027, 0.0075453296]\n",
      "epoch:9 batch_done:166 Gen Loss: 6.86169 Disc Loss: 0.0741526 Q Losses: [0.015232852, 0.010540254]\n",
      "epoch:9 batch_done:167 Gen Loss: 6.54511 Disc Loss: 0.0486509 Q Losses: [0.011111509, 0.011126839]\n",
      "epoch:9 batch_done:168 Gen Loss: 8.45939 Disc Loss: 0.0345754 Q Losses: [0.0090189874, 0.018906508]\n",
      "epoch:9 batch_done:169 Gen Loss: 8.69185 Disc Loss: 0.00398854 Q Losses: [0.0081635267, 0.0091935229]\n",
      "epoch:9 batch_done:170 Gen Loss: 7.3129 Disc Loss: 0.00784232 Q Losses: [0.0086200032, 0.015182806]\n",
      "epoch:9 batch_done:171 Gen Loss: 6.43824 Disc Loss: 0.00767305 Q Losses: [0.016750827, 0.0068301861]\n",
      "epoch:9 batch_done:172 Gen Loss: 24.5475 Disc Loss: 0.273203 Q Losses: [0.011169074, 0.0074948431]\n",
      "epoch:9 batch_done:173 Gen Loss: 15.2362 Disc Loss: 2.18802 Q Losses: [0.0066448003, 0.0080672046]\n",
      "epoch:9 batch_done:174 Gen Loss: 9.18568 Disc Loss: 0.00293903 Q Losses: [0.007267667, 0.0052524833]\n",
      "epoch:9 batch_done:175 Gen Loss: 5.96801 Disc Loss: 0.000575061 Q Losses: [0.0086261369, 0.007159296]\n",
      "epoch:9 batch_done:176 Gen Loss: 4.67238 Disc Loss: 0.0133427 Q Losses: [0.0095606185, 0.0074329288]\n",
      "epoch:9 batch_done:177 Gen Loss: 15.4029 Disc Loss: 0.17884 Q Losses: [0.0092710722, 0.00579591]\n",
      "epoch:9 batch_done:178 Gen Loss: 20.0765 Disc Loss: 0.000226105 Q Losses: [0.0055417153, 0.0089733563]\n",
      "epoch:9 batch_done:179 Gen Loss: 18.7562 Disc Loss: 0.220482 Q Losses: [0.011387974, 0.0060404236]\n",
      "epoch:9 batch_done:180 Gen Loss: 16.0533 Disc Loss: 0.0368157 Q Losses: [0.009122869, 0.0056253308]\n",
      "epoch:9 batch_done:181 Gen Loss: 13.5416 Disc Loss: 0.00513376 Q Losses: [0.012389099, 0.0058452412]\n",
      "epoch:9 batch_done:182 Gen Loss: 12.0529 Disc Loss: 0.00641137 Q Losses: [0.0083557451, 0.0054674787]\n",
      "epoch:9 batch_done:183 Gen Loss: 9.71421 Disc Loss: 0.000166231 Q Losses: [0.0076696095, 0.0052465699]\n",
      "epoch:9 batch_done:184 Gen Loss: 7.80465 Disc Loss: 0.0046826 Q Losses: [0.0093920678, 0.0054463218]\n",
      "epoch:9 batch_done:185 Gen Loss: 5.18784 Disc Loss: 0.0128968 Q Losses: [0.0087026209, 0.0050423257]\n",
      "epoch:9 batch_done:186 Gen Loss: 5.86706 Disc Loss: 0.027067 Q Losses: [0.012435599, 0.0058420738]\n",
      "epoch:9 batch_done:187 Gen Loss: 6.51286 Disc Loss: 0.01401 Q Losses: [0.0070041148, 0.0053713517]\n",
      "epoch:9 batch_done:188 Gen Loss: 6.68544 Disc Loss: 0.00704608 Q Losses: [0.0081579993, 0.005822584]\n",
      "epoch:9 batch_done:189 Gen Loss: 5.84231 Disc Loss: 0.0232233 Q Losses: [0.0095043769, 0.006244841]\n",
      "epoch:9 batch_done:190 Gen Loss: 6.66732 Disc Loss: 0.0273876 Q Losses: [0.010207732, 0.0083832424]\n",
      "epoch:9 batch_done:191 Gen Loss: 6.52405 Disc Loss: 0.0212357 Q Losses: [0.0058718612, 0.0047359574]\n",
      "epoch:9 batch_done:192 Gen Loss: 5.37576 Disc Loss: 0.0676855 Q Losses: [0.010674596, 0.0063142502]\n",
      "epoch:9 batch_done:193 Gen Loss: 7.0492 Disc Loss: 0.0523574 Q Losses: [0.0080183353, 0.0047772946]\n",
      "epoch:9 batch_done:194 Gen Loss: 7.18077 Disc Loss: 0.0163534 Q Losses: [0.0069390587, 0.008272036]\n",
      "epoch:9 batch_done:195 Gen Loss: 6.34419 Disc Loss: 0.0209036 Q Losses: [0.0054789898, 0.005674867]\n",
      "epoch:9 batch_done:196 Gen Loss: 2.90844 Disc Loss: 0.148465 Q Losses: [0.0095863407, 0.0052058697]\n",
      "epoch:9 batch_done:197 Gen Loss: 25.759 Disc Loss: 0.343662 Q Losses: [0.0067116655, 0.0043089576]\n",
      "epoch:9 batch_done:198 Gen Loss: 24.6255 Disc Loss: 1.32237 Q Losses: [0.0077993558, 0.0055008768]\n",
      "epoch:9 batch_done:199 Gen Loss: 21.2429 Disc Loss: 0.0235767 Q Losses: [0.0062806783, 0.0046297368]\n",
      "epoch:9 batch_done:200 Gen Loss: 17.86 Disc Loss: 0.011213 Q Losses: [0.013085514, 0.0057632439]\n",
      "epoch:9 batch_done:201 Gen Loss: 14.6543 Disc Loss: 0.00156154 Q Losses: [0.0079358676, 0.0049271379]\n",
      "epoch:9 batch_done:202 Gen Loss: 11.586 Disc Loss: 0.0188241 Q Losses: [0.0078045698, 0.004962374]\n",
      "epoch:9 batch_done:203 Gen Loss: 10.4858 Disc Loss: 0.000689302 Q Losses: [0.0075544259, 0.0051599541]\n",
      "epoch:9 batch_done:204 Gen Loss: 10.4797 Disc Loss: 0.000197147 Q Losses: [0.0081771025, 0.0055274437]\n",
      "epoch:9 batch_done:205 Gen Loss: 5.494 Disc Loss: 0.0140399 Q Losses: [0.0061989324, 0.0055716792]\n",
      "epoch:9 batch_done:206 Gen Loss: 7.45551 Disc Loss: 0.0493831 Q Losses: [0.0088919317, 0.0064297398]\n",
      "epoch:9 batch_done:207 Gen Loss: 10.6947 Disc Loss: 0.000818655 Q Losses: [0.013608044, 0.0064303535]\n",
      "epoch:10 batch_done:1 Gen Loss: 18.5022 Disc Loss: 0.000547328 Q Losses: [0.0056930017, 0.0076466841]\n",
      "epoch:10 batch_done:2 Gen Loss: 5.71406 Disc Loss: 0.0562597 Q Losses: [0.010631425, 0.0052670264]\n",
      "epoch:10 batch_done:3 Gen Loss: 17.528 Disc Loss: 0.00100553 Q Losses: [0.0099958321, 0.0054905927]\n",
      "epoch:10 batch_done:4 Gen Loss: 10.2826 Disc Loss: 0.0836942 Q Losses: [0.010206206, 0.0046025449]\n",
      "epoch:10 batch_done:5 Gen Loss: 19.3018 Disc Loss: 0.00188552 Q Losses: [0.0094586853, 0.004228346]\n",
      "epoch:10 batch_done:6 Gen Loss: 19.2385 Disc Loss: 0.0600854 Q Losses: [0.012057757, 0.0078186607]\n",
      "epoch:10 batch_done:7 Gen Loss: 12.3116 Disc Loss: 0.0222994 Q Losses: [0.0078774039, 0.0051779482]\n",
      "epoch:10 batch_done:8 Gen Loss: 10.3682 Disc Loss: 0.0668426 Q Losses: [0.013235839, 0.0046505365]\n",
      "epoch:10 batch_done:9 Gen Loss: 11.9553 Disc Loss: 0.00298621 Q Losses: [0.0099551622, 0.010441475]\n",
      "epoch:10 batch_done:10 Gen Loss: 6.38575 Disc Loss: 0.0159223 Q Losses: [0.0058276067, 0.0059910575]\n",
      "epoch:10 batch_done:11 Gen Loss: 52.4805 Disc Loss: 1.04001 Q Losses: [0.01160833, 0.0059106061]\n",
      "epoch:10 batch_done:12 Gen Loss: 45.6366 Disc Loss: 7.33759 Q Losses: [0.010256482, 0.0052431449]\n",
      "epoch:10 batch_done:13 Gen Loss: 36.8984 Disc Loss: 0.433729 Q Losses: [0.014731118, 0.0053794272]\n",
      "epoch:10 batch_done:14 Gen Loss: 29.6466 Disc Loss: 0.000235915 Q Losses: [0.015506804, 0.0049488279]\n",
      "epoch:10 batch_done:15 Gen Loss: 24.8038 Disc Loss: 6.12203e-06 Q Losses: [0.010667119, 0.0038656269]\n",
      "epoch:10 batch_done:16 Gen Loss: 18.3477 Disc Loss: 7.09676e-07 Q Losses: [0.013322965, 0.0049072234]\n",
      "epoch:10 batch_done:17 Gen Loss: 15.1791 Disc Loss: 2.92031e-05 Q Losses: [0.010258943, 0.0078008901]\n",
      "epoch:10 batch_done:18 Gen Loss: 5.73481 Disc Loss: 0.0323468 Q Losses: [0.0074504456, 0.0062307897]\n",
      "epoch:10 batch_done:19 Gen Loss: 28.7252 Disc Loss: 0.441683 Q Losses: [0.0064901346, 0.010808368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch_done:20 Gen Loss: 38.1876 Disc Loss: 0.00030841 Q Losses: [0.011689628, 0.0083836112]\n",
      "epoch:10 batch_done:21 Gen Loss: 34.968 Disc Loss: 0.0506349 Q Losses: [0.0095015205, 0.0050320458]\n",
      "epoch:10 batch_done:22 Gen Loss: 25.6092 Disc Loss: 0.263864 Q Losses: [0.0090417704, 0.0054139579]\n",
      "epoch:10 batch_done:23 Gen Loss: 15.4216 Disc Loss: 0.160865 Q Losses: [0.0080779269, 0.0059104371]\n",
      "epoch:10 batch_done:24 Gen Loss: 9.17992 Disc Loss: 0.0494824 Q Losses: [0.010069171, 0.0093113296]\n",
      "epoch:10 batch_done:25 Gen Loss: 6.11138 Disc Loss: 0.0623561 Q Losses: [0.012445221, 0.0087383017]\n",
      "epoch:10 batch_done:26 Gen Loss: 9.41524 Disc Loss: 0.0109306 Q Losses: [0.0079310052, 0.0069516157]\n",
      "epoch:10 batch_done:27 Gen Loss: 6.9598 Disc Loss: 0.0417436 Q Losses: [0.012005714, 0.0078403987]\n",
      "epoch:10 batch_done:28 Gen Loss: 37.3004 Disc Loss: 0.658504 Q Losses: [0.0092174998, 0.0086986618]\n",
      "epoch:10 batch_done:29 Gen Loss: 36.7272 Disc Loss: 1.69268 Q Losses: [0.012267983, 0.0078992266]\n",
      "epoch:10 batch_done:30 Gen Loss: 23.534 Disc Loss: 2.60927 Q Losses: [0.0096477829, 0.0093449354]\n",
      "epoch:10 batch_done:31 Gen Loss: 14.9829 Disc Loss: 0.0481816 Q Losses: [0.0089908531, 0.0095783239]\n",
      "epoch:10 batch_done:32 Gen Loss: 11.4625 Disc Loss: 2.07533e-05 Q Losses: [0.012282969, 0.0054916842]\n",
      "epoch:10 batch_done:33 Gen Loss: 9.97941 Disc Loss: 9.60198e-06 Q Losses: [0.01302016, 0.0070664352]\n",
      "epoch:10 batch_done:34 Gen Loss: 5.05238 Disc Loss: 0.00439739 Q Losses: [0.010692386, 0.0098376004]\n",
      "epoch:10 batch_done:35 Gen Loss: 5.62494 Disc Loss: 0.00931369 Q Losses: [0.006896609, 0.0065242061]\n",
      "epoch:10 batch_done:36 Gen Loss: 9.3 Disc Loss: 0.000142865 Q Losses: [0.015778232, 0.011031172]\n",
      "epoch:10 batch_done:37 Gen Loss: 6.16187 Disc Loss: 0.0120036 Q Losses: [0.011150878, 0.0060388199]\n",
      "epoch:10 batch_done:38 Gen Loss: 8.1188 Disc Loss: 0.000834063 Q Losses: [0.010484582, 0.0088447137]\n",
      "epoch:10 batch_done:39 Gen Loss: 6.27634 Disc Loss: 0.00803625 Q Losses: [0.012384092, 0.0083010541]\n",
      "epoch:10 batch_done:40 Gen Loss: 27.7889 Disc Loss: 0.437098 Q Losses: [0.0088356463, 0.0095718261]\n",
      "epoch:10 batch_done:41 Gen Loss: 32.9264 Disc Loss: 0.0224061 Q Losses: [0.0089386441, 0.0069778883]\n",
      "epoch:10 batch_done:42 Gen Loss: 27.7966 Disc Loss: 0.22398 Q Losses: [0.018148234, 0.0096987197]\n",
      "epoch:10 batch_done:43 Gen Loss: 24.0785 Disc Loss: 0.114207 Q Losses: [0.013040498, 0.0070088338]\n",
      "epoch:10 batch_done:44 Gen Loss: 18.7416 Disc Loss: 0.155277 Q Losses: [0.0091257403, 0.006539532]\n",
      "epoch:10 batch_done:45 Gen Loss: 14.7089 Disc Loss: 0.0276585 Q Losses: [0.0077362065, 0.010376122]\n",
      "epoch:10 batch_done:46 Gen Loss: 11.4356 Disc Loss: 0.00441229 Q Losses: [0.011888031, 0.013182903]\n",
      "epoch:10 batch_done:47 Gen Loss: 8.26629 Disc Loss: 0.000431559 Q Losses: [0.012000861, 0.0064980905]\n",
      "epoch:10 batch_done:48 Gen Loss: 5.04938 Disc Loss: 0.059756 Q Losses: [0.017683271, 0.011835832]\n",
      "epoch:10 batch_done:49 Gen Loss: 6.15834 Disc Loss: 0.0468296 Q Losses: [0.014328378, 0.0080802403]\n",
      "epoch:10 batch_done:50 Gen Loss: 8.32199 Disc Loss: 0.0301243 Q Losses: [0.013106586, 0.0091948817]\n",
      "epoch:10 batch_done:51 Gen Loss: 8.05235 Disc Loss: 0.0102887 Q Losses: [0.013078108, 0.0080345841]\n",
      "epoch:10 batch_done:52 Gen Loss: 4.48869 Disc Loss: 0.143849 Q Losses: [0.010217465, 0.010449618]\n",
      "epoch:10 batch_done:53 Gen Loss: 29.0174 Disc Loss: 0.297478 Q Losses: [0.008175524, 0.00645959]\n",
      "epoch:10 batch_done:54 Gen Loss: 28.9796 Disc Loss: 0.704672 Q Losses: [0.0064175399, 0.0072615906]\n",
      "epoch:10 batch_done:55 Gen Loss: 25.8572 Disc Loss: 0.0494844 Q Losses: [0.014244234, 0.0068526734]\n",
      "epoch:10 batch_done:56 Gen Loss: 19.3181 Disc Loss: 0.441374 Q Losses: [0.0057740435, 0.0059036105]\n",
      "epoch:10 batch_done:57 Gen Loss: 12.6402 Disc Loss: 0.121329 Q Losses: [0.0089653656, 0.0059650517]\n",
      "epoch:10 batch_done:58 Gen Loss: 7.95476 Disc Loss: 0.00207708 Q Losses: [0.010104788, 0.0047147064]\n",
      "epoch:10 batch_done:59 Gen Loss: 4.75143 Disc Loss: 0.00459302 Q Losses: [0.0072716298, 0.008337548]\n",
      "epoch:10 batch_done:60 Gen Loss: 14.5073 Disc Loss: 0.174956 Q Losses: [0.0068278243, 0.00769821]\n",
      "epoch:10 batch_done:61 Gen Loss: 17.2931 Disc Loss: 0.0014197 Q Losses: [0.0076576238, 0.0065174373]\n",
      "epoch:10 batch_done:62 Gen Loss: 17.3527 Disc Loss: 0.0309095 Q Losses: [0.018008446, 0.0042900518]\n",
      "epoch:10 batch_done:63 Gen Loss: 16.3971 Disc Loss: 0.0211408 Q Losses: [0.010575555, 0.0051270714]\n",
      "epoch:10 batch_done:64 Gen Loss: 15.0222 Disc Loss: 0.0302631 Q Losses: [0.0088590169, 0.0097302217]\n",
      "epoch:10 batch_done:65 Gen Loss: 11.8949 Disc Loss: 0.00398328 Q Losses: [0.010424335, 0.0050514247]\n",
      "epoch:10 batch_done:66 Gen Loss: 7.90902 Disc Loss: 0.0277034 Q Losses: [0.010966226, 0.0082196137]\n",
      "epoch:10 batch_done:67 Gen Loss: 5.02538 Disc Loss: 0.0129157 Q Losses: [0.0098771779, 0.0051663681]\n",
      "epoch:10 batch_done:68 Gen Loss: 5.85933 Disc Loss: 0.00847079 Q Losses: [0.0060673803, 0.0058917869]\n",
      "epoch:10 batch_done:69 Gen Loss: 5.3718 Disc Loss: 0.0798056 Q Losses: [0.0066734953, 0.0050204936]\n",
      "epoch:10 batch_done:70 Gen Loss: 15.007 Disc Loss: 0.191183 Q Losses: [0.0067542465, 0.0052031763]\n",
      "epoch:10 batch_done:71 Gen Loss: 13.0408 Disc Loss: 0.124431 Q Losses: [0.011455549, 0.0078113559]\n",
      "epoch:10 batch_done:72 Gen Loss: 11.0548 Disc Loss: 0.065515 Q Losses: [0.0092541315, 0.0052560954]\n",
      "epoch:10 batch_done:73 Gen Loss: 11.3919 Disc Loss: 0.0259547 Q Losses: [0.0099526923, 0.0080194082]\n",
      "epoch:10 batch_done:74 Gen Loss: 6.98607 Disc Loss: 0.0784572 Q Losses: [0.00494745, 0.0053440575]\n",
      "epoch:10 batch_done:75 Gen Loss: 5.33765 Disc Loss: 0.0436952 Q Losses: [0.012736995, 0.0062596505]\n",
      "epoch:10 batch_done:76 Gen Loss: 49.9454 Disc Loss: 2.32175 Q Losses: [0.0081063481, 0.0052768206]\n",
      "epoch:10 batch_done:77 Gen Loss: 40.8322 Disc Loss: 5.7008 Q Losses: [0.01024317, 0.005886185]\n",
      "epoch:10 batch_done:78 Gen Loss: 22.4367 Disc Loss: 1.64159 Q Losses: [0.0099666417, 0.0079748072]\n",
      "epoch:10 batch_done:79 Gen Loss: 11.5549 Disc Loss: 0.013719 Q Losses: [0.0082715228, 0.0092022084]\n",
      "epoch:10 batch_done:80 Gen Loss: 3.88229 Disc Loss: 0.00554172 Q Losses: [0.010933715, 0.0068822815]\n",
      "epoch:10 batch_done:81 Gen Loss: 29.6184 Disc Loss: 1.08051 Q Losses: [0.011141188, 0.0082564484]\n",
      "epoch:10 batch_done:82 Gen Loss: 38.6714 Disc Loss: 0.177395 Q Losses: [0.011387783, 0.0086568594]\n",
      "epoch:10 batch_done:83 Gen Loss: 34.8795 Disc Loss: 0.708744 Q Losses: [0.0076879594, 0.011386748]\n",
      "epoch:10 batch_done:84 Gen Loss: 27.3035 Disc Loss: 0.412943 Q Losses: [0.0064702737, 0.011683792]\n",
      "epoch:10 batch_done:85 Gen Loss: 20.4344 Disc Loss: 0.29363 Q Losses: [0.012342017, 0.0060926317]\n",
      "epoch:10 batch_done:86 Gen Loss: 17.0111 Disc Loss: 0.00381568 Q Losses: [0.020229135, 0.0089792106]\n",
      "epoch:10 batch_done:87 Gen Loss: 13.5622 Disc Loss: 0.0476711 Q Losses: [0.0093513597, 0.0093674622]\n",
      "epoch:10 batch_done:88 Gen Loss: 10.1664 Disc Loss: 0.0566824 Q Losses: [0.012579717, 0.0075905551]\n",
      "epoch:10 batch_done:89 Gen Loss: 6.66255 Disc Loss: 0.00561858 Q Losses: [0.023322603, 0.011972936]\n",
      "epoch:10 batch_done:90 Gen Loss: 5.14194 Disc Loss: 0.0307981 Q Losses: [0.0095760804, 0.0072371103]\n",
      "epoch:10 batch_done:91 Gen Loss: 7.73053 Disc Loss: 0.127849 Q Losses: [0.0059153344, 0.0086257299]\n",
      "epoch:10 batch_done:92 Gen Loss: 8.25499 Disc Loss: 0.00887353 Q Losses: [0.010371869, 0.0098934621]\n",
      "epoch:10 batch_done:93 Gen Loss: 7.71155 Disc Loss: 0.00668055 Q Losses: [0.00878671, 0.0066072019]\n",
      "epoch:10 batch_done:94 Gen Loss: 6.24286 Disc Loss: 0.0874196 Q Losses: [0.0097833443, 0.010932639]\n",
      "epoch:10 batch_done:95 Gen Loss: 5.50443 Disc Loss: 0.0193302 Q Losses: [0.0081697628, 0.0085866917]\n",
      "epoch:10 batch_done:96 Gen Loss: 6.77792 Disc Loss: 0.110556 Q Losses: [0.0063241725, 0.0084288698]\n",
      "epoch:10 batch_done:97 Gen Loss: 7.24202 Disc Loss: 0.0108757 Q Losses: [0.013668595, 0.0060232622]\n",
      "epoch:10 batch_done:98 Gen Loss: 5.85762 Disc Loss: 0.0576606 Q Losses: [0.0082948543, 0.010741526]\n",
      "epoch:10 batch_done:99 Gen Loss: 5.70186 Disc Loss: 0.0714181 Q Losses: [0.010726769, 0.010537419]\n",
      "epoch:10 batch_done:100 Gen Loss: 6.39943 Disc Loss: 0.219662 Q Losses: [0.010401044, 0.0084284861]\n",
      "epoch:10 batch_done:101 Gen Loss: 5.74078 Disc Loss: 0.185052 Q Losses: [0.0096169803, 0.014592967]\n",
      "epoch:10 batch_done:102 Gen Loss: 6.75856 Disc Loss: 0.0311024 Q Losses: [0.010355728, 0.0077746958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch_done:103 Gen Loss: 6.23786 Disc Loss: 0.0883282 Q Losses: [0.0059392368, 0.013398651]\n",
      "epoch:10 batch_done:104 Gen Loss: 6.5291 Disc Loss: 0.152524 Q Losses: [0.030552872, 0.0072561372]\n",
      "epoch:10 batch_done:105 Gen Loss: 3.54774 Disc Loss: 0.345355 Q Losses: [0.016483136, 0.015263208]\n",
      "epoch:10 batch_done:106 Gen Loss: 36.5614 Disc Loss: 0.641014 Q Losses: [0.010679618, 0.0083126677]\n",
      "epoch:10 batch_done:107 Gen Loss: 34.5585 Disc Loss: 2.09326 Q Losses: [0.010363664, 0.0093186991]\n",
      "epoch:10 batch_done:108 Gen Loss: 27.5967 Disc Loss: 0.499489 Q Losses: [0.00795775, 0.0063536097]\n",
      "epoch:10 batch_done:109 Gen Loss: 20.351 Disc Loss: 0.172108 Q Losses: [0.013495372, 0.0070690531]\n",
      "epoch:10 batch_done:110 Gen Loss: 15.2071 Disc Loss: 4.83429e-05 Q Losses: [0.0045672911, 0.0093449885]\n",
      "epoch:10 batch_done:111 Gen Loss: 10.6539 Disc Loss: 0.000203818 Q Losses: [0.0079351887, 0.01019518]\n",
      "epoch:10 batch_done:112 Gen Loss: 6.01614 Disc Loss: 0.0674292 Q Losses: [0.0080192611, 0.013148542]\n",
      "epoch:10 batch_done:113 Gen Loss: 6.30434 Disc Loss: 0.057284 Q Losses: [0.0073062894, 0.0083655696]\n",
      "epoch:10 batch_done:114 Gen Loss: 7.56935 Disc Loss: 0.035608 Q Losses: [0.0088315587, 0.0061194692]\n",
      "epoch:10 batch_done:115 Gen Loss: 7.10394 Disc Loss: 0.0256425 Q Losses: [0.0078222491, 0.0080292821]\n",
      "epoch:10 batch_done:116 Gen Loss: 6.47001 Disc Loss: 0.0143235 Q Losses: [0.015884448, 0.010189878]\n",
      "epoch:10 batch_done:117 Gen Loss: 7.56501 Disc Loss: 0.0540126 Q Losses: [0.018920606, 0.0062735397]\n",
      "epoch:10 batch_done:118 Gen Loss: 7.10699 Disc Loss: 0.0763292 Q Losses: [0.01134832, 0.0075748456]\n",
      "epoch:10 batch_done:119 Gen Loss: 6.98347 Disc Loss: 0.0206087 Q Losses: [0.009362855, 0.0094365729]\n",
      "epoch:10 batch_done:120 Gen Loss: 7.16214 Disc Loss: 0.0247489 Q Losses: [0.011437796, 0.011082516]\n",
      "epoch:10 batch_done:121 Gen Loss: 7.30014 Disc Loss: 0.03331 Q Losses: [0.0090781339, 0.0086230095]\n",
      "epoch:10 batch_done:122 Gen Loss: 7.16506 Disc Loss: 0.0318599 Q Losses: [0.011463486, 0.010355457]\n",
      "epoch:10 batch_done:123 Gen Loss: 13.728 Disc Loss: 0.187451 Q Losses: [0.010416899, 0.0080501195]\n",
      "epoch:10 batch_done:124 Gen Loss: 10.2322 Disc Loss: 0.342667 Q Losses: [0.012908272, 0.0073468196]\n",
      "epoch:10 batch_done:125 Gen Loss: 5.82114 Disc Loss: 0.183914 Q Losses: [0.010977849, 0.0091287494]\n",
      "epoch:10 batch_done:126 Gen Loss: 36.879 Disc Loss: 0.942712 Q Losses: [0.0089296745, 0.0076990053]\n",
      "epoch:10 batch_done:127 Gen Loss: 29.9744 Disc Loss: 2.91889 Q Losses: [0.010521595, 0.0074826917]\n",
      "epoch:10 batch_done:128 Gen Loss: 23.1573 Disc Loss: 0.141025 Q Losses: [0.015002238, 0.0068093529]\n",
      "epoch:10 batch_done:129 Gen Loss: 18.876 Disc Loss: 0.000710977 Q Losses: [0.0080223605, 0.0065150424]\n",
      "epoch:10 batch_done:130 Gen Loss: 15.2748 Disc Loss: 0.00011759 Q Losses: [0.011601242, 0.0062755342]\n",
      "epoch:10 batch_done:131 Gen Loss: 12.1816 Disc Loss: 0.00502971 Q Losses: [0.012800944, 0.0076801376]\n",
      "epoch:10 batch_done:132 Gen Loss: 9.10989 Disc Loss: 0.000129807 Q Losses: [0.0091306269, 0.0089333039]\n",
      "epoch:10 batch_done:133 Gen Loss: 6.36287 Disc Loss: 0.00181503 Q Losses: [0.0096192295, 0.0055612437]\n",
      "epoch:10 batch_done:134 Gen Loss: 5.58633 Disc Loss: 0.0201079 Q Losses: [0.0087590273, 0.010968639]\n",
      "epoch:10 batch_done:135 Gen Loss: 8.39973 Disc Loss: 0.0578061 Q Losses: [0.0096626999, 0.0051658796]\n",
      "epoch:10 batch_done:136 Gen Loss: 9.00274 Disc Loss: 0.00250156 Q Losses: [0.0092897164, 0.0077417772]\n",
      "epoch:10 batch_done:137 Gen Loss: 9.1588 Disc Loss: 0.00358161 Q Losses: [0.0064707566, 0.0053604497]\n",
      "epoch:10 batch_done:138 Gen Loss: 7.14841 Disc Loss: 0.0197313 Q Losses: [0.01076775, 0.0049270871]\n",
      "epoch:10 batch_done:139 Gen Loss: 5.71748 Disc Loss: 0.0141421 Q Losses: [0.0070513212, 0.0062104217]\n",
      "epoch:10 batch_done:140 Gen Loss: 7.83568 Disc Loss: 0.045204 Q Losses: [0.010439618, 0.0071089771]\n",
      "epoch:10 batch_done:141 Gen Loss: 8.01685 Disc Loss: 0.00823723 Q Losses: [0.0069881072, 0.0058644619]\n",
      "epoch:10 batch_done:142 Gen Loss: 6.4393 Disc Loss: 0.0768977 Q Losses: [0.0072323852, 0.0073600519]\n",
      "epoch:10 batch_done:143 Gen Loss: 5.82542 Disc Loss: 0.130055 Q Losses: [0.011972185, 0.0093437564]\n",
      "epoch:10 batch_done:144 Gen Loss: 24.523 Disc Loss: 0.356315 Q Losses: [0.0073856777, 0.006967017]\n",
      "epoch:10 batch_done:145 Gen Loss: 27.2666 Disc Loss: 0.544147 Q Losses: [0.012005906, 0.0077362331]\n",
      "epoch:10 batch_done:146 Gen Loss: 18.7397 Disc Loss: 0.610219 Q Losses: [0.0079512494, 0.0079153711]\n",
      "epoch:10 batch_done:147 Gen Loss: 16.341 Disc Loss: 0.0233124 Q Losses: [0.0076558599, 0.0081030652]\n",
      "epoch:10 batch_done:148 Gen Loss: 10.4701 Disc Loss: 0.0137494 Q Losses: [0.013121381, 0.0081193959]\n",
      "epoch:10 batch_done:149 Gen Loss: 10.4991 Disc Loss: 2.59985e-05 Q Losses: [0.0074073044, 0.0055780988]\n",
      "epoch:10 batch_done:150 Gen Loss: 41.2299 Disc Loss: 1.15902 Q Losses: [0.0055951821, 0.0059437705]\n",
      "epoch:10 batch_done:151 Gen Loss: 48.9115 Disc Loss: 1.14434 Q Losses: [0.0072988616, 0.0055825613]\n",
      "epoch:10 batch_done:152 Gen Loss: 38.5355 Disc Loss: 0.4743 Q Losses: [0.017545683, 0.012580661]\n",
      "epoch:10 batch_done:153 Gen Loss: 27.0164 Disc Loss: 0.3708 Q Losses: [0.011236394, 0.0082442127]\n",
      "epoch:10 batch_done:154 Gen Loss: 19.8039 Disc Loss: 0.246912 Q Losses: [0.0091717318, 0.0074398406]\n",
      "epoch:10 batch_done:155 Gen Loss: 9.85361 Disc Loss: 0.0131526 Q Losses: [0.011396536, 0.014558518]\n",
      "epoch:10 batch_done:156 Gen Loss: 5.25666 Disc Loss: 0.00307026 Q Losses: [0.013114793, 0.013591098]\n",
      "epoch:10 batch_done:157 Gen Loss: 57.7096 Disc Loss: 1.92914 Q Losses: [0.016763806, 0.033200614]\n",
      "epoch:10 batch_done:158 Gen Loss: 37.1792 Disc Loss: 2.32071 Q Losses: [0.012828307, 0.084883213]\n",
      "epoch:10 batch_done:159 Gen Loss: 16.1057 Disc Loss: 0.317425 Q Losses: [0.0096185412, 0.12942965]\n",
      "epoch:10 batch_done:160 Gen Loss: 43.7861 Disc Loss: 4.20044 Q Losses: [0.010208178, 0.18345445]\n",
      "epoch:10 batch_done:161 Gen Loss: 32.5701 Disc Loss: 3.5912 Q Losses: [0.038525216, 0.60705733]\n",
      "epoch:10 batch_done:162 Gen Loss: nan Disc Loss: inf Q Losses: [0.18672249, 2.3017778]\n",
      "epoch:10 batch_done:163 Gen Loss: nan Disc Loss: nan Q Losses: [0.1639742, 2.3014843]\n",
      "epoch:10 batch_done:164 Gen Loss: nan Disc Loss: nan Q Losses: [0.17195697, 2.3022394]\n",
      "epoch:10 batch_done:165 Gen Loss: nan Disc Loss: nan Q Losses: [0.15583263, 2.3019953]\n",
      "epoch:10 batch_done:166 Gen Loss: nan Disc Loss: nan Q Losses: [0.1681142, 2.3029103]\n",
      "epoch:10 batch_done:167 Gen Loss: nan Disc Loss: nan Q Losses: [0.18953742, 2.302465]\n",
      "epoch:10 batch_done:168 Gen Loss: nan Disc Loss: nan Q Losses: [0.19098714, 2.3052223]\n",
      "epoch:10 batch_done:169 Gen Loss: nan Disc Loss: nan Q Losses: [0.17832425, 2.3007259]\n",
      "epoch:10 batch_done:170 Gen Loss: nan Disc Loss: nan Q Losses: [0.14251462, 2.3022456]\n",
      "epoch:10 batch_done:171 Gen Loss: nan Disc Loss: nan Q Losses: [0.16984755, 2.302855]\n",
      "epoch:10 batch_done:172 Gen Loss: nan Disc Loss: nan Q Losses: [0.15593968, 2.3040919]\n",
      "epoch:10 batch_done:173 Gen Loss: nan Disc Loss: nan Q Losses: [0.15681353, 2.3032622]\n",
      "epoch:10 batch_done:174 Gen Loss: nan Disc Loss: nan Q Losses: [0.16440667, 2.3008161]\n",
      "epoch:10 batch_done:175 Gen Loss: nan Disc Loss: nan Q Losses: [0.17231424, 2.3046131]\n",
      "epoch:10 batch_done:176 Gen Loss: nan Disc Loss: nan Q Losses: [0.14718875, 2.3004003]\n",
      "epoch:10 batch_done:177 Gen Loss: nan Disc Loss: nan Q Losses: [0.14077272, 2.3026807]\n",
      "epoch:10 batch_done:178 Gen Loss: nan Disc Loss: nan Q Losses: [0.16782886, 2.302669]\n",
      "epoch:10 batch_done:179 Gen Loss: nan Disc Loss: nan Q Losses: [0.13864304, 2.3017256]\n",
      "epoch:10 batch_done:180 Gen Loss: nan Disc Loss: nan Q Losses: [0.1587576, 2.3032207]\n",
      "epoch:10 batch_done:181 Gen Loss: nan Disc Loss: nan Q Losses: [0.15013739, 2.2998509]\n",
      "epoch:10 batch_done:182 Gen Loss: nan Disc Loss: nan Q Losses: [0.17617393, 2.3032904]\n",
      "epoch:10 batch_done:183 Gen Loss: nan Disc Loss: nan Q Losses: [0.15253267, 2.3030148]\n",
      "epoch:10 batch_done:184 Gen Loss: nan Disc Loss: nan Q Losses: [0.16815996, 2.3001242]\n",
      "epoch:10 batch_done:185 Gen Loss: nan Disc Loss: nan Q Losses: [0.18893456, 2.3023767]\n",
      "epoch:10 batch_done:186 Gen Loss: nan Disc Loss: nan Q Losses: [0.15246683, 2.3071389]\n",
      "epoch:10 batch_done:187 Gen Loss: nan Disc Loss: nan Q Losses: [0.18307889, 2.3019114]\n",
      "epoch:10 batch_done:188 Gen Loss: nan Disc Loss: nan Q Losses: [0.18603016, 2.304074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch_done:189 Gen Loss: nan Disc Loss: nan Q Losses: [0.1927771, 2.3015885]\n",
      "epoch:10 batch_done:190 Gen Loss: nan Disc Loss: nan Q Losses: [0.18376932, 2.304894]\n",
      "epoch:10 batch_done:191 Gen Loss: nan Disc Loss: nan Q Losses: [0.18328318, 2.3026342]\n",
      "epoch:10 batch_done:192 Gen Loss: nan Disc Loss: nan Q Losses: [0.14758044, 2.301434]\n",
      "epoch:10 batch_done:193 Gen Loss: nan Disc Loss: nan Q Losses: [0.2000674, 2.3025129]\n",
      "epoch:10 batch_done:194 Gen Loss: nan Disc Loss: nan Q Losses: [0.16240129, 2.3042691]\n",
      "epoch:10 batch_done:195 Gen Loss: nan Disc Loss: nan Q Losses: [0.15483439, 2.3043442]\n",
      "epoch:10 batch_done:196 Gen Loss: nan Disc Loss: nan Q Losses: [0.18043047, 2.3029013]\n",
      "epoch:10 batch_done:197 Gen Loss: nan Disc Loss: nan Q Losses: [0.15961009, 2.3022304]\n",
      "epoch:10 batch_done:198 Gen Loss: nan Disc Loss: nan Q Losses: [0.15536895, 2.3061495]\n",
      "epoch:10 batch_done:199 Gen Loss: nan Disc Loss: nan Q Losses: [0.15462056, 2.3016303]\n",
      "epoch:10 batch_done:200 Gen Loss: nan Disc Loss: nan Q Losses: [0.13889331, 2.304122]\n",
      "epoch:10 batch_done:201 Gen Loss: nan Disc Loss: nan Q Losses: [0.1373755, 2.3031526]\n",
      "epoch:10 batch_done:202 Gen Loss: nan Disc Loss: nan Q Losses: [0.13625278, 2.3016644]\n",
      "epoch:10 batch_done:203 Gen Loss: nan Disc Loss: nan Q Losses: [0.16329929, 2.3036485]\n",
      "epoch:10 batch_done:204 Gen Loss: nan Disc Loss: nan Q Losses: [0.1768969, 2.3046064]\n",
      "epoch:10 batch_done:205 Gen Loss: nan Disc Loss: nan Q Losses: [0.1757603, 2.3024395]\n",
      "epoch:10 batch_done:206 Gen Loss: nan Disc Loss: nan Q Losses: [0.1614143, 2.302515]\n",
      "epoch:10 batch_done:207 Gen Loss: nan Disc Loss: nan Q Losses: [0.15689249, 2.3020482]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fae07538ba4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_directory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/model-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.cptk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved Model on \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         model_checkpoint_path = sess.run(\n\u001b[1;32m   1471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlpy35tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "# blow up after 81800\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/Session#run\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "c_val = 10\n",
    "\n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#train_data_filenames = read_train_data_fullname(\"/home/cli/CeleA/Img/img_align_celeba_png.7z/img_align_celeba_png/*.png\")\n",
    "train_data_filenames = read_train_data_fullname_lfw(\"/home/cli/LFW/lfw2\")\n",
    "\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 1000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        train_data_filenames=permutation(train_data_filenames) # mini-batch\n",
    "        data_left = len(train_data_filenames)\n",
    "        batch_counter = 0\n",
    "        while data_left>0:\n",
    "            batch_size_to_train = min(batch_size, data_left)          \n",
    "            \n",
    "            zs = np.random.uniform(-1.0,1.0,size=[batch_size_to_train,z_size]).astype(np.float32) #Generate a random z batch\n",
    "            #print(\"zs shape:\",zs.shape)\n",
    "            \n",
    "            #lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "            lcat = np.random.randint(0,c_val,[batch_size_to_train,len(categorical_list)]) #Generate random c batch\n",
    "            \n",
    "            lcont = np.random.uniform(-1,1,[batch_size_to_train,number_continuous]) #\n",
    "\n",
    "            #xs = read_train_data_random_batch(train_data_filenames, batchsize=batch_size_to_train)\n",
    "            xs = read_train_data_mini_batch(train_data_filenames, batch_counter*batch_size, batch_size_to_train)\n",
    "            \n",
    "            _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "            _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "            _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "\n",
    "            data_left = data_left - batch_size_to_train\n",
    "            batch_counter +=1\n",
    "            if batch_counter%1 == 0 or data_left == 0:\n",
    "                z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32)\n",
    "                lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "                a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "                b = np.reshape(a,[c_val*c_val,1])\n",
    "                lcont_sample = b\n",
    "                samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample})\n",
    "                if not os.path.exists(sample_directory):\n",
    "                    os.makedirs(sample_directory)\n",
    "                save_images(np.reshape(samples[0:100],[100,32,32,3]),[10,10],sample_directory+'/fig'\\\n",
    "                            +str(i)+'_'+str(batch_counter)+'.png')\n",
    "                \n",
    "                print (\"epoch:\"+str(i)+\" batch_done:\"+str(batch_counter) \\\n",
    "                       +\" Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            \n",
    "             \n",
    "        \"\"\"\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "            #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "            latent_fixed = np.ones((c_val*c_val,1))\n",
    "            lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "            \n",
    "            #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "            #b = np.reshape(a,[100,1])\n",
    "            b = np.reshape(a,[c_val*c_val,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            #\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "            save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig'+str(i)+'.png')\n",
    "        \"\"\"\n",
    "        \n",
    "        if i % 10 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model on \", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\"\"\"\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "c_val = 10\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "    #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #lcat_sample = np.reshape(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]),[100,1])\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "    #print(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]))\n",
    "    #latent_fixed = np.ones((c_val*c_val,1))*50\n",
    "    latent_fixed = np.zeros((c_val*c_val,1))\n",
    "    #lcat_sample = np.hstack([latent_fixed,lcat_sample])\n",
    "    # good shape\n",
    "    lcat_sample = np.hstack([lcat_sample,latent_fixed])\n",
    "            \n",
    "    #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #a = a = np.ones((c_val*c_val,1))*-0.5\n",
    "    #a = a = np.reshape(np.array([[(e*5/4.5 - 5.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "    #b = np.reshape(a,[100,1])\n",
    "    b = np.reshape(a,[c_val*c_val,1])\n",
    "    #c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)\n",
    "    c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)+8\n",
    "    #angle\n",
    "    lcont_sample = np.hstack([b,c])\n",
    "    # width\n",
    "    #lcont_sample = np.hstack([c,b])\n",
    "    \n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test_4'+'.png')\n",
    "    save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig_test_13'+'.png')\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35tf]",
   "language": "python",
   "name": "conda-env-dlpy35tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
