{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorials walks through an implementation of InfoGAN as described in [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657).\n",
    "\n",
    "To learn more about InfoGAN, see this [Medium post](https://medium.com/p/dd710852db46) on them. To lean more about GANs generally, see [this one](https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.692jyamki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/depth_to_space\n",
    "# http://qiita.com/tadOne/items/48302a399dcad44c69c8   Tensorflow - padding = VALID/SAMEの違いについて\n",
    "#     so 3 tf.depth_to_space(genX,2) gives 4x2^3 = 32\n",
    "# \n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d(\\\n",
    "        zCon,num_outputs=128,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    gen1 = tf.depth_to_space(gen1,2)\n",
    "    \n",
    "    gen2 = slim.convolution2d(\\\n",
    "        gen1,num_outputs=64,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    gen2 = tf.depth_to_space(gen2,2)\n",
    "    \n",
    "    gen3 = slim.convolution2d(\\\n",
    "        gen2,num_outputs=32,kernel_size=[3,3],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    gen3 = tf.depth_to_space(gen3,2)\n",
    "    \n",
    "    g_out = slim.convolution2d(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, cat_list,conts, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,32,[3,3],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis1 = tf.space_to_depth(dis1,2)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,64,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis2 = tf.space_to_depth(dis2,2)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,128,[3,3],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis3 = tf.space_to_depth(dis3,2)\n",
    "        \n",
    "    dis4 = slim.fully_connected(slim.flatten(dis3),1024,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_fc1', weights_initializer=initializer)\n",
    "        \n",
    "    d_out = slim.fully_connected(dis4,1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    q_a = slim.fully_connected(dis4,128,normalizer_fn=slim.batch_norm,\\\n",
    "        reuse=reuse,scope='q_fc1', weights_initializer=initializer)\n",
    "    \n",
    "    \n",
    "    ## Here we define the unique layers used for the q-network. The number of outputs depends on the number of \n",
    "    ## latent variables we choose to define.\n",
    "    q_cat_outs = []\n",
    "    for idx,var in enumerate(cat_list):\n",
    "        q_outA = slim.fully_connected(q_a,var,activation_fn=tf.nn.softmax,\\\n",
    "            reuse=reuse,scope='q_out_cat_'+str(idx), weights_initializer=initializer)\n",
    "        q_cat_outs.append(q_outA)\n",
    "    \n",
    "    q_cont_outs = None\n",
    "    if conts > 0:\n",
    "        q_cont_outs = slim.fully_connected(q_a,conts,activation_fn=tf.nn.tanh,\\\n",
    "            reuse=reuse,scope='q_out_cont_'+str(conts), weights_initializer=initializer)\n",
    "    \n",
    "    return d_out,q_cat_outs,q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/split\n",
    "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "# https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "# https://www.tensorflow.org/api_docs/python/tf/trainable_variables\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "# https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "# z_lat: one_hot_size + z_size + number_continuous = 10+64+2=76\n",
    "# g_loss def is interesting, my understanding: \n",
    "#        if Dg is the probablity to be told as feak data, then 1-Dg is the probabily of suceessfully cheating, \n",
    "#        so we cal KL(Dg/(1-Dg)), and readuce_mean works as sampling proceduce\n",
    "# \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 64 #Size of initial z vector used for generator.\n",
    "\n",
    "# Define latent variables.\n",
    "# categorical_list = [10] # Each entry in this list defines a categorical variable of a specific size.\n",
    "categorical_list = [20] # Each entry in this list defines a categorical variable of a specific size.\n",
    "number_continuous = 2 # The number of continous variables.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "#These placeholders load the latent variables.\n",
    "latent_cat_in = tf.placeholder(shape=[None,len(categorical_list)],dtype=tf.int32)\n",
    "#print(\"latent_cat_in:\", latent_cat_in)\n",
    "latent_cat_list = tf.split(latent_cat_in,len(categorical_list),1)\n",
    "#print(\"latent_cat_list: \",latent_cat_list)\n",
    "latent_cont_in = tf.placeholder(shape=[None,number_continuous],dtype=tf.float32)\n",
    "\n",
    "oh_list = []\n",
    "for idx,var in enumerate(categorical_list):\n",
    "    latent_oh = tf.one_hot(tf.reshape(latent_cat_list[idx],[-1]),var)\n",
    "    #print(latent_cat_list[idx])\n",
    "    #print(latent_oh),  woundn't print anything in sess.run()\n",
    "    oh_list.append(latent_oh)\n",
    "\n",
    "#Concatenate all c and z variables.\n",
    "z_lats = oh_list[:]\n",
    "#print(\"1st z_lats: \", z_lats )\n",
    "z_lats.append(z_in)\n",
    "#print(\"2nd z_lats: \", z_lats )\n",
    "z_lats.append(latent_cont_in)\n",
    "#print(\"3rd z_lats: \", z_lats )\n",
    "z_lat = tf.concat(z_lats,1)\n",
    "#print(\"z_lat: \", z_lat )\n",
    "\n",
    "Gz = generator(z_lat) #Generates images from random z vectors\n",
    "Dx,_,_ = discriminator(real_in,categorical_list,number_continuous) #Produces probabilities for real images\n",
    "Dg,QgCat,QgCont = discriminator(Gz,categorical_list,number_continuous,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log((Dg/(1-Dg)))) #KL Divergence optimizer\n",
    "\n",
    "#Combine losses for each of the categorical variables.\n",
    "cat_losses = []\n",
    "for idx,latent_var in enumerate(oh_list):\n",
    "    #print (\"latent_var: \", latent_var)\n",
    "    #print (\"tf.log(QgCat[idx]): \",tf.log(QgCat[idx]))\n",
    "    cat_loss = -tf.reduce_sum(latent_var*tf.log(QgCat[idx]),axis=1)\n",
    "    cat_losses.append(cat_loss)\n",
    "    \n",
    "#Combine losses for each of the continous variables.\n",
    "if number_continuous > 0:\n",
    "    q_cont_loss = tf.reduce_sum(0.5 * tf.square(latent_cont_in - QgCont),axis=1)\n",
    "else:\n",
    "    q_cont_loss = tf.constant(0.0)\n",
    "\n",
    "q_cont_loss = tf.reduce_mean(q_cont_loss)\n",
    "q_cat_loss = tf.reduce_mean(cat_losses)\n",
    "q_loss = tf.add(q_cat_loss,q_cont_loss)\n",
    "tvars = tf.trainable_variables()\n",
    "#print (len(tvars))\n",
    "#for i in tvars:\n",
    "#    print(i)\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.002,beta1=0.5)\n",
    "trainerQ = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:-2-((number_continuous>0)*2)-(len(categorical_list)*2)]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss, tvars[0:9]) #Only update the weights for the generator network.\n",
    "q_grads = trainerQ.compute_gradients(q_loss, tvars) \n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)\n",
    "update_Q = trainerQ.apply_gradients(q_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 3.11065 Disc Loss: 1.23146 Q Losses: [0.38988644, 3.011333]\n",
      "Gen Loss: 3.7254 Disc Loss: 0.526238 Q Losses: [0.29656106, 2.9896457]\n",
      "Gen Loss: 1.58489 Disc Loss: 0.854981 Q Losses: [0.31130534, 2.8965406]\n",
      "Gen Loss: 2.07305 Disc Loss: 0.734456 Q Losses: [0.30952907, 2.9539061]\n",
      "Gen Loss: 1.64596 Disc Loss: 0.744665 Q Losses: [0.31260398, 2.9448695]\n",
      "Gen Loss: 1.99018 Disc Loss: 0.659825 Q Losses: [0.22830687, 2.9057789]\n",
      "Gen Loss: 1.77015 Disc Loss: 0.600556 Q Losses: [0.2390309, 2.9076266]\n",
      "Gen Loss: 3.75663 Disc Loss: 1.02859 Q Losses: [0.27358037, 2.8704686]\n",
      "Gen Loss: 3.25688 Disc Loss: 0.106198 Q Losses: [0.19978608, 2.913341]\n",
      "Gen Loss: 0.544102 Disc Loss: 0.540586 Q Losses: [0.17956485, 2.8248923]\n",
      "Gen Loss: 4.39169 Disc Loss: 1.01063 Q Losses: [0.19592017, 2.742383]\n",
      "Saved Model on  1000\n",
      "Gen Loss: 0.950689 Disc Loss: 0.745644 Q Losses: [0.22024876, 2.5256977]\n",
      "Gen Loss: 1.1003 Disc Loss: 1.96406 Q Losses: [0.16696006, 2.2938721]\n",
      "Gen Loss: 0.337766 Disc Loss: 0.755196 Q Losses: [0.1825577, 2.0083475]\n",
      "Gen Loss: 2.4352 Disc Loss: 0.752242 Q Losses: [0.17296508, 1.8781781]\n",
      "Gen Loss: 1.37301 Disc Loss: 0.733316 Q Losses: [0.19634536, 1.6887929]\n",
      "Gen Loss: -1.08215 Disc Loss: 1.7034 Q Losses: [0.14873773, 1.5379817]\n",
      "Gen Loss: 2.3871 Disc Loss: 0.62443 Q Losses: [0.1585837, 1.2641559]\n",
      "Gen Loss: 1.52706 Disc Loss: 0.710452 Q Losses: [0.14383441, 1.2298768]\n",
      "Gen Loss: 1.28415 Disc Loss: 0.912363 Q Losses: [0.12685445, 0.97352409]\n",
      "Gen Loss: 2.31883 Disc Loss: 0.636737 Q Losses: [0.14519235, 0.6540221]\n",
      "Saved Model on  2000\n",
      "Gen Loss: 2.30342 Disc Loss: 0.567717 Q Losses: [0.14149377, 0.70844412]\n",
      "Gen Loss: 0.901249 Disc Loss: 0.643209 Q Losses: [0.11875965, 0.70212984]\n",
      "Gen Loss: 0.936158 Disc Loss: 0.85363 Q Losses: [0.12983793, 0.6254617]\n",
      "Gen Loss: 0.27173 Disc Loss: 0.689731 Q Losses: [0.15564334, 0.56772053]\n",
      "Gen Loss: 1.9264 Disc Loss: 0.778477 Q Losses: [0.15542868, 0.5016948]\n",
      "Gen Loss: 1.92463 Disc Loss: 0.64248 Q Losses: [0.10942616, 0.37642008]\n",
      "Gen Loss: 2.53972 Disc Loss: 0.689696 Q Losses: [0.13608386, 0.50189948]\n",
      "Gen Loss: 2.89313 Disc Loss: 0.59182 Q Losses: [0.12592508, 0.55642575]\n",
      "Gen Loss: 2.70445 Disc Loss: 0.662113 Q Losses: [0.11678325, 0.33780494]\n",
      "Gen Loss: 3.09895 Disc Loss: 0.565723 Q Losses: [0.12681162, 0.45648926]\n",
      "Saved Model on  3000\n",
      "Gen Loss: 1.18563 Disc Loss: 0.975586 Q Losses: [0.12086122, 0.38798839]\n",
      "Gen Loss: 1.15461 Disc Loss: 0.788665 Q Losses: [0.11933877, 0.32097733]\n",
      "Gen Loss: 0.23102 Disc Loss: 1.5846 Q Losses: [0.1559419, 0.4230662]\n",
      "Gen Loss: 1.64895 Disc Loss: 0.639195 Q Losses: [0.13841701, 0.27116346]\n",
      "Gen Loss: 1.73141 Disc Loss: 0.757179 Q Losses: [0.12553746, 0.26881531]\n",
      "Gen Loss: 0.733452 Disc Loss: 0.669801 Q Losses: [0.10318154, 0.22684701]\n",
      "Gen Loss: 0.749061 Disc Loss: 1.09696 Q Losses: [0.13091657, 0.19841951]\n",
      "Gen Loss: 1.04836 Disc Loss: 0.820655 Q Losses: [0.11945183, 0.27136758]\n",
      "Gen Loss: 1.61775 Disc Loss: 0.734716 Q Losses: [0.10856651, 0.18016152]\n",
      "Gen Loss: 0.778262 Disc Loss: 0.871014 Q Losses: [0.11922164, 0.31608284]\n",
      "Saved Model on  4000\n",
      "Gen Loss: 1.9027 Disc Loss: 0.674214 Q Losses: [0.083062373, 0.22830491]\n",
      "Gen Loss: 0.875418 Disc Loss: 0.634916 Q Losses: [0.100723, 0.18412961]\n",
      "Gen Loss: 1.24116 Disc Loss: 1.01181 Q Losses: [0.11191916, 0.1951037]\n",
      "Gen Loss: 0.778101 Disc Loss: 0.847891 Q Losses: [0.070347629, 0.12630543]\n",
      "Gen Loss: -0.0284375 Disc Loss: 0.902867 Q Losses: [0.080186345, 0.15433186]\n",
      "Gen Loss: 1.73735 Disc Loss: 0.643064 Q Losses: [0.082683772, 0.16757795]\n",
      "Gen Loss: 1.16081 Disc Loss: 0.543012 Q Losses: [0.078379467, 0.1687783]\n",
      "Gen Loss: 0.660223 Disc Loss: 0.661902 Q Losses: [0.095174655, 0.27958575]\n",
      "Gen Loss: 0.516803 Disc Loss: 0.757525 Q Losses: [0.088461846, 0.08357057]\n",
      "Gen Loss: 0.666653 Disc Loss: 0.955816 Q Losses: [0.078083068, 0.19410394]\n",
      "Saved Model on  5000\n",
      "Gen Loss: 1.4967 Disc Loss: 0.604905 Q Losses: [0.10054584, 0.13594764]\n",
      "Gen Loss: 2.74145 Disc Loss: 0.749084 Q Losses: [0.094756715, 0.18537319]\n",
      "Gen Loss: 1.62513 Disc Loss: 0.670849 Q Losses: [0.085305169, 0.36440489]\n",
      "Gen Loss: 2.70763 Disc Loss: 0.592794 Q Losses: [0.10699849, 0.20084418]\n",
      "Gen Loss: 0.849846 Disc Loss: 0.616533 Q Losses: [0.10413402, 0.21810353]\n",
      "Gen Loss: 0.184377 Disc Loss: 1.2713 Q Losses: [0.090539351, 0.12972388]\n",
      "Gen Loss: 2.06855 Disc Loss: 0.589723 Q Losses: [0.09891288, 0.13523099]\n",
      "Gen Loss: 2.3623 Disc Loss: 0.579249 Q Losses: [0.098177381, 0.13911754]\n",
      "Gen Loss: 0.168845 Disc Loss: 0.97315 Q Losses: [0.08208251, 0.12484716]\n",
      "Gen Loss: 1.55183 Disc Loss: 0.576255 Q Losses: [0.097972795, 0.091580316]\n",
      "Saved Model on  6000\n",
      "Gen Loss: 3.45129 Disc Loss: 0.82651 Q Losses: [0.060436718, 0.098221481]\n",
      "Gen Loss: -0.055461 Disc Loss: 0.732191 Q Losses: [0.093681589, 0.27328724]\n",
      "Gen Loss: 1.8227 Disc Loss: 0.52005 Q Losses: [0.073057607, 0.066727087]\n",
      "Gen Loss: 2.09752 Disc Loss: 0.620401 Q Losses: [0.08983016, 0.32813448]\n",
      "Gen Loss: 5.6505 Disc Loss: 0.959432 Q Losses: [0.090456232, 0.1466912]\n",
      "Gen Loss: 1.28307 Disc Loss: 0.786322 Q Losses: [0.090174355, 0.1041512]\n",
      "Gen Loss: 4.36557 Disc Loss: 1.37361 Q Losses: [0.092391983, 0.072434664]\n",
      "Gen Loss: 1.26613 Disc Loss: 0.611217 Q Losses: [0.075375289, 0.050803415]\n",
      "Gen Loss: 1.60451 Disc Loss: 0.651716 Q Losses: [0.077128872, 0.13317071]\n",
      "Gen Loss: 2.70281 Disc Loss: 0.610783 Q Losses: [0.10392357, 0.1856201]\n",
      "Saved Model on  7000\n",
      "Gen Loss: 1.63463 Disc Loss: 0.685338 Q Losses: [0.083645135, 0.034154404]\n",
      "Gen Loss: 2.12818 Disc Loss: 0.600432 Q Losses: [0.1021221, 0.2329765]\n",
      "Gen Loss: 1.50679 Disc Loss: 0.621728 Q Losses: [0.087301962, 0.19159587]\n",
      "Gen Loss: 4.00387 Disc Loss: 1.13488 Q Losses: [0.081917852, 0.12799749]\n",
      "Gen Loss: 1.2663 Disc Loss: 0.912529 Q Losses: [0.073568858, 0.088918969]\n",
      "Gen Loss: 1.17798 Disc Loss: 0.523405 Q Losses: [0.073359117, 0.14110398]\n",
      "Gen Loss: 2.84183 Disc Loss: 0.637496 Q Losses: [0.086669251, 0.077190757]\n",
      "Gen Loss: 1.743 Disc Loss: 0.42952 Q Losses: [0.075618431, 0.070896305]\n",
      "Gen Loss: 3.67999 Disc Loss: 0.990621 Q Losses: [0.075657785, 0.16695923]\n",
      "Gen Loss: 2.08235 Disc Loss: 0.528786 Q Losses: [0.060862787, 0.13147581]\n",
      "Saved Model on  8000\n",
      "Gen Loss: 1.92235 Disc Loss: 0.641268 Q Losses: [0.085516483, 0.015960349]\n",
      "Gen Loss: 2.46479 Disc Loss: 0.838462 Q Losses: [0.095954001, 0.17495134]\n",
      "Gen Loss: -0.349466 Disc Loss: 1.05004 Q Losses: [0.055106536, 0.23577884]\n",
      "Gen Loss: 2.52715 Disc Loss: 0.43644 Q Losses: [0.062880412, 0.10785073]\n",
      "Gen Loss: 1.70496 Disc Loss: 0.488271 Q Losses: [0.088122085, 0.076941781]\n",
      "Gen Loss: 0.729475 Disc Loss: 0.969588 Q Losses: [0.082346506, 0.10799799]\n",
      "Gen Loss: -0.988095 Disc Loss: 2.33496 Q Losses: [0.051804125, 0.0649243]\n",
      "Gen Loss: 3.22095 Disc Loss: 0.85326 Q Losses: [0.084333658, 0.13860846]\n",
      "Gen Loss: 1.27166 Disc Loss: 0.516925 Q Losses: [0.072218731, 0.10092338]\n",
      "Gen Loss: 1.39443 Disc Loss: 1.07027 Q Losses: [0.064063601, 0.070853248]\n",
      "Saved Model on  9000\n",
      "Gen Loss: 2.2971 Disc Loss: 0.644416 Q Losses: [0.077148266, 0.14562277]\n",
      "Gen Loss: 2.27698 Disc Loss: 0.610501 Q Losses: [0.076759577, 0.34318838]\n",
      "Gen Loss: 0.712444 Disc Loss: 0.766602 Q Losses: [0.074924886, 0.10037007]\n",
      "Gen Loss: 0.839217 Disc Loss: 0.849196 Q Losses: [0.076172553, 0.083896838]\n",
      "Gen Loss: 2.00048 Disc Loss: 0.38509 Q Losses: [0.093133979, 0.12682128]\n",
      "Gen Loss: 1.60123 Disc Loss: 0.58973 Q Losses: [0.076818347, 0.10264528]\n",
      "Gen Loss: -0.4507 Disc Loss: 1.32901 Q Losses: [0.084782347, 0.013822853]\n",
      "Gen Loss: 0.98641 Disc Loss: 0.472296 Q Losses: [0.074471295, 0.059997983]\n",
      "Gen Loss: 0.641313 Disc Loss: 0.837047 Q Losses: [0.064412616, 0.045407116]\n",
      "Gen Loss: 3.14046 Disc Loss: 0.490308 Q Losses: [0.087950803, 0.10148153]\n",
      "Saved Model on  10000\n",
      "Gen Loss: -0.721008 Disc Loss: 1.48309 Q Losses: [0.076164484, 0.078958973]\n",
      "Gen Loss: 2.56685 Disc Loss: 0.5125 Q Losses: [0.065826081, 0.12578696]\n",
      "Gen Loss: 3.48399 Disc Loss: 0.581229 Q Losses: [0.074319839, 0.13537629]\n",
      "Gen Loss: 0.51335 Disc Loss: 1.13457 Q Losses: [0.073956743, 0.025748245]\n",
      "Gen Loss: 3.48896 Disc Loss: 0.889657 Q Losses: [0.072496623, 0.052003052]\n",
      "Gen Loss: 2.7588 Disc Loss: 0.550197 Q Losses: [0.060365096, 0.035208359]\n",
      "Gen Loss: 1.63099 Disc Loss: 0.54247 Q Losses: [0.070935398, 0.070336625]\n",
      "Gen Loss: 0.143768 Disc Loss: 0.765192 Q Losses: [0.063652694, 0.043442406]\n",
      "Gen Loss: 2.95263 Disc Loss: 0.420726 Q Losses: [0.054831792, 0.034753159]\n",
      "Gen Loss: -1.10833 Disc Loss: 1.16487 Q Losses: [0.062809266, 0.027951464]\n",
      "Saved Model on  11000\n",
      "Gen Loss: 3.05626 Disc Loss: 0.598908 Q Losses: [0.050540306, 0.088638902]\n",
      "Gen Loss: 1.618 Disc Loss: 0.656877 Q Losses: [0.06394805, 0.012665349]\n",
      "Gen Loss: 4.7773 Disc Loss: 0.937829 Q Losses: [0.086077958, 0.10026558]\n",
      "Gen Loss: 5.30419 Disc Loss: 0.816539 Q Losses: [0.064293779, 0.087409079]\n",
      "Gen Loss: 2.24462 Disc Loss: 0.52385 Q Losses: [0.056543116, 0.075064987]\n",
      "Gen Loss: 1.3136 Disc Loss: 0.701477 Q Losses: [0.052184198, 0.027411407]\n",
      "Gen Loss: 2.18977 Disc Loss: 0.450095 Q Losses: [0.046956901, 0.14176035]\n",
      "Gen Loss: 4.60382 Disc Loss: 0.792539 Q Losses: [0.053742889, 0.12584417]\n",
      "Gen Loss: 1.46649 Disc Loss: 0.605766 Q Losses: [0.066961184, 0.069104299]\n",
      "Gen Loss: 2.88527 Disc Loss: 0.479683 Q Losses: [0.061430775, 0.030287046]\n",
      "Saved Model on  12000\n",
      "Gen Loss: 3.76202 Disc Loss: 1.22989 Q Losses: [0.063832015, 0.12293002]\n",
      "Gen Loss: 1.29925 Disc Loss: 0.551426 Q Losses: [0.059499945, 0.042117927]\n",
      "Gen Loss: 1.01465 Disc Loss: 0.658426 Q Losses: [0.06394168, 0.033352621]\n",
      "Gen Loss: 1.53449 Disc Loss: 0.668295 Q Losses: [0.039346423, 0.081038028]\n",
      "Gen Loss: 1.46755 Disc Loss: 0.474732 Q Losses: [0.053996265, 0.052908044]\n",
      "Gen Loss: 0.781577 Disc Loss: 0.657133 Q Losses: [0.049241442, 0.050865304]\n",
      "Gen Loss: 2.63793 Disc Loss: 0.548809 Q Losses: [0.050732225, 0.044502441]\n",
      "Gen Loss: 3.64625 Disc Loss: 0.905481 Q Losses: [0.04952269, 0.113528]\n",
      "Gen Loss: 1.96292 Disc Loss: 0.662584 Q Losses: [0.070290849, 0.01757028]\n",
      "Gen Loss: 0.944236 Disc Loss: 0.609106 Q Losses: [0.045605686, 0.010177057]\n",
      "Saved Model on  13000\n",
      "Gen Loss: 2.86293 Disc Loss: 0.491447 Q Losses: [0.067737758, 0.0041254708]\n",
      "Gen Loss: 1.13767 Disc Loss: 0.61384 Q Losses: [0.068648227, 0.031524796]\n",
      "Gen Loss: 1.97858 Disc Loss: 0.434209 Q Losses: [0.061284672, 0.062334608]\n",
      "Gen Loss: 2.23161 Disc Loss: 0.347773 Q Losses: [0.072076559, 0.03785985]\n",
      "Gen Loss: 0.998917 Disc Loss: 0.683391 Q Losses: [0.054411367, 0.015830111]\n",
      "Gen Loss: 0.812476 Disc Loss: 0.635538 Q Losses: [0.075834177, 0.08315064]\n",
      "Gen Loss: 3.89484 Disc Loss: 0.69535 Q Losses: [0.07978338, 0.068404496]\n",
      "Gen Loss: 2.15428 Disc Loss: 0.582554 Q Losses: [0.071588539, 0.0093577839]\n",
      "Gen Loss: 2.21366 Disc Loss: 0.565978 Q Losses: [0.07812424, 0.074276797]\n",
      "Gen Loss: 2.16194 Disc Loss: 0.484562 Q Losses: [0.089640632, 0.0031801262]\n",
      "Saved Model on  14000\n",
      "Gen Loss: 0.45155 Disc Loss: 0.961697 Q Losses: [0.067772374, 0.055401437]\n",
      "Gen Loss: -0.747785 Disc Loss: 1.17923 Q Losses: [0.072584949, 0.11987608]\n",
      "Gen Loss: 3.04502 Disc Loss: 0.479207 Q Losses: [0.054617561, 0.16396493]\n",
      "Gen Loss: 1.65051 Disc Loss: 0.485553 Q Losses: [0.045813337, 0.036347099]\n",
      "Gen Loss: 4.6775 Disc Loss: 0.912441 Q Losses: [0.041462161, 0.066433549]\n",
      "Gen Loss: 0.845814 Disc Loss: 0.566079 Q Losses: [0.05860012, 0.026315071]\n",
      "Gen Loss: -2.04311 Disc Loss: 1.75696 Q Losses: [0.065058693, 0.027327333]\n",
      "Gen Loss: 1.25704 Disc Loss: 0.391031 Q Losses: [0.067461483, 0.010623313]\n",
      "Gen Loss: 1.3567 Disc Loss: 0.642363 Q Losses: [0.069706276, 0.10243716]\n",
      "Gen Loss: 0.0475093 Disc Loss: 0.516143 Q Losses: [0.043921106, 0.083824106]\n",
      "Saved Model on  15000\n",
      "Gen Loss: 2.56757 Disc Loss: 0.361994 Q Losses: [0.045256924, 0.010926925]\n",
      "Gen Loss: 2.63124 Disc Loss: 0.375181 Q Losses: [0.041645508, 0.058347031]\n",
      "Gen Loss: 1.77542 Disc Loss: 0.334925 Q Losses: [0.070306942, 0.068951122]\n",
      "Gen Loss: 1.21764 Disc Loss: 0.683344 Q Losses: [0.039731577, 0.019113049]\n",
      "Gen Loss: 1.45969 Disc Loss: 0.789188 Q Losses: [0.049265258, 0.03805621]\n",
      "Gen Loss: 2.42236 Disc Loss: 0.667897 Q Losses: [0.057764158, 0.0050202962]\n",
      "Gen Loss: 2.80872 Disc Loss: 0.46666 Q Losses: [0.068819761, 0.019962577]\n",
      "Gen Loss: 2.17084 Disc Loss: 0.473427 Q Losses: [0.058497392, 0.089171007]\n",
      "Gen Loss: 1.89348 Disc Loss: 0.60528 Q Losses: [0.06130185, 0.076561332]\n",
      "Gen Loss: 3.03033 Disc Loss: 0.505362 Q Losses: [0.067504987, 0.019268414]\n",
      "Saved Model on  16000\n",
      "Gen Loss: 2.34673 Disc Loss: 0.637639 Q Losses: [0.049583055, 0.017110579]\n",
      "Gen Loss: 3.5258 Disc Loss: 0.659562 Q Losses: [0.075328767, 0.074525096]\n",
      "Gen Loss: -0.885883 Disc Loss: 1.42983 Q Losses: [0.056064174, 0.026605621]\n",
      "Gen Loss: 2.89074 Disc Loss: 0.804368 Q Losses: [0.062281236, 0.083999582]\n",
      "Gen Loss: 3.79583 Disc Loss: 0.841776 Q Losses: [0.06368883, 0.10545923]\n",
      "Gen Loss: 2.92406 Disc Loss: 0.658094 Q Losses: [0.045465734, 0.0051417323]\n",
      "Gen Loss: 4.25813 Disc Loss: 0.5038 Q Losses: [0.043780148, 0.045244608]\n",
      "Gen Loss: 3.36742 Disc Loss: 0.527837 Q Losses: [0.071366549, 0.025939144]\n",
      "Gen Loss: -0.567424 Disc Loss: 0.697303 Q Losses: [0.058189943, 0.034379136]\n",
      "Gen Loss: 3.80128 Disc Loss: 0.649052 Q Losses: [0.060484942, 0.055727877]\n",
      "Saved Model on  17000\n",
      "Gen Loss: 2.20166 Disc Loss: 0.596321 Q Losses: [0.046780214, 0.031094484]\n",
      "Gen Loss: 2.36525 Disc Loss: 0.48898 Q Losses: [0.073531277, 0.014585935]\n",
      "Gen Loss: 2.00024 Disc Loss: 0.528278 Q Losses: [0.064754367, 0.015041227]\n",
      "Gen Loss: 3.67853 Disc Loss: 0.570717 Q Losses: [0.06021136, 0.0022213161]\n",
      "Gen Loss: 3.18684 Disc Loss: 0.453202 Q Losses: [0.050332606, 0.046828791]\n",
      "Gen Loss: 2.19389 Disc Loss: 0.612994 Q Losses: [0.06505131, 0.043374479]\n",
      "Gen Loss: -0.866541 Disc Loss: 0.746875 Q Losses: [0.051536582, 0.059175998]\n",
      "Gen Loss: 2.93263 Disc Loss: 0.677993 Q Losses: [0.052806839, 0.0078916289]\n",
      "Gen Loss: 3.25526 Disc Loss: 1.07628 Q Losses: [0.050009176, 0.037647046]\n",
      "Gen Loss: -1.97341 Disc Loss: 1.25035 Q Losses: [0.056591995, 0.020433264]\n",
      "Saved Model on  18000\n",
      "Gen Loss: 4.39795 Disc Loss: 0.732303 Q Losses: [0.043920644, 0.0095210858]\n",
      "Gen Loss: 3.36183 Disc Loss: 0.51025 Q Losses: [0.050240435, 0.040321849]\n",
      "Gen Loss: 3.46334 Disc Loss: 0.384694 Q Losses: [0.067047402, 0.097483411]\n",
      "Gen Loss: 1.7957 Disc Loss: 0.361547 Q Losses: [0.053219128, 0.039694451]\n",
      "Gen Loss: 0.879369 Disc Loss: 0.792764 Q Losses: [0.06315697, 0.0070508746]\n",
      "Gen Loss: 2.52525 Disc Loss: 0.482061 Q Losses: [0.055633552, 0.11112912]\n",
      "Gen Loss: 1.1396 Disc Loss: 0.507288 Q Losses: [0.055321228, 0.0027826577]\n",
      "Gen Loss: 3.76019 Disc Loss: 0.539051 Q Losses: [0.048418462, 0.018825408]\n",
      "Gen Loss: 3.03214 Disc Loss: 0.592566 Q Losses: [0.048645888, 0.017652333]\n",
      "Gen Loss: 4.15645 Disc Loss: 0.632615 Q Losses: [0.042272311, 0.033601087]\n",
      "Saved Model on  19000\n",
      "Gen Loss: 3.61557 Disc Loss: 0.475008 Q Losses: [0.057256091, 0.041226603]\n",
      "Gen Loss: 2.2094 Disc Loss: 0.432278 Q Losses: [0.047165237, 0.026239118]\n",
      "Gen Loss: 4.04168 Disc Loss: 0.778267 Q Losses: [0.037012335, 0.12275559]\n",
      "Gen Loss: 3.03685 Disc Loss: 0.672505 Q Losses: [0.044921849, 0.014661757]\n",
      "Gen Loss: 1.65911 Disc Loss: 0.588261 Q Losses: [0.048776675, 0.009772175]\n",
      "Gen Loss: 3.29375 Disc Loss: 0.297302 Q Losses: [0.049378633, 0.0095416419]\n",
      "Gen Loss: 1.28428 Disc Loss: 0.426422 Q Losses: [0.041739129, 0.011568849]\n",
      "Gen Loss: 0.42785 Disc Loss: 0.784339 Q Losses: [0.049162362, 0.074518077]\n",
      "Gen Loss: 3.5722 Disc Loss: 0.428252 Q Losses: [0.059352953, 0.035721637]\n",
      "Gen Loss: 3.93988 Disc Loss: 0.922546 Q Losses: [0.054467544, 0.056901507]\n",
      "Saved Model on  20000\n",
      "Gen Loss: 2.56823 Disc Loss: 0.460983 Q Losses: [0.05952407, 0.0094753336]\n",
      "Gen Loss: 4.51079 Disc Loss: 0.827691 Q Losses: [0.04750137, 0.012489703]\n",
      "Gen Loss: 2.91588 Disc Loss: 0.364659 Q Losses: [0.054370154, 0.012374805]\n",
      "Gen Loss: 3.14644 Disc Loss: 0.532808 Q Losses: [0.046976, 0.048698798]\n",
      "Gen Loss: 2.72441 Disc Loss: 0.338259 Q Losses: [0.045235753, 0.044632021]\n",
      "Gen Loss: 4.11739 Disc Loss: 0.63522 Q Losses: [0.059348315, 0.1321452]\n",
      "Gen Loss: 2.25625 Disc Loss: 0.632466 Q Losses: [0.043490764, 0.087264962]\n",
      "Gen Loss: 1.57239 Disc Loss: 0.598728 Q Losses: [0.039802559, 0.034419488]\n",
      "Gen Loss: 3.15665 Disc Loss: 0.475692 Q Losses: [0.055004314, 0.037170552]\n",
      "Gen Loss: 1.70086 Disc Loss: 0.423292 Q Losses: [0.047047518, 0.02170861]\n",
      "Saved Model on  21000\n",
      "Gen Loss: 3.72349 Disc Loss: 0.648782 Q Losses: [0.070169613, 0.017200027]\n",
      "Gen Loss: 3.23327 Disc Loss: 0.346793 Q Losses: [0.065055445, 0.050305065]\n",
      "Gen Loss: 2.08912 Disc Loss: 0.834889 Q Losses: [0.066058517, 0.055245738]\n",
      "Gen Loss: 2.21974 Disc Loss: 0.459432 Q Losses: [0.051917829, 0.036770921]\n",
      "Gen Loss: 1.14173 Disc Loss: 0.63485 Q Losses: [0.041192994, 0.040849902]\n",
      "Gen Loss: 2.1938 Disc Loss: 0.289274 Q Losses: [0.039290365, 0.037212003]\n",
      "Gen Loss: 3.69453 Disc Loss: 0.337325 Q Losses: [0.051499873, 0.016936222]\n",
      "Gen Loss: 0.209346 Disc Loss: 0.385472 Q Losses: [0.060275979, 0.10392162]\n",
      "Gen Loss: 2.69746 Disc Loss: 0.359709 Q Losses: [0.04858698, 0.079038016]\n",
      "Gen Loss: 4.18235 Disc Loss: 0.665616 Q Losses: [0.057827212, 0.0094670849]\n",
      "Saved Model on  22000\n",
      "Gen Loss: 0.646891 Disc Loss: 0.499471 Q Losses: [0.043291189, 0.032458372]\n",
      "Gen Loss: 1.84928 Disc Loss: 0.458535 Q Losses: [0.037564665, 0.015993446]\n",
      "Gen Loss: 3.61112 Disc Loss: 0.720882 Q Losses: [0.057698973, 0.014722019]\n",
      "Gen Loss: 2.91897 Disc Loss: 0.508435 Q Losses: [0.060077526, 0.0049407175]\n",
      "Gen Loss: 2.02885 Disc Loss: 0.360942 Q Losses: [0.050847247, 0.0029663201]\n",
      "Gen Loss: 3.0293 Disc Loss: 0.552378 Q Losses: [0.054056279, 0.046958439]\n",
      "Gen Loss: 2.52948 Disc Loss: 0.394111 Q Losses: [0.055303697, 0.0063747689]\n",
      "Gen Loss: 1.34127 Disc Loss: 0.36029 Q Losses: [0.072570741, 0.007616302]\n",
      "Gen Loss: 0.391951 Disc Loss: 0.545307 Q Losses: [0.056868952, 0.023729794]\n",
      "Gen Loss: 4.63932 Disc Loss: 0.718715 Q Losses: [0.050311428, 0.060430847]\n",
      "Saved Model on  23000\n",
      "Gen Loss: 1.69536 Disc Loss: 0.562896 Q Losses: [0.04599278, 0.027555991]\n",
      "Gen Loss: 3.77407 Disc Loss: 0.415517 Q Losses: [0.04910069, 0.019214708]\n",
      "Gen Loss: 2.6379 Disc Loss: 0.46806 Q Losses: [0.035387866, 0.0043537756]\n",
      "Gen Loss: 0.512683 Disc Loss: 0.828524 Q Losses: [0.06636177, 0.0043058265]\n",
      "Gen Loss: 2.66126 Disc Loss: 0.41167 Q Losses: [0.050010696, 0.098651864]\n",
      "Gen Loss: 5.189 Disc Loss: 0.507503 Q Losses: [0.039101928, 0.0037146679]\n",
      "Gen Loss: 1.30621 Disc Loss: 0.578549 Q Losses: [0.048397571, 0.0090631424]\n",
      "Gen Loss: 4.97891 Disc Loss: 0.541542 Q Losses: [0.051584151, 0.015136818]\n",
      "Gen Loss: 2.45548 Disc Loss: 0.564142 Q Losses: [0.040099427, 0.0074083777]\n",
      "Gen Loss: 2.46775 Disc Loss: 0.67204 Q Losses: [0.057228211, 0.04976273]\n",
      "Saved Model on  24000\n",
      "Gen Loss: 2.69675 Disc Loss: 0.587654 Q Losses: [0.045117475, 0.0043931538]\n",
      "Gen Loss: 3.51491 Disc Loss: 0.593509 Q Losses: [0.049002513, 0.017907273]\n",
      "Gen Loss: -0.0824224 Disc Loss: 0.571352 Q Losses: [0.031088512, 0.054088473]\n",
      "Gen Loss: 3.32602 Disc Loss: 0.490699 Q Losses: [0.038901359, 0.10015536]\n",
      "Gen Loss: 2.24853 Disc Loss: 0.375053 Q Losses: [0.05017297, 0.016507067]\n",
      "Gen Loss: 1.57494 Disc Loss: 1.24683 Q Losses: [0.045087621, 0.012074844]\n",
      "Gen Loss: 3.4083 Disc Loss: 0.487869 Q Losses: [0.03823116, 0.047524881]\n",
      "Gen Loss: 1.70171 Disc Loss: 0.714459 Q Losses: [0.035910293, 0.051633377]\n",
      "Gen Loss: 4.026 Disc Loss: 0.541076 Q Losses: [0.050586548, 0.032029059]\n",
      "Gen Loss: 4.06042 Disc Loss: 0.749183 Q Losses: [0.050636906, 0.05297111]\n",
      "Saved Model on  25000\n",
      "Gen Loss: 0.397658 Disc Loss: 0.959766 Q Losses: [0.06044795, 0.044657893]\n",
      "Gen Loss: 4.01363 Disc Loss: 0.312843 Q Losses: [0.041801691, 0.067186579]\n",
      "Gen Loss: -0.110691 Disc Loss: 0.83625 Q Losses: [0.040344089, 0.0065128994]\n",
      "Gen Loss: 3.18666 Disc Loss: 0.291723 Q Losses: [0.038899101, 0.010284252]\n",
      "Gen Loss: 3.78077 Disc Loss: 0.719919 Q Losses: [0.050743591, 0.045145061]\n",
      "Gen Loss: 1.44319 Disc Loss: 1.63862 Q Losses: [0.040446188, 0.033668593]\n",
      "Gen Loss: 1.58945 Disc Loss: 0.587091 Q Losses: [0.041361578, 0.019884527]\n",
      "Gen Loss: 2.39061 Disc Loss: 0.310957 Q Losses: [0.047376912, 0.031577602]\n",
      "Gen Loss: 3.99616 Disc Loss: 0.313463 Q Losses: [0.029594526, 0.029196214]\n",
      "Gen Loss: 3.20702 Disc Loss: 0.372846 Q Losses: [0.047644801, 0.035719797]\n",
      "Saved Model on  26000\n",
      "Gen Loss: 3.20241 Disc Loss: 0.321291 Q Losses: [0.034077574, 0.03650583]\n",
      "Gen Loss: 1.2005 Disc Loss: 0.602956 Q Losses: [0.042482764, 0.0059089959]\n",
      "Gen Loss: 3.42318 Disc Loss: 0.346451 Q Losses: [0.034464106, 0.04542686]\n",
      "Gen Loss: 3.33781 Disc Loss: 0.656742 Q Losses: [0.04742207, 0.020697126]\n",
      "Gen Loss: 2.98624 Disc Loss: 0.394955 Q Losses: [0.039447188, 0.011409549]\n",
      "Gen Loss: 3.8998 Disc Loss: 0.506476 Q Losses: [0.045690805, 0.018746367]\n",
      "Gen Loss: 4.21755 Disc Loss: 0.497463 Q Losses: [0.047859259, 0.020999804]\n",
      "Gen Loss: 6.36486 Disc Loss: 1.37924 Q Losses: [0.056281589, 0.01956428]\n",
      "Gen Loss: 0.824498 Disc Loss: 0.62576 Q Losses: [0.047049213, 0.10905226]\n",
      "Gen Loss: 3.27928 Disc Loss: 0.357092 Q Losses: [0.044306614, 0.010563812]\n",
      "Saved Model on  27000\n",
      "Gen Loss: -0.930238 Disc Loss: 1.21555 Q Losses: [0.05423861, 0.0027728593]\n",
      "Gen Loss: 2.34019 Disc Loss: 0.511995 Q Losses: [0.053830795, 0.011622157]\n",
      "Gen Loss: 6.67813 Disc Loss: 1.31455 Q Losses: [0.035205718, 0.0071584666]\n",
      "Gen Loss: 3.9336 Disc Loss: 0.44459 Q Losses: [0.045306846, 0.0068220468]\n",
      "Gen Loss: 1.14392 Disc Loss: 0.647421 Q Losses: [0.040968359, 0.029954242]\n",
      "Gen Loss: 1.5331 Disc Loss: 0.621651 Q Losses: [0.044861004, 0.0038806899]\n",
      "Gen Loss: 2.15337 Disc Loss: 0.497401 Q Losses: [0.041267872, 0.038154908]\n",
      "Gen Loss: 6.0699 Disc Loss: 1.50098 Q Losses: [0.036896087, 0.01927417]\n",
      "Gen Loss: 1.16427 Disc Loss: 0.474232 Q Losses: [0.040962797, 0.0069657019]\n",
      "Gen Loss: 4.08496 Disc Loss: 0.266957 Q Losses: [0.04081957, 0.032895289]\n",
      "Saved Model on  28000\n",
      "Gen Loss: 3.95241 Disc Loss: 0.350927 Q Losses: [0.028921351, 0.011187034]\n",
      "Gen Loss: 4.412 Disc Loss: 0.407906 Q Losses: [0.043138701, 0.011400558]\n",
      "Gen Loss: 3.3738 Disc Loss: 0.337394 Q Losses: [0.035937555, 0.034571979]\n",
      "Gen Loss: 0.699167 Disc Loss: 0.801242 Q Losses: [0.035440836, 0.0088619366]\n",
      "Gen Loss: 5.70938 Disc Loss: 0.366111 Q Losses: [0.035483431, 0.054918494]\n",
      "Gen Loss: 1.86259 Disc Loss: 0.547089 Q Losses: [0.04407933, 0.0098257456]\n",
      "Gen Loss: 4.18819 Disc Loss: 0.389266 Q Losses: [0.04634203, 0.049582981]\n",
      "Gen Loss: 2.9287 Disc Loss: 0.382731 Q Losses: [0.044497661, 0.069956928]\n",
      "Gen Loss: 4.60043 Disc Loss: 0.498206 Q Losses: [0.041677453, 0.030880515]\n",
      "Gen Loss: 2.90046 Disc Loss: 0.386146 Q Losses: [0.036617178, 0.033727881]\n",
      "Saved Model on  29000\n",
      "Gen Loss: 3.67827 Disc Loss: 0.395231 Q Losses: [0.044812523, 0.11098285]\n",
      "Gen Loss: 4.48253 Disc Loss: 0.202282 Q Losses: [0.036960766, 0.030351572]\n",
      "Gen Loss: 4.93797 Disc Loss: 0.608109 Q Losses: [0.04006014, 0.0053116111]\n",
      "Gen Loss: 4.51244 Disc Loss: 0.607626 Q Losses: [0.055772904, 0.060742624]\n",
      "Gen Loss: 6.09093 Disc Loss: 0.523622 Q Losses: [0.044983875, 0.022543907]\n",
      "Gen Loss: 4.42318 Disc Loss: 0.616024 Q Losses: [0.047229074, 0.011871818]\n",
      "Gen Loss: 4.90463 Disc Loss: 1.61806 Q Losses: [0.055239048, 0.027887736]\n",
      "Gen Loss: 3.22725 Disc Loss: 0.417939 Q Losses: [0.039472688, 0.12208076]\n",
      "Gen Loss: 5.02516 Disc Loss: 0.513597 Q Losses: [0.047986064, 0.1234314]\n",
      "Gen Loss: 1.94245 Disc Loss: 0.493266 Q Losses: [0.046629861, 0.039147075]\n",
      "Saved Model on  30000\n",
      "Gen Loss: 4.03559 Disc Loss: 0.864379 Q Losses: [0.037235439, 0.010393323]\n",
      "Gen Loss: 4.98408 Disc Loss: 0.491329 Q Losses: [0.040799581, 0.0033986745]\n",
      "Gen Loss: 2.97341 Disc Loss: 0.70101 Q Losses: [0.037430551, 0.011025845]\n",
      "Gen Loss: 0.849052 Disc Loss: 0.837744 Q Losses: [0.048885658, 0.0051937615]\n",
      "Gen Loss: 5.46588 Disc Loss: 1.27682 Q Losses: [0.039483637, 0.016546901]\n",
      "Gen Loss: -0.179761 Disc Loss: 0.928204 Q Losses: [0.060116991, 0.027757974]\n",
      "Gen Loss: 2.6965 Disc Loss: 0.341226 Q Losses: [0.063510433, 0.21871382]\n",
      "Gen Loss: 5.4327 Disc Loss: 1.32975 Q Losses: [0.056594815, 0.091237478]\n",
      "Gen Loss: 3.89717 Disc Loss: 0.581045 Q Losses: [0.048953548, 0.003983649]\n",
      "Gen Loss: 2.64749 Disc Loss: 1.1013 Q Losses: [0.050057411, 0.056672055]\n",
      "Saved Model on  31000\n",
      "Gen Loss: 4.4974 Disc Loss: 0.392884 Q Losses: [0.034694944, 0.015841359]\n",
      "Gen Loss: 5.108 Disc Loss: 0.809398 Q Losses: [0.045437224, 0.012182916]\n",
      "Gen Loss: 2.60268 Disc Loss: 0.410526 Q Losses: [0.04925406, 0.010291064]\n",
      "Gen Loss: 2.59152 Disc Loss: 0.484913 Q Losses: [0.034440491, 0.014110781]\n",
      "Gen Loss: 2.86297 Disc Loss: 0.890232 Q Losses: [0.040048487, 0.032920379]\n",
      "Gen Loss: 4.599 Disc Loss: 0.36563 Q Losses: [0.040671781, 0.012418277]\n",
      "Gen Loss: 5.97343 Disc Loss: 0.856012 Q Losses: [0.042642366, 0.0064246734]\n",
      "Gen Loss: 3.69244 Disc Loss: 0.387585 Q Losses: [0.040913038, 0.053834427]\n",
      "Gen Loss: 1.66716 Disc Loss: 0.533784 Q Losses: [0.041558404, 0.049458761]\n",
      "Gen Loss: 3.59027 Disc Loss: 0.376774 Q Losses: [0.051287763, 0.046014573]\n",
      "Saved Model on  32000\n",
      "Gen Loss: 4.17433 Disc Loss: 0.184257 Q Losses: [0.053973317, 0.033157952]\n",
      "Gen Loss: 2.57115 Disc Loss: 0.612399 Q Losses: [0.047790892, 0.066142917]\n",
      "Gen Loss: 3.51737 Disc Loss: 0.405395 Q Losses: [0.058829084, 0.1000397]\n",
      "Gen Loss: 0.955802 Disc Loss: 0.526268 Q Losses: [0.040599912, 0.017456599]\n",
      "Gen Loss: 0.932561 Disc Loss: 1.0229 Q Losses: [0.036170747, 0.05707103]\n",
      "Gen Loss: 4.07304 Disc Loss: 0.626024 Q Losses: [0.047716178, 0.051153708]\n",
      "Gen Loss: 2.79318 Disc Loss: 0.511352 Q Losses: [0.038311247, 0.006463307]\n",
      "Gen Loss: 5.47057 Disc Loss: 1.45144 Q Losses: [0.052467965, 0.05567316]\n",
      "Gen Loss: 3.93547 Disc Loss: 0.40599 Q Losses: [0.03377641, 0.023236925]\n",
      "Gen Loss: 4.09113 Disc Loss: 0.385058 Q Losses: [0.041167505, 0.030673245]\n",
      "Saved Model on  33000\n",
      "Gen Loss: -0.19144 Disc Loss: 0.887127 Q Losses: [0.04330904, 0.095245726]\n",
      "Gen Loss: 6.43562 Disc Loss: 0.79989 Q Losses: [0.046472598, 0.02009294]\n",
      "Gen Loss: 4.00302 Disc Loss: 0.796817 Q Losses: [0.040673651, 0.061999686]\n",
      "Gen Loss: -0.294501 Disc Loss: 0.847386 Q Losses: [0.040884364, 0.016381066]\n",
      "Gen Loss: 4.40113 Disc Loss: 0.32466 Q Losses: [0.040639155, 0.017653298]\n",
      "Gen Loss: 4.6445 Disc Loss: 0.437543 Q Losses: [0.055313356, 0.091162361]\n",
      "Gen Loss: 4.38701 Disc Loss: 0.396947 Q Losses: [0.040503412, 0.025590217]\n",
      "Gen Loss: 4.5698 Disc Loss: 0.49878 Q Losses: [0.046843108, 0.073042728]\n",
      "Gen Loss: 2.23409 Disc Loss: 0.426489 Q Losses: [0.035992064, 0.020560138]\n",
      "Gen Loss: 0.248221 Disc Loss: 0.906647 Q Losses: [0.044837855, 0.024296755]\n",
      "Saved Model on  34000\n",
      "Gen Loss: 2.98298 Disc Loss: 0.276436 Q Losses: [0.043681588, 0.021597426]\n",
      "Gen Loss: 4.67749 Disc Loss: 0.432506 Q Losses: [0.045621358, 0.024785079]\n",
      "Gen Loss: 3.62438 Disc Loss: 0.290839 Q Losses: [0.041540645, 0.019850295]\n",
      "Gen Loss: 3.02949 Disc Loss: 0.398372 Q Losses: [0.04818166, 0.0045521995]\n",
      "Gen Loss: 4.34772 Disc Loss: 0.5157 Q Losses: [0.038857292, 0.008892172]\n",
      "Gen Loss: 3.19775 Disc Loss: 0.446957 Q Losses: [0.035507999, 0.015980093]\n",
      "Gen Loss: 4.23462 Disc Loss: 0.445495 Q Losses: [0.034352839, 0.033499785]\n",
      "Gen Loss: 1.2157 Disc Loss: 0.747736 Q Losses: [0.045527346, 0.012200201]\n",
      "Gen Loss: 2.32745 Disc Loss: 0.417514 Q Losses: [0.061729714, 0.023943588]\n",
      "Gen Loss: 4.87899 Disc Loss: 0.524437 Q Losses: [0.046013132, 0.02711625]\n",
      "Saved Model on  35000\n",
      "Gen Loss: 2.78575 Disc Loss: 0.399474 Q Losses: [0.040281974, 0.044646911]\n",
      "Gen Loss: 5.27869 Disc Loss: 0.694594 Q Losses: [0.0380418, 0.0075370013]\n",
      "Gen Loss: 4.58832 Disc Loss: 0.316917 Q Losses: [0.037591882, 0.023282815]\n",
      "Gen Loss: 3.0027 Disc Loss: 0.358176 Q Losses: [0.05080016, 0.033422168]\n",
      "Gen Loss: 3.35942 Disc Loss: 0.300733 Q Losses: [0.041484211, 0.13458258]\n",
      "Gen Loss: 0.246851 Disc Loss: 0.800079 Q Losses: [0.04762923, 0.042970102]\n",
      "Gen Loss: 3.1916 Disc Loss: 0.384986 Q Losses: [0.046421021, 0.019425148]\n",
      "Gen Loss: 3.32463 Disc Loss: 0.285939 Q Losses: [0.044378571, 0.040338047]\n",
      "Gen Loss: 3.24451 Disc Loss: 0.555264 Q Losses: [0.051152602, 0.070215359]\n",
      "Gen Loss: 3.71161 Disc Loss: 0.334814 Q Losses: [0.036188599, 0.078485154]\n",
      "Saved Model on  36000\n",
      "Gen Loss: 2.67772 Disc Loss: 0.281042 Q Losses: [0.040062226, 0.02724722]\n",
      "Gen Loss: 3.18171 Disc Loss: 0.307917 Q Losses: [0.040582944, 0.027338332]\n",
      "Gen Loss: 5.7042 Disc Loss: 0.427054 Q Losses: [0.044906832, 0.046929438]\n",
      "Gen Loss: 3.92179 Disc Loss: 0.239737 Q Losses: [0.037914887, 0.0043277619]\n",
      "Gen Loss: 0.661766 Disc Loss: 0.488223 Q Losses: [0.060116936, 0.022748273]\n",
      "Gen Loss: -1.65585 Disc Loss: 1.49344 Q Losses: [0.03816016, 0.072935991]\n",
      "Gen Loss: 4.68403 Disc Loss: 0.555832 Q Losses: [0.039488956, 0.0023563388]\n",
      "Gen Loss: 3.05564 Disc Loss: 0.284519 Q Losses: [0.053968839, 0.018261241]\n",
      "Gen Loss: 3.28898 Disc Loss: 0.270365 Q Losses: [0.033194192, 0.019547885]\n",
      "Gen Loss: 3.68513 Disc Loss: 0.333036 Q Losses: [0.042839535, 0.026689086]\n",
      "Saved Model on  37000\n",
      "Gen Loss: 3.90525 Disc Loss: 0.436799 Q Losses: [0.040962998, 0.011084565]\n",
      "Gen Loss: -0.30734 Disc Loss: 2.17362 Q Losses: [0.041961782, 0.012472201]\n",
      "Gen Loss: 3.84265 Disc Loss: 0.335834 Q Losses: [0.049079712, 0.0769374]\n",
      "Gen Loss: 2.75184 Disc Loss: 0.62406 Q Losses: [0.040012971, 0.058758747]\n",
      "Gen Loss: 3.5033 Disc Loss: 0.286879 Q Losses: [0.044836991, 0.033343829]\n",
      "Gen Loss: 4.71532 Disc Loss: 0.485867 Q Losses: [0.046934478, 0.003788424]\n",
      "Gen Loss: 8.06512 Disc Loss: 1.70945 Q Losses: [0.043890614, 0.039511621]\n",
      "Gen Loss: 0.945912 Disc Loss: 0.561782 Q Losses: [0.030310076, 0.01442308]\n",
      "Gen Loss: 3.73439 Disc Loss: 0.411143 Q Losses: [0.042632252, 0.0044362028]\n",
      "Gen Loss: 4.17864 Disc Loss: 0.42007 Q Losses: [0.045884632, 0.031090537]\n",
      "Saved Model on  38000\n",
      "Gen Loss: 2.51153 Disc Loss: 0.464937 Q Losses: [0.032919399, 0.0051598689]\n",
      "Gen Loss: 4.31376 Disc Loss: 0.435886 Q Losses: [0.031722218, 0.031751871]\n",
      "Gen Loss: 2.5052 Disc Loss: 0.395044 Q Losses: [0.045303904, 0.033230711]\n",
      "Gen Loss: 2.90144 Disc Loss: 0.489669 Q Losses: [0.034486383, 0.01668085]\n",
      "Gen Loss: 2.45851 Disc Loss: 0.603523 Q Losses: [0.051307768, 0.0053811641]\n",
      "Gen Loss: 6.11795 Disc Loss: 0.738417 Q Losses: [0.044819422, 0.010494689]\n",
      "Gen Loss: 4.18372 Disc Loss: 0.550142 Q Losses: [0.039684623, 0.056424297]\n",
      "Gen Loss: 3.55458 Disc Loss: 0.750387 Q Losses: [0.049622588, 0.017349971]\n",
      "Gen Loss: 1.80738 Disc Loss: 0.537764 Q Losses: [0.049488686, 0.036997288]\n",
      "Gen Loss: 0.342421 Disc Loss: 0.691423 Q Losses: [0.036587052, 0.037782364]\n",
      "Saved Model on  39000\n",
      "Gen Loss: 2.78496 Disc Loss: 0.423656 Q Losses: [0.042040229, 0.040694613]\n",
      "Gen Loss: 3.30061 Disc Loss: 0.440723 Q Losses: [0.047612075, 0.033526324]\n",
      "Gen Loss: 3.56098 Disc Loss: 0.243026 Q Losses: [0.035578527, 0.023963863]\n",
      "Gen Loss: 1.66973 Disc Loss: 0.402779 Q Losses: [0.058298916, 0.020345256]\n",
      "Gen Loss: 5.97331 Disc Loss: 1.25197 Q Losses: [0.044083353, 0.029068751]\n",
      "Gen Loss: 5.41862 Disc Loss: 0.318396 Q Losses: [0.057200596, 0.14314206]\n",
      "Gen Loss: 2.08574 Disc Loss: 0.552501 Q Losses: [0.037049621, 0.0036504406]\n",
      "Gen Loss: -0.384434 Disc Loss: 0.738461 Q Losses: [0.040253736, 0.015958544]\n",
      "Gen Loss: 3.03548 Disc Loss: 0.326947 Q Losses: [0.030542878, 0.0046339696]\n",
      "Gen Loss: 2.95136 Disc Loss: 0.363366 Q Losses: [0.032905534, 0.069269091]\n",
      "Saved Model on  40000\n",
      "Gen Loss: 4.16579 Disc Loss: 0.48325 Q Losses: [0.034875326, 0.018200006]\n",
      "Gen Loss: 4.03897 Disc Loss: 0.398522 Q Losses: [0.055374783, 0.037483133]\n",
      "Gen Loss: 3.47335 Disc Loss: 0.257293 Q Losses: [0.047788881, 0.072895437]\n",
      "Gen Loss: 2.88054 Disc Loss: 0.432164 Q Losses: [0.037775028, 0.01656088]\n",
      "Gen Loss: 4.32645 Disc Loss: 0.292328 Q Losses: [0.043888759, 0.014826764]\n",
      "Gen Loss: 4.15301 Disc Loss: 0.762881 Q Losses: [0.044287816, 0.0083746277]\n",
      "Gen Loss: 2.95431 Disc Loss: 0.214792 Q Losses: [0.048449159, 0.018460777]\n",
      "Gen Loss: 5.37465 Disc Loss: 0.60113 Q Losses: [0.038700253, 0.018303953]\n",
      "Gen Loss: 1.44357 Disc Loss: 0.312339 Q Losses: [0.02924609, 0.016273163]\n",
      "Gen Loss: 5.76391 Disc Loss: 0.45827 Q Losses: [0.049365278, 0.0053212536]\n",
      "Saved Model on  41000\n",
      "Gen Loss: 4.25549 Disc Loss: 0.529218 Q Losses: [0.048448138, 0.0067248205]\n",
      "Gen Loss: 3.61379 Disc Loss: 0.811746 Q Losses: [0.04165503, 0.03934921]\n",
      "Gen Loss: -1.09181 Disc Loss: 0.538333 Q Losses: [0.04709411, 0.058875628]\n",
      "Gen Loss: 5.01944 Disc Loss: 0.543953 Q Losses: [0.04551097, 0.0056131319]\n",
      "Gen Loss: 2.96311 Disc Loss: 0.199994 Q Losses: [0.035996929, 0.08777409]\n",
      "Gen Loss: 3.72169 Disc Loss: 0.322124 Q Losses: [0.046145029, 0.007206982]\n",
      "Gen Loss: 1.82715 Disc Loss: 0.657172 Q Losses: [0.047792539, 0.018725859]\n",
      "Gen Loss: 5.187 Disc Loss: 0.463648 Q Losses: [0.034224939, 0.043970764]\n",
      "Gen Loss: 2.50234 Disc Loss: 0.408084 Q Losses: [0.061000638, 0.048829455]\n",
      "Gen Loss: 5.19783 Disc Loss: 0.442435 Q Losses: [0.042663857, 0.037867766]\n",
      "Saved Model on  42000\n",
      "Gen Loss: -1.84019 Disc Loss: 0.911892 Q Losses: [0.047491618, 0.010128426]\n",
      "Gen Loss: 4.54129 Disc Loss: 0.385116 Q Losses: [0.042972311, 0.047680579]\n",
      "Gen Loss: 5.7894 Disc Loss: 1.17214 Q Losses: [0.041658588, 0.077916756]\n",
      "Gen Loss: 5.81688 Disc Loss: 0.858148 Q Losses: [0.043071084, 0.024468329]\n",
      "Gen Loss: 3.8584 Disc Loss: 0.409825 Q Losses: [0.04524488, 0.0048099002]\n",
      "Gen Loss: 2.85959 Disc Loss: 0.41323 Q Losses: [0.049371995, 0.080899224]\n",
      "Gen Loss: 0.614973 Disc Loss: 0.987732 Q Losses: [0.038017485, 0.017914198]\n",
      "Gen Loss: 1.20107 Disc Loss: 0.483129 Q Losses: [0.038321741, 0.022070548]\n",
      "Gen Loss: 5.14433 Disc Loss: 0.2702 Q Losses: [0.028272247, 0.0017348733]\n",
      "Gen Loss: 3.98192 Disc Loss: 0.359461 Q Losses: [0.053063497, 0.003458383]\n",
      "Saved Model on  43000\n",
      "Gen Loss: 4.02102 Disc Loss: 0.278087 Q Losses: [0.037572641, 0.0058274344]\n",
      "Gen Loss: 5.9816 Disc Loss: 0.720406 Q Losses: [0.041516751, 0.01204291]\n",
      "Gen Loss: 3.38882 Disc Loss: 0.391458 Q Losses: [0.050989769, 0.11391977]\n",
      "Gen Loss: 4.17016 Disc Loss: 0.23223 Q Losses: [0.043449178, 0.048894733]\n",
      "Gen Loss: 3.48479 Disc Loss: 0.414357 Q Losses: [0.035333458, 0.0032465416]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b2e3ff1d5fb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_Q\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_cont_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_cat_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cat_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_cont_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlcont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update to optimize mutual information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scl/anaconda2/envs/dlpy35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# on at52 (GTX1080), 15mins/10000 epochs , 5000000 is about 12.5 hrs　 \n",
    "# https://stackoverflow.com/questions/19349410/how-to-pad-with-zeros-a-tensor-along-some-axis-python\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "# blow up after 81800\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/Session#run\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "c_val = 20\n",
    "batch_size = 64 #Size of image batch to apply at each iteration.\n",
    "#iterations = 500000 #Total number of iterations to use.\n",
    "iterations = 81000 #Total number of iterations to use.\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        #print(\"zs shape:\",zs.shape)\n",
    "        #lcat = np.random.randint(0,10,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "        lcat = np.random.randint(0,c_val,[batch_size,len(categorical_list)]) #Generate random c batch\n",
    "        #print(\"lcat\", lcat)\n",
    "        #print(\"lcat shape: \", lcat.shape)\n",
    "        lcont = np.random.uniform(-1,1,[batch_size,number_continuous]) #\n",
    "        \n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        \n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update the generator, twice for good measure.\n",
    "        _,qLoss,qK,qC = sess.run([update_Q,q_loss,q_cont_loss,q_cat_loss],feed_dict={z_in:zs,latent_cat_in:lcat,latent_cont_in:lcont}) #Update to optimize mutual information.\n",
    "        if i % 100 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss) + \" Q Losses: \" + str([qK,qC]))\n",
    "            #z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "            z_sample = np.random.uniform(-1.0,1.0,size=[c_val*c_val,z_size]).astype(np.float32) #Generate another z batch\n",
    "            #lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "            lcat_sample = np.reshape(np.array([e for e in range(c_val) for _ in range(c_val)]),[c_val*c_val,1])\n",
    "            #a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "            a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(c_val) for _ in range(c_val)]),[c_val,c_val]).T\n",
    "            #b = np.reshape(a,[100,1])\n",
    "            b = np.reshape(a,[c_val*c_val,1])\n",
    "            c = np.zeros_like(b)\n",
    "            lcont_sample = np.hstack([b,c])\n",
    "            samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig'+str(i)+'.png')\n",
    "            save_images(np.reshape(samples[0:c_val*c_val],[c_val*c_val,32,32]),[c_val,c_val],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model on \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-80000.cptk\n"
     ]
    }
   ],
   "source": [
    "# http://qiita.com/TokyoMickey/items/f6a9251f5a59120e39f8\n",
    "\n",
    "sample_directory = './figsTut' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "\n",
    "#init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[100,z_size]).astype(np.float32) #Generate another z batch\n",
    "    lcat_sample = np.reshape(np.array([e for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #lcat_sample = np.reshape(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]),[100,1])\n",
    "    #print(np.array([np.random.randint(10) for e in range(10) for _ in range(10)]))\n",
    "    a = a = np.reshape(np.array([[(e/4.5 - 1.)] for e in range(10) for _ in range(10)]),[10,10]).T\n",
    "    b = np.reshape(a,[100,1])\n",
    "    c = np.zeros_like(b)\n",
    "    #c = np.zeros_like(b)+8\n",
    "    lcont_sample = np.hstack([b,c])\n",
    "    samples = sess.run(Gz,feed_dict={z_in:z_sample,latent_cat_in:lcat_sample,latent_cont_in:lcont_sample}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    #Save sample generator images for viewing training progress.\n",
    "    save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test'+'.png')\n",
    "    #save_images(np.reshape(samples[0:100],[100,32,32]),[10,10],sample_directory+'/fig_test_4'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlpy35]",
   "language": "python",
   "name": "conda-env-dlpy35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
